<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Added an `optimize_bbox` option for `geo_polygon` queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12457</link><project id="" key="" /><description>This setting is analogous to the one for geo_distance queries, and defaults to "memory". Note
that the optimization will need to be updated when addressing #5968.

Closes #10356.
</description><key id="97175214">12457</key><summary>Added an `optimize_bbox` option for `geo_polygon` queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jtibshirani</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2015-07-25T00:36:12Z</created><updated>2016-01-20T15:17:15Z</updated><resolved>2016-01-20T15:17:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jtibshirani" created="2015-07-25T00:44:09Z" id="124779709">This PR is based on #10541, but is up-to-date with master and addresses code review comments: 
    \* Moved the optimization to GeoPolygonQuery and switched to TwoPhaseIterator
    \* Added a test to ensure that the bounding box check is actually applied, and runs first

There were so many changes to master since the initial commit that I needed to do a major rebase, and it seemed cleaner to open a new PR.
</comment><comment author="clintongormley" created="2015-07-27T11:32:47Z" id="125173513">@nknize could you review please?
</comment><comment author="jtibshirani" created="2015-08-14T21:47:18Z" id="131245393">Just checking the status of this -- let me know if there's anything I can do!
</comment><comment author="nknize" created="2016-01-20T15:17:15Z" id="173234870">closing in favor of #14537
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delayed allocation "stuck" on 0 shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12456</link><project id="" key="" /><description>Upgraded from 1.6.1, a 1.7.0 node joins, we use allocation excludes to force data away from the nodes we intend to stop. When the 1.7.0 node has all the data, the old node is stopped.

1.7.0 becomes the master, then logs "delaying allocation for [66] unassigned shards, next check in [59.6s]". 22 _minutes_ later, it then starts logging "delaying allocation for [0] unassigned shards, next check in [0s]" forever.

We had another cluster doing the same, which stopped logging it after two more nodes joined the cluster.

NTP enabled, clocks seem fine, drift not out of the ordinary.
</description><key id="97168560">12456</key><summary>Delayed allocation "stuck" on 0 shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexbrasetvik</reporter><labels><label>:Allocation</label><label>bug</label><label>discuss</label></labels><created>2015-07-24T23:31:18Z</created><updated>2015-08-06T16:08:45Z</updated><resolved>2015-08-06T16:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T11:30:22Z" id="125173075">@kimchy any ideas?
</comment><comment author="dakrone" created="2015-07-27T15:50:17Z" id="125251526">@alexbrasetvik does this problem persist when the entire cluster is running 1.7.0, or does it only occur when the cluster is in a mixed-version state?
</comment><comment author="alexbrasetvik" created="2015-07-27T16:33:47Z" id="125266015">It happens also for clusters created with 1.7.0.
</comment><comment author="dakrone" created="2015-07-27T16:35:52Z" id="125266598">@alexbrasetvik I was able to reproduce this, still trying to figure out what causes it
</comment><comment author="mkliu" created="2015-07-29T19:43:09Z" id="126074771">I see this multiple times too. Besides the logging itself, actually it never allocates my unassigned shards. I have a 20 machine cluster, all on 1.7 already. I shutdown a node via kopf. I see ~100 unassigned shards. The node then joins the cluster again, it's not initializing the unassigned shards. 

I manually reroute one unassigned shard, then i start seeing the cluster initializing the other unassigned shards. 

In pending tasks, I'm seeing

```
 176228       29.1s URGENT   shard-started ([xxxx-2015.27][0], node[nBe-T6GzTPOKoFHz-sIz8A], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-07-29T19:18:21.992Z], details[node_left[Vsj8k-eIQX2PFgAQQWDIJA]]]), reason [master [...][DwayquBqT8u8Xvns_HiIag][CO3SCH020050240][inet[/10.65.207.36:9300]]{...} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]   
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file></files><comments><comment>Avoid extra reroutes of delayed shards in RoutingService</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file></files><comments><comment>Avoid extra reroutes of delayed shards in RoutingService</comment></comments></commit></commits></item><item><title>Configure Maven -source target as Java 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12455</link><project id="" key="" /><description>We require Java 1.7 as a minimum anyway, so this doesn't cause any breakage.
</description><key id="97151157">12455</key><summary>Configure Maven -source target as Java 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2015-07-24T21:30:13Z</created><updated>2016-02-21T20:22:06Z</updated><resolved>2015-07-24T22:45:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-24T22:45:34Z" id="124754307">Closing this, as I think it was a transient problem.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add randomized jitter to s3 client retries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12454</link><project id="" key="" /><description>Add jitter in AWS S3 retry and delete setting max retry when initializing S3 client , [#11934](https://github.com/elastic/elasticsearch/issues/11934) and [#12434](https://github.com/elastic/elasticsearch/issues/12434)
</description><key id="97141112">12454</key><summary>Add randomized jitter to s3 client retries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label></labels><created>2015-07-24T20:32:28Z</created><updated>2015-08-18T06:10:53Z</updated><resolved>2015-08-03T18:10:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-08-03T18:10:39Z" id="127356175">Close this PR, issue has been closed, review is not needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support field exclusion in alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12453</link><project id="" key="" /><description>Hi, I have same question of this user, why it not works?

http://stackoverflow.com/questions/26349705/how-to-create-elasticsearch-index-alias-that-excludes-specific-fields
</description><key id="97114410">12453</key><summary>Support field exclusion in alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felipegs</reporter><labels /><created>2015-07-24T18:47:17Z</created><updated>2015-07-27T11:44:04Z</updated><resolved>2015-07-27T11:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T11:27:56Z" id="125172623">Because it is not supported, and the documentation doesn't imply that it should work. You could use templated search requests to achieve something similar instead.
</comment><comment author="felipegs" created="2015-07-27T11:44:04Z" id="125175685">This could be, in my opinion, a good feature, to secure some data with alias

Thanks 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reroute shards when a node goes under disk watermarks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12452</link><project id="" key="" /><description>Previously we issued a reroute when a node went over the high watermark
in order to move shards away from the node. This change tracks nodes
that have previously been over the high or low watermarks and issues a
reroute when the node goes back underneath the watermark.

This allows shards that may be unassigned to be assigned back to a node
that was previously over the low watermark but no longer is.

Resolves #12422
</description><key id="97098103">12452</key><summary>Reroute shards when a node goes under disk watermarks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T17:31:34Z</created><updated>2016-03-03T19:12:07Z</updated><resolved>2015-07-30T22:54:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-30T16:51:25Z" id="126399993">Left one minor comment. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add explicit check that we have reached the end of the settings stream when parsing settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12451</link><project id="" key="" /><description>Settings are currently parsed by looping over the tokens until an END_OBJECT token is reached. However, this does not mean that the end of the settings stream was reached. This can occur, for example, when parsing a YAML settings file with inconsistent indentation. Currently in this case, some settings will be silently ignored. This commit forces a check that we have in fact reached the end of the settings stream.

Closes #12382
</description><key id="97096984">12451</key><summary>Add explicit check that we have reached the end of the settings stream when parsing settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>bug</label><label>v1.6.2</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T17:26:12Z</created><updated>2015-07-28T15:25:58Z</updated><resolved>2015-07-24T17:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-24T17:49:42Z" id="124597859">LGTM, can you add the versions this will be merged to?
</comment><comment author="jasontedor" created="2015-07-24T17:51:54Z" id="124598324">@dakrone Done. Thanks for reviewing.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/YamlSettingsLoaderTests.java</file></files><comments><comment>Merge pull request #12451 from jasontedor/fix/12382</comment></comments></commit></commits></item><item><title>Add new `public` option for containers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12450</link><project id="" key="" /><description>From https://github.com/elastic/elasticsearch-cloud-azure/issues/58:

&gt; When we create a container using Java API, we can easily make it public:
&gt; 
&gt; ``` java
&gt; BlobContainerPermissions containerPermissions = new BlobContainerPermissions();
&gt; containerPermissions.setPublicAccess(BlobContainerPublicAccessType.CONTAINER);
&gt; container.uploadPermissions(containerPermissions);
&gt; ```
&gt; 
&gt; So we could add an option to the repository such as `"public": true` (default to `false`), so people would be able to share their data using a public read-only URL.
</description><key id="97090911">12450</key><summary>Add new `public` option for containers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-24T16:49:09Z</created><updated>2016-07-21T14:39:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>support new `host_type=rdma` option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12449</link><project id="" key="" /><description>From https://github.com/elastic/elasticsearch-cloud-azure/issues/41 by @ppf2:

&gt; Would like to see support for a `host_type` similar to what we have for the AWS plugin.  For azure, it can be one of the following types:
&gt; - `private_ip`
&gt; - `public_ip`
&gt; - `RDMA` (http://blogs.technet.com/b/windowshpc/archive/2014/01/30/new-high-performance-capabilities-for-windows-azure.aspx)
&gt; 
&gt; It will also be nice for it to default to use the better performing RDMA connection and fall back to `private_ip` if none is provided.
</description><key id="97090560">12449</key><summary>support new `host_type=rdma` option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure ARM</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-24T16:46:59Z</created><updated>2016-07-21T14:40:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>[cloud-azure] Support repository length &gt; 64mb</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12448</link><project id="" key="" /><description>Coming from https://github.com/elastic/elasticsearch-cloud-azure/issues/6

We currently only support repository with length &lt;= 64mb.

To support larger blobs, we need to implement http://msdn.microsoft.com/en-us/library/windowsazure/dd135726.aspx
</description><key id="97089820">12448</key><summary>[cloud-azure] Support repository length &gt; 64mb</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Snapshot/Restore</label><label>adoptme</label></labels><created>2015-07-24T16:41:26Z</created><updated>2016-01-26T16:04:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T15:03:29Z" id="175063214">@dadoonet is this still relevant?
</comment><comment author="dadoonet" created="2016-01-26T16:04:24Z" id="175092737">@clintongormley Yes. We need to implement `Put Block` API to be able to upload bigger chunks than 64mb.
I added the adoptme label.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: don't cache type filter in DocumentMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12447</link><project id="" key="" /><description>If we cache the type filter and we e.g. set its boost which is now settable on all queries, the boost will change for all subsequent queries. We should rather create a new query every time.

This PR is against the query-refactoring branch.
</description><key id="97089405">12447</key><summary>Query DSL: don't cache type filter in DocumentMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T16:38:27Z</created><updated>2015-07-27T09:37:01Z</updated><resolved>2015-07-24T16:50:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-24T16:38:52Z" id="124575146">LGTM
</comment><comment author="javanna" created="2015-07-27T09:37:01Z" id="125146509">After talking to @rjernst , we decided to backport this change to master, that's why I added the 2.0 label.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file></files><comments><comment>Merge pull request #12447 from javanna/enhancement/dont_cache_type_query</comment></comments></commit></commits></item><item><title>[cloud-azure] specify blob account settings at Repository creation time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12446</link><project id="" key="" /><description>from https://github.com/elastic/elasticsearch-cloud-azure/issues/96 by @smflorentino:

&gt; Currently, when creating an Azure Repository for ES, creation fails if the proper blob storage settings aren't in elasticsearch.yml. Changing the storage account settings can be cumbersome if there is a large number of nodes, as it requires a full cluster restart.
&gt; 
&gt; I want to do a one-time back up some repositories for testing, but I don't have access to do a full reboot of the ES cluster. I'd like to be able to specify the blob storage account details right in the request (the AWS plugin supports this). I'm willing to do the work on this if it would be received positively.
</description><key id="97087950">12446</key><summary>[cloud-azure] specify blob account settings at Repository creation time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Snapshot/Restore</label></labels><created>2015-07-24T16:28:53Z</created><updated>2015-08-10T07:52:07Z</updated><resolved>2015-08-10T07:52:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-10T07:52:06Z" id="129344569">Closing this one in favor of https://github.com/elastic/elasticsearch/issues/12759
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PluginManager: Add Support for basic auth</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12445</link><project id="" key="" /><description>In order to support the URL notation including a user/pass combination
(like http://user:pass@host/plugin.zip) the auth info needs to be added
manually.
</description><key id="97084248">12445</key><summary>PluginManager: Add Support for basic auth</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T16:06:35Z</created><updated>2015-08-05T13:58:26Z</updated><resolved>2015-08-05T13:58:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-04T09:05:05Z" id="127533626">LGTM
</comment><comment author="s1monw" created="2015-08-04T09:21:01Z" id="127539231">I think we should fail if we don't use SSL here - we should never share the password with the rest of the world
</comment><comment author="spinscale" created="2015-08-04T09:30:56Z" id="127540877">true, will change the PR accordingly... testing is going to be tricky with SSL I guess
</comment><comment author="spinscale" created="2015-08-05T09:41:26Z" id="127935929">updated this PR to only support basic auth for HTTPS.

Test implementation note: I am temporarily replacing the HttpsUrlConnection SSL socket factory while running this test in order to accept a self-signed certificate - if someone has a smarter idea I am all ears.
</comment><comment author="s1monw" created="2015-08-05T10:36:50Z" id="127951726">LGTM left one minor comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Refactoring: Move null-checks to validate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12444</link><project id="" key="" /><description>Following up to #12427, this PR does same changes, moving null-checks from construtors
and setters in query builder to the validate() method.

PR against query-refactoring branch
</description><key id="97079885">12444</key><summary>Query Refactoring: Move null-checks to validate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-24T15:42:10Z</created><updated>2016-03-11T11:51:03Z</updated><resolved>2015-07-24T16:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-24T15:52:38Z" id="124564886">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file></files><comments><comment>Merge pull request #12444 from cbuescher/feature/query-refactoring-nullCheckCleanup</comment></comments></commit></commits></item><item><title>Copy headers from the MLT request before calling the multi-termvectors API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12443</link><project id="" key="" /><description>Today, we call the fetch service with a MultiTermVectorsRequest and then after that copy the headers
and context to the MultiTermVectorsRequest. The fetch service executes the request so the headers
and context are not used when the request is executed. This changes the code to copy the headers
and context first, so the headers and information from the context are available when executing the
request.
</description><key id="97078705">12443</key><summary>Copy headers from the MLT request before calling the multi-termvectors API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Search</label><label>bug</label><label>v1.6.2</label><label>v1.7.1</label></labels><created>2015-07-24T15:35:32Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-24T17:56:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-24T15:41:36Z" id="124561890">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mlt copy headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12442</link><project id="" key="" /><description /><key id="97078059">12442</key><summary>Mlt copy headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels /><created>2015-07-24T15:32:02Z</created><updated>2015-07-24T15:32:11Z</updated><resolved>2015-07-24T15:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>improve java version comparison in JarHell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12441</link><project id="" key="" /><description>Currently this uses a pretty crappy approach, to try to not break for future jdk versions.

But @jasontedor suggests something that looks much cleaner on #12424 

What do you think of instead of doing this parsing and relying on a specific version format, we just use [`Package.isCompatibleWith`](https://docs.oracle.com/javase/8/docs/api/java/lang/Package.html#isCompatibleWith-java.lang.String-)? Something like this:

```
Package p = Runtime.class.getPackage();
if (!p.isCompatibleWith(targetVersion)) {
  throw new IllegalStateException(
    String.format(
       "%s requires Java %s:, your system: %s",
       resource,
       targetVersion,
       p.getSpecificationVersion()
    )
  );
}
```
</description><key id="97055336">12441</key><summary>improve java version comparison in JarHell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>discuss</label><label>enhancement</label><label>v2.1.0</label></labels><created>2015-07-24T13:40:16Z</created><updated>2015-08-21T12:30:50Z</updated><resolved>2015-08-21T12:30:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JavaVersion.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JavaVersionTests.java</file></files><comments><comment>Improve Java version comparison in JarHell</comment></comments></commit></commits></item><item><title>Extended SpanFirstQueryBuilderTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12440</link><project id="" key="" /><description>Leftover changes for #12427.

This PR is agains query refactoring branch.
</description><key id="97053251">12440</key><summary>Extended SpanFirstQueryBuilderTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-24T13:28:33Z</created><updated>2016-03-11T11:51:04Z</updated><resolved>2015-07-24T13:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-24T13:35:23Z" id="124521782">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12440 from cbuescher/feature/query-refactoring-constructorCleanup</comment></comments></commit></commits></item><item><title>NPE when querying `_all` field with no fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12439</link><project id="" key="" /><description>```
DELETE my_index

POST my_index/my_type/1
{}

GET my_index/_search
{
  "query": {
    "match": {
      "_all": "foo"
    }
  }
}
```

Returns an NPE with the following stack trace:

```
Failed to execute [org.elasticsearch.action.search.SearchRequest@402fa749]
RemoteTransportException[[Havok][inet[/127.0.0.1:9300]][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ElasticsearchException; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ElasticsearchException; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:334)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:346)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:340)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: ElasticsearchException; nested: NullPointerException;
    at org.elasticsearch.ExceptionsHelper.convertToElastic(ExceptionsHelper.java:57)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:133)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:485)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:694)
    at org.apache.lucene.search.IndexSearcher.searchAfter(IndexSearcher.java:410)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:439)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:151)
    ... 9 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.lucene.all.AllTermQuery.rewrite(AllTermQuery.java:130)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:786)
    at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:833)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:130)
    ... 15 more
```
</description><key id="97051352">12439</key><summary>NPE when querying `_all` field with no fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T13:19:27Z</created><updated>2015-07-28T14:21:00Z</updated><resolved>2015-07-28T14:21:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file></files><comments><comment>_all: Stop NPE querying _all when it doesn't exist</comment></comments></commit></commits></item><item><title>documenting plugins custom install location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12438</link><project id="" key="" /><description>documenting use of -Des.plugins.path with regards to plugin script
</description><key id="97038446">12438</key><summary>documenting plugins custom install location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>docs</label></labels><created>2015-07-24T12:02:53Z</created><updated>2015-08-15T16:04:43Z</updated><resolved>2015-08-15T16:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T11:09:35Z" id="125167802">@spinscale / @tlrx could you review this please?  Is this still true in master?
</comment><comment author="tlrx" created="2015-07-28T09:52:08Z" id="125527621">@nellicus I'm not sure why you add this documentation.

In 1.7 &amp; master, the custom plugins folder must be set in the `elasticsearch.yml` file under the `path.plugins` entry. When executed, the `bin/plugin` looks for the configuration file in the `$ES_HOME/config` then it loads this file and finally resolves the `path.plugins` folder. So if the configuration file is found and loaded, you don't need to specify `-Des.plugins.path` in the command.

Related to #10673
</comment><comment author="nellicus" created="2015-07-28T09:57:49Z" id="125534272">@tlrx and what if you use a custom -Des.config.path ?
</comment><comment author="tlrx" created="2015-07-28T10:09:13Z" id="125538958">In this case you still need to update the `elasticsearch.yml` file with the custom `path.plugins` entry and you need to pass the location of the configuration dir and/or file to the `bin\plugin` script.

You can pass the location as an environment variable like this:

```
CONF_DIR="/tmp/config" bin/plugin install elasticsearch/shield/latest
```

or use the `es.path.conf` property:

```
bin/plugin -Des.path.conf=/tmp/config/ install elasticsearch/shield/latest
```

or directly the configuration file:

```
/bin/plugin -Des.config=/tmp/config/elasticsearch.yml remove shield
```

Note that the plugin manager is changing for 2.0, hopefully this behavior will remain for 2.0
</comment><comment author="nellicus" created="2015-07-28T10:11:20Z" id="125539381">is this anywhere in the documentation? @tlrx 
</comment><comment author="tlrx" created="2015-07-28T10:18:27Z" id="125540881">@nellicus I don't think so
</comment><comment author="nellicus" created="2015-07-28T10:58:21Z" id="125556978">ok the purpose of this PR was to clarify a scenario we don't document which leads users into this problematic scenario 

-&gt; installing/uninstalling a plugin fails if both a custom config and plugin path are in use.

@tlrx do you suggest a complete rewrite explaining how plugins config paths work or just leaving this to address this dark corner?
</comment><comment author="tlrx" created="2015-07-29T07:15:17Z" id="125866811">&gt; @tlrx do you suggest a complete rewrite explaining how plugins config paths work or just leaving this to address this dark corner?

I think we should update the doc to make things easier to understand, yes.
</comment><comment author="nellicus" created="2015-07-29T07:57:47Z" id="125874261">@tlrx sure I agree

so 

&gt; do you suggest a complete rewrite explaining how plugins config paths work or just leaving this to address this dark corner or anything else?

I do not want this to get stalled.

CC @palecur @debadair 
</comment><comment author="clintongormley" created="2015-07-30T12:34:53Z" id="126308166">@nellicus I'm doing a big rewrite of the plugins documentation: see https://github.com/elastic/elasticsearch/pull/12040

Want to check that and see if anything needs updating there?
</comment><comment author="nellicus" created="2015-07-30T14:40:53Z" id="126351434">ok @clintongormley I guess I don't have enough knowledge on 2.0 plugins however I'll take a look FWIW
</comment><comment author="palecur" created="2015-07-31T04:50:43Z" id="126566608">Thanks for taking this on, Clinton. I'll take some time to review.

pe

On Thu, Jul 30, 2015 at 7:41 AM, Antonio Bonuccelli &lt;
notifications@github.com&gt; wrote:

&gt; ok @clintongormley https://github.com/clintongormley I guess I don't
&gt; have enough knowledge on 2.0 plugins however I'll take a look FWIW
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12438#issuecomment-126351434
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>ShardUtils#getElasticsearchLeafReader() should use FilterLeafReader#getDelegate() instead of FilterLeafReader#unwrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12437</link><project id="" key="" /><description>If there are multiple levels of filtered leaf readers then with the unwrap() method it immediately returns the most inner leaf reader and thus skipping of over any other filtered leaf reader that may be instance of ElasticsearchLeafReader. By using #getDelegate() method we can check each filter reader layer if it is instance of ElasticsearchLeafReader, so that we never skip over any wrapped filtered leaf reader and lose the shard id.
</description><key id="97036333">12437</key><summary>ShardUtils#getElasticsearchLeafReader() should use FilterLeafReader#getDelegate() instead of FilterLeafReader#unwrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T11:44:32Z</created><updated>2015-07-24T11:50:18Z</updated><resolved>2015-07-24T11:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-24T11:46:14Z" id="124497401">LGTM

Can you add a comment to the code about why we use getDelegate and not unwrap?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>install/remove plugin script when using non-default location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12436</link><project id="" key="" /><description>adding note regarding installation/removal of plugins in non-default location
</description><key id="97029466">12436</key><summary>install/remove plugin script when using non-default location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels><label>docs</label></labels><created>2015-07-24T11:13:43Z</created><updated>2015-07-24T11:27:04Z</updated><resolved>2015-07-24T11:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2015-07-24T11:26:45Z" id="124488750">recreating as "Custom location" paragraph
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>No need to find replica copy when index is created</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12435</link><project id="" key="" /><description>There is no need to try and go fetch replica copies for best allocation when the index is created
</description><key id="97023179">12435</key><summary>No need to find replica copy when index is created</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T10:34:31Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-26T22:49:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-24T10:40:19Z" id="124469485">LGTM.  Left  a minor comment.
</comment><comment author="dakrone" created="2015-07-24T17:52:47Z" id="124598604">LGTM also.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>S3 client retry logic </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12434</link><project id="" key="" /><description>From my understanding, we are using default exponential backoff policy for S3 client and we make max number of retries configurable, and we also pass this number into our S3 client when we creating it ( I don't think we should do it [here](https://github.com/elastic/elasticsearch/blob/4c981ff4bfc250080d521af105b5e8589c9fc517/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L126).

So for example, if user put 3 for max number of tries into the setting, aws java client will do 3 times retries for us (hopefully) . AND PLUS, during doing snapshot, we would do another 3 times [retries](https://github.com/elastic/elasticsearch-cloud-aws/blob/master/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java#L86) at our side if we got retryable exception, which is 9 retryies. If we put 5, the total number of retry would become 25? 

Personally, I think this is a bug.
</description><key id="96979894">12434</key><summary>S3 client retry logic </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>bug</label><label>discuss</label></labels><created>2015-07-24T06:53:08Z</created><updated>2016-02-01T14:25:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T10:59:21Z" id="125165718">As I understand it, S3 retries when there is a timeout, while our own retries happen if there is a 404.  So the only way you'd get 3 \* 3 retries is if all 2 S3 requests timed out, then the third returned a 404, and the same pattern is repeated for the next 3 attempts.
</comment><comment author="xuzha" created="2015-07-29T01:07:44Z" id="125794845">Yes. AWS client only do retry for their retryable task. 
That's why we shouldn't change the default 3 times retry setting when we init S3 client.
</comment><comment author="clintongormley" created="2016-01-26T15:02:27Z" id="175062194">@dadoonet @imotov any thoughts on this?
</comment><comment author="imotov" created="2016-02-01T14:25:23Z" id="177992295">I agree it looks like we have two essentially different settings (blob-level max retry count and client level max retry count) and we use the same that same repository setting to control both. I think should at least add an ability to specify them separately. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PluginService should ignore .DS_Store and other hidden files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12433</link><project id="" key="" /><description>If you traverse into a plugin directory with Finder, OSX will start creating `.DS_Store` files everywhere.  The PluginService will then attempt to read these files on startup and bomb out:

```
[2015-07-24 00:35:29,943][WARN ][bootstrap                ] failed to add plugin [/Users/tongz/Documents/elasticsearch_master/core/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT/plugins/.DS_Store]
java.nio.file.FileSystemException: /Users/tongz/Documents/elasticsearch_master/core/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT/plugins/.DS_Store/plugin-descriptor.properties: Not a directory
at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
at java.nio.file.Files.newByteChannel(Files.java:361)
at java.nio.file.Files.newByteChannel(Files.java:407)
at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
at java.nio.file.Files.newInputStream(Files.java:152)
at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:85)
at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:316)
at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:147)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:176)
at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:277)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
[2015-07-24 00:35:29,946][WARN ][bootstrap                ] failed to add plugin [/Users/tongz/Documents/elasticsearch_master/core/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT/plugins/sight]
java.lang.IllegalArgumentException: Plugin [sight] must be at least a jvm or site plugin
at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:100)
at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:316)
at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:147)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:176)
at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:277)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
```

It would make sense for `.DS_Store` and other hidden files to be ignored by default.
</description><key id="96968249">12433</key><summary>PluginService should ignore .DS_Store and other hidden files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-24T05:21:03Z</created><updated>2015-07-27T08:06:58Z</updated><resolved>2015-07-27T08:06:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-27T08:06:58Z" id="125117554">Closed via #12465
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cannot instantiate SPI class: org.apache.lucene.codecs.lucene40.Lucene40Codec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12432</link><project id="" key="" /><description>Both JDK 7 &amp; 8:

```
java version "1.8.0_51"
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)
```

```
java version "1.7.0_79"
Java(TM) SE Runtime Environment (build 1.7.0_79-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)
```

When calling `NodeBuilder.nodeBuilder().node();`, I get the following exception. I've tried adding `lucene-codec:4.10.4` as a dependency, since I didn't see it in `elasticsearch-1.7.0.pom`, but no luck... Am I doing it wrong? I've successfully used previous versions of the library.

```
INFO  [2015-07-24 00:10:45,431] org.elasticsearch.node: [Shooting Star] version[1.7.0], pid[45231], build[929b973/2015-07-16T14:31:07Z]
INFO  [2015-07-24 00:10:45,431] org.elasticsearch.node: [Shooting Star] initializing ...
Exception in thread "main" java.util.ServiceConfigurationError: Cannot instantiate SPI class: org.apache.lucene.codecs.lucene40.Lucene40Codec
    at org.apache.lucene.util.NamedSPILoader.reload(NamedSPILoader.java:77)
    at org.apache.lucene.util.NamedSPILoader.&lt;init&gt;(NamedSPILoader.java:47)
    at org.apache.lucene.util.NamedSPILoader.&lt;init&gt;(NamedSPILoader.java:37)
    at org.apache.lucene.codecs.Codec.&lt;clinit&gt;(Codec.java:41)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:155)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)
...
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene40' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [IDVersion]
    at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
    at org.apache.lucene.codecs.PostingsFormat.forName(PostingsFormat.java:100)
    at org.apache.lucene.codecs.lucene40.Lucene40Codec.&lt;init&gt;(Lucene40Codec.java:117)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at java.lang.Class.newInstance(Class.java:442)
    at org.apache.lucene.util.NamedSPILoader.reload(NamedSPILoader.java:67)
    ... 13 more
```
</description><key id="96932411">12432</key><summary>Cannot instantiate SPI class: org.apache.lucene.codecs.lucene40.Lucene40Codec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devoncrouse</reporter><labels /><created>2015-07-24T00:20:08Z</created><updated>2015-09-24T21:05:22Z</updated><resolved>2015-07-24T17:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="devoncrouse" created="2015-07-24T17:27:02Z" id="124586645">Can no longer reproduce this; things are working now. Not sure what I changed :shrug:
</comment><comment author="jpv" created="2015-09-24T21:05:22Z" id="143050804">same trouble. Arrrg
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard awareness leaves replica shards unassigned when racks are unevenly sized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12431</link><project id="" key="" /><description>There are a few parts to this ticket:

**Functionality**
There is this logic (https://github.com/elastic/elasticsearch/blob/c57951780e0132c50b723d78038ab73e10d176c5/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java#L222) which prevents the allocation of shards when the number of shards per unique node attribute used in the awareness (eg. rack_id) is exceeded (`if (currentNodeCount &gt; (requiredCountPerAttribute + leftoverPerAttribute)) {`).

In this particular use case, the user has 69 nodes with an index that has 1 primary shard defined with 68 replicas (so 69 copies of the shard).  The expectation is that it will put 1 shard on each node.  The issue here is that it doesn't and it ends up leaving 2 copies unassigned.

For example, one of the rack_id's used in allocation awareness is associated to 5 different nodes in the cluster.  It ended up allocating 1 shard on 4 of the 5 nodes.  At this point, (currentNodeCount &gt; (requiredCountPerAttribute + leftoverPerAttribute)), so it prevents the 5th node from getting a copy of this shard.  In this case averagePerAttribute is 2 (with 28 unique attributes) so requiredCountPerAttribute is 2.  totalLeftover is not 0, so leftoverPerAttribute is 1.  So 4 &gt; 3 and it prevents the 5th node from getting a shard copy.

There is quite an uneven distribution of nodes per rack_id, some rack_ids in this setup has 1 node, others have up to 5 nodes in them.  The allocation decider logic is preventing all shards for that index from being allocated and leaving a few unassigned.

For this use case, the user is not that concerned about distribution of shards for this particular index across racks.  But awareness is currently enabled for the node/cluster and there does not appear to be a way to configure a specific index to be excluded from the awareness logic.

**Documentation**
Not much available on the above logic.

**Logging/Debugging**
This may get a bit verbose so I am not sure about the feasibility of it.  For troubleshooting purposes, it will be helpful if we can write a trace line with a bit more details than just the current message in the reroute explain ("explanation": "too many shards on nodes for attribute: [rack_id]").  Like it will be useful if it will tell you the problem index, how many shards are allowed, and how many are on there currently, etc..  Doesn't have to be logging either, maybe add to the explanation string that is returned by reroute explain to give some more specific information.
</description><key id="96921851">12431</key><summary>Shard awareness leaves replica shards unassigned when racks are unevenly sized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>discuss</label><label>docs</label><label>enhancement</label></labels><created>2015-07-23T22:55:52Z</created><updated>2016-01-15T12:40:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-27T09:02:14Z" id="125135408">@imotov @dakrone any thoughts on this one?
</comment><comment author="dakrone" created="2015-07-27T17:54:58Z" id="125286952">&gt; Doesn't have to be logging either, maybe add to the explanation string that is returned by reroute explain to give some more specific information.

I added additional messages for this in #12490
</comment><comment author="GlenRSmith" created="2015-07-30T00:14:02Z" id="126133920">I think it makes sense to allow a client to interrogate the cluster for the state of shard distribution under the context of any/all awareness attributes, possibly including index-level allocation filtering.

Possibly something like this:

```
{
  "attributes": {
    "rack_id": {
      "rack_one": {
        "nodes": {
          "_all": {
            "logstash-2015.07.24": {
              "0": ["0", "2", "3"],
              "1": ["1", "2"],
              "2": ["0", "3"],
              "3": ["1"],
              "4": ["1", "3"]
            }
          },
          "abcxyz0123456": {
            "logstash-2015.07.24": ["0-0", "1-1", "2-0", "3-1", "4-1"]
          }
        }
      }
    }
  }
}
```

I'm not sure what endpoint is most logical to include it. Possibly cluster stats?
</comment><comment author="clintongormley" created="2015-07-30T15:22:57Z" id="126369197">It'd be nice to think of a better algorithm, so that we can deal with unevenly sized groups.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file></files><comments><comment>Add more debugging information to the Awareness Decider</comment></comments></commit></commits></item><item><title>Fix cidr mask conversion issue for 0.0.0.0/0 and add tests #12005</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12430</link><project id="" key="" /><description>This fixes the issue raised in #12005 and adds test to verify it.
</description><key id="96889202">12430</key><summary>Fix cidr mask conversion issue for 0.0.0.0/0 and add tests #12005</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ruflin</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.6.2</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T19:33:32Z</created><updated>2015-07-27T10:56:49Z</updated><resolved>2015-07-27T10:12:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ruflin" created="2015-07-27T07:15:02Z" id="125108517">@jpountz I updated the tests. Based on https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java#L133 the -1 has to stay.
</comment><comment author="jpountz" created="2015-07-27T08:00:43Z" id="125116092">@ruflin Would you mind signing the [CLA](https://www.elastic.co/contributor-agreement). This will be required so that your pull request can be merged.
</comment><comment author="ruflin" created="2015-07-27T08:33:48Z" id="125126526">@jpountz I just signed it. BTW: The CLA should probably be updated to Elastic. It is still Elasticsearch.
</comment><comment author="ruflin" created="2015-07-27T08:40:31Z" id="125128154">Test moved up and CLA signed.
</comment><comment author="jpountz" created="2015-07-27T10:32:44Z" id="125159597">Thanks!

&gt; The CLA should probably be updated to Elastic. It is still Elasticsearch.

Elastic is the trade name of the company, but Elasticsearch BV is still the registed name of the company, this is why the CLA still mentions Elasticsearch BV.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file></files><comments><comment>Merge pull request #12430 from ruflin/ip_range-mask-fix</comment></comments></commit></commits></item><item><title>Use underscores instead of camel-casing for date formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12429</link><project id="" key="" /><description>The date formats stand out as one of the few place we use camel casing, eg `strictDateOptionalTime`

These should be changed to `strict_date_optional_time` etc
</description><key id="96887121">12429</key><summary>Use underscores instead of camel-casing for date formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/metadave/following{/other_user}', u'events_url': u'https://api.github.com/users/metadave/events{/privacy}', u'organizations_url': u'https://api.github.com/users/metadave/orgs', u'url': u'https://api.github.com/users/metadave', u'gists_url': u'https://api.github.com/users/metadave/gists{/gist_id}', u'html_url': u'https://github.com/metadave', u'subscriptions_url': u'https://api.github.com/users/metadave/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/58244?v=4', u'repos_url': u'https://api.github.com/users/metadave/repos', u'received_events_url': u'https://api.github.com/users/metadave/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/metadave/starred{/owner}{/repo}', u'site_admin': False, u'login': u'metadave', u'type': u'User', u'id': 58244, u'followers_url': u'https://api.github.com/users/metadave/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Dates</label><label>adoptme</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T19:20:36Z</created><updated>2015-07-28T20:57:45Z</updated><resolved>2015-07-28T20:57:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="metadave" created="2015-07-27T17:57:46Z" id="125287679">@clintongormley It looks like the formats have been updated in the [Joda](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/joda/Joda.java#L55..L218) class already, is this simply changing tests and docs to reflect these changes?
</comment><comment author="clintongormley" created="2015-07-27T18:30:45Z" id="125298142">@metadave It's the stringification which defaults to camelcase.  Try:

```
PUT my_index/my_type/1
{
  "date": "2007-01-01T01:00:00Z"
}

GET my_index/_mapping
```

Don't worry about the docs, I'm reworking those in a bigger change
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Use underscores for date formats</comment></comments></commit></commits></item><item><title>setRoutingService change breaks discovery plugins in 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12428</link><project id="" key="" /><description>This change 

https://github.com/elastic/elasticsearch/commit/182c59f5b46d3a666d21ba369452844220736a72 

is breaking existing discovery plugins and may scare people away from upgrading to 1.7. I've put comments on that pull request too.

While the fix is easy but it worries me about ES's upgrading philosophy as I'm not expecting API changes in minor version upgrades. 

I've already fixed my plugin so I'm good now, just want to bring this up to your attention.
</description><key id="96863589">12428</key><summary>setRoutingService change breaks discovery plugins in 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkliu</reporter><labels /><created>2015-07-23T17:29:50Z</created><updated>2015-07-24T09:08:04Z</updated><resolved>2015-07-24T09:08:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-24T09:08:04Z" id="124450468">As we already discussed on the commit, we don't guarantee BWC on the internal APIs. We do try, where possible to maintain it and way pros against the cons. This is such a minor change that I don't think it's a problem to upgrade any plugin. I'm going to close for now... 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Moving null-checks from constructors to validate()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12427</link><project id="" key="" /><description>After joint discussion we decided to move the validation of inner queries etc... not being null to the validate() method only and rely on it being called rather than fail in constructors already. This PR moves those null-checks for already refactored span queries.

PR: query refactoring branch
</description><key id="96861286">12427</key><summary>Moving null-checks from constructors to validate()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-23T17:17:48Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-24T12:16:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-07-23T19:07:38Z" id="124213730">LGTM
</comment><comment author="javanna" created="2015-07-24T07:53:54Z" id="124406549">left one suggestion, LGTM otherwise
</comment><comment author="cbuescher" created="2015-07-24T10:55:45Z" id="124473754">Added changes to validate() to only perform inner validation when queries are non-null.
</comment><comment author="cbuescher" created="2015-07-24T13:26:11Z" id="124519976">@javanna  seems like I can't append more commits here after I merged, I added the other changes to SpanFirstQueryBuilder but will include that in different PR.
</comment><comment author="javanna" created="2015-07-24T13:27:50Z" id="124520481">no worris @cbuescher 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file></files><comments><comment>Query Refactoring: Move null-checks from constructors and setters to validate</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12427 from cbuescher/feature/query-refactoring-constructorCleanup</comment></comments></commit></commits></item><item><title>bug with list indexing in groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12426</link><project id="" key="" /><description>There seems to be a problem with list indexing in Groovy scripts in elasticsearch 1.7.0. I have the following document in my database:

```
{
    "name": "Test",
    "numbers": [20, 15, 10, 5]
}
```

I use function_score to boost on a specific element of "numbers" field:

```
POST _search
{
    "query" : {
        "function_score" : {
             "query" : {"match" : { "name" : "Test"}},
             "script_score" : {"script" : "doc['numbers'].get(0)"},
             "boost_mode" : "replace"
        }
    }
}
```

This returns

```
"hits": {
  "total": 1,
  "max_score": 5,
  "hits": [
     {
        "_index": "myindex",
        "_type": "mytitle",
        "_id": "AU67v7GsYH4LIlLusnu1",
        "_score": 5,
        "_source": {
           "name": "Test",
           "numbers": [
              20,
              15,
              10,
              5
           ]
        }
     }
  ]
}
```

The score ends up being the last element of the list. get(1) gives me the next-to-last element, and get(-1) the first element. I assume that's not the inteded behaviour?
</description><key id="96858605">12426</key><summary>bug with list indexing in groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">mwiebusch78</reporter><labels /><created>2015-07-23T17:01:20Z</created><updated>2015-07-23T18:42:34Z</updated><resolved>2015-07-23T18:42:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-07-23T18:42:22Z" id="124206285">@mwiebusch78 This is due to the way that [multi-value fields are indexed](https://www.elastic.co/guide/en/elasticsearch/guide/current/complex-core-fields.html#_multivalue_fields). In particular, the ordering of the elements is not available at search time.

My recommendation would be to at index time extract the first value of the `numbers` field into another field. This will make the first value available at scoring time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dump thread stack traces on AssertionError when executing tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12425</link><project id="" key="" /><description>We are currently seeing some spooky test failures due to assertion errors deep in the JVM. It's unclear what the root cause of these failures is, but it appears to be due to a race condition. We need to collect more information about what exactly the JVM is doing when we see failures like this. Useful information here would be the stack traces of all threads at the time of the assertion failure.
</description><key id="96839714">12425</key><summary>Dump thread stack traces on AssertionError when executing tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-07-23T15:26:23Z</created><updated>2015-09-30T15:43:39Z</updated><resolved>2015-07-23T15:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/ESTestCase.java</file><file>core/src/test/java/org/elasticsearch/test/StackTraces.java</file><file>core/src/test/java/org/elasticsearch/test/junit/listeners/AssertionErrorThreadDumpPrinter.java</file></files><comments><comment>Do not dump stack traces of threads on test failure</comment></comments></commit><commit><files><file>core/src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>core/src/test/java/org/elasticsearch/test/StackTraces.java</file><file>core/src/test/java/org/elasticsearch/test/junit/listeners/AssertionErrorThreadDumpPrinter.java</file></files><comments><comment>Dump stack traces of all threads on test failure due to AssertionError</comment></comments></commit></commits></item><item><title>make java.version mandatory for jvm plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12424</link><project id="" key="" /><description>JarHell has a low level check, but its more of a best effort one,
only checking if X-Compile-Target-JDK is set in the manifest. This
is the case for all lucene- and elasticsearch- generated jars, but
lets just be explicit for plugins.

I also reorganized the descriptor file a bit and added a site plugin example.
</description><key id="96834849">12424</key><summary>make java.version mandatory for jvm plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T15:02:32Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-24T15:37:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-23T15:41:13Z" id="124144992">cc @clintongormley can you review the docs added in this PR? https://github.com/rmuir/elasticsearch/blob/plugin_java_version/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
</comment><comment author="clintongormley" created="2015-07-23T15:45:50Z" id="124147268">@rmuir docs look good to me.  perhaps just add a note that `elasticsearch.version` can also be specified for site plugins, as they have dependencies on the APIs in a particular version?
</comment><comment author="rmuir" created="2015-07-23T15:48:37Z" id="124148324">&gt; @rmuir docs look good to me. perhaps just add a note that elasticsearch.version can also be specified for site plugins, as they have dependencies on the APIs in a particular version?

But currently this property is only functional for jvm plugins. If we want to make it optional for site plugins then we need to implement that.
</comment><comment author="jasontedor" created="2015-07-24T13:37:25Z" id="124522267">@rmuir I left a minor comment, otherwise it LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginInfo.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file></files><comments><comment>Merge pull request #12424 from rmuir/plugin_java_version</comment></comments></commit></commits></item><item><title>Medium Interval time for ResourceWatcher should be 30 seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12423</link><project id="" key="" /><description>I think the value is 30.

I'm always thankful for this software.
</description><key id="96834317">12423</key><summary>Medium Interval time for ResourceWatcher should be 30 seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">khiraiwa</reporter><labels><label>:Settings</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T15:00:03Z</created><updated>2015-07-27T11:11:01Z</updated><resolved>2015-07-24T12:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-24T12:19:39Z" id="124501662">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file></files><comments><comment>Merge pull request #12423 from khiraiwa/patch-1</comment></comments></commit></commits></item><item><title>After disk allocation hi-watermark hit, freeing disk space does not cause automatic reallocation of new UNASSIGNED shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12422</link><project id="" key="" /><description>reproduced on 1.7.0

steps in short

1) start 2 nodes cluster on same host
2) create huge file to cause disk to be &gt;95% full (high disk allocation watermark hit)
3) create new index with one doc
4) index is created with UNASSIGNED shards
5) remove huge file to go below disk allocation thresholds
6) shards sit in UNASSIGNED (waited 10m)
7) a manual reroute is required to have them reassigned

steps
https://gist.github.com/nellicus/8a7f5a70160346b76237

debug logs
https://gist.github.com/nellicus/c779113544c5f44f6d18

output of cat shards / cluster health during repro
https://gist.github.com/nellicus/b3744dfd85b1343fd944
</description><key id="96828372">12422</key><summary>After disk allocation hi-watermark hit, freeing disk space does not cause automatic reallocation of new UNASSIGNED shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2015-07-23T14:32:05Z</created><updated>2015-08-12T10:28:56Z</updated><resolved>2015-07-30T22:54:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-24T09:14:00Z" id="124451298">When a node goes from high-watermark -&gt; low-watermark (any state change really), we should issue a reroute.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesTests.java</file></files><comments><comment>Reroute shards when a node goes under disk watermarks</comment></comments></commit></commits></item><item><title>Cancel replica recovery on another sync option copy found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12421</link><project id="" key="" /><description>When a replica is initializing from the primary, and we find a better node that has full sync id match, it is better to cancel the existing replica allocation and allocate it to the new node with sync id match (eventually)
</description><key id="96824691">12421</key><summary>Cancel replica recovery on another sync option copy found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T14:13:42Z</created><updated>2015-08-07T17:45:11Z</updated><resolved>2015-07-24T10:14:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-23T15:07:45Z" id="124134715">This is awesome. LGTM. Left some small suggestions... 
</comment><comment author="bleskes" created="2015-07-23T15:08:18Z" id="124134926">OH. We probably need to document this. I'm also wondering if a release highlight is in order...
</comment><comment author="kimchy" created="2015-07-23T15:52:37Z" id="124149616">@bleskes pushed changes, @clintongormley I am not sure where to document this?
</comment><comment author="bleskes" created="2015-07-23T15:54:08Z" id="124149967">still awesome
</comment><comment author="pickypg" created="2015-07-23T16:02:33Z" id="124151964">This is awesome and it's definitely a worthwhile release highlight!
</comment><comment author="clintongormley" created="2015-07-23T16:21:05Z" id="124156064">@kimchy should be documented here i think: https://www.elastic.co/guide/en/elasticsearch/reference/master/delayed-allocation.html

I can add a note if you like.  What do you mean by (eventually) in your PR description?
</comment><comment author="kimchy" created="2015-07-24T10:14:52Z" id="124463055">thanks @clintongormley!, I just pushed it. What I meant by eventually is that we cancel the replica allocation to the node, and then we will try and allocate it to the node with the sync id match, but on that node, we might be throttling allocation, so it will end up getting assigned to it, but not right away, in order to respect the allocation deciders.
</comment><comment author="clintongormley" created="2015-08-07T17:45:11Z" id="128775977">Docs added in c22e179e874e654c5e31c2cb258ee737ecfd6fa6
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Documented cancelation of shard recovery</comment></comments></commit></commits></item><item><title>This javadoc comment is wrong.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12420</link><project id="" key="" /><description>I think the default value is 25.

I am very grateful to your software.
</description><key id="96821422">12420</key><summary>This javadoc comment is wrong.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">khiraiwa</reporter><labels /><created>2015-07-23T13:55:45Z</created><updated>2015-07-23T13:58:16Z</updated><resolved>2015-07-23T13:58:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Copy the classloader from the original settings when checking for prompts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12419</link><project id="" key="" /><description>Today, when a user provides settings and specifies a classloader to be used, the classloader gets
dropped when we copy the settings to check for prompt entries. This change copies the classloader
when replacing the prompt placeholders and adds a test to ensure the InternalSettingsPreparer
always retains the classloader.

Closes #12340
</description><key id="96805064">12419</key><summary>Copy the classloader from the original settings when checking for prompts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>bug</label><label>v1.6.2</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T12:44:20Z</created><updated>2015-07-24T14:05:46Z</updated><resolved>2015-07-23T17:06:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lytvynenkoinvest" created="2015-07-23T16:05:39Z" id="124152659">Hello, is there ETA for fix of this issue?
</comment><comment author="rmuir" created="2015-07-23T16:11:49Z" id="124154115">Looks good to me. just one note
</comment><comment author="lytvynenkoinvest" created="2015-07-23T18:16:44Z" id="124192590">Could you please tell when release with fix of this bug will be built?
</comment><comment author="jaymode" created="2015-07-23T19:00:09Z" id="124211825">@lytvynenkoinvest We try to release regularly and the fix will be released with our next bug fix release, 1.7.1.
</comment><comment author="lytvynenkoinvest" created="2015-07-24T14:05:46Z" id="124533779">Are there expectations about the date of the next release?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file></files><comments><comment>Merge pull request #12419 from jaymode/fix_settings_classloader</comment></comments></commit></commits></item><item><title>Fix fielddata handling for the `_parent` field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12418</link><project id="" key="" /><description>Change #12371 broke fielddata on the `_parent` child for indices created before
2.0. This pull request adds back caching of the `_parent` fielddata for indices
created before 2.0 and cleans some related stuff. For instance
DocumentTypeListener doesn't need to take care of removed mappings anymore since
mappings can't be removed in 2.0.
</description><key id="96800485">12418</key><summary>Fix fielddata handling for the `_parent` field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2015-07-23T12:14:30Z</created><updated>2015-07-23T14:07:38Z</updated><resolved>2015-07-23T14:07:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-23T12:21:26Z" id="124079275">Left one question. LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentTypeListener.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/KeyedLockTests.java</file></files><comments><comment>Merge pull request #12418 from jpountz/fix/parent_fielddata_caching</comment></comments></commit></commits></item><item><title>Remove unused QueryParseContext argument in MappedFieldType#rangeQuery()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12417</link><project id="" key="" /><description>The rangeQuery() method in MappedFieldType and some overwriting subtypes takes
a nullable QueryParseContext argument which is unused and can be deleted.
This is also useful for the current query parsing refactoring, since
we want to avoid passing the parse context object around unnecessarily.
</description><key id="96781815">12417</key><summary>Remove unused QueryParseContext argument in MappedFieldType#rangeQuery()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T10:30:42Z</created><updated>2016-03-11T11:51:07Z</updated><resolved>2015-07-24T12:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-23T13:05:42Z" id="124089736">LGTM. I'm curious if you checked whether other *Query methods also needed this context? The more methods we can remove this parameter from, the better!
</comment><comment author="cbuescher" created="2015-07-23T13:34:13Z" id="124101620">@jpountz not yet, but I think I will because we need to reduce the amount the context is passed around a bit for the query refactoring. Just want to go step by step here.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxQuery.java</file><file>core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Merge pull request #12417 from cbuescher/remove-rangequery-context</comment></comments></commit></commits></item><item><title>CI: testCorruptedShards fails sporadically </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12416</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_metal/10479/testReport/junit/org.elasticsearch.action.admin.indices.shards/IndicesShardStoreRequestTests/testCorruptedShards/

```
expected null, but was:&lt;ElasticsearchException[/home/jenkins/workspace/es_core_master_metal/core/target/J2/temp/org.elasticsearch.action.admin.indices.shards.IndicesShardStoreRequestTests_A587044FC4F41BF6-001/tempDir-003/data/TEST-slave19.build.ci.hetz.es.io-CHILD_VM=[2]-CLUSTER_SEED=[-6063786116933768492]-HASH=[715B7D4EEE90F3]-cluster/nodes/0/indices/test/4/_state/state-2.st]; nested: NoSuchFileException[/home/jenkins/workspace/es_core_master_metal/core/target/J2/temp/org.elasticsearch.action.admin.indices.shards.IndicesShardStoreRequestTests_A587044FC4F41BF6-001/tempDir-003/data/TEST-slave19.build.ci.hetz.es.io-CHILD_VM=[2]-CLUSTER_SEED=[-6063786116933768492]-HASH=[715B7D4EEE90F3]-cluster/nodes/0/indices/test/4/_state/state-2.st];&gt;
```
</description><key id="96776038">12416</key><summary>CI: testCorruptedShards fails sporadically </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2015-07-23T10:03:28Z</created><updated>2016-01-26T16:53:51Z</updated><resolved>2016-01-26T15:13:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T15:00:34Z" id="175060349">@areek @bleskes is this still relevant?
</comment><comment author="bleskes" created="2016-01-26T15:13:06Z" id="175068094">this is so long ago and we didn't research (or at least, not analysis here). I will close this and reopen when it and if it surfaces again. Please reopen if anyone disagrees
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>master node was force to rejoin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12415</link><project id="" key="" /><description>Elasticsearch 1.6.0

The master node is 10.19.0.100, the es.log record as follow. It discover itself as a `also master but with an older cluster_state`, then force itself to rejoin...

```
[2015-07-23 15:00:12,976][INFO ][cluster.service          ] [10.19.0.100] new_master [10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}, reason: zen-disco-join (elected_as_master)
[2015-07-23 15:00:14,356][INFO ][cluster.service          ] [10.19.0.100] added {[10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false},}, reason: zen-disco-receive(join from node[[10.19.0.69][Kn1ghOA4SFyI12Qo9ZEXEg][esnode069.mweibo.bx.sinanode.com][inet[/10.19.0.69:9300]]{max_local_storage_nodes=1, data=false, master=false}])
[2015-07-23 15:00:44,366][WARN ][discovery.zen.publish    ] [10.19.0.100] timed out waiting for all nodes to process published state [357982] (timeout [30s], pending nodes: [[10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false}])
[2015-07-23 15:00:44,369][WARN ][cluster.service          ] [10.19.0.100] cluster state update task [zen-disco-receive(join from node[[10.19.0.69][Kn1ghOA4SFyI12Qo9ZEXEg][esnode069.mweibo.bx.sinanode.com][inet[/10.19.0.69:9300]]{max_local_storage_nodes=1, data=false, master=false}])] took 30s above the warn threshold of 30s
[2015-07-23 15:00:44,383][INFO ][cluster.service          ] [10.19.0.100] removed {[10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false},}, reason: zen-disco-node_failed([10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false}), reason failed to ping, tried [3] times, each with maximum [1.6m] timeout
[2015-07-23 15:00:44,464][WARN ][discovery.zen            ] [10.19.0.100] discovered [[10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}] which is also master but with an older cluster_state, telling [[10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}] to rejoin the cluster ([via a new cluster state])
[2015-07-23 15:00:44,465][WARN ][discovery.zen            ] [10.19.0.100] received a request to rejoin the cluster from [144L7QgTSMahE2MVpWDffw], current nodes: {[10.19.0.81][IbIErGUAQja_oh3Pu6CiHQ][esnode081.mweibo.bx.sinanode.com][inet[/10.19.0.81:9300]]{max_local_storage_nodes=1, master=false},[10.19.0.82][HCzM5N_1Rd6_V551rXA4fA][esnode082.mweibo.bx.sinanode.com][inet[/10.19.0.82:9300]]{max_local_storage_nodes=1, master=false},[10.19.0.72][Ww2_E4LQT-K8Au8boXT2Fg][esnode072.mweibo.bx.sinanode.com][inet[/10.19.0.72:9300]]{max_local_storage_nodes=1, master=false},[10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true},...(many nodes here)...[10.19.0.99][zcn7tAnoS4SrsalwzmK_Jw][localhost][inet[/10.19.0.99:9300]]{max_local_storage_nodes=1, data=false, master=true},}
[2015-07-23 15:02:24,499][INFO ][cluster.service          ] [10.19.0.100] new_master [10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}, reason: zen-disco-join (elected_as_master)
[2015-07-23 15:02:24,897][INFO ][cluster.service          ] [10.19.0.100] added {[10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false},}, reason: zen-disco-receive(join from node[[10.19.0.80][DycB4xRISgSN4xUGe7bhog][esnode080.mweibo.bx.sinanode.com][inet[/10.19.0.80:9300]]{max_local_storage_nodes=1, master=false}])
[2015-07-23 15:02:54,906][WARN ][discovery.zen.publish    ] [10.19.0.100] timed out waiting for all nodes to process published state [357985] (timeout [30s], pending nodes: [[10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false}])
[2015-07-23 15:02:54,909][WARN ][cluster.service          ] [10.19.0.100] cluster state update task [zen-disco-receive(join from node[[10.19.0.80][DycB4xRISgSN4xUGe7bhog][esnode080.mweibo.bx.sinanode.com][inet[/10.19.0.80:9300]]{max_local_storage_nodes=1, master=false}])] took 30s above the warn threshold of 30s
[2015-07-23 15:02:54,923][INFO ][cluster.service          ] [10.19.0.100] removed {[10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false},}, reason: zen-disco-node_failed([10.19.0.96][KUI5p0MhTFyjQyhIAj-sHA][localhost.localdomain][inet[/127.0.0.1:9300]]{max_local_storage_nodes=1, data=false, master=false}), reason failed to ping, tried [3] times, each with maximum [1.6m] timeout
[2015-07-23 15:02:55,070][WARN ][discovery.zen            ] [10.19.0.100] discovered [[10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}] which is also master but with an older cluster_state, telling [[10.19.0.100][144L7QgTSMahE2MVpWDffw][localhost][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}] to rejoin the cluster ([via a new cluster state])
[2015-07-23 15:02:55,080][WARN ][discovery.zen            ] [10.19.0.100] received a request to rejoin the cluster from [144L7QgTSMahE2MVpWDffw], current nodes: {[10.19.0.81][IbIErGUAQja_oh3Pu6CiHQ][esnode081.mweibo.bx.sinanode.com][inet[/10.19.0.81:9300]]{max_local_storage_nodes=1, master=false...(many nodes here)
```

This happened time after time.

I had try to restart 10.19.0.100 but no effect. Then I had to stop such master, restart all other nodes to detect another master, start this node. Now the cluster health is green.
</description><key id="96776016">12415</key><summary>master node was force to rejoin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">chenryn</reporter><labels><label>:Cluster</label><label>feedback_needed</label></labels><created>2015-07-23T10:03:17Z</created><updated>2016-01-26T15:56:36Z</updated><resolved>2016-01-26T14:59:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-23T10:14:38Z" id="124045516">@chenryn can you share you cluster state on a gist? you can get it via `GET _cluster/state` . 

Also, can you post the complete logs? you redacted some things for brevity (...(many nodes here)...) but it is important to get a complete picture...
</comment><comment author="chenryn" created="2015-07-23T12:39:59Z" id="124082847">@bleskes I upload cluster state and one circle rejoin logs to https://gist.github.com/chenryn/0aa3ba4742b3741d1f01
</comment><comment author="bleskes" created="2015-07-24T09:20:39Z" id="124453430">@chenryn thx. The cluster state in the ES is the same on all nodes except for a little flag indicating witch of the nodes is the local node. Your cluster state misses that flag, which causes the master to publish a new cluster state to itself (which we shouldn't do). This cause it the think there is another master active, responding with telling the other master to stop down. The other master (i.e., the same node) receives the command and steps down only to re-elect it self. 

The biggest question here is how did the node end up not having a local flag set. Do you have any custom plugins installed? Was anything else out of order before this started happening?
</comment><comment author="chenryn" created="2015-07-31T09:14:35Z" id="126618180">No plugin installed. There was one client node died and reboot before the first rejoin happen, the "10.19.0.96" in above log.
</comment><comment author="chenryn" created="2015-07-31T09:25:29Z" id="126620868">btw: what the local flag like? I check the state of another cluster, seems no different of this cluster.
</comment><comment author="chenryn" created="2015-08-17T05:53:21Z" id="131687334">I got the same problem again:

```
[2015-08-16 19:05:27,898][WARN ][discovery.zen            ] [10.19.0.100] discovered [[10.19.0.100][DCTdoPzARimCnC3ZAdq2yQ][esnode100.mweibo.bx.sinanode.com][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}] which is also master but with an old
er cluster_state, telling [[10.19.0.100][DCTdoPzARimCnC3ZAdq2yQ][esnode100.mweibo.bx.sinanode.com][inet[/10.19.0.100:9300]]{max_local_storage_nodes=1, data=false, master=true}] to rejoin the cluster ([via a new cluster state])
```
</comment><comment author="bleskes" created="2015-08-17T12:34:25Z" id="131800303">@chenryn sorry for not getting back to you - I was out for two weeks. The flag is something internal and is not serialized to the rest api. You can see it if you connect via the Java API. You say you don't use any plugins. Is there anything else in your deployment that may be unusual? Do you embed ES? Can you reproduce this by any chance with a small setup you can share? 
</comment><comment author="clintongormley" created="2016-01-26T14:59:52Z" id="175059717">No more feedback - closing
</comment><comment author="chenryn" created="2016-01-26T15:56:36Z" id="175089368">yes, I didn;t reproduce this too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds template support to _msearch resource</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12414</link><project id="" key="" /><description>Much like we already do with search this adds templating support to the _msearch resource.

Closes #10885

Note: Tried it out with a slightly simpler query than the one given in the original issue:

``` shell
$ cat requests
{"index" : "main"}
{ "inline" : "{ \"query\": { \"match_{{template}}\": {} } }", "params": { "template": "all" } }
{"index" : "main"}
{ "inline" : "{ \"query\": { \"match_{{template}}\": {} } }", "params": { "template": "all" } }

$ curl -XGET localhost:9200/_msearch/template --data-binary @requests; echo
```
</description><key id="96765099">12414</key><summary>Adds template support to _msearch resource</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Templates</label><label>enhancement</label><label>review</label><label>v2.1.0</label></labels><created>2015-07-23T08:55:13Z</created><updated>2015-09-03T07:30:11Z</updated><resolved>2015-09-02T08:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-14T09:51:16Z" id="131053148">@MaineC I left a couple of comments, but I think this change is pretty good
</comment><comment author="MaineC" created="2015-08-27T09:29:40Z" id="135363339">@colings86 Adjusted PR according to your comments.

While at it I noticed that the resource files the test for MultiSearchTemplates uses was located under src/test/java - moved them to src/test/resources where according to my maven knowledge they actually belong.
</comment><comment author="colings86" created="2015-08-27T10:28:17Z" id="135374733">LGTM
</comment><comment author="MaineC" created="2015-09-02T08:23:43Z" id="136974482">Merged to master only so far. 

Not sure if this should go into 2.0.0, or only be added to 2.1.0 as a new feature.

@clintongormley @colings86 What do you think?
</comment><comment author="colings86" created="2015-09-02T09:55:05Z" id="137008062">personally I think this is ok as a 2.1 feature
</comment><comment author="s1monw" created="2015-09-02T09:56:03Z" id="137008566">+1 for 2.1
</comment><comment author="MaineC" created="2015-09-02T18:29:09Z" id="137200871">Makes sense.

Not seeing a 2.x branch and assuming master is post-2.something this will have to wait to be backported until a 2.x branch is there, right?
</comment><comment author="colings86" created="2015-09-03T07:28:23Z" id="137359621">The master branch is 2.1.0 at the moment so I think pushing to master only should be enough as 2.x will be spawned from master when we need to.
</comment><comment author="MaineC" created="2015-09-03T07:30:11Z" id="137359925">Gotcha. Thanks for clarifying.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file></files><comments><comment>Merge pull request #12414 from MaineC/feature/10885</comment></comments></commit></commits></item><item><title>_search option limiting returned hits to only top scorers </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12413</link><project id="" key="" /><description>Hello, 

I have tried searching for this feature through the doc, issues and mailing list but couldn't find it, I apologize if I missed something. 
The only parameter I could find to limit the number of returned hits is the size parameter but we can't know in advance how many hits will have the top score for a given query, 

In order to limit the amount of processing and network traffic for some of our requests I would like to only get the top scoring hits highlighted and returned, we discard anything which doesn't have the top score anyway. 

This new parameter can be either a boolean (only the top scoring hits) or maybe an int (the hits for the `n` top scores), not sure which param makes the most sense.

I am thinking of something like : 

```
GET index/type/_search
{
 "topScorers": true,
 "query": {"match": {
   "name": "the quick brown fox"
 }} 
} 
# OR
GET index/type/_search
{
 "topScorers": 3,
 "query": {"match": {
   "name": "the quick brown fox"
 }} 
} 
```

I am also definitely not sure about the parameter name 
</description><key id="96762712">12413</key><summary>_search option limiting returned hits to only top scorers </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeantil</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-07-23T08:41:00Z</created><updated>2015-07-27T11:05:40Z</updated><resolved>2015-07-24T09:21:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-24T09:21:43Z" id="124453829">Hiya

We discussed this in our FixItFriday session and we're against adding this feature.  It feels very use case specific.  If the top 6 documents all have the same score and the 7th is 0.0001 less, is that sufficient to truncate the result list?  For you maybe, but somebody else will want a different behaviour.  You could of course get back a million top-scoring documents, so either way this needs to be limited by `size`.

You could write a custom plugin that does this for you, but I don't think this is a feature we want in core.

thanks
</comment><comment author="jeantil" created="2015-07-24T09:43:31Z" id="124457688">Hello, 

Thanks for your answer. 

&gt; If the top 6 documents all have the same score and the 7th is 0.0001 less, is that sufficient to truncate the result list
&gt; Yes, the idea was to select exactly the hits which have the exact score reported by the max_score property on hits.

A pointer on a plugin which interfaces/extends the same modules I would need to extend would be really nice (or to the relevant classes/module in the 1.7 code base).
I have a very shallow knowledge of the ES code base and have absolutely no idea where to start (apart from setting up the maven project with the couple property files for a plugin) 

Google hasn't been very forthcoming with such a pointer : lots of stuff around adding an analyzer, creating a REST endpoint or extending the infrastructure but not so much seems to extend the search result processing 

thanks
</comment><comment author="jeantil" created="2015-07-24T10:15:35Z" id="124463159">I tried to have a look at the code base: 
I would need to extend the search context to add my custom property but it sounds like I would then have to reimplement all three of DefaultSearchContext, FilteredSearchContext and PercolateContext
once I have the property at hand in the context, I would need to create a custom QueryPhase override its execute method call the super then manipulate the  topdocs in the queryResult to limit the score docs to only the docs with the max score. 

is that viable ?

thanks
</comment><comment author="clintongormley" created="2015-07-27T11:05:40Z" id="125167032">Hi @jeantil 

I suggest asking about this in the forum: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better logging on unrecoverable shard allocation failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12412</link><project id="" key="" /><description>Allocation a shard is subject to some pretty complex logic. It's easy to get into a unresolvable state where shards cannot be allocated. In this case, it would be great to have `ERROR` level log messages showing up so I know what the heck is going on as an operator.

Here's a scenario I ran into tonight.

&lt;WARNING - LONG BORING STORY AHEAD. SKIP TO THE BOTTOM FOR TL;DR&gt;

---

I'm upgrading a cluster with 3 master nodes and 6 data nodes which lives in AWS from 1.6.0 to 1.6.1. We've configured our cluster with `cluster.routing.allocation.awareness.attributes` set to `aws_availability_zone`. We use Ansible to orchestrate rollouts. Ansible started going through each node, disabling cluster allocation + applying changes / upgrading ES + bringing ES back up and waiting for cluster state to go back to green before moving on to the next node.

Everything was going great, until after we bounced the third data node and realised that the cluster was refusing to go back into a green state. Investigation revealed that a shard was stuck in UNASSIGNED. No clue why, no logs, no pending recoveries / relocations. Just stuck in unassigned.

So, I tried to relocate the shard manually using the `_reroute` api:

```
{
    "commands" : [ 
        {
          "allocate" : {
              "index" : "&lt;index with a stuck shard&gt;", "shard" : 0, "node" : "i-0ab4dfc1"
          }
        }
    ]
}
```

... except that request was being denied. Why? Because at this point I had upgraded 3 of my data nodes to 1.6.1, but _all_ of them were in us-west-1a. The other 3 data nodes in us-west-1c were 1.6.0. Turns out ES will refuse to relocate a shard to a node which is an older version (which makes perfect sense). So, the end result is the shard couldn't go anywhere in us-west-1a (because it's the same availability zone), but it also couldn't go anywhere in us-west-1c (because no nodes in that AZ were 1.6.1 yet).

I resolved the issue by setting a transient cluster setting to disable `cluster.routing.allocation.awareness.attributes`, completed the rollout, then reenabled that config.

---

Anyway, long story is long, just wanted to paint a picture of how easy it is to create a rather complex situation that would have been significantly easier to debug if I had simply seen a log message from ES saying "hey, I can't relocate this replica because _blah_" with the same helpful explanation output that `_reroute` gives you on bad relocation requests.
</description><key id="96761676">12412</key><summary>Better logging on unrecoverable shard allocation failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-23T08:34:08Z</created><updated>2016-09-27T15:36:10Z</updated><resolved>2016-09-27T15:36:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T12:21:04Z" id="124079226">Hi @samcday 

I think the `explain` parameter on the reroute command will help you here:

&gt; If the explain parameter is specified, a detailed explanation of why the commands could or could not be executed is returned.

That said, i think it'd be a nice improvement to just say "assign this shard" without having to specify a destination, and then be able to get the explanations of why assignment failed.

Logging is problematic because routing decisions are made all the time, and so can produce copious logs.
</comment><comment author="samcday" created="2015-07-27T05:26:42Z" id="125090115">&gt; Routing decisions are made all the time

Yeah, I see your point. I just wonder if it would be possible to maybe crank up the logging for a short period after mutative changes have happened? Things like cluster nodes leaving / joining, or cluster allocation being completely disabled for a brief period. I'm sure such a request is terrifying and subject to a dizzying number of race conditions and nightmares attributed to distributed consensus, but I figure I'd ask anyway :P`
</comment><comment author="dakrone" created="2016-09-27T15:36:10Z" id="249902923">This is resolved with the cluster allocation explain API added in https://github.com/elastic/elasticsearch/pull/17305 (available in 5.0+)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms agg: calculate aggs on 'other' bucket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12411</link><project id="" key="" /><description>The terms aggregation now provides an 'other' bucket with a count, I'd like to see the same aggregations performed on the 'other' bucket. Eg if I'm doing a stats aggregation I have stats (sum) for docs with term `foo` and `bar` but not for docs where the field is missing or has a `null` value. 

This is _really_ important for analytics type services as all the values must add up to 100% of the total.

There's quite a bit of discussion about it in #5324.
</description><key id="96761619">12411</key><summary>Terms agg: calculate aggs on 'other' bucket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">j0hnsmith</reporter><labels><label>:Aggregations</label><label>stalled</label></labels><created>2015-07-23T08:33:46Z</created><updated>2016-05-25T13:15:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T12:11:07Z" id="124077832">@j0hnsmith calculating sub-aggs on an other bucket requires two round trips. The first calculates the top-ten terms (plus their sub-aggs).  The second calculates the sub-aggs on everything except the top-ten terms.

To support this in Elasticsearch, we'd need to implement #12316 first.  However this is something you can do yourself today.
</comment><comment author="j0hnsmith" created="2015-07-24T11:14:27Z" id="124478771">I know there are workarounds, but with every level of sub aggregation they get progressively more complex, this could simplify some very complex queries.
</comment><comment author="vivekmoosani" created="2016-01-14T06:04:09Z" id="171540018">+1
</comment><comment author="PaulGrandperrin" created="2016-01-15T18:03:26Z" id="172037390">+1
</comment><comment author="powermik" created="2016-02-19T12:39:42Z" id="186199960">+1
</comment><comment author="dynomeat" created="2016-03-10T01:49:48Z" id="194614999">+1
</comment><comment author="EdwardKaravakis" created="2016-05-25T13:15:02Z" id="221572084">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filtering on a deeper nested object within a nested aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12410</link><project id="" key="" /><description>There might be some technical reason why this can't work.
But I feel like this is a bug.  For example:

```

{
   "aggs": {
      "nested_aggregation": {
         "nested": {
            "path": "Applications"
         },
         "aggs": {
            "deeper_nested_filter": {
               "filter": {
                   "nested": {
                      "path": "Applications.CheckItems",
                      "filter": {
                          "term": {
                             "Applications.CheckItems.CheckType": "somechecktype"
                          }
                      }
                   }
               }
            }
         }
      }
   }
}

```

I realise I could do another nested aggregation onto checkitems, filter on check type and reverse nest back up.  That works for this simple example.

What I actually want to do is filters where an application only has certain check types, and the only way to do this is from the context of the application.
</description><key id="96759613">12410</key><summary>Filtering on a deeper nested object within a nested aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hisuwh</reporter><labels><label>:Nested Docs</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-23T08:21:21Z</created><updated>2016-07-26T07:21:33Z</updated><resolved>2016-07-26T07:21:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T12:06:19Z" id="124076867">I tend to agree with you.  @martijnvg what are your thoughts?
</comment><comment author="martijnvg" created="2015-07-23T12:13:10Z" id="124078114">@hisuwh @clintongormley When developing  `nested` / `reverse_nested` agg my thoughts around nesting is that aggs should be used for this and not filters, it is aggs not query... but I've seen this usage before and I think we should change it, it makes the aggs itself easier.

There is an technical issue that needs to be fixed first and then these kinds of aggs should work:
https://github.com/elastic/elasticsearch/issues/11749#issuecomment-114034328
</comment><comment author="hisuwh" created="2015-08-18T13:07:30Z" id="132200048">Any progress on this? Just ran up against this again.
Main problem is that I'm using the filters aggregation to create multiple filters.

The only other way to do this is to have multiple aggregatoins but this is going to get a lot more complicated.
</comment><comment author="hisuwh" created="2015-08-18T13:19:00Z" id="132202329">What I have had to (which I personally think is horrible)

```

{
   "aggs": {
      "applications": {
         "nested": {
            "path": "Applications"
         },
         "aggs": {
            "another_aggregation": {
               "filter": {
                   .... some other filter.....
               },
               "aggs": {
                  "checkitems": {
                     "nested": {
                        "path": "Applications.CheckItems"
                     },
                     "aggs": {
                        "adverseFinancial": {
                           "filter": {
                              "term": {
                                 "Applications.CheckItems.CheckTypeHierarchy": "adversefinancialcheck"
                              }
                           },
                           "aggs": {
                              "backToApplications": {
                                 "reverse_nested": {
                                    "path": "Applications"
                                 },
                                 "aggs": {
                                    ...... my aggregation......
                                 }
                              }
                           }
                        }
                     }
                  }
               }
            }
         }
      }
   }
}

```

Rather than just:

```

{
   "aggs": {
      "nested_aggregation": {
         "nested": {
            "path": "Applications"
         },
         "aggs": {
            "deeper_nested_filter": {
               "filter": {
                   "nested": {
                      "path": "Applications.CheckItems",
                      "filter": {
                         "and" : {
                             "filters" : [
                                 {
                                      "term": {
                                           "Applications.CheckItems.CheckType": "somechecktype"
                                      }
                                 },
                                 {
                                      .....some other filter ......
                                 }
                             ]
                         }
                      }
                   }
               },
               "aggs" : {
                    ...... my aggregation.....
               }
            }
         }
      }
   }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java</file></files><comments><comment>aggs: Changed how `nested` and `reverse_nested` aggs know about their nested depth level.</comment></comments></commit></commits></item><item><title>ThrottlingAllocationDecider should not counting relocating shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12409</link><project id="" key="" /><description>The ThrottlingAllocationDecider is responsible to limit the number of incoming/local recoveries on a node. It therefore shouldn't count shards marked as relocating which represent the source of the recovery.
</description><key id="96755828">12409</key><summary>ThrottlingAllocationDecider should not counting relocating shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T08:01:36Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-24T14:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-23T16:39:44Z" id="124163017">@bleskes can you update the Javadoc for this decider then? It currently says it limits the number of recovery operations (incoming and outgoing) but this change would make it only check incoming.

Also, is this causing an issue? I'm curious if we want the behavior changed.
</comment><comment author="bleskes" created="2015-07-24T09:47:52Z" id="124458561">@dakrone I'm not sure I follow your comment (I don't see the incoming and outgoing part). The problem is that this supposed to limit the amount of shards intializing on a node. This can happen in two ways - a primary assignment (Recover from gateway), a replica assignment and relocation to the node. The later ends up copying the data from the current primary.  

At the moment I don't think the intention of this class is limit out going recoveries (we do see using `INDICES_RECOVERY_CONCURRENT_STREAMS` currently, in a totally different way). If we want to also count outgoing recoveries, the problem with the current relocation check is that it doesn't necessarily mark the source node. Replicas the relocate will still recovery from the primary. We should then count any initializing replica against the node holding the primary.  

In both cases the current code is wrong. I opted to make it just count incoming recoveries which I think is the current model. We can make a bigger change later.
</comment><comment author="dakrone" created="2015-07-24T14:16:00Z" id="124537462">@bleskes I was thinking the current javadoc should be changed from:

```
  * &lt;li&gt;&lt;tt&gt;cluster.routing.allocation.node_concurrent_recoveries&lt;/tt&gt; -
  * restricts the number of concurrent recovery operations on a single node. The
  * default is &lt;tt&gt;2&lt;/tt&gt;&lt;/li&gt;
```

To something like:

```
  * &lt;li&gt;&lt;tt&gt;cluster.routing.allocation.node_concurrent_recoveries&lt;/tt&gt; -
  * restricts the number of shards initializing from recovery on a single node. The
  * default is &lt;tt&gt;2&lt;/tt&gt;&lt;/li&gt;
```

to make it clearer which shard state this is checking. Other than that, the code part LGTM :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file></files><comments><comment>Allocation: ThrottlingAllocationDecider should not counting relocating shards</comment></comments></commit></commits></item><item><title>Adapt pluginmanager to the new world</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12408</link><project id="" key="" /><description>Now that plugins have some structure / e.g. required descriptor file, make pluginmanager strict and prevent mistakes from screwing up the installation. 

Today it is still lenient and will basically accept any .zip or .jar and who knows what will happen. There is no sense in leniency, PluginService is going to reject any bullshit on startup.
- Read/validate descriptor file, remove heuristics around _site or not
- Look for descriptor file to find the plugin "root" in a simpler/safer way, when plugin-a.zip expands to plugin-a/ and we have to deal with that.
- Proper jar hell check for non-isolated plugins: we check the candidate against any other non-isolated plugins already installed.
- Fix tests to not use embedded zip files, instead create them on the fly for testing.
</description><key id="96722882">12408</key><summary>Adapt pluginmanager to the new world</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T03:58:09Z</created><updated>2015-12-04T09:32:59Z</updated><resolved>2015-07-23T11:16:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-07-23T10:52:36Z" id="124056409">left a few minor comments about exception messages. other than that LGTM
</comment><comment author="clintongormley" created="2015-07-24T12:38:47Z" id="124507020">Contacting community plugin authors: 

@sscarduzio, @lukas-vlcek, @mobz, @jprante, @royrusso, @andrewvc, @lmenezes, @karmi, @grantr, @shikhar, @grmblfrz, @yakaz, @synhershko, @medcl, @chytreg, @imotov, @duydo, @carrot2, @wikimedia, @kzwang, @YannBrrd, @NLPchina, @codelibs

Hi all 

Just to let you know about a breaking change coming to plugins in 2.0.  Each plugin will require a top-level `plugin-descriptor.properties` which declares metadata about the plugin.  The details can be found here:

https://github.com/elastic/elasticsearch/blob/master/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties

Also, site plugins will need to have a `_site` directory.  We no longer move anything that isn't a jar into an auto-created `_site` dir.
</comment><comment author="clintongormley" created="2015-07-24T12:42:59Z" id="124507610">@sscarduzio, @lukas-vlcek, @mobz, @jprante, @royrusso, @andrewvc, @lmenezes, @karmi, @grantr, @shikhar, @grmblfrz, @yakaz, @synhershko, @medcl, @chytreg, @imotov, @duydo, @carrot2, @wikimedia, @kzwang, @YannBrrd, @NLPchina, @codelibs

Sorry: updated link (this is still in the process of being merged, but wanted to make sure you were made aware of it as soon as possible):

https://github.com/rmuir/elasticsearch/blob/plugin_java_version/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
</comment><comment author="synhershko" created="2015-07-24T12:44:57Z" id="124508098">Ack, thanks Clinton
</comment><comment author="jprante" created="2015-07-24T12:50:36Z" id="124509677">I saw this yesterday :) https://twitter.com/xbib/status/624236671551283201

Just a side note, for testing custom jvm plugins, it is also important to add the plugin class name to the key `plugin.types` to the node settings to get it loaded, because auto-loading from classpath has been disabled. Like this:

```
settingsBuilder()
                .put("cluster.name", cluster)
                .put("path.home", getHome())
                .put("plugin.types", MyCustomPlugin.class.getName())
                .build();
```

Will a blog post follow to instruct the new plugin authoring?
</comment><comment author="clintongormley" created="2015-07-24T13:34:28Z" id="124521647">thanks @jprante - i'm in the process of redoing the plugin docs for 2.0 (see https://github.com/elastic/elasticsearch/pull/12040)  and I'll update this PR to include documentation.

(btw, why are none of your plugins listed? :) )
</comment><comment author="jprante" created="2015-07-24T19:11:26Z" id="124645868">@clintongormley I'm not sure, maybe they are not important, maybe I'm too lazy to work through the stuff and submit pull requests. I can't even properly document my plugins for myself or write blog posts :( I know it's really a shame.
</comment><comment author="lmenezes" created="2015-07-25T21:02:19Z" id="124893533">:+1: 
</comment><comment author="YannBrrd" created="2015-08-22T13:19:03Z" id="133702892">Ok. I'll release a new version asap. :-)

Le sam. 25 juil. 2015 à 23:03, leonardo menezes notifications@github.com
a écrit :

&gt; [image: :+1:]
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12408#issuecomment-124893533
&gt; .
&gt; 
&gt; ## 
&gt; 
&gt; Cordialement,
&gt; Yann Barraud
</comment><comment author="duydo" created="2015-08-27T03:16:16Z" id="135271032">Well noted. Thank @clintongormley for your information :-)
</comment><comment author="YannBrrd" created="2015-09-13T09:33:49Z" id="139856011">@clintongormley @rmuir @imotov where are the Cache, CacheBuilder &amp; ElasticsearchIllegalArgumentException gone ? 
How do I manage cached configuration now ? 
</comment><comment author="jprante" created="2015-09-13T10:34:00Z" id="139858709">@YannBrrd Cache/CacheBuilder classes are part of Guava, and you can use the dependency package of Guava, `com.google.common.cache`. Instead of ElasticsearchIllegalArgumentException you can either use `java.lang.IllegalArgumentException` or use the checks in `com.google.common.base.Preconditions`
</comment><comment author="YannBrrd" created="2015-09-13T11:26:32Z" id="139862150">Nice thanks !

Le dim. 13 sept. 2015 12:34, Jörg Prante notifications@github.com a
écrit :

&gt; @YannBrrd https://github.com/YannBrrd Cache/CacheBuilder classes are
&gt; part of Guava, and you can use the dependency package of Guava,
&gt; com.google.common.cache. Instead of ElasticsearchIllegalArgumentException
&gt; you can either use java.lang.IllegalArgumentException or use the checks
&gt; in com.google.common.base.Preconditions
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12408#issuecomment-139858709
&gt; .
&gt; 
&gt; ## 
&gt; 
&gt; Cordialement,
&gt; Yann Barraud
</comment><comment author="YannBrrd" created="2015-09-14T14:41:18Z" id="140102202">\o/
https://github.com/YannBrrd/elasticsearch-entity-resolution/tree/2.0
</comment><comment author="sscarduzio" created="2015-11-10T15:14:34Z" id="155447701">Hi guys, Readonly REST plugin is updated as well. :+1: 

https://github.com/sscarduzio/elasticsearch-readonlyrest-plugin
</comment><comment author="YannBrrd" created="2015-12-04T09:27:49Z" id="161918392">@jprante how do I fix the jar hell after adding guava as a dependency ? Integrations tests failing... /o\

Edit : My bad, was fixed as per ES issue
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginInfo.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file></files><comments><comment>Merge pull request #12408 from rmuir/adapt_pluginmanager</comment></comments></commit></commits></item><item><title>Specifying an analyzer for the _all field breaks mapping updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12407</link><project id="" key="" /><description>Specifying an analyzer for the _all field breaks mapping updates.

```
curl -X DELETE http://localhost:29200/twitter
# {"acknowledged":true}

curl -X PUT http://localhost:29200/twitter -d '
{
  "mappings": {
    "tweet": {
      "_all": {
        "analyzer": "snowball"
      },
      "properties": {
        "tweet": {
          "type": "string",
          "analyzer": "english"
        }
      }
    }
  }
}
# {"acknowledged":true}

curl -X PUT http://localhost:29200/twitter/_mapping/tweet -d '
{
  "properties": {
    "tag": {
      "type": "string",
      "index": "not_analyzed"
    }
  }
}
'
# {"error":"MergeMappingException[Merge failed with failures {[mapper [_all] has different index_analyzer]}]","status":400}
```

If no analyzer is specified for the _all field when creating the index, the mapping update call succeeds.

I've tested the above against Elasticsearch 1.7.0.

Thanks!

Perry
</description><key id="96701979">12407</key><summary>Specifying an analyzer for the _all field breaks mapping updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lperry</reporter><labels /><created>2015-07-23T01:08:29Z</created><updated>2015-07-27T18:21:35Z</updated><resolved>2015-07-23T10:59:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lperry" created="2015-07-23T01:15:27Z" id="123924430">It looks like specifying a new field with the same analyzer as the _all field also returns an error:

```
curl -X PUT http://localhost:29200/twitter/_mapping/tweet -d '
{
  "properties": {
    "tag": {
      "type": "string",
      "analyzer": "snowball"
    }
  }
}
'
# {"error":"MergeMappingException[Merge failed with failures {[mapper [_all] has different index_analyzer]}]","status":400}
```
</comment><comment author="clintongormley" created="2015-07-23T10:59:40Z" id="124057988">Hi @lperry 

You can't change an analyzer after the field has been created.  It would invalidate your existing data.
</comment><comment author="lperry" created="2015-07-23T15:21:11Z" id="124138105">Hi @clintongormley,

I'm not trying to change an existing field's analyzer. I'm trying to add a new field.

The provided curl commands first create an index with an analyzer specified for the _all field and a field tweet, and then tries to add a new tag field.

Thanks,

Perry
</comment><comment author="clintongormley" created="2015-07-27T10:47:07Z" id="125163227">Apologies @lperry - I misread the issue.  You are correct.  This has already been fixed in 2.0.

For now you can work around it by respecifying the `_all` mapping:

```
PUT /twitter/_mapping/tweet
{
  "_all": {
    "analyzer": "snowball"
  },
  "properties": {
    "tag": {
      "type": "string",
      "index": "not_analyzed"
    }
  }
}
```
</comment><comment author="lperry" created="2015-07-27T18:21:35Z" id="125294858">Thanks for the workaround and fix!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Check for incompatible mappings while upgrading old indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12406</link><project id="" key="" /><description>Conflicting mappings that were allowed before v2.0 can cause runaway shard failures on upgrade. This commit adds a check that prevents a cluster from starting if it contains such indices as well as restoring such indices from a snapshot into already running cluster.

Closes #11857
</description><key id="96694757">12406</key><summary>Check for incompatible mappings while upgrading old indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Recovery</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-23T00:04:12Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-08-05T00:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-23T08:36:01Z" id="124022018">@imotov correct me if I'm wrong, but since we now check the mapping in the constructor of GatewayMetaState, if it runs into a mapping that's incompatible with 2.0 it will throw an exception and prevent the node from starting... 
</comment><comment author="imotov" created="2015-07-23T13:09:48Z" id="124091538">@bleskes you are right and this is the behavior described in the commit comment. It is consistent with a node behavior in case of other detected incompatibilities. We had a discussion about this and came to the conclusion that this is the best user experience we can provide. A user trying to upgrade a cluster with incompatible indices will not be able to start a node and since we haven't updated any metadata or data files at this point yet, they still will be able to downgrade to the previous version and fix the issue before trying to upgrade again. An alternative would be to start the cluster with potentially all indices closed and no way back. 
</comment><comment author="clintongormley" created="2015-07-27T09:21:49Z" id="125140170">@imotov i like this approach
</comment><comment author="jpountz" created="2015-08-03T13:48:11Z" id="127240633">@imotov I like the behaviour. I guess something that I'm missing from the PR is an upgrade test where there are two types that have a common field which has the same mappings, to make sure we don't reject that case?
</comment><comment author="imotov" created="2015-08-03T14:02:23Z" id="127249774">@jpountz because I couldn't create conflicting mapping in 2.0, I had to generate an index with a conflicting mapping using [this script](https://github.com/elastic/elasticsearch/pull/12406/files#diff-e51a08a43dde51ab5c488e98ac24b538R1) and then test it as part [UpgradeReallyOldIndexTest](https://github.com/elastic/elasticsearch/pull/12406/files#diff-0c8235e2974b7061d9f8e9476b11688cR52). 
</comment><comment author="imotov" created="2015-08-03T15:51:38Z" id="127285185">@jpountz I pushed an update. 
</comment><comment author="jpountz" created="2015-08-03T16:12:02Z" id="127292902">This looks good to me in general but I would appreciate if someone else could give it a look to make sure I didn't overlook anything.
</comment><comment author="rjernst" created="2015-08-04T14:28:06Z" id="127631750">This looks good. I left a handful of minor comments/questions.
</comment><comment author="imotov" created="2015-08-04T20:36:13Z" id="127753468">@rjernst I pushed an update
</comment><comment author="rjernst" created="2015-08-04T21:32:27Z" id="127768583">LGTM, just one minor comment about naming.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Term indexed as stemmed version but not available when searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12405</link><project id="" key="" /><description>I have term `accused` where during indexing I apply a `porter stemmer` which converts it to `accus`. But when I search this term I get zero hits! I know in which document `accused` occurs so I request a `term_vector` of that document and in that `term_vector` I had an entry of `accus`!!

Why is this happening?
</description><key id="96684383">12405</key><summary>Term indexed as stemmed version but not available when searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apanimesh061</reporter><labels /><created>2015-07-22T22:50:00Z</created><updated>2015-07-23T10:58:17Z</updated><resolved>2015-07-23T10:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T10:58:17Z" id="124057524">Hi @apanimesh061 

You haven't provided any JSON so it is quite difficult to guess what you are doing.  This sounds like user error.  I suggest asking the question (with supporting JSON) on the forum: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Installation from package repository on Debian fails to pickup limits config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12404</link><project id="" key="" /><description>Hi,
I've ran into an issue with ES not picking up limits configuration when installing using apt-get.

System descrtiption:
Debian8 with openJDK:

```
java version "1.7.0_79"
OpenJDK Runtime Environment (IcedTea 2.5.5) (7u79-2.5.5-1~deb8u1)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)
```

I've configured /etc/deafult/elasticsearch to support mlock.all and increased the nofile for the ES user, but even doing so, when cURL-ed `es:9200/_nodes/process?pretty`, I got mlock.all false and max_file_descriptors=65K (where I set it to 262144).

After digging throughout the system and with the community, I scanned again `/etc/default/elasticsearch` and found a pointer to `/usr/lib/systemd/system/elasticsearch.service`.

After modifying `LimitNOFILE=262144` and `LimitMEMLOCK=infinity` and restarting ES, mlock.all was true and max_file_descriptors was set to 262144.

Is this the way it intended to be used? if so, should be documented more explicitly on ES site.
If not, what should be done (ontop of [this](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/setup-service.html)) in order to enable mlock.all and increase the max_file_descriptors ?

I'll be happy to provide additional info if needed.

Thanks,
Yarden
</description><key id="96655419">12404</key><summary>Installation from package repository on Debian fails to pickup limits config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ayashjorden</reporter><labels><label>:Packaging</label><label>adoptme</label><label>docs</label><label>enhancement</label></labels><created>2015-07-22T20:21:42Z</created><updated>2016-04-29T11:15:32Z</updated><resolved>2016-04-29T11:05:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T10:52:39Z" id="124056420">Yeah I think you're not the first person to run into this issue.  We should definitely document it more explicitly.  I wonder if we should actually change the comments in `config/elasticsearch.yml` in the RPM/deb to point users to the right file.
</comment><comment author="ayashjorden" created="2015-07-23T12:04:09Z" id="124076559">Hi Clinton,
I'm not the systemd expert, but if not to much of a bother for you, can you elaborate about the relationship between _/etc/elasticsearch/elasticsearch.yml_ &lt;=&gt; _/etc/default/elasticsearch_ &lt;=&gt; _/usr/lib/systemd/system/elasticsearch.service_, that would be awesome.

The thing that I'm missing is whether should I've edit only _elasticsearch.service_ or _/etc/default/elasticsearch_ or both?

Thanks,
Yarden
</comment><comment author="clintongormley" created="2015-07-23T12:55:56Z" id="124086475">the settings in elasticsearch.service are passed as custom settings (not sure if in an env variable or on the command line) but they override what is set in the config file
</comment><comment author="ayashjorden" created="2015-07-23T13:10:37Z" id="124091955">Thanks
</comment><comment author="tlrx" created="2015-07-23T13:20:56Z" id="124096328">&gt; I'm not the systemd expert, but if not to much of a bother for you, can you elaborate about the relationship between /etc/elasticsearch/elasticsearch.yml &lt;=&gt; /etc/default/elasticsearch &lt;=&gt; /usr/lib/systemd/system/elasticsearch.service, that would be awesome.

Systemd uses the file `/usr/lib/systemd/system/elasticsearch.service` to manage the elasticsearch service. Every custom setting like `LimitNOFILE=262144` must be defined directly in this file.

The executable to launch as a service by Systemd is indicated in the `ExecStart` setting. This setting is special compared to other settings because it is one of those that accept environment variables (ex: `-Des.default.config=$CONF_FILE`). We cannot do things like `LimitNOFILE=$BLA`.

The environment variables that can be used when starting/stopping the service are defined which `Environment` settings. We defined default values in the `elasticsearch.service` file but it is possible to override the default values in the file '/etc/default/elasticsearch' (that's the role of the `EnvironmentFile' setting).

The environment variables indicate useful paths to elasticsearch, like where to load the the configuration file `elasticsearch.yml`.

Finally, Systemd uses /usr/lib/systemd/system/elasticsearch.service
to start the service, then potentially overrides some paths using /etc/default/elasticsearch, then start elasticsearch which loads /etc/elasticsearch/elasticsearch.yml.
</comment><comment author="ayashjorden" created="2015-07-23T13:33:11Z" id="124101347">@tlrx , thanks for the detailed explanation.
So, my question now is: why the configuration in `/etc/default/elasticsearch` didn't kick in when started ES service on my hosts? **only** when I modified `/usr/lib/systemd/system/elasticsearch.service` file, the settings in it worked.
</comment><comment author="clintongormley" created="2015-07-24T09:25:29Z" id="124454564">We should:
- update docs to explain process
- add comment to config file pointing to elasticsearch.service, etc
- add comments above config elements overwritten by other files, pointing to those files
</comment><comment author="abraxxa" created="2016-04-28T10:22:33Z" id="215379047">This is **really** annoying as you have to edit /usr/lib/systemd/system/elasticsearch.service on every `apt update` to uncomment LimitMEMLOCK=infinity which makes automating cluster updates hard because a manual edit is required!
Furthermore the last line `# Built for Distribution: Deb-2.3.2 (deb)` also changes with every version and is displayed in the diff.
I'm wondering why `LimitMEMLOCK=infinity` isn't the default which would also circumvent the problem.
Having two different debian packages, one for systemd and one for SysV might be helpful as well.
</comment><comment author="jasontedor" created="2016-04-28T11:42:47Z" id="215398688">@abraxxa You do not have to edit `elasticsearch.service` on every update. Instead, create a directory `/etc/systemd/system/elasticsearch.service.d/memlock.conf` with the contents

```
[Service]
LimitMEMLOCK=infinity
```

and set the correct permissions. These are called drop-in units and `man systemd.unit` covers them (in particular, you can place this drop-in unit in a few other locations, but `/etc/systemd/system` is preferred). With this above solution, you no longer need to edit the unit file.

&gt; I'm wondering why `LimitMEMLOCK=infinity` isn't the default which would also circumvent the problem.

Because locking memory requires special permissions and should be a deliberate choice made by system operators.
</comment><comment author="abraxxa" created="2016-04-28T13:04:48Z" id="215416679">@jasontedor thanks for the great info! Can this be added to the package docs?
The Debian 8 manpage for systemd.unit seems to be too old to contain any infos about drop-in unit but the online version at https://www.freedesktop.org/software/systemd/man/systemd.unit.html does.
</comment><comment author="clintongormley" created="2016-04-29T11:05:43Z" id="215685119">@abraxxa this has been added to the documentation here: https://www.elastic.co/guide/en/elasticsearch/reference/master/setting-system-settings.html#systemd
</comment><comment author="abraxxa" created="2016-04-29T11:15:32Z" id="215686622">Thanks Clinton!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consult field info before fetching field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12403</link><project id="" key="" /><description>If a field doesn't exist in field info then always return a non cached empty atomic reader.
</description><key id="96652124">12403</key><summary>Consult field info before fetching field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-22T20:03:28Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-23T12:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-22T20:42:28Z" id="123855809">looks good to me
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ImmutableSettings.classLoader() is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12402</link><project id="" key="" /><description>During nodeBuilder.node() on init of InternalNode passed to settings classloader is ignored.
It happens in InternalNode (line 136, elasticsearch 1.7.0) -&gt; InternalSettingsPreparer(line: 153).

I create Settings instance and set classloader to the instance.
The settings instance I set to node builder and execute nodebuilder.node().
After that in constructor of TransportService I see settings instance without my classloader.

Does workarround exists for this problem?
</description><key id="96629364">12402</key><summary>ImmutableSettings.classLoader() is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lytvynenkoinvest</reporter><labels /><created>2015-07-22T18:04:14Z</created><updated>2015-07-23T12:38:54Z</updated><resolved>2015-07-23T12:38:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lytvynenkoinvest" created="2015-07-23T10:09:49Z" id="124042849">Cluster contains 2 nodes.
All ES and lucene classes are loaded by classloader that loaded my main class(where I create node builder) and this classloader I set into immutable settings.
But because of ThreadContext classloader is used I received next Exception:

Caused by: org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:178)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:130)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)[:1.7.0_75]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)[:1.7.0_75]
    at java.lang.Thread.run(Thread.java:745)[:1.7.0_75]
Caused by: java.io.InvalidClassException: failed to read class descriptor
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1603)[:1.7.0_75]
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)[:1.7.0_75]
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)[:1.7.0_75]
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)[:1.7.0_75]
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)[:1.7.0_75]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:176)
    ... 23 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.transport.RemoteTransportException
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)[:1.7.0_75]
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)[:1.7.0_75]
    at java.security.AccessController.doPrivileged(Native Method)[:1.7.0_75]
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)[:1.7.0_75]
    at com.shared.classloader.MyClassLoader.findClass(XESClassLoader.java:195)[etools-commons-cl.jar:8.6.0.0-SNAPSHOT.5007]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)[:1.7.0_75]
    at com.shared.classloader.MyClassLoader.loadClass(XESClassLoader.java:183)[etools-commons-cl.jar:8.6.0.0-SNAPSHOT.5007]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)[:1.7.0_75]
    at org.elasticsearch.common.io.ThrowableObjectInputStream.loadClass(ThrowableObjectInputStream.java:99)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.readClassDescriptor(ThrowableObjectInputStream.java:73)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1601)[:1.7.0_75]
    ... 28 more
</comment><comment author="clintongormley" created="2015-07-23T12:38:54Z" id="124082625">Closing in favour of https://github.com/elastic/elasticsearch/issues/12340
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify Replica Allocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12401</link><project id="" key="" /><description>Simplify the codebase of replica allocator and add more unit tests for it
</description><key id="96612957">12401</key><summary>Simplify Replica Allocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-22T16:43:20Z</created><updated>2015-08-07T10:07:06Z</updated><resolved>2015-07-23T05:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-22T17:19:13Z" id="123796805">LGTM, I like the encapsulation, it's much easier to read! Left one question (not blocking).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make fetch sub phases pluggable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12400</link><project id="" key="" /><description>This is wip, only opening this pull request so that it is easier to discuss.
I made the fetch sub phases pluggable to see if this adds too much complexity. With this change new or existing fetch sub phases such as fielddata fields or script fields can be plugged in.
As a proof of concept I changed fielddata fields to use the plugin mechanism and added an example for 
a plugged in fetch phase in the test.
While this adds a little more guice stuff, it also removes the need for implementing methods like `hasFieldDataFields()` and `fieldDataFields()` in every subclass of SearchContext. 
In addition we could easily add features like  #10729 and also move features like fielddatafields from core to a plugin immediately. Overall my feeling is that it would be a win to make sub phases pluggable.

It is work in progress because converting inner hits to use the plugin mechanism will be a little more 
tricky. Also not all fetch phases are structured the same so converting all sub phases might need more work as well.
</description><key id="96604393">12400</key><summary>Make fetch sub phases pluggable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>discuss</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-22T16:07:08Z</created><updated>2015-08-03T13:21:53Z</updated><resolved>2015-08-03T13:19:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-07-22T16:20:43Z" id="123778462">@clintongormley could you take a look and let me know what you think?
</comment><comment author="jpountz" created="2015-07-22T17:06:09Z" id="123793661">I haven't looked at the pull request yet, but for more context: I was first on the fence about adding more entry points to plugins, until Britta brought up the selling point that it would allow to move some current fetch sub phases, like fielddata fields, to plugins, which I like as it would help keep the core small and focused on major features (see #10368).
</comment><comment author="martijnvg" created="2015-07-22T17:18:54Z" id="123796742">+1 to make sub fetch phases pluggable. If sub fetch phase implementations are better isolated, then this will automatically improve the design and code of these sub fetch phase implementations.
</comment><comment author="jpountz" created="2015-07-23T13:27:05Z" id="124097765">It looks good to me overall. Maybe @martijnvg and @dakrone should have a look since they worked on inner hits (which is handled differently in your PR) and fielddata fields.

One thing I'm wondering when looking at the changes is whether we actually need hasFetchSubPhaseContext. It looks to me that we always do something like: `if (hasSubFetchPhase) { execute fetch sub phase }` so maybe we could just have fetch sub phase execution be smart enough to not do anything if it has not been configured? This would help limit the number of methods we need on the SearchContext class.
</comment><comment author="dakrone" created="2015-07-24T18:11:51Z" id="124604734">left some comments, I agree with Adrien's comment about potentially getting rid of `hasFetchSubPhaseContext`, I think it would be a cleaner interface. Other than that I like this!
</comment><comment author="brwe" created="2015-07-27T10:08:33Z" id="125154529">Thanks a lot for the feedback! I removed `hasFetchSubPhaseContext()` now. Because I still need some way to let the fetch sub phase know if it should execute or not I now put this information in the `FetchSubPhaseContext` and the `FetchSubPhaseParseElement` makes sure that this is always set when the sub phase is parsed. Let me know if this makes sense.
I also added documentation - did not mean to make anyone sad :)

To proceed: Do we want to merge this after review cycles are done and then later convert each of the other sub phases in separate pull requests? Or should I do all at once?
</comment><comment author="nik9000" created="2015-07-28T19:44:20Z" id="125733149">&gt; To proceed: Do we want to merge this after review cycles are done and then later convert each of the other sub phases in separate pull requests? Or should I do all at once?

I'm all for doing things incrementally. Less long lived branches the better.
</comment><comment author="martijnvg" created="2015-07-29T08:39:16Z" id="125881744">+1 this looks good. I'm good with inner hits being handled differently than all the other sub fetch phases.
</comment><comment author="brwe" created="2015-07-29T15:47:54Z" id="125995413">Thanks again for the feedback! I did my best to parameterize all places where I had casts before. It works but I am unsure if this is the right way. Can you take another critical look?
</comment><comment author="brwe" created="2015-07-30T15:58:08Z" id="126380294">@nik9000 thanks again for the comments! updated pr and happily awaiting next review round
</comment><comment author="nik9000" created="2015-07-30T18:21:22Z" id="126423961">Ok - I left one comment but it can wait. I feel good about this.
</comment><comment author="jpountz" created="2015-08-03T13:17:13Z" id="127228965">LGTM
</comment><comment author="nik9000" created="2015-08-03T13:18:01Z" id="127229202">LGTM
</comment><comment author="brwe" created="2015-08-03T13:21:53Z" id="127231553">Uh. forgot to squash before merging. sorry...will remember next time.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseModule.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsContext.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java</file><file>core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginTests.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Merge pull request #12400 from brwe/plug-fetch-sub-phases</comment></comments></commit></commits></item><item><title>Add shadow indicator when using shadow replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12399</link><project id="" key="" /><description>Very simple PR that changes:

```
[2015-07-22 09:42:44,088][INFO ][org.elasticsearch.cluster.metadata] [node_t0] [test] creating index, cause [api], templates [], shards [1]/[1], mappings [doc]
```

Into:

```
[2015-07-22 09:42:44,088][INFO ][org.elasticsearch.cluster.metadata] [node_t0] [test] creating index, cause [api], templates [], shards [1]/[1s], mappings [doc]
```

When shadow replicas are used.
</description><key id="96598610">12399</key><summary>Add shadow indicator when using shadow replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-22T15:44:15Z</created><updated>2015-08-07T13:43:07Z</updated><resolved>2015-07-23T15:17:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-07-23T05:46:05Z" id="123979643">LGTM
</comment><comment author="bleskes" created="2015-07-23T07:19:13Z" id="124003145">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>It is unclear how security-related issues have been fixed in 1.6.1 / 1.7.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12398</link><project id="" key="" /><description>From looking a the commit log, I can't figure out how exactly a few vulnerabilities have been fixed in recent versions of Elasticsearch. It would be very helpful if the specific commits that fix such issues were marked as such. I am specifically looking for the commits fixing
- CVE-2015-4165 (File modification)
- CVE-2015-5377 (Remote code execution)
- CVE-2015-5531 (Directory traversal)

The background here is that there are concerns that the 1.0.3-based package shipped as part of Debian 8.x (jessie) may still be vulnerable.
</description><key id="96572203">12398</key><summary>It is unclear how security-related issues have been fixed in 1.6.1 / 1.7.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hillu</reporter><labels /><created>2015-07-22T13:45:11Z</created><updated>2015-11-02T17:25:40Z</updated><resolved>2015-07-23T08:27:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-22T14:21:32Z" id="123738724">@hillu the security page of the website lists all the CVE's for Elasticsearch and which versions are affected: https://www.elastic.co/community/security

This should help you identify which CVEs are relevant to your version. To avoid know security vulnerabilities it is recommended that you stay up to date with the latest version of Elasticsearch. Details of how to add the Elasticsearch debian repository can be found here: https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html
</comment><comment author="hillu" created="2015-07-22T14:29:50Z" id="123741196">@colings86 I am looking for the specific commits that fix the three CVEs so I can find out whether there is any need to fix the 1.0.3-based packages that come as part of Debian/jessie which I help maintain.
</comment><comment author="clintongormley" created="2015-07-23T08:27:47Z" id="124020609">Hi @hillu 

We have discussed this issue internally.  The consensus that we arrived at was that we don't want to make it any easier than absolutely necessary for blackhats to find exploitable code.  Even having whitehats commit patches makes it easy for blackhats.  We want to give our users as must chance to upgrade as possible before these exploits become public.

For this reason we won't publish links to the actual commits.  The affected versions are listed on https://www.elastic.co/community/security

Honestly, we've fixed so many non-security related bugs since 1.0.3 that it would be a mistake for any user to continue to such an old version.

Hope you understand our stance

thanks
</comment><comment author="hillu" created="2015-07-23T11:11:03Z" id="124060057">Sorry, I don't understand -- the point about blackhats having to invest a bit less time to find exploits seems a bit like a red hering to me -- and it is not far from Oracle-style "advisories" ("Undisclosed vulnerability in component $FOO").

I'm afraid it's probably not going to possible to upgrade the Elasticsearch packages shipped with Debian/jessie to 1.6.1 or 1.7.0, so we'd very much like to fix the problems in the 1.0.3 codebase if they exist there.

Couldn't you just tell me via private mail to bengen@debian.org how those CVE-worthy bugs were fixed?

Thanks,
-Hilko
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adapt IndicesClusterStateService to use allocation ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12397</link><project id="" key="" /><description>#12242 introduced a unique id for an assignment of shard to a node. We should use these id's to drive the decisions made by IndicesClusterStateService when processing the new cluster state sent by the master. If the local shard has a different allocation id than the new cluster state, the shard will be removed and a new one will be created. This fixes a couple of subtle bugs,  most notably a node previously got confused if an incoming cluster state had a newly allocated shard in the initializing state and the local copy was started (which can happen if cluster state updates are bulk processed). In that case, the node have previously re-used the local copy instead of initializing a new one.

 Also, as set of utility methods was introduced on ShardRouting to do various types of matching with other shard routings, giving control about what exactly should be matched (same shard id, same allocation id, all but version and shard info etc.). This is useful here, but also prepares the grounds for the change needed in #12387 (making ShardRouting.equals be strict and perform exact equality).
</description><key id="96567272">12397</key><summary>Adapt IndicesClusterStateService to use allocation ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-07-22T13:25:10Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-24T11:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-22T13:25:48Z" id="123718391">@kimchy @imotov  can you have a look? 
</comment><comment author="imotov" created="2015-07-23T16:19:14Z" id="124155671">Looks good to me. I have also rebased  #12387 on this change and it no longer fails.  
</comment><comment author="bleskes" created="2015-07-24T11:15:43Z" id="124479574">thx @dakrone @imotov . pushed to master... 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Core: Adapt IndicesClusterStateService to use allocation ids</comment></comments></commit></commits></item><item><title>Query refactoring: SpanWithinQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12396</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217
</description><key id="96562185">12396</key><summary>Query refactoring: SpanWithinQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-07-22T12:59:31Z</created><updated>2016-03-11T11:51:08Z</updated><resolved>2015-07-22T14:23:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-22T13:05:56Z" id="123713925">left a very minor comment, LGTM otherwise
</comment><comment author="javanna" created="2015-08-28T10:18:50Z" id="135727912">This change is breaking for the java api as it removed setters for mandatory big/little inner span queries. Both arguments have to be supplied at construction time instead and have to be non-null.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12396 from cbuescher/feature/query-refactoring-spanwithin</comment></comments></commit></commits></item><item><title>Cleanup TransportReplicationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12395</link><project id="" key="" /><description>- Split the actual primary operation from the primary phase into a dedicated AsyncPrimaryAction class (similar to AsyncReplicaAction)
- Removed threaded_operation option from replication based transport actions.
- Let the threading be handled by by the transport service and drop forking from new threads from the transport replication action.
</description><key id="96539986">12395</key><summary>Cleanup TransportReplicationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label></labels><created>2015-07-22T11:16:23Z</created><updated>2015-08-27T09:29:51Z</updated><resolved>2015-08-26T13:14:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-23T19:06:21Z" id="124213337">@brwe I've fixed the test and made sure that the newly added `ReplicationRequest#internalShardRouting` is always used in the test.
</comment><comment author="martijnvg" created="2015-08-04T15:28:11Z" id="127649074">I rebased this PR and moved the retry primary handling entirely to the coordinating node. 

Also I included @brwe test from #12574 which passes with this PR. This test simulates a situation where 2 nodes endlessly redirect same index request to each other caused by a number of events described in #12573.

The reason this test is included is because the way primary write requests are redirected has changed in the PR. The coordinating node remains in charge of the entire write operation, even if the primary shard is on a different node than the coordinating node. While before the node that holds the primary shard is always in charge of a write request. In a situation that is described in #12573 this can lead to nodes endlessly redirecting the same write request to each other until the cluster state caught up or something bad happens. With this PR with the #12573 situation, the write request will be retried when a new cluster state comes or the write timeout has been met.
</comment><comment author="brwe" created="2015-08-05T11:57:14Z" id="127972166">Overall I really like this split of primary operation and primary phase. I think this might make it much easier to understand what is happening. I left some comments and questions around testing and where we should retry though. Hope they are not too confusing... 
</comment><comment author="martijnvg" created="2015-08-25T12:06:27Z" id="134565203">@bleskes I brought back the chasing of the primary shard to `PrimaryPhase` inner class and delegate to the primary action if the primary shard is local.
</comment><comment author="martijnvg" created="2015-08-26T13:14:54Z" id="135011282">The tricky part about the PR is that the cluster state is observed twice, one time in the primary phase and one time in the async primary action. In cases we retry, we might miss the update the to the cluster state. This would only be likely to occur more if the primary action was executed remotely which, is what the plan was after as follow up issue. But this can be dangerous. It is fixable if we remember on what version we decided to execute the primary operation, but that would make this change bigger than the plan is and we should maybe to a break from this change and reconsider it post 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>String field mappings and fielddata / doc values settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12394</link><project id="" key="" /><description>This issue addresses a few topics:
- separating `string` fields out into `text` and `keyword` fields (#11901)
- deprecating in-memory fielddata for field-types that support doc-values (to remove in 3.x)
- `fielddata` and `doc_values` settings (#8693) and `norms`
- good out-of-the-box dynamic mappings for string fields
## `string` → `text`/ `keyword`

Today, we use `string` both for full-text and for structured keywords.  We don't support doc-values on `analyzed` string fields, which means that strings which are essentially keywords (but eg need to be lowercased) cannot use doc-values.

**Proposal:**
- deprecate `string` fields
- add `text` fields which support the full analysis chain and don't support doc-values
- add `keywords` fields which support only the `keyword` tokenizer, and have doc-values enabled by default
- change `index` to accept `true` | `false`

Question:  Should `keyword` fields allow token filters that introduce new tokens? 
## Deprecating fielddata for fields that support doc values

In-memory fielddata is limited by the size of the heap, and has been one of the biggest pain-points for users. Doc-values are slightly slower but: (1) don't suffer from the same latency as fielddata, (2) are not limited by heap size, (3) don't impact garbage collection, (4) allow much greater scaling.

All fields that support doc values already have them enabled by default.  

Proposal:
- Deprecate fielddata implementations (except for analyzed string fields) in 2.x 
- Remove them in 3.x.  

The question arises: what happens if the user disables doc values then decides that actually they DO want to aggregate on that field after all?  The answer is the same as if they have set a field to `index:false` - they have to reindex.
## Fielddata and doc values settings 

Today we have these settings:
- `doc_values`: `true`|`false`
- `fielddata.format`: `disabled` | `doc_values` | `paged_bytes` | `array`
- `fielddata.loading`: `lazy` | `eager` | `eager_global_ordinals`
- `fielddata.filters`: `frequency:{}`, `regex:{}`

These become a lot easier to simplify if we deprecate fielddata for all but analyzed string fields. 

**Proposal for fields that support doc values:**
- `doc_values` : `true` (default) | `false`
- `global_ordinals` : `lazy` (default) | `eager`

**Proposal for analyzed string fields:**
- `fielddata`: `disabled` (default) | `lazy` | `eager`
- `global_ordinals` : `lazy` (default) | `eager`
- `fielddata.filters`: `frequency:{}`, `regex:{}`

If, in the future, we can automatically figure out which global ordinals need to be built eagerly, then we can remove the `global_ordinals` setting.
## Norms settings

Similar to the above, we have:
- `norms.enabled` : `true` | `false`
- `norms.loading` : `lazy` | `eager`

In Lucene 5.3, norms are disk based, so the lazy/eager issue is less important (eager in this case would mean force-loading the norms into the file system cache, a decision which we can probably make automatically in the future).

**Proposal:**
- `norms`: `true` | `false`
- only supported on `text` fields
## Good out-of-the-box dynamic mappings for string fields

Today, when we detect a new string field, we add it as an `analyzed` `string`, with `lazy` fielddata loading enabled.  While this allows users to get going with full text search, sorting and aggregations (with limitations, eg `new` + `york`), it's a poor default for heap usage.

**Proposal:**

Add a `text` main field (with fielddata loading disabled) and a `keyword` multi-field by default, ie:

```
{
  "my_string": {
    "type": "text",
    "fields": {
      "keyword": {
        "type": "keyword",
        "ignore_above": 256
      }
    }
  }
}
```

With the default settings these fields would look like this:

```
{
  "my_string": {
    "type":                "text",
    "analyzer":            "default",
    "boost":               1,
    "fielddata":           "disabled",
    "fielddata_filters":   {},
    "ignore_above":        -1,
    "include_in_all":      true,
    "index":               true,
    "index_options":       "positions",
    "norms":               true,
    "null_value":          null,
    "position_offset_gap": 0,
    "search_analyzer":     "default",
    "similarity":          "default",
    "store":               false,
    "term_vector":         "no"
  }
}

{
  "my_string.keyword": {
    "type":                "keyword",
    "analyzer":            "keyword",
    "boost":               1,
    "doc_values":          true,
    "ignore_above":        256,
    "include_in_all":      true,
    "index":               true,
    "index_options":       "docs",
    "null_value":          null,
    "position_offset_gap": 0,
    "search_analyzer":     "keyword",
    "similarity":          "default",
    "store":               false,
    "term_vector":         "no"
  }
}
```
</description><key id="96539450">12394</key><summary>String field mappings and fielddata / doc values settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>discuss</label><label>Meta</label></labels><created>2015-07-22T11:14:18Z</created><updated>2016-10-18T08:50:58Z</updated><resolved>2016-03-30T12:42:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-22T11:27:02Z" id="123677537">+1

One issue with the default dynamic mappings that you propose is that we would fail at indexing large string fields given that the keyword mapping could generate large tokens while Lucene refuses to index tokens that are greater than 32k. So we might want to set `ignore_above` to a reasonable value, eg. 256?
</comment><comment author="clintongormley" created="2015-07-22T11:29:21Z" id="123679221">++  I meant to mention that. I'll update the description and add it
</comment><comment author="rmuir" created="2015-07-22T11:39:03Z" id="123683875">+1
</comment><comment author="javanna" created="2015-07-22T11:39:24Z" id="123684005">+1 !!!
</comment><comment author="gmoskovicz" created="2015-07-22T12:21:17Z" id="123701766">This is a big win. It makes strings much more clear, and should make a faster default behaviour!
</comment><comment author="imotov" created="2015-07-22T14:45:41Z" id="123746095">I am ±0 on this one. I think the change is useful but I feel like it's solving a wrong problem. I think what we really need is a mechanism to define custom types. So, users could define "text" the way they want with reasonable defaults in a single place and then just refer to this type everywhere in a mapping. We can still pre-populate a set of default custom types such as "text" and "keyword" proposed above, but this should be done through the same mechanism that is available to users who should be able to redefine these custom types.
</comment><comment author="colings86" created="2015-07-22T14:47:00Z" id="123746467">+1 to this issue, it will make string settings much easier for users to understand.
</comment><comment author="pickypg" created="2015-07-22T15:09:22Z" id="123753418">Huge +1. This simplifies so much and it does what you want out of the box.

@imotov I think that's an interesting point, but this significantly improves what is currently happening/allowed with not_analyzed strings (e.g., adding a filter). Perhaps your idea alone should be a separate feature? I like the concept of globally controlled type defaults.
</comment><comment author="dakrone" created="2015-07-22T16:30:52Z" id="123781485">+1 on this, I think if implemented properly it could lead to possibly adding something like @imotov suggested in the future, but for now just starting with these two is a good idea.
</comment><comment author="markwalkom" created="2015-07-23T09:45:53Z" id="124035605">+1 2
</comment><comment author="mbonaci" created="2015-07-29T11:02:52Z" id="125920169">+[insert some ridiculously large number]
Do you still expect this to land in 2.0?
</comment><comment author="clintongormley" created="2015-07-30T12:44:56Z" id="126310329">i hope so!
</comment><comment author="mattweber" created="2015-08-05T18:26:09Z" id="128099778">@clintongormley What is the current thinking for: 

`Question: Should keyword fields allow token filters that introduce new tokens?`

Is there a way to know if a token filter outputs more than a single value? 
</comment><comment author="clintongormley" created="2015-08-05T18:28:31Z" id="128100343">Hi @mattweber - long time! 

@rmuir can confirm but I believe it is not a problem, in the same way that a numeric field can have multiple values.  What we need to avoid is proper full text tokenization.
</comment><comment author="mattweber" created="2015-08-05T18:34:35Z" id="128102301">@clintongormley Thanks.  I was thinking token filters like (edge)ngram and word delimiter might be an issue.  Probably rare though.  
</comment><comment author="rmuir" created="2015-08-05T18:35:28Z" id="128102606">No, we don't need to allow filters that have multiple values. That is really really trappy!

Yes, in lucene we annotate filters that are "safe". This can also be used to make things like wildcard "analysis" more intuitive. But elasticsearch does not make use of this right now.
</comment><comment author="rayward" created="2015-08-05T23:20:06Z" id="128181887">:heart_eyes: +1
</comment><comment author="ariasdelrio" created="2015-08-06T12:49:14Z" id="128352353">+1
</comment><comment author="ppf2" created="2015-10-19T18:59:47Z" id="149313047">+1
</comment><comment author="blankyao" created="2016-02-02T15:18:10Z" id="178628356">+10086 good idea!
</comment><comment author="pweerd" created="2016-02-11T06:36:31Z" id="182737504">Great, I really like this effort! 
Just stumbled across the present limitations.

&gt; No, we don't need to allow filters that have multiple values. That is really really trappy!

One use case is for instance a comma-separated field. 
If not supported, I should break the field in the ingestion app and feed ES with an array...
Can do that, but it needs more code at my side to circumvent the added core-code in ES that prevented me. So more code at both sides, more bugs :-)
To me it make sense to have no limitations. Or at least to be able to switch that limitation off.

As an alternative, we may impose limitations only at the **number** of emitted doc-values. Not at the level of this filter is OK, that filter is not.
Even can make that limit a global setting.
</comment><comment author="bleskes" created="2016-02-11T07:26:23Z" id="182746590">@pweerd 

&gt; One use case is for instance a comma-separated field. 

Just a small note that the coming [ingest feature](https://github.com/elastic/elasticsearch/issues/14049) will allow to preprocess the docs in ES and break the comma separated field into a multi field in ES with a simpler configuration. 
</comment><comment author="pweerd" created="2016-02-15T07:06:12Z" id="184089078">I am really happy with the ingest feature. Great addition!

But...
My point was not about being unable to implement things with ES. I can. Just not in an optimal way.

The net effect of these forced limitations is that I end up having 2 fields, and that I now have to **educate** the end-users (Kibana dashboard designers) when to use what field.

If they do that wrong, ES happily does the aggregations on the fieldcache instead of the docvalues and the enduser doesn't notice that. Apart from OOM's, maybe.

My point was that I'm not sure that the described limitations are a good/valid thing. The way I see it, is that I am forced to implement things in a non-optimal way, while the limitations did not really solve a/the problem. We are just shifting problems to a different party.

With a hammer you can kill yourself. But that shouldn't be a reason to produce only soft rubber hammers.

Apart from that: I really love ES! 
</comment><comment author="javanna" created="2016-02-15T08:43:03Z" id="184114624">&gt; If they do that wrong, ES happily does the aggregations on the fieldcache instead of the docvalues and the enduser doesn't notice that.

Hi @pweerd , something that should help a bit: you can disable fielddata in the mappings (see https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/309/files for an example), so that the aggregation will not work on the wrong field and users will notice that, they'll have to switch to the correct field that has doc_values. 
</comment><comment author="brusic" created="2016-02-16T02:42:06Z" id="184485573">Completely agree with this issue, not only because of the simplification and better defaults, but it also because it better matches Lucene, which has been using TextField and StringField since version 4. Useful is someone is transitioning from Lucene or Solr.

Would creating a multi-field automatically for any string field catch users off-guard when it comes to index size? Many have already been confused by the additional disk space used by doc values.

My question is regarding aggregations and analyzed strings. Perhaps I read another issue or am just confused (which very well could be the case!), but will aggregations still work on fields that do not use doc_values? Using aggregations on analyzed text is a great way for exploratory text mining such as getting counts for unique stems, bigram occurrences, etc.. Yes, there is a big performance hit, but many of us understand the consequences. Many times I feel like I am the only one using elasticsearch as a search engine, so just making sure my goals are covered. :)
</comment><comment author="clintongormley" created="2016-02-17T15:49:43Z" id="185265930">&gt; Would creating a multi-field automatically for any string field catch users off-guard when it comes to index size? Many have already been confused by the additional disk space used by doc values.

Yeah, I'm still undecided on this one. I'm leaning towards creating the multi-field by default so that the out of the box experience is a good one: you get full text search AND analytics by default, without having to go back and reindex your data.

Changing this default could be achieved very simply with a simple index template.

&gt; will aggregations still work on fields that do not use doc_values?

By default, no.  Fielddata loading is disabled by default to prevent unaware users from hurting themselves by eg mistakenly sorting on `foo`  instead of `foo.keyword`.  However, fielddata loading can be enabled dynamically on an existing field, which will then allow aggregations to be run as before.
</comment><comment author="jpountz" created="2016-02-26T15:23:31Z" id="189321260">&gt; Proposal:
&gt;    norms: true | false
&gt;    only supported on text fields

@clintongormley  I started working on this but one issue I have is that the `boost` parameter implicitly enables norms. So should we keep accepting norms on all fields or should we remove `boost` too?
</comment><comment author="clintongormley" created="2016-02-28T23:39:03Z" id="189969323">Bah, I really dislike index-time boosting.  The only purpose it serves that can't be replaced entirely by query time boosting is that terms in the `_all` field inherit the field boost, and I know that some people actively use it. See the "why index time boosting is bad" box on https://www.elastic.co/guide/en/elasticsearch/reference/2.2/index-boost.html

That said, queries on the `_all` field are analyzed/full text, while `keyword` fields are typically used for filtering, not scoring.  I'd remove support for index time boosting from `keyword` fields.  I doubt any other field types support index time boosting?
</comment><comment author="pweerd" created="2016-02-29T08:21:40Z" id="190090906">Though I agree in disliking index-time boosting, I dislike limiting features even more.
I don't want to be protected against myself. I'm a grownup. I think :-)

Proposal: can we have an API in ES that returns warnings that are issued. So, if people sort/aggregate on analyzed fields (I do that sometimes!) they are informed. If they keep doing it, They were aware and thought it was useful.

This #warnings is pretty small I guess. But it informs people about potential issues.
</comment><comment author="clintongormley" created="2016-02-29T08:38:28Z" id="190103350">&gt;  if people sort/aggregate on analyzed fields (I do that sometimes!) they are informed

The plan is to disable aggs on analyzed fields by default and to throw an exception if the user tries to do so.  I believe this is the right thing to do as it is much more common for an unaware user to sort/aggregate on the wrong field (full text instead of raw) and blow up their heap.  Aggs can be enabled on analyzed fields on a live index by updating the mappings, so it is quite easy for users who know what they are doing to resolve the situation to their liking

&gt; Though I agree in disliking index-time boosting, I dislike limiting features even more.

Index time boosting and aggs are unrelated.  This is actually to do with whether field length norms are written for a not_analyzed field or not.  Field length norms are for full text relevance calculations (but index time boosting is hacked in by adjusting the norm).  I would argue that enabling norms on keyword (not_analyzed) fields counts as spooky action at a distance.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport: allow to de-serialize arbitrary objects given their name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12393</link><project id="" key="" /><description>This commit makes it possible to serialize arbitrary objects by having them extend Writeable. When reading them though, we need to be able to identify which object we have to create, based on its name. This is useful for queries once we move to parsing on the coordinating node, as well as with aggregations and so on.

Introduced a new abstraction called NamedWriteable, which is supported by StreamOutput and StreamInput through writeNamedWriteable and readNamedWriteable methods. A new NamedWriteableRegistry is introduced also where named writeable prototypes need to be registered so that we are able to retrieve the proper instance of the writeable given its name and then de-serialize it calling readFrom against it.

Note that this same change was previously reviewed and pushed to the query-refactoring branch (#11553), the goal of this PR is to backport the change to master.

The main question is whether we should use this mechanism for exceptions or not. It seems like it would require a class per exception, maybe a bit too verbose compared to the switch that we have in `StreamInput#readThrowable` and `StreamOutput#writeThrowable`. Looking for a second opinion there.
</description><key id="96531824">12393</key><summary>Transport: allow to de-serialize arbitrary objects given their name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2015-07-22T10:33:41Z</created><updated>2015-07-31T08:41:34Z</updated><resolved>2015-07-31T08:41:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-22T13:13:03Z" id="123715297">LGTM but I'm not too familiar with the Netty stuff so might be worth getting someone to look at that bit. I don't have a strong opinion on the Throwable stuff but it might be nice if everything uses the same mechanism.
</comment><comment author="javanna" created="2015-07-22T14:01:16Z" id="123732142">maybe @jpountz can have a look?
</comment><comment author="jpountz" created="2015-07-22T17:18:08Z" id="123796562">&gt; The main question is whether we should use this mechanism for exceptions or not. It seems like it would require a class per exception, maybe a bit too verbose compared to the switch that we have in StreamInput#readThrowable and StreamOutput#writeThrowable. Looking for a second opinion there.

I like the current logic as it prevents from being tempted to make exception types pluggable?
</comment><comment author="jpountz" created="2015-07-22T17:30:31Z" id="123801005">I'm a bit concerned that this pull request adds coupling between StreamInput and NamedWriteableRegistry on one hand, and transport modules and NamedWriteableRegistry on the other hand.

Could we only have this NamedWriteableRegistry handling in a subclass of StreamInput so that StreamInput would remain unaware of NamedWriteableRegistry? Also I think it would be cleaner if every component maintained its own registry (could be eg. one for queries, one for aggs, etc.) instead of sharing a single one for everything?
</comment><comment author="javanna" created="2015-07-31T08:41:34Z" id="126610004">Replaced by #12571 .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/compress/CompressedStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NamedWriteable.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NamedWriteableAwareStreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/NamedWriteableRegistry.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportModule.java</file><file>core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java</file><file>core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingIT.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluggableTransportModuleIT.java</file><file>core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java</file><file>core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java</file><file>core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortTests.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java</file></files><comments><comment>Transport: allow to de-serialize arbitrary objects given their name</comment></comments></commit></commits></item><item><title> nested: ElasticsearchIllegalArgumentException[found no fielddata type for field [location]]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12392</link><project id="" key="" /><description>{
   "geospatial": {
      "mappings": {
         "gis_basic": {
            "properties": {
               "id": {
                  "type": "string"
               },
               "location": {
                  "properties": {
                     "lat": {
                        "type": "double"
                     },
                     "lon": {
                        "type": "double"
                     }
                  }
               }
            }
         }
      }
   }
}
</description><key id="96505859">12392</key><summary> nested: ElasticsearchIllegalArgumentException[found no fielddata type for field [location]]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">defineconst</reporter><labels /><created>2015-07-22T08:17:37Z</created><updated>2015-07-22T09:02:34Z</updated><resolved>2015-07-22T09:02:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="defineconst" created="2015-07-22T08:18:39Z" id="123612582"> rror": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[LZphOkj4SpaJMTldwumuWw][sooilspatial][0]: SearchParseException[[sooilspatial][0]: query[ConstantScore(GeoBoundingBoxFilter(location, [40.8, -74.1], [40.4, -73.7]))],from[-1],size[-1]: Parse Failure [Failed to parse source [{

nested: ElasticsearchIllegalArgumentException[found no fielddata type for field [location]]; }{[LZphOkj4SpaJMTldwumuWw][sooilspatial][4]: SearchParseException[[sooilspatial][4]: query[ConstantScore(GeoBoundingBoxFilter(location, [40.8, -74.1], [40.4, -73.7]))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"geo_bounding_box\": {\n          \"location\": {\n            \"top_left\": {\n              \"lat\": 40.8,\n              \"lon\": -74.1\n            },\n            \"bottom_right\": {\n              \"lat\": 40.4,\n              \"lon\": -73.7\n            }\n          }\n        }\n      }\n    }\n  },\n  \"aggs\": {\n    \"new_york\": {\n      \"geohash_grid\": {\n        \"field\": \"location\",\n        \"precision\": 5\n      }\n    }\n  }\n}\n]]]; nested: ElasticsearchIllegalArgumentException[found no fielddata type for field [location]]; }]",
   "status": 400
}
</comment><comment author="defineconst" created="2015-07-22T08:19:06Z" id="123612640">Who Can Help Me
</comment><comment author="dadoonet" created="2015-07-22T09:02:34Z" id="123629805">Please use discuss.elastic.co for such questions.

Probably you need to define a geo_point here: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cryptic error message when mis-spelling a field in geo-distance aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12391</link><project id="" key="" /><description>Request:

``` js
GET _search
{
   "aggs": {
      "nested_geodistance": {
         "nested": {
            "path": "addresses"
         },
         "aggs": {
           "per_ring": {
             "geo_distance": {
               "field": "addresses.location",
               "unit": "km",
               "orgin": {
                 "lat": 56.78,
                 "lon": 12.34
               },
               "ranges": [
                 {
                   "from": 100,
                   "to": 200
                 }
               ]
             }
           }
         }
      }
   }
}
```

Note that `origin` field is mis-spelt as `orgin`. The error that comes back in the response is:

```
{
   "error": {
      "root_cause": [
         {
            "type": "search_parse_exception",
            "reason": "Unexpected token START_OBJECT in [per_ring].",
            "line": 11,
            "col": 28
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": ".scripts",
            "node": "IzyWUpmzTbmgk1L8LDCRhQ",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Unexpected token START_OBJECT in [per_ring].",
               "line": 11,
               "col": 28
            }
         }
      ]
   },
   "status": 400
}
```

This is very confusing and doesn't actually point to the error. We should improve this
</description><key id="96498565">12391</key><summary>Cryptic error message when mis-spelling a field in geo-distance aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-07-22T07:49:50Z</created><updated>2015-09-14T17:17:36Z</updated><resolved>2015-08-26T12:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HarishAtGitHub" created="2015-08-18T15:18:51Z" id="132247163">FYI: Just now started working on this .... Will record my observations soon .
</comment><comment author="HarishAtGitHub" created="2015-08-18T19:35:39Z" id="132327551">made a pull request : https://github.com/elastic/elasticsearch/pull/12971 .
Feedback welcome.
</comment><comment author="xuzha" created="2015-08-21T08:19:05Z" id="133327177">I think we just need to print fieldname to keep consistency with other exceptions.
</comment><comment author="HarishAtGitHub" created="2015-08-21T13:21:01Z" id="133423921">Hi Friend @xuzha , 

that is what has been done ..
my PR takes care of 
1) what u said
2) REMOVING the unnecessary CRYTIC parts in the message that distracts users.
3) changing the message into a READABLE one
4) changing all the places in that Geo Distance where such error can occur in future. and changing all into readable one. (with a NOTE: in the commit msg stating wherelse it can occur)
5) also reusing some redundant code.

By the way,
comments or new opinions can be given as new review comments and not new PR's ....
this statement is made by me with the thinking that "We are all working as a team . So we should not allow wasting time working on issues others are already working on. If N people work on a project and N people raise PR's on the same issue I feel that it is a waste of time" .

Happy Learning,
Thanks, it was just my opinion given to my friend.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file></files><comments><comment>Print field name when meet unexpected token.</comment></comments></commit></commits></item><item><title>Bloom filter setting still recognized in post-ES 1.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12390</link><project id="" key="" /><description>Even though bloom filters have been removed from product (https://github.com/elastic/elasticsearch/issues/8571) in 1.5.0, its setting is still recognized by index settings today (ES 1.7.0):

To repro:

Create an index "test"
Close the index

```
PUT /test/_settings
{
  "index": {
    "codec": {
      "bloom": {
        "load": "true"
      }
    }
  }
}
```

Open the index, and the setting is "accepted":

```
{
   "test": {
      "settings": {
         "index": {
            "creation_date": "1437518044482",
            "codec": {
               "bloom": {
                  "load": "true"
               }
            },
...
...
```
</description><key id="96475510">12390</key><summary>Bloom filter setting still recognized in post-ES 1.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-07-22T05:05:13Z</created><updated>2015-07-23T10:43:48Z</updated><resolved>2015-07-23T10:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-22T14:50:24Z" id="123747610">@ppf2 I think this is because the index settings are not validated so that plugins can add their own settings? So this setting is probably silently ignored.

Not necessarily saying that means it's ok, but it at least provides an explanation.
</comment><comment author="ppf2" created="2015-07-22T18:40:47Z" id="123821376">Yah, I already checked with @mikemccand previously and he has confirmed that there is no such thing as enabling bloom filters post 1.5, so this is just for us to do some clean up on the settings side, thx!
</comment><comment author="clintongormley" created="2015-07-23T10:43:48Z" id="124054356">Closing in favour of #6732
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticsearchIllegalArgumentException when _only_nodes is used as a search preference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12389</link><project id="" key="" /><description>I can reproduce this using REST and Java API + ES 1.7.0

The following works:

GET /_search?explain&amp;preference=_only_node:6qdURxYySpCWugwxuvIH3g

The following throws the error:

GET /_search?explain&amp;preference=_only_nodes:pod:A

```
{"error":"ElasticsearchIllegalArgumentException[no Preference for [_only_nodes]]","status":400}
```

I am able to workaround this by using the ENUM value from org.elasticsearch.cluster.routing.operation.plain.Preference directly:

The following works:
GET /_search?explain&amp;preference=ONLY_NODES:pod:A

_only_nodes should work just like _only_node (and not require using the ONLY_NODES enum)?
</description><key id="96443703">12389</key><summary>ElasticsearchIllegalArgumentException when _only_nodes is used as a search preference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Search</label><label>bug</label><label>v1.7.1</label></labels><created>2015-07-22T00:07:37Z</created><updated>2015-07-28T11:41:56Z</updated><resolved>2015-07-28T11:41:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2015-07-22T14:05:23Z" id="123733621">its broken in 1.7 as "_only_nodes" is not parsed to right preference. (
Preference.java - parse )

Thanks
Nirmal

On Tue, Jul 21, 2015 at 7:08 PM, Pius notifications@github.com wrote:

&gt; I can reproduce this using REST and Java API + ES 1.7.0
&gt; 
&gt; The following works:
&gt; 
&gt; GET /_search?explain&amp;preference=_only_node:6qdURxYySpCWugwxuvIH3g
&gt; 
&gt; The following throws the error:
&gt; 
&gt; GET /_search?explain&amp;preference=_only_nodes:pod:A
&gt; 
&gt; {"error":"ElasticsearchIllegalArgumentException[no Preference for [_only_nodes]]","status":400}
&gt; 
&gt; I am able to workaround this by using the ENUM value from
&gt; org.elasticsearch.cluster.routing.operation.plain.Preference directly:
&gt; 
&gt; The following works:
&gt; GET /_search?explain&amp;preference=ONLY_NODES:pod:A
&gt; 
&gt; _only_nodes should work just like _only_node?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12389.
</comment><comment author="clintongormley" created="2015-07-23T10:37:29Z" id="124052164">@nirmalc want to send a fix?
</comment><comment author="nirmalc" created="2015-07-23T13:30:53Z" id="124100208">@clintongormley , will look at it today and send fix.  I think its minor  merge issue with 1.7 tree
</comment><comment author="nirmalc" created="2015-07-25T12:27:49Z" id="124841268">@ppf2 - using enum directly doesnt actually work ; its considering preference as custom string value probably. we consider it as custom preference if preference value begins with "_"
</comment><comment author="ppf2" created="2015-07-27T06:38:43Z" id="125103950">@nirmalc yah you are right, was testing with just 2 nodes, so it appeared that it worked with ONLY_NODES when switching the node attributes but in fact it was just treating them as custom routing values.  Thx for submitting the fix :+1: 
</comment><comment author="spinscale" created="2015-07-28T11:41:56Z" id="125569897">Closed by https://github.com/elastic/elasticsearch/commit/2fa7404c129b9406d98821acfc7dec7ab1e2a3a7
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get field mapping with defaults doesn't populate `fielddata` and `norms` fully</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12388</link><project id="" key="" /><description>```
PUT my_index/my_type/1
{
   "foo": "bar"
}

GET _mapping/field/foo?include_defaults
```

returns:

```
              "foo": {
                 "type": "string",
                 "boost": 1,
                 "index": "analyzed",
                 "store": false,
                 "doc_values": false,
                 "term_vector": "no",
                 "norms": {
                    "enabled": true
                 },
                 "index_options": "positions",
                 "analyzer": "default",
                 "similarity": "default",
                 "fielddata": {},
                 "null_value": null,
                 "include_in_all": false,
                 "position_offset_gap": 0,
                 "search_quote_analyzer": "default",
                 "ignore_above": -1
              }
```

It doesn't include:
- `norms.loading`
- `fielddata.loading`
- `fielddata.format`
</description><key id="96436527">12388</key><summary>Get field mapping with defaults doesn't populate `fielddata` and `norms` fully</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2015-07-21T23:09:13Z</created><updated>2016-11-06T07:56:08Z</updated><resolved>2016-11-06T07:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T14:45:21Z" id="175053386">Related to #12394
</comment><comment author="clintongormley" created="2016-11-06T07:56:08Z" id="258665990">Fixed in 5.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changes in unassigned info and version might not be transferred as part of cluster state diffs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12387</link><project id="" key="" /><description>The equalTo logic of ShardRouting doesn't take version and unassignedInfo into the account when compares shard routings.  Since cluster state diff relies on equal to detect the changes that needs to be sent to other cluster, this omission might lead to changes not being properly propagated to other nodes in the cluster.
</description><key id="96430491">12387</key><summary>Changes in unassigned info and version might not be transferred as part of cluster state diffs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Cluster</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T22:24:20Z</created><updated>2015-08-11T13:40:52Z</updated><resolved>2015-08-11T00:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-21T22:25:53Z" id="123497547">@bleskes could you take a look when you have a chance? Somehow, adding version to the equal triggers retry failures in RecoveryPercolatorTests. Any idea why?
</comment><comment author="bleskes" created="2015-07-23T10:23:56Z" id="124049865">I had a look. like the change (didn’t do a proper review). The source of trouble was IndicesClusterStateService being confused by the new equal semantics. I fixed this in #12397 . Once it’s in we can try again..

&gt; On 22 Jul 2015, at 00:26, Igor Motov notifications@github.com wrote:
&gt; 
&gt; @bleskes could you take a look when you have a chance? Somehow, adding version to the equal triggers retry failures in RecoveryPercolatorTests. Any idea why?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="dakrone" created="2015-08-07T13:17:41Z" id="128696673">@imotov I think this needs to get in before the 2.0 beta, but needs to be rebased now that Boaz fixed the remaining issue in #12397. Can you rebase and see if the nocommit can be removed?
</comment><comment author="imotov" created="2015-08-07T21:43:43Z" id="128839801">@dakrone rebased, force pushed, and can use a review
</comment><comment author="dakrone" created="2015-08-07T21:56:55Z" id="128841961">Left a couple of comments about the testing methods, but the actual diff code looks good to me.
</comment><comment author="dakrone" created="2015-08-10T15:53:44Z" id="129504083">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RandomShardRoutingMutator.java</file><file>core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>core/src/test/java/org/elasticsearch/test/XContentTestUtils.java</file></files><comments><comment>Changes in unassigned info and version might not be transferred as part of cluster state diffs</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Core: Adapt IndicesClusterStateService to use allocation ids</comment></comments></commit></commits></item><item><title>[Test] make cluster state blocking more reliable in IndicesStoreIntegrationTests.indexCleanup() </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12386</link><project id="" key="" /><description>IndicesStoreIntegrationTests.indexCleanup() tests if the shard files on disk are actually removed
after relocation. In particular it tests the following:
Whenever a node deletes a shard because it was relocated somewhere else, it first
checks if enough other copies are started somewhere else. The node sends a
ShardActiveRequest to the nodes that should have a copy.
The nodes that receive this request check if the shard is in state STARTED in which case
they respond with true. If they have the shard in POST_RECOVERY they register a cluster state
observer that checks at each update if the shard has moved to STARTED and respond with true when
this happens.

To test that the cluster state observer mechanism actually works, the latter can be triggered by
blocking the cluster state processing when a recover starts and only unblocking it shortly
after the node receives the ShardActiveRequest.

This is more reliable than using random cluster state processing delays
because the random delays make it hard to reason about different timeouts
that can be reached.

closes #11989
</description><key id="96421821">12386</key><summary>[Test] make cluster state blocking more reliable in IndicesStoreIntegrationTests.indexCleanup() </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T21:37:07Z</created><updated>2015-07-23T12:34:09Z</updated><resolved>2015-07-23T09:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-07-22T09:39:37Z" id="123643450">@bleskes can you take a look?
</comment><comment author="bleskes" created="2015-07-22T13:38:56Z" id="123723852">Looks great. Left some comments.
</comment><comment author="brwe" created="2015-07-22T18:30:37Z" id="123818415">Thanks for the review! I addressed all comments. Let me know if this is what you meant with the latches for signaling. 
</comment><comment author="bleskes" created="2015-07-23T09:06:19Z" id="124028199">LGTM. Left some minor comments. No need for another review..
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file></files><comments><comment>Merge pull request #12386 from brwe/shard-active-test</comment></comments></commit></commits></item><item><title>XContentBuilder.writeValue() serializes java.math.BigDecimal as String</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12385</link><project id="" key="" /><description>When adding documents to the index that are represented by `Map&lt;String, Object&gt;`, and the documents contain instances of `java.math.BigDecimal` or `java.math.BigInteger`, they are serialized as strings, and are subsequently mapped by Elasticsearch to string fields.
Although `java.math.BigDecimal` and `java.math.BigInteger` are supported by Jackson, `XContentBuilder.writeValue()` defaults to represent these data types as strings.
</description><key id="96407914">12385</key><summary>XContentBuilder.writeValue() serializes java.math.BigDecimal as String</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickivanov</reporter><labels /><created>2015-07-21T20:19:28Z</created><updated>2015-07-23T10:19:43Z</updated><resolved>2015-07-23T10:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T10:19:43Z" id="124047844">Lucene currently has no support for BigInteger/Decimal.

Closing in favour of https://github.com/elastic/elasticsearch/pull/5683
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ignore EngineClosedException during translog fysnc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12384</link><project id="" key="" /><description>When performing an operation on a primary, the state is captured and the
operation is performed on the primary shard. The original request is then
modified to increment the version of the operation as preparation for it to
be sent to the replicas.

If the request first fails on the primary during the translog sync
(because the Engine is already closed due to shadow primaries closing
the engine on relocation), then the operation is retried on the new primary
_after being modified for the replica shards_. It will then fail due to the
version being incorrect (the document does not yet exist but the request
expects a version of "1").

Order of operations:
- Request is executed against primary
- Request is modified (version incremented) so it can be sent to replicas
- Engine's translog is fsync'd if necessary (failing, and throwing an exception)
- Modified request is retried against new primary

This change ignores the exception where the engine is already closed
when syncing the translog (similar to how we ignore exceptions when
refreshing the shard if the `?refresh=true` flag is used).
</description><key id="96404358">12384</key><summary>Ignore EngineClosedException during translog fysnc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T20:00:36Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-21T21:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-21T20:01:00Z" id="123461986">Example failure this caused: http://build-us-00.elastic.co/job/es_g1gc_master_metal/11343/testReport/junit/org.elasticsearch.index/IndexWithShadowReplicasTests/testPrimaryRelocationWithConcurrentIndexing/
</comment><comment author="bleskes" created="2015-07-21T20:40:41Z" id="123471212">Left a minor comment. LGTM. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Empty and/orFilter inside hasParent filter causes NULLPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12383</link><project id="" key="" /><description>In my application, a bunch of filters are generated on the fly from client side requests. As a result there might be situations where there can be empty and filter list. This is causing a NPE in elasticsearch.

I have provided a detailed test case and the output log [here](https://gist.github.com/santanusinha/45c5bcbf6afb6c67eb63)
</description><key id="96391477">12383</key><summary>Empty and/orFilter inside hasParent filter causes NULLPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">santanusinha</reporter><labels><label>:Query Refactoring</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-21T18:57:00Z</created><updated>2015-10-15T13:43:26Z</updated><resolved>2015-10-15T13:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-23T10:17:52Z" id="124047056">@javanna one for the list
</comment><comment author="javanna" created="2015-10-15T13:42:54Z" id="148390815">I would expect this to be fixed in master after the query refactoring changes. That said it is hard to test given that queries and filters got merged. Also and and or queries have been removed. Anyways we did find different cases where queries would convert to null and that could cause NPEs which we finally added lots of tests for. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Leading space breaks YML parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12382</link><project id="" key="" /><description>If you accidentally manage to place a leading space in front of a YML setting in `elasticsearch.yml`, then all follow-on settings will be silently ignored.

```
 cluster.name: my-cluster
 other.setting: xyz

# The rest is ignored
name.node: my-node
```

If you turn around and start the node, then it fails to recognize the `node.name` without any error:

``` bash
$ bin/elasticsearch
[2015-07-21 11:43:08,883][INFO ][node                     ] [Screaming Mimi] version[1.6.0], pid[35094], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-21 11:43:08,883][INFO ][node                     ] [Screaming Mimi] initializing ...
```
</description><key id="96389444">12382</key><summary>Leading space breaks YML parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Settings</label><label>bug</label><label>v1.6.2</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T18:48:39Z</created><updated>2015-07-28T15:50:22Z</updated><resolved>2015-07-24T17:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-22T09:33:23Z" id="123641054">This should throw an inconsistent indentation error.  Not sure if this is our parsing or Jackson that has th issue
</comment><comment author="jasontedor" created="2015-07-24T17:33:46Z" id="124590212">@clintongormley It's our issue. We parse until we reach an end object token. The YAML spec requires that all sibling nodes be at the same indentation level. Once the indentation is reduced in the example that @pickypg gave, a YAML parser that parses according to the spec will return an end object token and we will stop parsing. Therefore, to solve this we should just ensure that when we encounter an end object token while parsing settings, we are in fact at the end of the settings stream. I've submitted PR #12451 to do this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/test/java/org/elasticsearch/common/settings/loader/YamlSettingsLoaderTests.java</file></files><comments><comment>Add explicit check that we have reached the end of the settings stream when parsing settings</comment></comments></commit></commits></item><item><title>don't fail junit4 integration tests if there are no tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12381</link><project id="" key="" /><description>instead fail the failsafe plugin, which means the external cluster will still get shut down
</description><key id="96381596">12381</key><summary>don't fail junit4 integration tests if there are no tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>build</label></labels><created>2015-07-21T18:16:45Z</created><updated>2015-07-21T18:49:50Z</updated><resolved>2015-07-21T18:49:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-07-21T18:29:49Z" id="123428870">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12381 from rmuir/fail_if_no_tests</comment></comments></commit></commits></item><item><title>Correctly list blobs in Azure Storage to prevent snapshot corruption and do not unnecessarily duplicate Lucene segments in Azure Storage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12380</link><project id="" key="" /><description>This commit addresses an issue that was leading to snapshot corruption for snapshots stored as blobs in Azure Storage.

The underlying issue is that in cases when multiple snapshots of an index were taken and persisted into Azure Storage, snapshots subsequent to the first would repeatedly overwrite the snapshot files. This issue does render useless all snapshots except the final snapshot.

The root cause of this is due to String concatenation involving null. In particular, to list all of the blobs in a snapshot directory in Azure the code would use the method `listBlobsByPrefix` where the prefix is `null`. In the `listBlobsByPrefix` method, the path `keyPath + prefix` is constructed. However, per [5.1.11](https://docs.oracle.com/javase/specs/jls/se8/html/jls-5.html#jls-5.1.11), [5.4](https://docs.oracle.com/javase/specs/jls/se8/html/jls-5.html#jls-5.4) and [15.18.1](https://docs.oracle.com/javase/specs/jls/se8/html/jls-15.html#jls-15.18.1) of the Java Language Specification, the reference null is first converted to the string `"null"` before performing the concatenation. This leads to no blobs being returned and therefore the snapshot mechanism would operate as if it were writing the first snapshot of the index. The fix is simply to check if prefix is null and handle the concatenation accordingly.

Upon fixing this issue so that subsequent snapshots would no longer overwrite earlier snapshots, it was discovered that the snapshot metadata returned by the `listBlobsByPrefix` method was not sufficient for the snapshot layer to detect whether or not the Lucene segments had already been copied to the Azure storage layer in an earlier snapshot. This led the snapshot layer to unnecessarily duplicate these Lucene segments in Azure Storage.

The root cause of this is due to known behavior in the `CloudBlobContainer.getBlockBlobReference` method in the Azure API. Namely, this method does not fetch blob attributes from Azure. As such, the lengths of all the blobs appeared to the snapshot layer to be of length zero and therefore they would compare as not equal to any new blobs that the snapshot layer is going to persist. To remediate this, the method `CloudBlockBlob.downloadAttributes` must be invoked. This will fetch the attributes from Azure Storage so that a proper comparison of the blobs can be performed.

Closes elastic/elasticsearch-cloud-azure#51, closes elastic/elasticsearch-cloud-azure#99
</description><key id="96378930">12380</key><summary>Correctly list blobs in Azure Storage to prevent snapshot corruption and do not unnecessarily duplicate Lucene segments in Azure Storage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugin Cloud Azure</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T18:04:30Z</created><updated>2015-07-23T10:09:47Z</updated><resolved>2015-07-21T20:47:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-21T18:42:02Z" id="123434570">I left some minor comments and great work @jasontedor! 

I'm wondering if we can add a test that creates multiple snapshots and checks that we don't snapshot everything for each snapshots (using the Snapshot status API and check the number of files / total size of each snapshot)?
</comment><comment author="jasontedor" created="2015-07-21T18:48:02Z" id="123437550">Thanks @tlrx for the quick review! I pushed two commits to address your comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java</file><file>plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java</file></files><comments><comment>Merge pull request #12380 from jasontedor/fix/elasticsearch-cloud-azure-51</comment></comments></commit></commits></item><item><title>Testing: Fix help displaying tests under windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12379</link><project id="" key="" /><description>The help files are using a unix based line separator, where as
the test relies on the help being based on the line separator.

This commit fixes the test to check only for contents, not for line separators.

The test has also been moved into its own CliToolTestCase, as it does
not need to be an integration test.
</description><key id="96368804">12379</key><summary>Testing: Fix help displaying tests under windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T17:10:52Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-21T17:14:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-07-21T17:12:45Z" id="123405839">LGTM. Ran the test on a windows box and it passed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>lat/lon appears to not be used even if you index it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12378</link><project id="" key="" /><description>The default optimizeBbox claims to be "memory", even if you index it...

```
    if (optimizeBbox != null &amp;&amp; !"none".equals(optimizeBbox)) {
        distanceBoundingCheck = GeoDistance.distanceBoundingCheck(lat, lon, inclusiveUpperPoint, DistanceUnit.DEFAULT);
        if ("memory".equals(optimizeBbox)) {
            boundingBoxFilter = null;
```
</description><key id="96367067">12378</key><summary>lat/lon appears to not be used even if you index it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-07-21T17:01:32Z</created><updated>2015-07-23T10:06:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactored check_license_and_sha.pl to accept a license dir and package path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12377</link><project id="" key="" /><description>In preparation for the move to building the core zip, tar.gz, rpm, and deb as separate modules, refactored check_license_and_sha.pl to:
- accept a license dir and path to the package to check on the command line
- to be able to extract zip, tar.gz, deb, and rpm
- all packages except rpm will work on Windows

Relates to #12286
</description><key id="96354171">12377</key><summary>Refactored check_license_and_sha.pl to accept a license dir and package path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>build</label></labels><created>2015-07-21T16:00:50Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-21T17:54:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12377 from clintongormley/license_check_all_packages</comment></comments></commit></commits></item><item><title>Highlighting: expose merge_contiguos plain highlighter options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12376</link><project id="" key="" /><description>Some users end up using the fast vector highlighter because it is able to merge contiguos fragments and add the highlighting tags only once e.g. `&lt;em&gt;onetwothree&lt;/em&gt;` rather than once per match e.g. `&lt;em&gt;one&lt;/em&gt;&lt;em&gt;two&lt;/em&gt;&lt;em&gt;three&lt;/em&gt;`. I think this is not a good reason to use the fast vector highlighter, and I see that the lucene plain highlighter supports merging contiguous fragments but we don't expose this option to the users and always pass in `false` to lucene. I wonder if it would make sense to expose this setting, which would be read only by the plain highlighter and ignored by other implementations, so that the plain highlighter is able to merge contiguos fragments if requested.
</description><key id="96347972">12376</key><summary>Highlighting: expose merge_contiguos plain highlighter options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>enhancement</label></labels><created>2015-07-21T15:36:10Z</created><updated>2015-07-24T09:35:24Z</updated><resolved>2015-07-24T09:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-21T17:26:54Z" id="123409169">In theory this sounds good to me. This is exactly the kind of thing that I'd just fix on the client side if I didn't like it though. But if its simple to turn on then why not?
</comment><comment author="javanna" created="2015-07-24T09:35:20Z" id="124456023">We discussed this during fix-it friday, I totally misunderstood the `mergeContiguous` option, which merges contiguous fragments rather than contiguous tokens. This might be a useful option but given all the options that we already have and which work only for some of the highlighters (this one would work only for the plain one), I would not expose it for now. We will also start some more general discussion around highlighting and how we want to improve things there post 2.0. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create Memory Index with Elasticsearch.net </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12375</link><project id="" key="" /><description>Hi, 

I try to create memory index with following code but it creates regular index.
Any idea?

```
        var node = new Uri("http://localhost:9200");
        var settings = new ConnectionSettings(node);
        var client = new Elasticsearch.Net.ElasticsearchClient(settings);
        var js = JsonConvert.SerializeObject("{settings: { index.store.type: memory}");
        var index = client.IndicesCreate("sampleIndex", js);
```
</description><key id="96329319">12375</key><summary>Create Memory Index with Elasticsearch.net </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serhattopkaya</reporter><labels /><created>2015-07-21T14:26:50Z</created><updated>2015-07-23T12:08:27Z</updated><resolved>2015-07-23T12:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="serhattopkaya" created="2015-07-23T08:24:28Z" id="124020028">Problem solved like that:

var js = new
                   {
                       settings = new
                       {
                           index = new
                           {
                               store = new
                               {
                                   type = "memory"
                               }
                           }
                       }
                   };
</comment><comment author="clintongormley" created="2015-07-23T12:08:19Z" id="124077351">Be aware that memory indices have been removed in 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace primaryPostAllocated flag and use UnassignedInfo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12374</link><project id="" key="" /><description>There is no need to maintain additional state as to if a primary was allocated post api creation on the index routing table, we hold all this information already in the UnassignedInfo class.
</description><key id="96325466">12374</key><summary>Replace primaryPostAllocated flag and use UnassignedInfo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T14:11:28Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-21T15:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-21T14:52:06Z" id="123356046">LGTM. A little dance for this flag going away... 
</comment><comment author="dakrone" created="2015-07-21T14:55:54Z" id="123358512">LGTM also, left one question out of curiosity.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginInfo.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfoTest.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/current/CurrentLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/newer/NewerLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/old/OldLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryITest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsITest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AbstractAzureComputeServiceTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AbstractAzureRepositoryServiceTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/AbstractGceTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeEngineTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/gce/itest/GceSimpleITest.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryTests.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/test/rest/DeleteByQueryRestTests.java</file></files><comments><comment>Refactor pluginservice</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/gateway/PrimaryShardAllocator.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocatePostApiFlagTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestBase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file></files><comments><comment>Replace primaryPostAllocated flag and use UnassignedInfo</comment><comment>There is no need to maintain additional state as to if a primary was allocated post api creation on the index routing table, we hold all this information already in the UnassignedInfo class.</comment><comment>closes #12374</comment></comments></commit></commits></item><item><title>PluginManager: Fix bin/plugin calls in scripts/bats test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12373</link><project id="" key="" /><description>The release and smoke test python scripts used to install
plugins in the old fashion.

Also the BATS testing suite installed/removed plugins in that
way. Here the marvel tests have been removed, as marvel currently
does not work with the master branch.

In addition documentation has been updated as well, where it was
still missing.
</description><key id="96318688">12373</key><summary>PluginManager: Fix bin/plugin calls in scripts/bats test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T13:45:59Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-21T14:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-21T13:46:31Z" id="123311146">@tlrx if you can spare a few seconds, this should hopefully be a quick one for you, just replaced the calls in the bats script
</comment><comment author="tlrx" created="2015-07-21T14:06:25Z" id="123322212">Left some comments, otherwise LGTM and works on Fedora 21, CentOS6.6 and Ubuntu 14.04 with the -i/remove adjustment.

Also, I'd change the titles of the PR and commit because it's not only BATS file here.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make sure NodeJoinController.ElectionCallback is always called from the update cluster state thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12372</link><project id="" key="" /><description>This is important for correct handling of the joining thread. This causes assertions to trip in our test runs. See http://build-us-00.elastic.co/job/es_g1gc_master_metal/11653/ as an example
</description><key id="96317793">12372</key><summary>Make sure NodeJoinController.ElectionCallback is always called from the update cluster state thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T13:43:00Z</created><updated>2015-08-07T10:07:05Z</updated><resolved>2015-07-21T14:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-21T14:08:01Z" id="123323335">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginInfo.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfoTest.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/current/CurrentLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/newer/NewerLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/old/OldLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryITest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsITest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AbstractAzureComputeServiceTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AbstractAzureRepositoryServiceTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/AbstractGceTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeEngineTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/gce/itest/GceSimpleITest.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryTests.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/test/rest/DeleteByQueryRestTests.java</file></files><comments><comment>Refactor pluginservice</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java</file></files><comments><comment>Discovery: make sure NodeJoinController.ElectionCallback is always called from the update cluster state thread</comment></comments></commit></commits></item><item><title>Remove the dependecy on IndexFielddataService from MapperService.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12371</link><project id="" key="" /><description>This dependency was used in order for mapping updates that change the fielddata
format to take effect immediately. And the way it worked was by clearing the
cache of fielddata instances that were already loaded. However, we do not need
to cache the already loaded (logical) fielddata instances, they are cheap to
regenerate. Note that the fielddata _caches_ are still kept around so that we
don't keep on rebuilding costly (physical) fielddata values.
</description><key id="96317348">12371</key><summary>Remove the dependecy on IndexFielddataService from MapperService.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T13:41:22Z</created><updated>2015-07-23T10:33:28Z</updated><resolved>2015-07-22T12:37:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-21T17:03:28Z" id="123403463">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentTypeListener.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/KeyedLockTests.java</file></files><comments><comment>Fix fielddata handling for the `_parent` field.</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file></files><comments><comment>Merge pull request #12371 from jpountz/fix/mapper_fielddata_dependency</comment></comments></commit></commits></item><item><title>Distance Score query gives wrong sort order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12370</link><project id="" key="" /><description>Using _geo_distance query gives me correct order by closest distance, geoCenter field is created using a transform in the mapping. 

```
{
  "from": 0,
  "size": 200,
  "sort": [
    {
      "_geo_distance": {
        "geoCenter": {
          "lat": 29.745703,
          "lon": -95.740514
        },
        "order": "asc",
        "unit": "mi",
        "distance_type": "plane"
      }
    }
  ]
}
```

However using Distance Score query on the same field gives wrong sort, every 4th item or so in the results gets lower score even though it closer, the deeper you go through the results the more wrong sorts I get, tried using different score functions and parameters but still getting same wrong sort consistently.

```
{
  "from": 0,
  "size": 100,
  "script_fields": {
    "MilesAway": {
      "script": "doc['geoCenter'].distanceInMiles(lat,lon)",
      "params": {
        "lat": 29.745703,
        "lon": -95.740514
      }
    }
  },
  "_source": {
    "include": [
      "doc.*"
    ]
  },
  "query": {
    "function_score": {
      "functions": [
        {
          "gauss": {
            "geoCenter": {
              "origin": {
                "lat": 29.745703,
                "lon": -95.740514
              },
              "scale": "500m",
              "offset": "1km",
              "decay": 0.99
            }
          }
        }
      ]
    }
  }
```
</description><key id="96304781">12370</key><summary>Distance Score query gives wrong sort order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Molenx</reporter><labels /><created>2015-07-21T12:30:51Z</created><updated>2015-07-23T10:23:21Z</updated><resolved>2015-07-23T10:23:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-07-21T13:02:27Z" id="123297608">function score uses sloppy arc for distance computation and `distanceInMiles` in scripts uses "plane"  so I would expect results to be slightly different. Can you try `arcDistanceInMiles` instead in the script and see if results are as expected? 
</comment><comment author="Molenx" created="2015-07-22T02:36:03Z" id="123539508">That worked, thank you!
</comment><comment author="clintongormley" created="2015-07-23T10:23:21Z" id="124049560">Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Imprecise field name matching with varying results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12369</link><project id="" key="" /><description>Had a query with ever-changing results:

```
curl -XPOST localhost:9200/event/_search?size=0 -d '{
    "query": {
        "term": {
            "id": 31
        }
    }
}'
```

Only about half of the relevant documents were returned and it was changing by +-50% between requests on a stable index.
I realised that the real field name was `event.data.id` and changing the query fixed the problem. But the field shouldn't match at all, it should not match just a sub section, and it should not vary between requests.
I have a few documents indexed where the field name is `event.id`. It seems this is what is triggering the bug, since I can't reproduce on other `event.data.*` fields.
</description><key id="96301305">12369</key><summary>Imprecise field name matching with varying results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Lensi</reporter><labels /><created>2015-07-21T12:09:41Z</created><updated>2015-07-21T12:14:01Z</updated><resolved>2015-07-21T12:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-21T12:14:01Z" id="123286675">@Lensi Totally agreed. As of 2.0 the full field name will be required, see https://github.com/elastic/elasticsearch/pull/9670
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CliTool: Add support for mutually exclusive options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12368</link><project id="" key="" /><description>Right now, the configured options are not mutually exlcusive. This means you can specify `--silent --verbose` and one wins.

This should throw an exception.
</description><key id="96294169">12368</key><summary>CliTool: Add support for mutually exclusive options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Core</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-21T11:32:56Z</created><updated>2016-07-07T06:50:10Z</updated><resolved>2016-07-07T06:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>core/src/main/java/org/elasticsearch/cli/Command.java</file><file>core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java</file><file>core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/ElasticsearchCliTests.java</file><file>core/src/test/java/org/elasticsearch/cli/CommandTests.java</file><file>test/framework/src/main/java/org/elasticsearch/bootstrap/ESElasticsearchCliTestCase.java</file></files><comments><comment>Dependencies: Update to jopt-5.0 (#19278)</comment></comments></commit></commits></item><item><title>Refactor pluginservice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12367</link><project id="" key="" /><description>Today plugins are all dumped into the main classloader and can easily have jar hell dependencies with each other (great example is the python scripting plugin). Also pluginservice is messy, there is way too much leniency, and most logic is accomplished with heuristics.

Instead we need mandatory metadata that is explicit: enough to easily iterate/manipulate/validate plugins from the filesystem. This simplifies the code and we can move forward to make this stuff really reliable. There are currently also a ton of ways to do things, I removed any esoteric options/unnecessary stuff from this code here: back to basics guys.

Instead of dumping all code into one classloader, plugins can just have a standard URLClassLoader with ES loader as the parent. We don't need any special classloader because we simply don't allow conflicts with the parent (jarhell will simply fail).

Unfortunately some plugins might not be so clean, and share hard dependencies with each other. For now those plugins can set `isolated` to false until they clean their act up. All plugins that do this stuff just share a loader with each other. We should consider this deprecated since it makes things more complicated than it needs to be. Loose coupling, please.

In all cases we aggressively jarhell check, and all plugin manipulation is underneath securitymanager protection.

With this branch all plugin integration tests are passing, and moreover the python script that loads all of them at once finally passes:

```
[INFO ][plugins                  ] [smoke_tester] loaded [lang-python, analysis-kuromoji, analysis-smartcn, cloud-gce, analysis-stempel, delete-by-query, cloud-aws, analysis-phonetic, cloud-azure, lang-javascript, analysis-icu], sites []
```

I disabled most of the unit tests, but they can be fixed / new ones written after sleep. There are also a lot of checks we should add in the future, and PluginManager should reuse some of this logic to prevent installing a plugin that isn't going to work or somehow screw the installation.

Closes #11917
</description><key id="96293774">12367</key><summary>Refactor pluginservice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T11:30:39Z</created><updated>2015-07-23T10:39:30Z</updated><resolved>2015-07-22T14:46:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-07-21T14:03:17Z" id="123319951">+1 Nice work!
</comment><comment author="rjernst" created="2015-07-22T05:42:09Z" id="123570573">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove obsolete plugins.info_refresh_interval setting</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginInfo.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfoTest.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file><file>core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryTests.java</file><file>core/src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/current/CurrentLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/newer/NewerLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/plugins/lucene/old/OldLucenePlugin.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryITest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsITest.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AbstractAzureComputeServiceTest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AbstractAzureRepositoryServiceTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/AbstractGceTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeEngineTest.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/gce/itest/GceSimpleITest.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryTests.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/test/rest/DeleteByQueryRestTests.java</file></files><comments><comment>Refactor pluginservice</comment></comments></commit></commits></item><item><title>Invalid mapping case not handled by index.mapping.ignore_malformed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12366</link><project id="" key="" /><description>Indexing a document with an object type on a field that has already been mapped as a string type causes `MapperParsingException`, even if `index.mapping.ignore_malformed` has been enabled.
## Reproducible test case

On Elasticsearch 1.6.0:

```
$ curl -XPUT localhost:9200/broken -d'{"settings":{"index.mapping.ignore_malformed": true}}'
{"acknowledged":true}

$ curl -XPOST localhost:9200/broken/type -d '{"test":"a string"}'
{"_index":"broken","_type":"type","_id":"AU6wNDGa_qDGqxty2Dvw","_version":1,"created":true}

$ curl -XPOST localhost:9200/broken/type -d '{"test":{"nested":"a string"}}'
{"error":"MapperParsingException[failed to parse [test]]; nested: ElasticsearchIllegalArgumentException[unknown property [nested]]; ","status":400}

$ curl localhost:9200/broken/_mapping
{"broken":{"mappings":{"type":{"properties":{"test":{"type":"string"}}}}}}
```
## Expected behaviour

Indexing a document with an object field where Elasticsearch expected a string field to be will not fail the whole document when `index.mapping.ignore_malformed` is enabled. Instead, it will ignore the invalid object field.
</description><key id="96285497">12366</key><summary>Invalid mapping case not handled by index.mapping.ignore_malformed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-21T10:45:26Z</created><updated>2017-07-18T01:06:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-24T09:35:05Z" id="124455974">+1
</comment><comment author="andrestc" created="2015-09-26T05:33:35Z" id="143400321">While working on this issue, I found out that it fails on other types too, but for another reason: For example, for integer:

```
$ curl -XPUT localhost:9200/broken -d'{"settings":{"index.mapping.ignore_malformed": true}}'
{"acknowledged":true}

$ curl -XPOST localhost:9200/broken/type -d '{"test2": 10}'
{"_index":"broken","_type":"type","_id":"AU6wNDGa_qDGqxty2Dvw","_version":1,"created":true}

$ curl -XPOST localhost:9200/broken/type -d '{"test2":{"nested": 20}}'
```

```
[elasticsearch] [2015-09-26 02:20:23,380][DEBUG][action.index             ] [Tyrant] [broken][1], node[7WAPN-92TAeuFYbRLVqf8g], [P], v[2], s[STARTED], a[id=WlYpBZ6vTXS-4WMvAypeTA]: Failed to execute [index {[broken][type][AVAIGFNQZ9WMajLk5l0S], source[{"test2":{"nested":1}}]}]
[elasticsearch] MapperParsingException[failed to parse]; nested: IllegalArgumentException[Malformed content, found extra data after parsing: END_OBJECT];
[elasticsearch]     at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:157)
[elasticsearch]     at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:77)
[elasticsearch]     at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:319)
[elasticsearch]     at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:475)
[elasticsearch]     at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:466)
[elasticsearch]     at org.elasticsearch.action.support.replication.TransportReplicationAction.prepareIndexOperationOnPrimary(TransportReplicationAction.java:1053)
[elasticsearch]     at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1061)
[elasticsearch]     at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:170)
[elasticsearch]     at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:580)
[elasticsearch]     at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:453)
[elasticsearch]     at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
[elasticsearch]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[elasticsearch]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[elasticsearch]     at java.lang.Thread.run(Thread.java:745)
[elasticsearch] Caused by: java.lang.IllegalArgumentException: Malformed content, found extra data after parsing: END_OBJECT
[elasticsearch]     at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:142)
[elasticsearch]     ... 13 more
```

Thats happening because, unlike in the `string` case, we are handling the ignoreMalformed for numeric types but, when we throw the exception [here](https://github.com/elastic/elasticsearch/blob/52113e752747e79d817a1d8ef14e441d839d4d9b/core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java#L278) we didn't parse the field object until `XContentParser.Token.END_OBJECT` and that comes to bite us later, [here](https://github.com/elastic/elasticsearch/blob/6885ab96805990d83e57dd0baded83252d7a2e0e/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L142).

So, I think two things must be done:
(1) Use the ignoreMalformed settings in StringFieldMapper, which is not happening (hence the original reported issue)
(2) Parse until the end of the current object before throwing `IllegalArgumentException("unknown property [" + currentFieldName + "]");` in the Mapper classes. To prevent the exception I reported from happening. Or maybe just ignore this exception, in `innerParseDocument`, when ignoreMalformed is set?

Does this make sense, @clintongormley? I'll happily send a PR for this. 
</comment><comment author="clintongormley" created="2015-09-27T10:05:14Z" id="143536217">ah - i just realised that the original post refers to a string field, which doesn't support ignore_malformed...

@andrestc i agree with your second point, but i'm unsure about the first...

@rjernst what do you think?
</comment><comment author="rjernst" created="2015-11-09T01:22:50Z" id="154897532">Sorry for the delayed response, I lost this one in email.

@clintongormley I think it is probably worth making the behavior consistent, and it does seem to me finding an object where a specific piece of data is expected constitutes "malformed" data.

@andrestc A PR would be great.
</comment><comment author="abulhol" created="2015-12-21T14:54:17Z" id="166323189">I want to upvote this issue!
I have fields in my JSON that are objects, but when they are empty, they contain an empty string, i.e. "" (this is the result of an XML2JSON parser). Now when I add a document where this is the case, I get a

```
 MapperParsingException[object mapping for [xxx] tried to parse field [xxx] as object, but found a concrete value]
```

This is not at all what I would expect from the documentation https://www.elastic.co/guide/en/elasticsearch/reference/2.0/ignore-malformed.html; please improve the documentation or fix the behavior (preferred!). 

@clintongormley "i just realised that the original post refers to a string field, which doesn't support ignore_malformed..." Why should string fields not support ignore_malformed?
</comment><comment author="megastef" created="2016-01-27T17:05:03Z" id="175748358">+1 

I think there could be done much more e.g. set the field to a default value and add an  annotation to the document - so users can see what went wrong. In my case all documents from Apache Logs having "-" in the size field (Integer) got ignored. I could tell you 100 stories, why Elasticsearch don't take documents from real data sources ... (just to mention one more https://github.com/elastic/elasticsearch/issues/3714)

I think this problem could be handled much better:
1. if a type error appears, try to convert the value (optional server/index setting). Often a JSON has number without quotes (correct), some put numbers as string in quotes. In this case the string could be converted to integer. 
2. if the type does not fit, take a default value for this type (0,null) - or ignore the field as you do today, but very bad if it is a larger object ...
3. add a comment field like "_es_error_report: MapperParsingException: ...." 
In that way users can see that there was something wrong, today - data just disappears, when it fails to be indexed or the field is ignored. And the sysadmin might see error message in some logs ... but users wonder that data in elasticsearch is not complete and might have no access to elasticsearch logs. In my case I missed all  Apache messages with status code 500 and size "-" instead of 0 - which is really bad - and depends on the log parser ... 

A good example is Logsene, it adds Error-Annotations to failed documents together with the String version of the original source document (@sematext can catch Elasticsearch errors during the indexing process). So at least Logsene users can see failed index operations and orginal document in their UI or in Kibana. Thanks to this feature I'm able to report this issue to you. 

It would be nice when such improvements would be available out of box for all Elasticsearch users. 
</comment><comment author="abulhol" created="2016-02-01T20:41:21Z" id="178178546">any news here? 
</comment><comment author="balooka" created="2016-03-22T10:09:44Z" id="199732779">I wish to upvote the issue too.
My understanding of the ignore_malformed purpose is to not lose events, even when you might lose some of its content.
In the current situation I'm in, a issue similar to what has been described here is occurring, and although it's identified and multiple mid-term approaches are looked into - Issue in our case relates to multiple sources sending similar event, so options like splitting the events in separate mappings, or even cleaning up the events before reaching elasticsearch could be done - I would have liked a short term approach similar to ignore_malformed functionality to be in place to help sort term.
</comment><comment author="BeccaMG" created="2016-05-03T12:54:39Z" id="216518042">Same problem with dates.

When adding an object with a field of type "date", in my DB whenever it is empty it's represented as "" (empty string) causing this error:

```
[DEBUG][action.admin.indices.mapping.put] [x] failed to put mappings on indices [[all]], type [seedMember]
java.lang.IllegalArgumentException: mapper [nms_recipient.birthDate] of different type, current_type [string], merged_type [date]
```
</comment><comment author="satazor" created="2016-05-06T14:21:55Z" id="217453177">Same problem with me. I'm using the ELK stack in which people may use the same properties but with different types. I don't want those properties to be searchable but I don't want to loose the entity event neither. I though `ignore_malformed` would do that but apparently is not working for all cases.
</comment><comment author="jarlelin" created="2016-06-20T12:30:16Z" id="227128700">We are having issues with this same feature. We have documents that sometimes decide to have objects inside something that was intedended to have strings. We would like to not lose the whole document just because one of the nodes of data are malformed.

This is the behaviour I expected to get from setting ignore_malformed on the properties, and I would applaude such a feature.
</comment><comment author="DaTebe" created="2016-08-26T15:40:43Z" id="242771038">Hay, I have the same problem. Is there any solution (even if it is a bit hacky) out there?
</comment><comment author="goodfella1408" created="2016-09-09T07:17:47Z" id="245838332">Facing this in elasticsearch 2.3.1 . Before this bug is fixed we should atleast have a list of bad fields inside mapper_parsing_exception error so that the app can choose to remove them . Currently there is no standard field in the error through which these keys can be retrieved -

"error":{"type":"mapper_parsing_exception","reason":"object mapping for [A.B.C.D] tried to parse field [D] as object, but found a concrete value"}}

The app would have to parse the reason string and extract A.B.C.D which will fail if the error doc format changes . Additionally mapper_parsing_exception error itself must be using different formats for different parsing error scenarios all of which need to be handled by the app
</comment><comment author="BeccaMG" created="2016-09-14T04:05:27Z" id="246900846">I used a workaround for this matter following the recommendations from Elasticsearch forums and official documentation.

Declaring the mapping of the objects you want to index (if you know it), choosing `ignore_malfored` in dates and numbers, should do the trick. Those tricky ones that could have `string` or `nested` content could be simply declared as `object`.
</comment><comment author="derEremit" created="2016-09-15T16:30:01Z" id="247379677">for usage as a real log stash I would say something like https://github.com/elastic/elasticsearch/issues/12366#issuecomment-175748358
is a must have!
I can get accustomed to losing indexed fields but losing log entries is a no-go for ELK from my perspective
</comment><comment author="micpotts" created="2016-10-31T21:17:59Z" id="257423457">Bumping, this issue is preventing a number of my messages to successfully be processed as a field object is returned as an empty string on rare cases.
</comment><comment author="patrick-oyst" created="2017-01-19T14:17:32Z" id="273787163">Bump, this is proving to be an extremely tedious (non) feature to work around.</comment><comment author="patrick-oyst" created="2017-01-19T15:01:12Z" id="273798499">I've found a way around this but it comes at a cost. It could be worth it for those like me who are in a situation where intervening directly on your data flow (like checking and fixing the log line yourself before sending it to ES) is something you'd like to avoid in the short term. Set the `enabled` setting of your field to `false`. This will make the field non searchable though. This isn't too big of an issue in my context because the reason this field is so unpredictable is the reason I need `ignore_malformed` to begin with, so it's not a particularly useful field to search on anyways, though you still have access to the data when you search for that document using another field. Incidentally, this solves both situations : writing an `object` to a non-object field and vice versa.

Hope this helps. It certainly saved *me* a lot of trouble...</comment><comment author="jarlelin" created="2017-01-19T15:53:14Z" id="273813505">Thats a good trick. Ill try that out.

On 19 Jan 2017 16:01, "patrick-oyst" &lt;notifications@github.com&gt; wrote:

&gt; I've found a way around this but it comes at a cost. It could be worth it
&gt; for those like me who are in a situation where intervening directly on your
&gt; data flow (like checking and fixing the log line yourself before sending it
&gt; to ES) is something you'd like to avoid in the short term. Set your
&gt; object's enabled setting to false. This will make the fields non
&gt; searchable though. This isn't too big of an issue in my context because the
&gt; reason this field is so unpredictable is the reason I need
&gt; ignore_malformed to begin with, so it's not a particularly useful field
&gt; to search on anyways, though you still have access to the data when you
&gt; search for that document using another field.
&gt;
&gt; Hope this helps. It certainly saved *me* a lot of trouble...
&gt;
&gt; —
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/12366#issuecomment-273798499&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AGC4v4w0ZIXlOGN40nAgl_8fpy0dj2CUks5rT3rhgaJpZM4Fcpph&gt;
&gt; .
&gt;
</comment><comment author="robinjha" created="2017-01-25T19:47:00Z" id="275212887">+1</comment><comment author="senseysensor" created="2017-03-15T07:44:49Z" id="286664911">+1</comment><comment author="marcovdkuur" created="2017-03-28T08:57:31Z" id="289706747">+1</comment><comment author="jonesn" created="2017-04-04T05:14:44Z" id="291395502">+1</comment><comment author="rholloway" created="2017-04-05T13:56:59Z" id="291869369">Also an issue on ES 5.2.1. Very frustrating when dealing with some unexpected input that may possibly be malformed.</comment><comment author="hartfordfive" created="2017-04-06T15:01:46Z" id="292202409">👍 
Would definitely be great to enable the `ignore_malformed` property for object.   I've had many cases of mapping errors due to the fact that someone tried to index a string where a nested object should be and vice versa.</comment><comment author="sorinescu" created="2017-04-11T13:09:57Z" id="293255854">👍 </comment><comment author="aivis" created="2017-05-14T07:49:27Z" id="301296774">👍 </comment><comment author="EamonHetherton" created="2017-05-31T04:19:05Z" id="305078483">+1</comment><comment author="fkoclas" created="2017-06-01T15:28:13Z" id="305529410">👍 </comment><comment author="flashfm" created="2017-06-14T16:46:01Z" id="308490566">👍</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: SpanNotQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12365</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new doToQuery() method analogous to other recent query refactorings.

Relates to #10217
</description><key id="96278440">12365</key><summary>Query refactoring: SpanNotQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-07-21T10:08:51Z</created><updated>2016-03-11T11:51:09Z</updated><resolved>2015-07-22T12:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-22T07:35:02Z" id="123591578">left a few comments
</comment><comment author="cbuescher" created="2015-07-22T10:11:46Z" id="123651091">@javanna Went through your comments, no change needed other than the minor typo I will delete. Will rebase and squash, maybe this can get in before #12342 (waiting for minor question there) and I will reuse the method for test introduced when that PR is ready?
</comment><comment author="javanna" created="2015-07-22T10:12:18Z" id="123651258">sounds good @cbuescher 
</comment><comment author="cbuescher" created="2015-07-22T11:39:13Z" id="123683940">Added separate test for `dist` parameter that we don't output from the builder any longer.
</comment><comment author="javanna" created="2015-07-22T11:40:27Z" id="123684387">LGTM
</comment><comment author="javanna" created="2015-08-28T10:18:01Z" id="135727802">This change is breaking for the Java API as it removes setter for mandatory include/exclude span query clause, which needs to be set in the constructor instead.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file></files><comments><comment>Merge pull request #12365 from cbuescher/feature/query-refactoring-spannot</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file></files><comments><comment>Query refactoring: SpanNotQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Add the ability to wrap an index searcher.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12364</link><project id="" key="" /><description /><key id="96272084">12364</key><summary>Add the ability to wrap an index searcher.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T09:34:55Z</created><updated>2015-08-13T14:00:02Z</updated><resolved>2015-07-24T14:24:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-23T13:18:36Z" id="124095123">I updated the PR and added `IndexSearcherWrapper#order()` method to have control over the order of which the `IndexSearchWrapper` instances do their wrapping.

I think as a follow up issue we should implement a `IndexSearcherWrapper` that applies the alias filters if there are any. I think this will be cleaner then how do this currently via the ContextIndexSearcher. Also this will unify logic between search and percolate api with regard to applying filtered aliases.
</comment><comment author="jpountz" created="2015-07-23T14:44:01Z" id="124128757">One issue I have with this change is that the IndexSearcher class is really hard to wrap.

For instance, imagine you both need to wrap the underlying IndexReader being the searcher. Since IndexSearcher doesn't allow you to replace the wrapped reader, you need to create a new IndexSearcher around this wrapped reader. So if the original IndexSearcher was a sub-class of IndexSearcher that customized, say, how search() is implemented, then these changes will be lost. And you can't delegate to the original IndexSearcher since it is wrapping the wrong reader.

Maybe one way to work around this issue would be to wrap readers and searchers separately: first you would wpply all reader wrappers, then all searcher wrappers?
</comment><comment author="martijnvg" created="2015-07-23T16:32:22Z" id="124160797">@jpountz good point. I changed the IndexSearcherWrapper interface to wrap directory readers and searcher independently. I think this resolves your concern.
</comment><comment author="martijnvg" created="2015-07-24T12:30:45Z" id="124504528">After talking with @jpountz we decided that it is for now better to allow only one IndexSearcherWrapper to wrap the underlying IndexSearcher from the engine. The order in which IndexSearcher get wrapped is too lenient, which can lead to confusing issues between plugins. Also an custom filtered IndexSearcher implementation needs to be really careful what methods to override. For example if IndexSearcher#rewrite gets overwritten than also IndexSearcher#createNormalizedWeight needs to be overwritten.
</comment><comment author="jpountz" created="2015-07-24T13:05:12Z" id="124513808">I think this can go in now. However can you add TODOs in the code that this stuff needs to be cleaned up and that we should make IndexSearcher more wrapping-friendly?
</comment><comment author="jpountz" created="2015-07-24T13:28:00Z" id="124520514">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tribe node binding to default ports rather than specified in the configuration. elasticsearch 1.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12363</link><project id="" key="" /><description>Details as below:

Software versions
a) Elasticsearch 1.6.0
b) Logstash 1.5.1
c) Kibana 4.1.0-linux-x64

Configuration of tribe node: 
"-Des.http.port=29200"
"-Des.transport.tcp.port=29300-29349"
"-Des.network.publish_host=localhost"
"-Des.node.name=global.node1"
"-Des.tribe.hk.cluster.name=dev.hk"
"-Des.tribe.uk.cluster.name=dev.uk"
"-Des.discovery.zen.ping.unicast.hosts=localhost:29350,localhost:29351"
"-Des.node.data=false"
"-Des.node.master=false"

Configuration of node UK: 
"-Des.http.port=29250"
"-Des.transport.tcp.port=29350"
"-Des.network.publish_host=localhost"
"-Des.discovery.zen.ping.unicast.hosts=localhost:29200"
"-Des.discovery.zen.ping.multicast.enabled=false"

Configuration of node HK: 
"-Des.http.port=29251"
"-Des.transport.tcp.port=29351"
"-Des.network.publish_host=localhost"
"-Des.discovery.zen.ping.unicast.hosts=localhost:29200"
"-Des.discovery.zen.ping.multicast.enabled=false"

Issue
log of the tribe, where it is clearly visible that, global.node1/uk and global.node1/hk are trying to connect to the 9300 and 9301 port instead of the 29300 and 29301 ports as specified in the configuration files.

[2015-07-20 17:06:50,872][INFO ][node ] [global.node1] starting ...
[2015-07-20 17:06:50,973][INFO ][transport ] [global.node1] bound_address {inet[/0.0.0.0:29300]}, publish_address {inet[localhost/10.117.20.24:29300]}
[2015-07-20 17:06:50,984][INFO ][discovery ] [global.node1] elasticsearch/bUO32MNNScSAjZYHNwJIlw
[2015-07-20 17:06:50,984][WARN ][discovery ] [global.node1] waited for 0s and no initial state was set by the discovery
[2015-07-20 17:06:51,002][INFO ][http ] [global.node1] bound_address {inet[/0.0.0.0:29200]}, publish_address {inet[localhost/10.117.20.24:29200]}
[2015-07-20 17:06:51,002][INFO ][node ] [global.node1/hk] starting ...
[2015-07-20 17:06:51,046][INFO ][transport ] [global.node1/hk] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/169.254.95.119:9300]}
[2015-07-20 17:06:51,069][INFO ][discovery ] [global.node1/hk] es.hk/Ja3FpoMXTKqMs4_q7AhIpw
</description><key id="96271533">12363</key><summary>Tribe node binding to default ports rather than specified in the configuration. elasticsearch 1.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dhramveersingh</reporter><labels /><created>2015-07-21T09:32:03Z</created><updated>2015-07-22T09:30:30Z</updated><resolved>2015-07-22T09:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dhramveersingh" created="2015-07-22T09:30:30Z" id="123639824">Removing the discovery tag from both the client node does the trick for me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add HDRHistogram as an option in percentiles and percentile_ranks aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12362</link><project id="" key="" /><description>HDRHistogram has been added as an option in the percentiles and percentile_ranks aggregation. It has one option `number_significant_digits` which controls the accuracy and memory size for the algorithm

Closes #8324
</description><key id="96270185">12362</key><summary>Add HDRHistogram as an option in percentiles and percentile_ranks aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T09:25:32Z</created><updated>2015-08-07T10:07:18Z</updated><resolved>2015-07-24T16:56:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-21T11:29:06Z" id="123272993">Results from [HDRPercentilesAggregationBenchmark](https://github.com/elastic/elasticsearch/pull/12362/files#diff-f14c882e09ffe9c17b5b85aa8adc3545):

| Field | Method | Avg Aggregation Time (ms) | Avg Estimated Memory (bytes) | Avg Estimated Memory (kB) |
| --- | --- | --- | --- | --- |
| gauss | HDR | 143.7 | 74240 | 72.50 |
| gauss | TDIGEST | 1098.1 | 17540.8 | 17.13 |
| high_card | HDR | 158.3 | 172544 | 168.50 |
| high_card | TDIGEST | 1096.1 | 17477.2 | 17.07 |
| low_card | HDR | 149 | 90624 | 88.50 |
| low_card | TDIGEST | 1256.3 | 10060 | 9.82 |
</comment><comment author="kimchy" created="2015-07-21T14:18:39Z" id="123329706">this is very exciting!, quite note, I think we need to add this to our assembly to include in the distribution, also, we have the notice stuff happening as well, right @clintongormley?

I am not terribly sure why the same dependencies are defined twice, both in the main pom and the core/pom, but that is a different question I guess.
</comment><comment author="colings86" created="2015-07-21T14:22:41Z" id="123331878">@kimchy wasn't aware of the assembly stuff needing to be changed, I assumed it would just take all the non-optional dependencies, but I will make sure it gets added before this PR is merged.

What do you mean by the "notice stuff"?

The dependencies are defined in the dependencyManagement part of the parent pom so that all the sub-modules use the same version of of things. The projects then define which of the dependencies in the dependencyManagement section they need to use.
</comment><comment author="colings86" created="2015-07-22T13:09:46Z" id="123714649">@jpountz I pushed a new commit with your comments addressed and also fixing the packaging for the tar.gz and zip so the HDRHistogram library is included in the lib directory
</comment><comment author="jpountz" created="2015-07-22T17:43:33Z" id="123804310">Thanks @colings86 it looks good. I think we should beef up documentation a bit to make it clearer when HDRHistogram is a better trade-off. Also now that we have several impls, I'm wondering if we want to put impl-specific properties under a namespace, such as:

``` json
"percentiles": {
  "field": "latency",
  "t-digest": {
    "compression": 50
  }
}
```

@clintongormley Maybe we could make use of your API design powers here?
</comment><comment author="colings86" created="2015-07-24T13:34:15Z" id="124521608">@jpountz I pushed some new commits
</comment><comment author="jpountz" created="2015-07-24T15:01:21Z" id="124551698">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup TransportSingleShardAction and TransportInstanceSingleOperationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12361</link><project id="" key="" /><description>Remove custom execute local logic in TransportSingleShardAction and TransportInstanceSingleOperationAction and rely on transport service to execute locally. (forking thread etc.)
</description><key id="96261691">12361</key><summary>Cleanup TransportSingleShardAction and TransportInstanceSingleOperationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T08:46:02Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-21T12:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-21T09:51:21Z" id="123244852">LGTM. Do you dare taking care of ReplicationRequest.operationThreaded ? :) good work.
</comment><comment author="martijnvg" created="2015-07-21T10:10:55Z" id="123249705">@bleskes that is my next target :) but that is more difficult
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Issue an error when a pipeline aggs references a  muliti-bucket aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12360</link><project id="" key="" /><description>Currently we silently ignore the pipeline agg in question. Until #12336 is implemented we should report an error.

For a reproduction see:

```
GET logstash-2015.01/_search?search_type=count
{
  "aggs": {
    "time": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "day"
      },
      "aggs": {
        "test": {
          "filters": {
            "filters": {
              "get": {
                "term": {
                  "verb": "get"
                }
              },
              "post": {
                "term": {
                  "verb": "post"
                }
              }
            }
          }
        },
        "get_derive": {
          "derivative": {
            "buckets_path": "test&gt;get&gt;_count"
          }
        }
      }
    }
  }
}
```

Which outputs (not the missing `get_derive` agg):

```
            {
               "key_as_string": "1420243200000",
               "key": 1420243200000,
               "doc_count": 10664,
               "test": {
                  "buckets": {
                     "get": {
                        "doc_count": 10592
                     },
                     "post": {
                        "doc_count": 15
                     }
                  }
               }
            },
```
</description><key id="96257618">12360</key><summary>Issue an error when a pipeline aggs references a  muliti-bucket aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T08:23:00Z</created><updated>2015-08-03T10:24:32Z</updated><resolved>2015-08-03T10:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java</file></files><comments><comment>Aggregations: Full path validation for pipeline aggregations</comment></comments></commit></commits></item><item><title>Percentiles aggregation generates json field names containing dots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12359</link><project id="" key="" /><description>The percentiles aggregation can generate field names containing dots:

``` json
"aggregations": {
  "percentiles": {
    "values": {
      "99.0": 4
    }
  }
}
```

The reason why it annoys me a bit is that we started rejecting documents that contain dots in field name, which means the result of a percentiles aggregation could not be added back to an elasticsearch index.
</description><key id="96253301">12359</key><summary>Percentiles aggregation generates json field names containing dots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-07-21T07:55:37Z</created><updated>2015-07-24T09:40:27Z</updated><resolved>2015-07-24T09:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-24T09:40:27Z" id="124457163">The `percentiles` and `percentiles_rank` aggregation have a `keyed` parameter which when set to `false` will change the response format to an array containing an object for each percentile with two fields `percentile` and `value`. This can be used to get around this issue
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>dynamic field mapping plus strict dynamic mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12358</link><project id="" key="" /><description>It appears that one cannot force strict dynamic mapping while also having field dynamic mappings.

For example what if one wants to mapping that all fields that start with "s_*" are valid and should be mapped to string, but reject anything that does not start with that, such as "foobar". Consider the following mapping:

```
PUT _template/mystack_wildcard
{
  "template": "mystack*",
  "mappings": {
    "question": {
      "dynamic": "strict",
      "dynamic_templates": [
        {
          "strings": {
            "match": "s_*",
            "mapping": { "type": "string" }
          }
        }
      ]
    }
  }
}
```

I would expect the first index below to succeed but the second to fail.

Expected succeed:

```
PUT /mystack_foo/question/1
{
  "s_name": "This should work"
}
```

Expected to fail:

```
PUT /mystack_foo/question/1
{
  "foobar": "This should be ignored"
}
```

Not sure if I'm misinterpreting or if there is an issue, but seems like what was mentioned above should be possible and valid.
</description><key id="96250786">12358</key><summary>dynamic field mapping plus strict dynamic mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-21T07:39:57Z</created><updated>2017-01-05T09:54:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-21T08:31:00Z" id="123212497">I think both should fail given that we need to map some fields dynamically while `dynamic` is `strict`? We could potentially add a new option that would only allow dynamic mappings if there is an explicit matching template.

There are several other open issues that are trying to find middle grounds between `dynamic: strict` and `dynamic: true`, for instance #11443 proposes a maximum number of dynamically mapped fields, and I think I remember someone proposing to limit dynamically mapped fields to a regular expression but I can't find the issue number. Recently, we have been working on removing options from mappings so I would be careful before adding new ones. In particular, I'd like to make sure it would be useful to a significant part of the user base so that it would be worth maintaining.
</comment><comment author="djschny" created="2015-07-21T09:12:01Z" id="123227737">From an external perspective it is confusing IMO, as the user has entered a definition in the mapping that matches on fields that start with "s_*", therefore Elasticsearch is not looking at data and inferring what new field to map to. Instead the field coming in matches a user defined mapping.

I don't think a new option is needed here, but rather a correction of the behavior of the existing mappings. I hope I am articulating myself correctly.
</comment><comment author="rjernst" created="2015-07-21T09:15:39Z" id="123229689">&gt; Instead the field coming in matches a user defined mapping

No, the data matches a _template_ for a field. `s_*` is not a field name, it is a pattern to match when _creating_ a field _dynamically_. Hence, setting `dynamic: false` would mean not allowing dynamic mappings (adding fields dynamically).

I agree with @jpountz that this could be solved through an additional setting, but I am very weary of adding such a setting.
</comment><comment author="clintongormley" created="2015-07-23T09:56:15Z" id="124037963">I think a clean way to do it would be to leave the `dynamic: true` setting in place, but allow a final rule in the dynamic templates that says either: ignore this field or throw an error.
</comment><comment author="sb1977" created="2015-11-10T10:37:52Z" id="155384427">+1 for the rule to ignore a field in dynamic templates
</comment><comment author="fabiank88" created="2015-12-08T18:34:46Z" id="162973408">+1 for the ignore field in dynamic templates!
</comment><comment author="gmoskovicz" created="2016-07-08T16:09:21Z" id="231401392">+1 for this!

I think that we need a way to discard fields.

Another option is to add a new `dynamic` setting as a pattern like the Automatic Index [Creation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-creation). My proposal is that dynamic has 4 possible values:
- `false`
- `strict`
- `true`
- Or an field pattern such `s_*`

@clintongormley @djschny @rjernst @jpountz how does it sounds?
</comment><comment author="rjernst" created="2016-07-08T16:23:05Z" id="231404789">I think that adds more complication than we should deal with. We already have the ability to control dynamic on a per-field-and-subfields basis. If we were to allow a pattern based approach, then it should be a single setting at the root of the mapping, otherwise there are weird cases you can get into (what if the setting disagrees with the explicitly set dynamic in a field already existing that matches the pattern).
</comment><comment author="rjernst" created="2016-07-08T16:23:48Z" id="231404973">Note that by "single field at the root of the mapping" i mean we would need to remove the ability to set dynamic within the mappings altogether, except at the root.
</comment><comment author="gmoskovicz" created="2016-07-08T16:25:12Z" id="231405393">I see, @rjernst so maybe we need a new setting to avoid this. Currently one cannot control per level, which "dynamic" fields should be created or not. 
</comment><comment author="gmoskovicz" created="2016-07-08T16:26:26Z" id="231405824">Example:

```
POST /my_index
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "my_type": {
      "dynamic_templates": [
        {
          "item": {
            "match": "field_*", &lt;------ i want at the root level, to ONLY create fields that starts with this name
            "match_mapping_type": "string",
            "mapping": {
              "type": "string",
              "analyzer": "english"
            }
          }
        }
      ]
    }
  }
}
```
</comment><comment author="clembac" created="2017-01-05T09:54:12Z" id="270605599">hi @jpountz, do you know if this setting has been implemented in the new release ?  </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update time_zone specification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12357</link><project id="" key="" /><description /><key id="96247971">12357</key><summary>Update time_zone specification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T07:21:06Z</created><updated>2015-07-21T07:27:32Z</updated><resolved>2015-07-21T07:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-21T07:25:19Z" id="123197640">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12357 from rjernst/fix/12317</comment></comments></commit></commits></item><item><title>Remove ability to configure _index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12356</link><project id="" key="" /><description>The `_index` field is now a completely virtual field thanks
to #12027. It is no longer necessary to index the actual value
of the index name.

closes #12329
</description><key id="96244539">12356</key><summary>Remove ability to configure _index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T06:57:06Z</created><updated>2015-07-21T07:30:03Z</updated><resolved>2015-07-21T07:30:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-21T07:26:20Z" id="123197922">Woohoo! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file></files><comments><comment>Merge pull request #12356 from rjernst/fix/12329</comment></comments></commit></commits></item><item><title>"invalid internal transport message format" error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12355</link><project id="" key="" /><description>It'd be nice if we made this error a little more human readable - 
`java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)`

From what I understand it's due to someone talking to Elasticsearch via HTTP but to the binary API port - eg `curl localhost:9300` - so we should really just say something along those lines.
</description><key id="96231829">12355</key><summary>"invalid internal transport message format" error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2015-07-21T05:32:41Z</created><updated>2015-07-21T08:11:45Z</updated><resolved>2015-07-21T08:10:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-21T07:08:06Z" id="123194109">Here is where the exception comes from. As you can see we already have a best effort attempt to detect and report http traffic

```
 if (buffer.getByte(readerIndex) != 'E' || buffer.getByte(readerIndex + 1) != 'S') {
            // special handling for what is probably HTTP
            if (bufferStartsWith(buffer, readerIndex, "GET ") ||
                bufferStartsWith(buffer, readerIndex, "POST ") ||
                bufferStartsWith(buffer, readerIndex, "PUT ") ||
                bufferStartsWith(buffer, readerIndex, "HEAD ") ||
                bufferStartsWith(buffer, readerIndex, "DELETE ") ||
                bufferStartsWith(buffer, readerIndex, "OPTIONS ") ||
                bufferStartsWith(buffer, readerIndex, "PATCH ") ||
                bufferStartsWith(buffer, readerIndex, "TRACE ")) {

                throw new HttpOnTransportException("This is not a HTTP port");
            }

            // we have 6 readable bytes, show 4 (should be enough)
            throw new StreamCorruptedException("invalid internal transport message format, got ("
                    + Integer.toHexString(buffer.getByte(readerIndex) &amp; 0xFF) + ","
                    + Integer.toHexString(buffer.getByte(readerIndex + 1) &amp; 0xFF) + ","
                    + Integer.toHexString(buffer.getByte(readerIndex + 2) &amp; 0xFF) + ","
                    + Integer.toHexString(buffer.getByte(readerIndex + 3) &amp; 0xFF) + ")");
        }
```

This works for me:

```
$ curl http://127.0.0.1:9300
This is not a HTTP port
```

Can you dig some more?
</comment><comment author="markwalkom" created="2015-07-21T08:06:29Z" id="123207191">This error is from 1.4.5, I can see 1.7.0 reports this response.
When did we add this?

On 21 July 2015 at 17:08, Boaz Leskes notifications@github.com wrote:

&gt; Here is where the exception comes from. As you can see we already have a
&gt; best effort attempt to detect and report http traffic
&gt; 
&gt;  if (buffer.getByte(readerIndex) != 'E' || buffer.getByte(readerIndex + 1) != 'S') {
&gt;             // special handling for what is probably HTTP
&gt;             if (bufferStartsWith(buffer, readerIndex, "GET ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "POST ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "PUT ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "HEAD ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "DELETE ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "OPTIONS ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "PATCH ") ||
&gt;                 bufferStartsWith(buffer, readerIndex, "TRACE ")) {
&gt; 
&gt; ```
&gt;             throw new HttpOnTransportException("This is not a HTTP port");
&gt;         }
&gt; 
&gt;         // we have 6 readable bytes, show 4 (should be enough)
&gt;         throw new StreamCorruptedException("invalid internal transport message format, got ("
&gt;                 + Integer.toHexString(buffer.getByte(readerIndex) &amp; 0xFF) + ","
&gt;                 + Integer.toHexString(buffer.getByte(readerIndex + 1) &amp; 0xFF) + ","
&gt;                 + Integer.toHexString(buffer.getByte(readerIndex + 2) &amp; 0xFF) + ","
&gt;                 + Integer.toHexString(buffer.getByte(readerIndex + 3) &amp; 0xFF) + ")");
&gt;     }
&gt; ```
&gt; 
&gt; This works for me:
&gt; 
&gt; $ curl http://127.0.0.1:9300
&gt; This is not a HTTP port
&gt; 
&gt; Can you dig some more?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12355#issuecomment-123194109
&gt; .
</comment><comment author="bleskes" created="2015-07-21T08:10:17Z" id="123208300">issue demystified : https://github.com/elastic/elasticsearch/pull/10108

Closing as a duplicate then. I think it covers it well enough?
</comment><comment author="markwalkom" created="2015-07-21T08:11:45Z" id="123208541">Thanks mate.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndicesShardStoreRequestTests flaky</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12354</link><project id="" key="" /><description>Suite: org.elasticsearch.action.admin.indices.segments.IndicesShardStoreRequestTests
  1&gt; [2015-07-20 22:03:39,297][TRACE][action.admin.indices.shards] [node_t0] cluster state used to determine shards [version: 14
  1&gt; state uuid: fLv4zaosRpSgl9raZAw-2Q
  1&gt; from_diff: false
  1&gt; meta data version: 5
  1&gt; nodes: 
  1&gt;    [node_t1][AyweLoM9SDuPdgv3kTB_EQ][beast][local[462]]{mode=local, enable_custom_paths=true}
  1&gt;    [node_t2][q0NaofS5SGGHkn2DKmfKoA][beast][local[463]]{mode=local, enable_custom_paths=true}
  1&gt;    [node_t0][sKWP13YWSHyhpJ8fUgdhJw][beast][local[461]]{mode=local, enable_custom_paths=true}, local, master
  1&gt;    [node_t3][hfvftAiyS_yNlxicAGMhHQ][beast][local[464]]{mode=local, client=true, data=false, enable_custom_paths=true}
  1&gt; routing_table (version 7):
  1&gt; -- index [test]
  1&gt; ----shard_id [test][0]
  1&gt; --------[test][0], node[q0NaofS5SGGHkn2DKmfKoA], [P], v[4], s[STARTED], a[id=LRZukkcqQyKbTtaY9llqWA]
  1&gt; --------[test][0], node[AyweLoM9SDuPdgv3kTB_EQ], [R], v[4], s[STARTED], a[id=S3MYag4yRDejlaG5VjXwTw]
  1&gt; ----shard_id [test][1]
  1&gt; --------[test][1], node[AyweLoM9SDuPdgv3kTB_EQ], [P], v[4], s[STARTED], a[id=eVreOkM2S5W6rqvDubvg5A]
  1&gt; --------[test][1], node[sKWP13YWSHyhpJ8fUgdhJw], [R], v[4], s[STARTED], a[id=2hRWi2Y_QQujB4UlFRrkvw]
  1&gt; 
  1&gt; routing_nodes:
  1&gt; -----node_id[q0NaofS5SGGHkn2DKmfKoA][V]
  1&gt; --------[test][0], node[q0NaofS5SGGHkn2DKmfKoA], [P], v[4], s[STARTED], a[id=LRZukkcqQyKbTtaY9llqWA]
  1&gt; -----node_id[AyweLoM9SDuPdgv3kTB_EQ][V]
  1&gt; --------[test][1], node[AyweLoM9SDuPdgv3kTB_EQ], [P], v[4], s[STARTED], a[id=eVreOkM2S5W6rqvDubvg5A]
  1&gt; --------[test][0], node[AyweLoM9SDuPdgv3kTB_EQ], [R], v[4], s[STARTED], a[id=S3MYag4yRDejlaG5VjXwTw]
  1&gt; -----node_id[sKWP13YWSHyhpJ8fUgdhJw][V]
  1&gt; --------[test][1], node[sKWP13YWSHyhpJ8fUgdhJw], [R], v[4], s[STARTED], a[id=2hRWi2Y_QQujB4UlFRrkvw]
  1&gt; ---- unassigned
  1&gt; ]
  1&gt; [2015-07-20 22:03:39,299][TRACE][action.admin.indices.shards] [node_t0] cluster state used to determine shards [version: 14
  1&gt; state uuid: fLv4zaosRpSgl9raZAw-2Q
  1&gt; from_diff: false
  1&gt; meta data version: 5
  1&gt; nodes: 
  1&gt;    [node_t1][AyweLoM9SDuPdgv3kTB_EQ][beast][local[462]]{mode=local, enable_custom_paths=true}
  1&gt;    [node_t2][q0NaofS5SGGHkn2DKmfKoA][beast][local[463]]{mode=local, enable_custom_paths=true}
  1&gt;    [node_t0][sKWP13YWSHyhpJ8fUgdhJw][beast][local[461]]{mode=local, enable_custom_paths=true}, local, master
  1&gt;    [node_t3][hfvftAiyS_yNlxicAGMhHQ][beast][local[464]]{mode=local, client=true, data=false, enable_custom_paths=true}
  1&gt; routing_table (version 7):
  1&gt; -- index [test]
  1&gt; ----shard_id [test][0]
  1&gt; --------[test][0], node[q0NaofS5SGGHkn2DKmfKoA], [P], v[4], s[STARTED], a[id=LRZukkcqQyKbTtaY9llqWA]
  1&gt; --------[test][0], node[AyweLoM9SDuPdgv3kTB_EQ], [R], v[4], s[STARTED], a[id=S3MYag4yRDejlaG5VjXwTw]
  1&gt; ----shard_id [test][1]
  1&gt; --------[test][1], node[AyweLoM9SDuPdgv3kTB_EQ], [P], v[4], s[STARTED], a[id=eVreOkM2S5W6rqvDubvg5A]
  1&gt; --------[test][1], node[sKWP13YWSHyhpJ8fUgdhJw], [R], v[4], s[STARTED], a[id=2hRWi2Y_QQujB4UlFRrkvw]
  1&gt; 
  1&gt; routing_nodes:
  1&gt; -----node_id[q0NaofS5SGGHkn2DKmfKoA][V]
  1&gt; --------[test][0], node[q0NaofS5SGGHkn2DKmfKoA], [P], v[4], s[STARTED], a[id=LRZukkcqQyKbTtaY9llqWA]
  1&gt; -----node_id[AyweLoM9SDuPdgv3kTB_EQ][V]
  1&gt; --------[test][1], node[AyweLoM9SDuPdgv3kTB_EQ], [P], v[4], s[STARTED], a[id=eVreOkM2S5W6rqvDubvg5A]
  1&gt; --------[test][0], node[AyweLoM9SDuPdgv3kTB_EQ], [R], v[4], s[STARTED], a[id=S3MYag4yRDejlaG5VjXwTw]
  1&gt; -----node_id[sKWP13YWSHyhpJ8fUgdhJw][V]
  1&gt; --------[test][1], node[sKWP13YWSHyhpJ8fUgdhJw], [R], v[4], s[STARTED], a[id=2hRWi2Y_QQujB4UlFRrkvw]
  1&gt; ---- unassigned
  1&gt; ]
  1&gt; [2015-07-20 22:03:39,303][TRACE][action.admin.indices.shards] [node_t0] [test][0] fetching [shard_stores] from [sKWP13YWSHyhpJ8fUgdhJw, AyweLoM9SDuPdgv3kTB_EQ, q0NaofS5SGGHkn2DKmfKoA]
  1&gt; [2015-07-20 22:03:39,304][TRACE][action.admin.indices.shards] [node_t0] [test][1] fetching [shard_stores] from [q0NaofS5SGGHkn2DKmfKoA, AyweLoM9SDuPdgv3kTB_EQ, sKWP13YWSHyhpJ8fUgdhJw]
  1&gt; [2015-07-20 22:03:39,350][TRACE][action.admin.indices.shards] [node_t0] cluster state used to determine shards [version: 16
  1&gt; state uuid: 46odQ1B4R0CbA_UCxM708Q
  1&gt; from_diff: false
  1&gt; meta data version: 6
  1&gt; nodes: 
  1&gt;    [node_t2][q0NaofS5SGGHkn2DKmfKoA][beast][local[463]]{mode=local, enable_custom_paths=true}
  1&gt;    [node_t0][sKWP13YWSHyhpJ8fUgdhJw][beast][local[461]]{mode=local, enable_custom_paths=true}, local, master
  1&gt;    [node_t3][hfvftAiyS_yNlxicAGMhHQ][beast][local[464]]{mode=local, client=true, data=false, enable_custom_paths=true}
  1&gt; routing_table (version 9):
  1&gt; -- index [test]
  1&gt; ----shard_id [test][0]
  1&gt; --------[test][0], node[q0NaofS5SGGHkn2DKmfKoA], [P], v[5], s[STARTED], a[id=LRZukkcqQyKbTtaY9llqWA]
  1&gt; --------[test][0], node[null], [R], v[5], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-07-21T02:03:39.340Z], details[node_left[AyweLoM9SDuPdgv3kTB_EQ]]]
  1&gt; ----shard_id [test][1]
  1&gt; --------[test][1], node[sKWP13YWSHyhpJ8fUgdhJw], [P], v[6], s[STARTED], a[id=2hRWi2Y_QQujB4UlFRrkvw]
  1&gt; --------[test][1], node[null], [R], v[6], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-07-21T02:03:39.340Z], details[node_left[AyweLoM9SDuPdgv3kTB_EQ]]]
  1&gt; 
  1&gt; routing_nodes:
  1&gt; -----node_id[q0NaofS5SGGHkn2DKmfKoA][V]
  1&gt; --------[test][0], node[q0NaofS5SGGHkn2DKmfKoA], [P], v[5], s[STARTED], a[id=LRZukkcqQyKbTtaY9llqWA]
  1&gt; -----node_id[sKWP13YWSHyhpJ8fUgdhJw][V]
  1&gt; --------[test][1], node[sKWP13YWSHyhpJ8fUgdhJw], [P], v[6], s[STARTED], a[id=2hRWi2Y_QQujB4UlFRrkvw]
  1&gt; ---- unassigned
  1&gt; --------[test][1], node[null], [R], v[6], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-07-21T02:03:39.340Z], details[node_left[AyweLoM9SDuPdgv3kTB_EQ]]]
  1&gt; --------[test][0], node[null], [R], v[5], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-07-21T02:03:39.340Z], details[node_left[AyweLoM9SDuPdgv3kTB_EQ]]]
  1&gt; ]
  1&gt; [2015-07-20 22:03:39,350][TRACE][action.admin.indices.shards] [node_t0] [test][0] fetching [shard_stores] from [q0NaofS5SGGHkn2DKmfKoA, sKWP13YWSHyhpJ8fUgdhJw]
  1&gt; [2015-07-20 22:03:39,350][TRACE][action.admin.indices.shards] [node_t0] [test][1] fetching [shard_stores] from [q0NaofS5SGGHkn2DKmfKoA, sKWP13YWSHyhpJ8fUgdhJw]
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=989A264CC8E20C32 -Dtests.class=org.elasticsearch.action.admin.indices.segments.IndicesShardStoreRequestTests -Dtests.method="testBasic" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=ja_JP -Dtests.timezone=Pacific/Easter
FAILURE 0.97s J1 | IndicesShardStoreRequestTests.testBasic &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: 
&gt; Expected: &lt;0&gt;
&gt;      but: was &lt;2&gt;
&gt;    at __randomizedtesting.SeedInfo.seed([989A264CC8E20C32:33603B59173E8A1C]:0)
&gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
&gt;    at org.elasticsearch.action.admin.indices.segments.IndicesShardStoreRequestTests.testBasic(IndicesShardStoreRequestTests.java:100)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   2&gt; NOTE: leaving temporary files on disk at: /home/rmuir/workspace/elasticsearch/core/target/J1/temp/org.elasticsearch.action.admin.indices.segments.IndicesShardStoreRequestTests_989A264CC8E20C32-001
&gt;   2&gt; NOTE: test params are: codec=Asserting(Lucene50): {_size=PostingsFormat(name=Asserting), field=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting)}, docValues:{_version=DocValuesFormat(name=Asserting)}, sim=RandomSimilarityProvider(queryNorm=true,coord=yes): {}, locale=ja_JP, timezone=Pacific/Easter
&gt;   2&gt; NOTE: Linux 3.19.0-15-generic amd64/Oracle Corporation 1.8.0_45 (64-bit)/cpus=8,threads=1,free=258551784,total=527433728
&gt;   2&gt; NOTE: All tests run in this JVM: [FieldDataFilterIntegrationTests, IndexTemplateBlocksTests, JsonFilteringGeneratorTests, DynamicMappingTests, SimpleBlocksTests, SingleShardNoReplicasRoutingTests, MultiTermVectorsTests, AliasRoutingTests, SimpleCountTests, CommitStatsTests, DateBackwardsCompatibilityTests, GroovySecurityTests, ExplainableScriptTests, NaNSortingTests, BooleansTests, TimestampMappingTests, PluginsInfoTest, SimpleNodesInfoTests, ASCIIFoldingTokenFilterFactoryTests, RecoveriesCollectionTests, ShapeBuilderTests, MinDocCountTests, StopAnalyzerTests, SearchSourceBuilderTest, ClusterSettingsTests, PrimaryNotRelocatedWhileBeingRecoveredTests, MemoryCircuitBreakerTests, CircuitBreakerServiceTests, NetworkPartitionTests, HeadersAndContextCopyClientTests, DisabledFieldDataFormatTests, ExplainActionTests, FieldDataLoadingTests, ExistsMissingTests, ClusterBlockTests, ToAndFromJsonMetaDataTests, SimpleStringMappingTests, SnapshotBackwardsCompatibilityTest, SitePluginRelativePathConfigTests, NettyTransportMultiPortIntegrationTests, BlendedTermQueryTest, ShardsAllocatorModuleTests, GeoJSONShapeParserTests, FilterFieldDataTest, HotThreadsTest, WildcardExpressionResolverTests, PutWarmerRequestTests, CopyToMapperTests, ParentFieldLoadingTest, TransportClientTests, SuggestSearchTests, UpdateTests, SimpleNettyTransportTests, StartedShardsRoutingTests, ResourceWatcherServiceTests, DistanceUnitTests, CardinalityTests, SimpleAllMapperTests, IndexRequestBuilderTests, SettingsFilterTests, ExceptionRetryTests, CodecTests, CumulativeSumTests, CustomPostingsHighlighterTests, UpdateMappingIntegrationTests, IndexFieldTypeTests, ShardUtilsTests, SimpleValidateQueryTests, IndicesServiceTest, PreBuiltAnalyzerIntegrationTests, YamlFilteringGeneratorTests, ChildrenQueryTests, TribeUnitTests, AggregationsBinaryTests, AnalysisFactoryTests, BytesRestResponseTests, MustacheTest, StatsTests, HppcMapsTests, GeoShapeQueryBuilderTests, DuelScrollTests, SearchStatsUnitTests, BlobStoreTest, IndicesLifecycleListenerSingleNodeTests, GeoPointFieldTypeTests, LimitTokenCountFilterFactoryTests, MultiFieldTests, BytesStreamsTests, SignificantTermsBackwardCompatibilityTests, TransportTwoNodesSearchTests, BucketSelectorTests, Base64Test, SimpleMultiSearchTests, SimpleLuceneTests, HunspellServiceTests, SizeValueTests, DoubleNestedSortingTests, BigArraysTests, SearchScrollTests, SimpleRecoveryTests, UpdateByNativeScriptTests, UpdateNumberOfReplicasTests, IndexQueryParserFilterDateRangeTimezoneTests, PagedBytesStringFieldDataTests, PluginManagerTests, StemmerTokenFilterFactoryTests, IndexedScriptTests, FreqTermsEnumTests, WriteConsistencyLevelTests, AliasedIndexDocumentActionsTests, SearchServiceTests, NettyTransportTests, IdMappingTests, ScriptModesTests, Rest1Tests, UpgradeTest, Rest4Tests, NestedAggregatorTest, SimpleGetMappingsTests, CreateIndexRequestBuilderTest, InternalEngineIntegrationTest, TemplateQueryBuilderTest, TransportMessageTests, ChannelsTests, DedicatedClusterSnapshotRestoreTests, MoreLikeThisQueryTests, CancellableThreadsTest, BulkProcessorTests, RandomAllocationDeciderTests, FilterTests, RoundingTests, StoredNumericValuesTest, LongNestedSortingTests, MinBucketTests, AnalysisModuleTests, StartRecoveryRequestTest, DiscoveryWithServiceDisruptionsTests, IndexLifecycleActionTests, ItemSerializationTests, CircuitBreakerUnitTests, ConcurrentDynamicTemplateTests, MetaDataWriteDataNodesTests, IndexShardTests, ShardInfoTests, IndexRequestBuilderTest, PercolatorFacetsAndAggregationsTests, GeoDistanceTests, RecoveryWithUnsupportedIndicesTests, SyncedFlushUnitTests, ChildrenTests, CborXContentParserTests, UpdateSettingsTests, GeoLocationContextMappingTest, TemplateQueryTest, InnerHitsTests, OpenCloseIndexTests, SuggestStatsTests, TransportIndexFailuresTest, IndicesShardStoreRequestTests]
</description><key id="96208949">12354</key><summary>IndicesShardStoreRequestTests flaky</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-21T02:07:48Z</created><updated>2015-07-21T03:59:18Z</updated><resolved>2015-07-21T03:59:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-07-21T02:24:15Z" id="123131013">I will take a look
</comment><comment author="areek" created="2015-07-21T03:59:18Z" id="123146328">Test bug: after killing a data node, we have to wait till yellow state to ensure that the cluster state reflects the unassigned shards. pushed a fix https://github.com/elastic/elasticsearch/commit/ae750bca699495b93a1ecb46cc487de96bee1150 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Load scripts from subdirectory of config/scripts - compiles, but not found on execution.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12353</link><project id="" key="" /><description>I've placed in my config/scripts directory a symlink to another directory containing a script. It appears that elasticsearch found and compiled the script, as it states "compiling script file," in the elasticsearch.log.

When I try to access it in a bulk update, I get "ElasticsearchIllegalArgumentException[Unable to find on disk script ..."
</description><key id="96203369">12353</key><summary>Load scripts from subdirectory of config/scripts - compiles, but not found on execution.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">twigbranch</reporter><labels><label>feedback_needed</label></labels><created>2015-07-21T01:14:12Z</created><updated>2015-07-22T21:39:10Z</updated><resolved>2015-07-22T21:39:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-22T07:14:56Z" id="123586443">Please stick to asking these types of questions on the http://discuss.elastic.co forums. This issues list is used for bug and new feature tracking. Also, posting in multiple places can often lead to separate disjointed conversation which can be hard to follow not only for the people involved now but also for people trying to answer the same question later.
</comment><comment author="twigbranch" created="2015-07-22T07:26:41Z" id="123589558">This appears to be a bug. it says the script compiled, but it won't execute.  

if it isn't a bug then the documentation needs to be fixed.

Please reopen.
</comment><comment author="colings86" created="2015-07-22T08:25:50Z" id="123613900">Apologies, you also have this raised in the discuss forums (https://discuss.elastic.co/t/script-file-in-symlinked-subdirectory-of-config-scripts-not-executing/26016) as a question and I was trying to keep the conversation in one place so it can be followed easily. The error you quoted above is truncated, could you paste the whole error message (ideally the whole response for the single index operation in the bulk) here as well as the corresponding part of the bulk request for that index operation?
</comment><comment author="twigbranch" created="2015-07-22T19:12:19Z" id="123830611">Thanks- my apologies, I had not seen any response to that post (and had checked it a few minutes prior to finding the closure).  Here is the request response for a single 'update' (it is the same error for a bulk update):

```
/myapp/release_path/config/elasticsearch/scripts/ includes:
myscript_name.groovy
```

```
ln -nfs /myapp/release_path/config/elasticsearch/scripts /etc/elasticsearch/scripts/myapp
```

Elasticsearch.log:

```
compiling script file [/etc/elasticsearch/scripts/myapp/myscript_name.groovy]
```

Request:

```
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
     "script" : {
          "lang": "groovy",
          "script_file": "myscript_name",
          "params": {
               operation: false,
               new_doc: true
          }
     }
}'
```

Response:

```
Elasticsearch::Transport::Transport::Errors::BadRequest: [400] {"error":"ElasticsearchIllegalArgumentException[failed to execute script]; nested: ElasticsearchIllegalArgumentException[Unable to find on disk script myscript_name]; ","status":400}
```
</comment><comment author="twigbranch" created="2015-07-22T19:38:31Z" id="123839343">I also tried physically copying the script into a subdirectory of /etc/elasticsearch/scripts.  As above, elasticsearch claims to find and compile the script, but upon attempt to execute, returns the 'unable to find' error.

I also tried symlinking to a script file in /etc/elasticsearch/scripts (as opposed to symlinking the directory), and I do not see a "compiling script file" in the log for that symlinked file.

So, it appears, elasticsearch finds and compiles files in a symlinked subdirectory, but it isn't clear how to access and execute them.  And, elasticsearch does not find and/or compile symlinked files in the config/scripts directory itself.
</comment><comment author="colings86" created="2015-07-22T21:39:10Z" id="123874554">I have replied on the discourse thread above. In order to keep the conversation in one place I am goijng to close this issue. If this doesn't turn out to be a bug I will either reopen it or raise a new issue for the bug.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove index name from mapping parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12352</link><project id="" key="" /><description>The index name was passed along through many levels of mapping parsing,
just so that it could be used for _index. However, the index name
is really metadata that should exist alongside things like type and
id in SourceToParse.

This change moves index name to SourceToParse, and eliminates it from the
DocumentMapperParser.
</description><key id="96200123">12352</key><summary>Remove index name from mapping parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T00:53:05Z</created><updated>2015-07-23T09:41:49Z</updated><resolved>2015-07-21T07:31:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-21T07:29:44Z" id="123198667">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/SourceToParse.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/boost/CustomBoostMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/boost/FieldLevelBoostTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/camelcase/CamelCaseFieldNameTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/compound/CompoundTypesTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/SimpleExternalMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ip/SimpleIpMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTest.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/nested/NestedMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/object/NullValueObjectMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/object/SimpleObjectMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseDocumentTypeLevelsTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #12352 from rjernst/fix/index-name-in-mapper-service</comment></comments></commit></commits></item><item><title>Remove Environment.homeFile()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12351</link><project id="" key="" /><description>Today we grant read+write+delete access to any files underneath the home.
But we have to remove this, if we want to have improved security of files
underneath elasticsearch.
</description><key id="96197449">12351</key><summary>Remove Environment.homeFile()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-21T00:30:07Z</created><updated>2015-08-03T17:21:22Z</updated><resolved>2015-08-03T17:21:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-21T00:55:35Z" id="123113909">LGTM
</comment><comment author="bleskes" created="2015-07-21T06:54:33Z" id="123191215">LGTM2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file></files><comments><comment>Merge pull request #12351 from rmuir/perms</comment></comments></commit></commits></item><item><title>Remove TransportSingleCustomOperationAction in favour of TransportSingleShardAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12350</link><project id="" key="" /><description>The TransportSingleCustomOperationAction `prefer_local` option has been removed as it isn't worth the effort.

The TransportSingleShardAction will execute the operation on the receiving node if a concrete list doesn't provide a list of candite shards routings to perform the operation on.
</description><key id="96178969">12350</key><summary>Remove TransportSingleCustomOperationAction in favour of TransportSingleShardAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-20T22:36:29Z</created><updated>2015-07-23T14:52:00Z</updated><resolved>2015-07-23T14:51:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-21T07:27:15Z" id="123198193">I like the clean up initiative. Left a question about the approach... 
</comment><comment author="martijnvg" created="2015-07-21T12:01:39Z" id="123283816">@bleskes I left an answer.
</comment><comment author="bleskes" created="2015-07-21T13:55:27Z" id="123314874">Discussed this with @martijnvg and we decided to fold this class into TransportSingleShardAction…

&gt; On 21 Jul 2015, at 14:01, Martijn van Groningen notifications@github.com wrote:
&gt; 
&gt; @bleskes I left an answer.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="martijnvg" created="2015-07-21T14:46:43Z" id="123351669">@bleskes I updated the PR
</comment><comment author="bleskes" created="2015-07-23T11:50:22Z" id="124070322">I think this is indeed a better approach. One base class less to maintain. Left some comments...
</comment><comment author="martijnvg" created="2015-07-23T12:52:40Z" id="124085261">@bleskes Thanks for looking at this. I updated the PR.
</comment><comment author="bleskes" created="2015-07-23T13:51:39Z" id="124112837">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>path.repo not being recognized?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12349</link><project id="" key="" /><description>So today I wanted to backup our teams kibana index because it was wiped over the weekend and we wanted to make sure we could recover from this if it ever happened again. I went onto the snapshot and restore doc (https://www.elastic.co/guide/en/elasticsearch/reference/1.7/modules-snapshots.html) on the elasticsearch website for version 1.7 and tried to follow the instructions but have been failing to get a file repo setup.

Specs: elasticsearch 1.7.0, java 8, red hat enterprise linux.

After setting the field ( path.repo: ["opt/velocity/backups"]  ) in elasticsearch.yml I do this curl command:

curl -XPUT http://localhost:9200/_snapshot/backups -d '{ "type": "fs", "settings": { "location": "/opt/velocity/backups" } }'

This results in a RemoteTransportException as so: 

{"error":"RemoteTransportException[[Ectokid][inet[/169.176.42.123:9300]][cluster:admin/repository/put]]; nested: RepositoryException[[backups] failed to create repository]; nested: CreationException[Guice creation errors:\n\n1) Error injecting constructor, org.elasticsearch.repositories.RepositoryException: [backups] location [/opt/velocity/backups] doesn't match any of the locations specified by path.repo because this setting is empty\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;()\n  at org.elasticsearch.repositories.fs.FsRepository\n  at Key[type=org.elasticsearch.repositories.Repository, annotation=[none]]\n\n1 error]; nested: RepositoryException[[backups] location [/opt/velocity/backups] doesn't match any of the locations specified by path.repo because this setting is empty]; ","status":500}{"error":"RemoteTransportException[[Ectokid][inet[/169.176.42.123:9300]][cluster:admin/repository/put]]; nested: RepositoryException[[backups] failed to create repository]; nested: CreationException[Guice creation errors:\n\n1) Error injecting constructor, org.elasticsearch.repositories.RepositoryException: [backups] location [/opt/velocity/backups] doesn't match any of the locations specified by path.repo because this setting is empty\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;()\n  at org.elasticsearch.repositories.fs.FsRepository\n  at Key[type=org.elasticsearch.repositories.Repository, annotation=[none]]\n\n1 error]; nested: RepositoryException[[backups] location [/opt/velocity/backups] doesn't match any of the locations specified by path.repo because this setting is empty]

It seems as though elasticsearch is not recognizing the path.repo field I setup in elasticsearch.yml. Basically now I am stuck because I believe I have correctly setup my config. I know the path is exists.

I tried playing around with the syntax a little bit like changing location to be "backups" and changing path.repo to be "opt/velocity/backups" (without the bracket) but nothing I have tried works. 

I think I read somewhere online that a person was able to set the path.repo field by command line but I don't see any good documentation mentioning this on the elasticsearch docs.

Is there something I am missing here? I hope my issue is a simple config or syntax thing which can be easily fixed.
</description><key id="96144815">12349</key><summary>path.repo not being recognized?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CoryBond</reporter><labels><label>non-issue</label></labels><created>2015-07-20T19:52:28Z</created><updated>2015-07-21T18:31:42Z</updated><resolved>2015-07-21T18:31:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-20T20:22:21Z" id="123020338">Did you add the `path.repo` setting to all data and master nodes and did you restart these nodes after making the change?
</comment><comment author="CoryBond" created="2015-07-20T20:53:57Z" id="123033962">I only have a local node. I used the shutdown api and then started up elasticsearch. I still see the issue however.

Also thanks for helping me out.
</comment><comment author="imotov" created="2015-07-20T21:18:57Z" id="123043038">Oh, sorry, I didn't notice when I first looked at the ticket. It should be 

```
path.repo: ["/opt/velocity/backups"]
```

Please, notice the leading `/` before `opt`
</comment><comment author="CoryBond" created="2015-07-20T21:50:34Z" id="123055784">I have tried adding the leading / but that doesn't work either. I still get the same error.
</comment><comment author="imotov" created="2015-07-20T21:56:52Z" id="123058712">Can you try one more time adding leading "/" to both `elasticsearch.yml` and `location` in the register repository request, restart the node and if you still get an error, please post here the error that you are getting, the log message that is getting generated if any? Please, make sure that there are no spaces before `path` in `elasticsearch.yml`
</comment><comment author="CoryBond" created="2015-07-21T13:32:41Z" id="123304770">Im sorry imotov but I just tried adding "/" to both the location in the curl command and to the elasticsearch.yml. Neither successfully work and it looks like the same message pops up again:

[velo@ny4ufxvel16 bin]$ curl -XPUT http://localhost:9200/_snapshot/backups -d '{ "type": "fs", "settings": { "location": "/opt/velocity/backups" } }'
{"error":"RemoteTransportException[[Ectokid][inet[/169.176.42.123:9300]][cluster:admin/repository/put]]; nested: RepositoryException[[backups] failed to create repository]; nested: CreationException[Guice creation errors:\n\n1) Error injecting constructor, org.elasticsearch.repositories.RepositoryException: [backups] location [/opt/velocity/backups] doesn't match any of the locations specified by path.repo because this setting is empty\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;()\n  at org.elasticsearch.repositories.fs.FsRepository\n  at Key[type=org.elasticsearch.repositories.Repository, annotation=[none]]\n\n1 error]; nested: RepositoryException[[backups] location [/opt/velocity/backups] doesn't match any of the locations specified by path.repo because this setting is empty]; ","status":500}

I am looking through the log files after executing the curl command but it seems nothing is being written to them. ls -lrt shows no changes to log sizes and looking at each log file individually shows they were only modified a few minutes before executing the curl command.

I am even tried putting path.repo at the top of the elasticsearch.yml file to see if that helps. I know the elasticsearch.yml is being loaded because one of the configurations in it (script.groovy.sandbox.enabled: true) is allowing us to use scripted fields in kibana.
</comment><comment author="imotov" created="2015-07-21T13:40:03Z" id="123308151">OK, it looks like the setting is not getting picked up from the `elaticsearch.yml`. Could you run `curl "localhost:9200/_nodes/settings?pretty"` to see where paths are pointing to make sure you are changing the right config file and could you send this config file to me? I tested it with a fresh installation of 1.7.0 and everything seems to work fine.
</comment><comment author="CoryBond" created="2015-07-21T13:49:20Z" id="123312901">First, let me post what I got back:

[velo@ny4ufxvel16 config]$ curl "localhost:9200/_nodes/settings?pretty"
{
  "cluster_name" : "elasticsearch",
  "nodes" : {
    "Ill8ykIfQGacAvtZr43flA" : {
      "name" : "logstash-ny4ufxvel16.dev.sti-24805-13246",
      "transport_address" : "inet[/10.99.92.147:9301]",
      "host" : "ny4ufxvel16.dev.sti",
      "ip" : "10.99.92.147",
      "version" : "1.5.1",
      "build" : "5e38401",
      "attributes" : {
        "data" : "false",
        "client" : "true"
      },
      "settings" : {
        "node" : {
          "name" : "logstash-ny4ufxvel16.dev.sti-24805-13246",
          "client" : "true"
        },
        "cluster" : {
          "name" : "elasticsearch"
        },
        "path" : {
          "logs" : "/opt/velocity/logstash-1.5.1/bin/logs"
        },
        "discovery" : {
          "zen" : {
            "ping" : {
              "multicast" : {
                "enabled" : "false"
              },
              "unicast" : {
                "hosts" : "localhost:9300,localhost:9301,localhost:9302,localhost:9303,localhost:9304,localhost:9305"
              }
            }
          }
        },
        "name" : "logstash-ny4ufxvel16.dev.sti-24805-13246",
        "http" : {
          "enabled" : "false"
        },
        "client" : {
          "type" : "node",
          "transport" : {
            "sniff" : "false"
          }
        }
      }
    },
    "YGrmZ3TgQG2JWjsjq1CTdA" : {
      "name" : "Ectokid",
      "transport_address" : "inet[/169.176.42.123:9300]",
      "host" : "ny4dlsor01.dev.sti",
      "ip" : "169.176.42.123",
      "version" : "1.6.0",
      "build" : "cdd3ac4",
      "http_address" : "inet[/169.176.42.123:9200]",
      "settings" : {
        "name" : "Ectokid",
        "path" : {
          "logs" : "/export/tb76300/elk/elasticsearch-1.6.0/logs",
          "home" : "/export/tb76300/elk/elasticsearch-1.6.0"
        },
        "cluster" : {
          "name" : "elasticsearch"
        },
        "client" : {
          "type" : "node"
        },
        "foreground" : "yes",
        "config" : {
          "ignore_system_properties" : "true"
        }
      }
    },
    "sBNtWMIrRbSyHEB9WHcmUA" : {
      "name" : "Velocity Server Logs",
      "transport_address" : "inet[/10.99.92.147:9300]",
      "host" : "ny4ufxvel16.dev.sti",
      "ip" : "10.99.92.147",
      "version" : "1.7.0",
      "build" : "929b973",
      "http_address" : "inet[/10.99.92.147:9200]",
      "settings" : {
        "path" : {
          "logs" : "/opt/velocity/elasticsearch/logs",
          "home" : "/opt/velocity/elasticsearch",
          "repo" : [ "/opt/velocity/backups" ]
        },
        "node" : {
          "name" : "Velocity Server Logs",
          "data" : "true"
        },
        "cluster" : {
          "name" : "elasticsearch"
        },
        "name" : "Velocity Server Logs",
        "http" : {
          "cors" : {
            "allow-origin" : "/.*/",
            "enabled" : "true"
          }
        },
        "index" : {
          "number_of_shards" : "5",
          "number_of_replicas" : "1"
        },
        "client" : {
          "type" : "node"
        },
        "foreground" : "yes",
        "config" : {
          "ignore_system_properties" : "true"
        },
        "script" : {
          "groovy" : {
            "sandbox" : {
              "enabled" : "true"
            }
          }
        }
      }
    },
    "AOaEZ9uHSJu9iI1WxQn2jw" : {
      "name" : "logstash-ny4dlsor01.dev.sti-5862-13484",
      "transport_address" : "inet[/169.176.42.123:9301]",
      "host" : "ny4dlsor01.dev.sti",
      "ip" : "169.176.42.123",
      "version" : "1.5.1",
      "build" : "5e38401",
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "settings" : {
        "node" : {
          "name" : "logstash-ny4dlsor01.dev.sti-5862-13484",
          "client" : "true"
        },
        "cluster" : {
          "name" : "elasticsearch"
        },
        "path" : {
          "logs" : "/export/tb76300/elk/logs"
        },
        "discovery" : {
          "zen" : {
            "ping" : {
              "multicast" : {
                "enabled" : "false"
              },
              "unicast" : {
                "hosts" : "localhost:9300,localhost:9301,localhost:9302,localhost:9303,localhost:9304,localhost:9305"
              }
            }
          }
        },
        "name" : "logstash-ny4dlsor01.dev.sti-5862-13484",
        "http" : {
          "enabled" : "false"
        },
        "client" : {
          "type" : "node",
          "transport" : {
            "sniff" : "false"
          }
        }
      }
    }
  }
}
</comment><comment author="CoryBond" created="2015-07-21T13:52:49Z" id="123313832">I see this in the file "repo" : [ "/opt/velocity/backups" ]
</comment><comment author="CoryBond" created="2015-07-21T13:59:22Z" id="123316591">When I try to perform this curl command though I get this:

[velo@ny4ufxvel16 config]$ curl -XPUT localhost:9200/_snapshot/backups/snapshot_1 -d '{ "indices": ".kibana", "include_global_state": false}'
{"error":"RemoteTransportException[[Ectokid][inet[/169.176.42.123:9300]][cluster:admin/snapshot/create]]; nested: RepositoryMissingException[[backups] missing]; ","status":404}

It claims the repo doesn't exist...
</comment><comment author="imotov" created="2015-07-21T14:00:12Z" id="123317166">You need to add this setting to all data and master nodes. It looks like you didn't add it to `Ectokid`, `169.176.42.123:9200`.
</comment><comment author="CoryBond" created="2015-07-21T14:28:58Z" id="123337142">Im going to sound ignorant by asking but how do I add this setting to Ectokid?... If I may also ask, what is the purpose of the Ectokid node? The only node I familiar in this list is the Velocity Server Logs because that is the one that has my data and is configured in my elasticsearch.yml.
</comment><comment author="imotov" created="2015-07-21T14:47:22Z" id="123352361">It depends on how you run this server. Is it stand-alone server or it runs embedded somewhere? If it's standalone service, you can modify its config file that can be found in `/export/tb76300/elk/elasticsearch-1.6.0/config/elasticsearch.yml`. If it's embedded, it really depends on the application it's embedded into. 
</comment><comment author="CoryBond" created="2015-07-21T15:01:54Z" id="123360929">Right so I am a little confused about this. We don't seem to have a directory called /export/ anywhere in our file-system. I tried  "find  . -type d -name "export" -ls" and all I am getting is permission denied for some directories.

Also tried "find . ! -readable -prune -type d -name "elk" -ls" but I don't get anything
</comment><comment author="imotov" created="2015-07-21T15:10:06Z" id="123364815">Try running `ps -aef | grep java` on this machine. If it runs embedded elasticsearch it has to run jvm somehow. 
</comment><comment author="CoryBond" created="2015-07-21T15:20:14Z" id="123368011">I also greped over elasticsearch.

[velo@ny4ufxvel16 bin]$ ps -aef | grep java | grep elasticsearch
velo     19456     1 80 11:18 pts/6    00:00:32 /usr/java/1.8.0_31l64/bin/java -Xms24g -Xmx24g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/opt/velocity/elasticsearch -cp :/opt/velocity/elasticsearch/lib/elasticsearch-1.7.0.jar:/opt/velocity/elasticsearch/lib/_:/opt/velocity/elasticsearch/lib/sigar/_ org.elasticsearch.bootstrap.Elasticsearch
</comment><comment author="imotov" created="2015-07-21T15:26:20Z" id="123369677">You need to run these commands on `169.176.42.123` where the misconfigured server is located, not on 10.99.92.147, where you already configured it properly. 
</comment><comment author="CoryBond" created="2015-07-21T15:56:46Z" id="123383498">Hey Thanks Imotov I got the snapshot and recovery api to work. I now see that the reason for my problems is because my ES was in using the default multicast mode rather then unicast mode. It was picking up all sorts of nodes across our network that weren't related to my teams work.

I looked over this documentation here: https://www.elastic.co/guide/en/elasticsearch/guide/current/_important_configuration_changes.html and see that changing the modes this is a necessary configuration that our own ES needs.
</comment><comment author="imotov" created="2015-07-21T18:31:27Z" id="123429224">Sounds like I can close this issue then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Custom default mapping partially overwritten by ES </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12348</link><project id="" key="" /><description>I am using mapping _.json_ files in ES 1.7.0 applied to document types to modify the default _dynamic_ mapping for _string_ data so as **not** to store the _raw_ field by using the definition _"fields" : {}_

I am finding, by querying the mapping, that although ES implements the mapping on the whole, it is redefining _raw_ for string defaults.

Is this an issue or a feature? If a feature, how do I turn off _raw_ for dynamic string fields?

Here is the _dynamic_templates_ definition part put into ES for _message_ and for _strings_, both with raw turned off:

```
        "dynamic_templates" : [ {
          "message_field" : {
            "mapping" : {
                "index" : "not_analyzed",
                "omit_norms" : true,
                "type" : "string",
                "fields" : {}
            },
            "match" : "message",
            "match_mapping_type" : "string"
          }
        }, {
          "string_fields" : {
            "mapping" : {
              "index" : "not_analyzed",
              "omit_norms" : true,
              "type" : "string",
              "fields" : {}
            },
            "match" : "*",
            "match_mapping_type" : "string"
          }
        } ],
```

Here is the definition pulled from ES - notice that the _raw_ definition is back in for _strings_, but _message_ is still set correctly:

```
        "dynamic_templates" : [ {
          "message_field" : {
            "mapping" : {
              "index" : "not_analyzed",
              "omit_norms" : true,
              "type" : "string",
              "fields" : { }
            },
            "match" : "message",
            "match_mapping_type" : "string"
          }
        }, {
          "string_fields" : {
            "mapping" : {
              "index" : "not_analyzed",
              "omit_norms" : true,
              "type" : "string",
              "fields" : {
                "raw" : {
                  "index" : "not_analyzed",
                  "ignore_above" : 256,
                  "type" : "string"
                }
              }
            },
            "match" : "*",
            "match_mapping_type" : "string"
          }
        } ],
```
</description><key id="96123652">12348</key><summary>Custom default mapping partially overwritten by ES </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">technosf</reporter><labels /><created>2015-07-20T18:10:53Z</created><updated>2015-07-21T19:09:46Z</updated><resolved>2015-07-21T19:09:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-20T23:39:31Z" id="123094229">`raw` fields are a feature of logstash. Are you using logstash? Do you have logstash configured to manage your index templates? Also, specifying mappings through on disk files has been removed in the upcoming ES 2.0. You should explicitly define your mappings, and/or use index templates (as it seems you are here, so I don't understand why you would be using mapping files).
</comment><comment author="technosf" created="2015-07-21T19:09:45Z" id="123447037">I am using Config Mappings (per 1.7 doc), as __default__ is inefficient for the data I'm putting in ES (yes via LS). 

testing this out, LS 1.4.3 is modifying the default for the mapping, whether "manage_template" is true or false. 

Looks like LS templates are the way to go - thanks for the pointer. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When a query_string contains stop words and the default_operator is "AND" no results are returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12347</link><project id="" key="" /><description>I would expect stop words to be ignored for an "AND" search?
</description><key id="96120417">12347</key><summary>When a query_string contains stop words and the default_operator is "AND" no results are returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdchmiel</reporter><labels /><created>2015-07-20T17:52:57Z</created><updated>2015-07-21T13:57:50Z</updated><resolved>2015-07-20T17:57:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-20T17:57:31Z" id="122968036">It is really helpful if you provide an example of what you're doing.  Without an example I have to guess what the problem is. Stopwords are ignored:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "text": {
          "type": "string", 
          "analyzer": "english"
        }
      }
    }
  }
}

PUT t/t/1
{
  "text": "black and white"
}

PUT t/t/2
{
  "text": "black white"
}

GET t/_search
{
  "query": {
    "query_string": {
      "default_field": "text",
      "default_operator": "AND", 
      "query": "black and white"
    }
  }
}
```

Returns:

```
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "2",
        "_score": 0.2712221,
        "_source": {
           "text": "black white"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "1",
        "_score": 0.2712221,
        "_source": {
           "text": "black and white"
        }
     }
  ]
```

I'm guessing that you aren't specifying the field that uses the analyzer with the stopwords, and instead you're querying the `_all` field, which uses the `standard` analyzer (not stopwords)
</comment><comment author="jdchmiel" created="2015-07-21T13:57:49Z" id="123315539">Ahh you are exactly correct. I was specifying the field, but not specifying the analyzer. When I set it to "snowball" (with a coworkers help) it all acted as expected.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use time with nanosecond resolution calculated at the executing node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12346</link><project id="" key="" /><description>Use time with nanosecond resolution calculated at the executing node to measure the time that contexts are held open

Closes #12345
</description><key id="96106180">12346</key><summary>Use time with nanosecond resolution calculated at the executing node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-20T16:44:50Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-20T19:54:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-20T19:52:14Z" id="123008043">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Merge pull request #12346 from jasontedor/fix/12345</comment></comments></commit></commits></item><item><title>search.scroll_time_in_millis might not be accurate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12345</link><project id="" key="" /><description>Metrics for scrolls (open scroll contexts, time scroll contexts held open, and completed scroll contexts) were added to the search stats in #9109. The time scroll contexts were held open is calculated at millisecond resolution using the time the context was created on the coordinating node. This can lead to two issues:
1. Millisecond resolution is too coarse to accurately capture the time that very short-lived scroll contexts are held open, especially on systems with low-granualarity clocks.
2. Discrepancies in the clocks between the coordinating node and the nodes where the scrolls are executed will lead to inaccuracies in measuring the time that the context is held open at the shard level.

Instead, an origin time in nanosecond resolution that is calculated on the executing node should be used to measure the time that scroll contexts are held open.
</description><key id="96105722">12345</key><summary>search.scroll_time_in_millis might not be accurate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Scroll</label><label>:Stats</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-20T16:41:55Z</created><updated>2015-07-20T19:54:54Z</updated><resolved>2015-07-20T19:54:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Use time with nanosecond resolution calculated at the executing node to measure the time that contexts are held open</comment></comments></commit></commits></item><item><title>Update randomizedtesting to 2.1.16</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12344</link><project id="" key="" /><description>This fixes a bug in failsafe-summary formatting, so we can remove our workaround.
</description><key id="96084066">12344</key><summary>Update randomizedtesting to 2.1.16</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-20T15:03:43Z</created><updated>2015-07-20T15:13:57Z</updated><resolved>2015-07-20T15:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-20T15:06:53Z" id="122913089">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12344 from rmuir/2.1.16</comment></comments></commit></commits></item><item><title>Unit tests for `nodesAndVersions` on shared filesystems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12343</link><project id="" key="" /><description>With the `recover_on_any_node` setting, these unit tests check that the
correct node list and versions are returned.
</description><key id="96082748">12343</key><summary>Unit tests for `nodesAndVersions` on shared filesystems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-20T14:56:19Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-21T20:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-20T16:10:24Z" id="122931056">@kimchy can you take a look? This is the unit tests for the part that you wanted to refactor.
</comment><comment author="bleskes" created="2015-07-20T20:46:44Z" id="123030831">Tests look good to me. Wonder if we should add/change things to make sure that ignoreNodes is honoured. 
</comment><comment author="dakrone" created="2015-07-21T20:18:31Z" id="123466263">@bleskes I'll add a test for `ignoredNodes` when I push this, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: SpanOrQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12342</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new doToQuery() method analogous to other recent query refactorings.

Relates to #10217
</description><key id="96072963">12342</key><summary>Query refactoring: SpanOrQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-20T14:11:34Z</created><updated>2016-03-11T11:51:07Z</updated><resolved>2015-07-23T15:10:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-07-20T21:21:16Z" id="123044418">Left some minor comments.
</comment><comment author="cbuescher" created="2015-07-21T10:03:36Z" id="123247094">@MaineC thanks for the review, just pushed new commit addressing your comments. Mind to have another look?
</comment><comment author="MaineC" created="2015-07-21T19:11:31Z" id="123448050">LGTM - I'm sure @javanna will will find a few more areas for improvment
</comment><comment author="javanna" created="2015-07-22T07:30:14Z" id="123590191">left one small comment
</comment><comment author="cbuescher" created="2015-07-22T10:02:05Z" id="123648419">Addressed comments and removed check for empty clauses in doXContent since this will either be caught by parser or in validate. Left one question regarding tests to get another opinion from @MaineC.
</comment><comment author="MaineC" created="2015-07-23T09:05:07Z" id="124027789">After checking the modifications in person with @cbuescher this morning: Looks all good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12342 from cbuescher/feature/query-refactoring-spanor</comment></comments></commit></commits></item><item><title>[CI] Failure: IndicesStoreIntegrationTests.testShardActiveElseWhere</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12341</link><project id="" key="" /><description>Tests with failures:
- org.elasticsearch.indices.store.IndicesStoreIntegrationTests.testShardActiveElseWhere

This failed a few times lately and seems to be reproducible, e.g.

http://build-us-00.elastic.co/job/es_g1gc_master_metal/12111/
http://build-us-00.elastic.co/job/es_feature_query_refactoring/5367/

I could reproduce it on master at ca3e0c6d4976b24c6b6a8f833603e92159271ba5 with the following:

mvn test -Pdev -Dtests.seed=2EC73F5447E49994 -Dtests.class=org.elasticsearch.indices.store.IndicesStoreIntegrationTests -Dtests.slow=true -Dtests.method="testShardActiveElseWhere" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.heap.size=1024m -Dtests.locale=mt -Dtests.timezone=Asia/Brunei
</description><key id="96069121">12341</key><summary>[CI] Failure: IndicesStoreIntegrationTests.testShardActiveElseWhere</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label></labels><created>2015-07-20T13:53:33Z</created><updated>2015-07-20T21:22:55Z</updated><resolved>2015-07-20T21:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-20T21:22:55Z" id="123045476">This should be fixed with https://github.com/elastic/elasticsearch/commit/068fae218f3c59fe6bc51adcd8d5f9b5a1dbccaf
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Settings' ClassLoader is lost </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12340</link><project id="" key="" /><description>Starting with 1.6.0, custom ClassLoader is lost in org.elasticsearch.node.internal.InternalSettingsPreparer.replacePromptPlaceholders(Settings, Terminal)
</description><key id="96068718">12340</key><summary>Settings' ClassLoader is lost </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">ojusti</reporter><labels><label>:Settings</label><label>bug</label></labels><created>2015-07-20T13:51:33Z</created><updated>2015-07-23T17:06:09Z</updated><resolved>2015-07-23T17:06:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file></files><comments><comment>copy the classloader from the original settings when checking for prompts</comment></comments></commit></commits></item><item><title>Simplify handling of ignored unassigned shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12339</link><project id="" key="" /><description>Fold ignored unassigned to a UnassignedShards and have simpler handling of them. Also remove the trappy way of adding an ignored unassigned shards today directly to the list, and have dedicated methods for it.
</description><key id="96058278">12339</key><summary>Simplify handling of ignored unassigned shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-20T12:58:47Z</created><updated>2015-08-13T14:04:40Z</updated><resolved>2015-07-20T14:27:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-20T14:25:35Z" id="122901920">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Known issues / production deployments should mention keepalive kernel parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12338</link><project id="" key="" /><description>Cluster might be rendered useless if deployed with some firewall or on some hosting providers.
For example GCE (Google Cloud Engine) has an internal FW that drops idle connection after 10 minutes.

These parameters in sysctl should have recommended values for production deployments: 
net.ipv4.tcp_keepalive_time
net.ipv4.tcp_keepalive_intvl
net.ipv4.tcp_keepalive_probes

Deep explanation here:
http://blog.trifork.com/2015/04/08/dealing-with-nodenotavailableexceptions-in-elasticsearch/

Bonus points for rethinking how this works and make it work out of the box.
</description><key id="96054776">12338</key><summary>Known issues / production deployments should mention keepalive kernel parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ofir-petrushka</reporter><labels><label>:Settings</label><label>adoptme</label><label>docs</label></labels><created>2015-07-20T12:37:06Z</created><updated>2016-01-15T12:41:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-24T09:44:36Z" id="124457851">Let's see if Frans Flippo would be up for sending a docs PR
</comment><comment author="fransflippo" created="2015-08-17T13:05:18Z" id="131811468">I will work on a PR for amending the documentation (probably in https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html). If there's a production deployments page I'll see if I can add a comment about setting keepalive there as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>logs should be under /var/logs/elasticsearch on linux, data under /var/elasticsearch...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12337</link><project id="" key="" /><description>Currently everything is under the "product" directory /usr/share/elasticsearch "windows style", see:
https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-dir-layout.html

As with the config that is linked to /etc...
ls -l /usr/share/elasticsearch/
config -&gt; /etc/elasticsearch

Should have 
logs - &gt; /var/logs/elasticsearch
data -&gt; /var/elasticsearch/data

Why?
This is standard linux structure and elasticsearch is the only big software that breaks it.

Many admin tools and procedures assume this structure.
Like doing log aggregation on all files in /var/log
Checking disk space etc.

Not to mention that the bin should bin in /bin...
</description><key id="96053949">12337</key><summary>logs should be under /var/logs/elasticsearch on linux, data under /var/elasticsearch...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofir-petrushka</reporter><labels><label>feedback_needed</label></labels><created>2015-07-20T12:31:44Z</created><updated>2015-08-03T14:21:29Z</updated><resolved>2015-08-03T14:21:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-20T19:47:53Z" id="123005614">What type of installation are you using and how do you start elasticsearch? If you are using DEB or RPM package the directory setup is different. Please see https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-dir-layout.html#_deb_and_rpm for more information. 
</comment><comment author="ofir-petrushka" created="2015-07-22T14:56:24Z" id="123749091">Using elastic.co apt package for version 1.5, see and try:
https://github.com/rounds/10M-Docker-Images/blob/master/elastic_search/Dockerfile
https://registry.hub.docker.com/u/rounds/10m-elastic-search/
</comment><comment author="imotov" created="2015-07-22T16:35:18Z" id="123782594">Yes, I see the issue with 1.5. However, it's no longer reproducible with 1.7.0.  @spinscale, could you comment on this?
</comment><comment author="spinscale" created="2015-08-03T14:21:29Z" id="127259047">Hey,

this is indeed a bug in the 1.5 package. The problem is that in `/usr/lib/systemd/system/elasticsearch.service` we try to load data from the `/etc/default/elasticsearch` file, where usually everything is commented out and this leads to empty specified directories on startup.

You can either upgrade, or change your `.service` file to something like this

```
[Service]
Environment=ES_HOME=/usr/share/elasticsearch
Environment=CONF_DIR=/etc/elasticsearch
Environment=CONF_FILE=/etc/elasticsearch/elasticsearch.yml
Environment=DATA_DIR=/var/lib/elasticsearch
Environment=LOG_DIR=/var/log/elasticsearch
Environment=PID_DIR=/var/run/elasticsearch
EnvironmentFile=-/etc/default/elasticsearch

User=elasticsearch
Group=elasticsearch

ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -Des.pidfile=$PID_DIR/elasticsearch.pid \
                                                -Des.default.path.home=$ES_HOME \
                                                -Des.default.path.logs=$LOG_DIR \
                                                -Des.default.path.data=$DATA_DIR \
                                                -Des.default.config=$CONF_FILE \
                                                -Des.default.path.conf=$CONF_DIR
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to specify specific buckets in Aggregation Path syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12336</link><project id="" key="" /><description>The syntax used in both the terms aggregation ordering and in the pipeline aggregations is at the moment limited to referencing numeric metric aggregations and single-bucket aggregations. We should extend the syntax so users are able to specify a specific bucket in a multi-bucket aggregation. So, for example you could do the following:

``` json
{
  "aggs": {
    "fruits": {
      "terms": {
        "field": "fruit",
        "order": {
          "colours.yellow": "asc"
        }
      },
      "aggs": {
        "colours": {
          "terms": {
            "field": "colour"
          }
        }
      }
    }
  }
}
```

which would order the `fruits` terms by the number of `yellow` documents for each `fruit`.

We would need to decide what we would do if `yellow` didn't appear in the top 10 terms for a particular fruit. Do we just say the sort value is `0`?

Bonus Points:

We could also support very basic operators on the multi-bucket aggregation's buckets so you could do something like:

``` json
"order": {
    "avg(colours)": "asc"
}
```

which would sort by the average (mean) document count value across all the colours for each fruit. and:

``` json
"order": {
    "avg(colours&gt;total_sales)": "asc"
}
```

which would sort by the average (mean) value of `total_sales` across all the colours for each fruit.
</description><key id="96035705">12336</key><summary>Ability to specify specific buckets in Aggregation Path syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-20T10:31:45Z</created><updated>2016-01-26T14:49:06Z</updated><resolved>2016-01-26T14:49:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T14:49:06Z" id="175055498">There does't seem to be much call for this syntax, so I'm going to close this issue for now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Initial Refactor Gateway Allocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12335</link><project id="" key="" /><description>Break it into more manageable code by separating allocation primaries and allocating replicas. Start adding basic unit tests for primary shard allocator.
</description><key id="96027413">12335</key><summary>Initial Refactor Gateway Allocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-20T09:50:05Z</created><updated>2015-08-13T14:04:29Z</updated><resolved>2015-07-20T12:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-20T11:56:58Z" id="122862368">LGTM. Left one assertion ask and minor comments. Nice breakage - much easier to test.
</comment><comment author="kimchy" created="2015-07-20T12:05:36Z" id="122864509">pushed, not sure why it didn't pick up on merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove primaryAllocatedPostApi flag and use UnassignedInfo information </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12334</link><project id="" key="" /><description>Conceptually, we can remove the need for the flag, which is tricky to manage and reason about, and just use the `UnassignedInfo` and reason of `INDEX_CREATED`.
</description><key id="95950578">12334</key><summary>Remove primaryAllocatedPostApi flag and use UnassignedInfo information </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-19T21:29:51Z</created><updated>2016-01-26T15:04:08Z</updated><resolved>2016-01-26T15:04:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T14:38:10Z" id="175049004">@bleskes i assume this is still worth doing?
</comment><comment author="bleskes" created="2016-01-26T15:04:08Z" id="175063790">yep and it has been removed  in 2.x and completely replaced in master with #14739 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`ignore_missing` ignored in `_timestamp` mapping </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12333</link><project id="" key="" /><description>```
DELETE my_index

PUT my_index
{
  "mappings": {
    "my_type": {
      "_timestamp": {
        "enabled": true,
        "ignore_missing": true
      }
    }
  }
}

PUT my_index/my_type/1
{
  "text": "This is a document"
}
```

This document shouldn't have a `_timestamp`:

```
GET _search
```

But it does:

```
     {
        "_index": "my_index",
        "_type": "my_type",
        "_id": "1",
        "_score": 1,
        "_timestamp": 1437339461591,
        "_source": {
           "text": "This is a document"
        }
     }
```
</description><key id="95948730">12333</key><summary>`ignore_missing` ignored in `_timestamp` mapping </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-07-19T20:58:24Z</created><updated>2015-08-13T13:52:02Z</updated><resolved>2015-07-19T21:26:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-19T21:26:39Z" id="122707966">Misread the docs - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `index_options` to `_all` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12332</link><project id="" key="" /><description>The `_all` field accepts the `term_vector` parameter (enabling the FVH) but silently ignored the `index_options` parameter, meaning that it cannot be used with the postings highlighter.
</description><key id="95935935">12332</key><summary>Add `index_options` to `_all` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-07-19T17:57:07Z</created><updated>2016-03-16T19:39:00Z</updated><resolved>2016-01-26T14:37:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-21T05:54:20Z" id="123172151">I actually think we should do the opposite, and remove most options from `_all`. If an advanced user wants to query over a catchall field with advanced settings, they can do so by setting up a field with copy_to, and querying on that field.
</comment><comment author="rmuir" created="2015-07-21T05:58:34Z" id="123174094">You can't use _all with postingshighlighter anyway, where would the offsets point? In order for that to work, highlighting would need to work differently, e.g. offsets corresponding to _source so that they are meaningful and valid.
</comment><comment author="clintongormley" created="2015-07-22T12:51:10Z" id="123709855">ok, makes sense - let's remove options from `_all` then
</comment><comment author="rmuir" created="2015-07-22T13:17:07Z" id="123716403">at least for now until we can do it nicely in the future. maybe one day we can get them corresponding to _source or osmething if you do this.
</comment><comment author="clintongormley" created="2016-01-26T14:37:25Z" id="175048639">The all field has been locked down. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add scroll stats to cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12331</link><project id="" key="" /><description>Adds scroll stats at the node, shard and index levels to the cat API.

Closes #12330
</description><key id="95935254">12331</key><summary>Add scroll stats to cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-19T17:43:35Z</created><updated>2015-07-26T02:07:36Z</updated><resolved>2015-07-20T09:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-20T07:03:42Z" id="122782540">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>Merge pull request #12331 from jasontedor/feature/12330</comment></comments></commit></commits></item><item><title>Add scroll stats to cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12330</link><project id="" key="" /><description>Metrics for scrolls (open scroll contexts, time scroll contexts held open, and completed scroll contexts) were added to the search stats in #9109. These metrics should be added to the cat API.
</description><key id="95935071">12330</key><summary>Add scroll stats to cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:CAT API</label><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-19T17:41:32Z</created><updated>2015-07-20T09:23:04Z</updated><resolved>2015-07-20T09:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>Add scroll stats to cat API</comment></comments></commit></commits></item><item><title>Make `_index` field non-configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12329</link><project id="" key="" /><description>Since #12027 was merged, the `_index` field can be used in queries, aggregations, sorting, and scripting, even though it is not indexed.

I can't see any reason now why we would allow users to set:

```
"_index": { "enabled": true }
```
</description><key id="95914877">12329</key><summary>Make `_index` field non-configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-07-19T13:13:47Z</created><updated>2016-03-20T09:29:40Z</updated><resolved>2015-07-21T07:30:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-19T13:14:17Z" id="122660403">@rjernst do you agree?
</comment><comment author="rjernst" created="2015-07-21T06:57:21Z" id="123192203">I do agree. I opened #12356.
</comment><comment author="JohnTian" created="2016-03-20T09:29:40Z" id="198881630">Soga.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file></files><comments><comment>Mappings: Remove ability to configure _index</comment></comments></commit></commits></item><item><title>Fix malformed query generation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12328</link><project id="" key="" /><description>Fix for issue #12327 
</description><key id="95887717">12328</key><summary>Fix malformed query generation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">alexclare</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.7.1</label></labels><created>2015-07-19T05:07:54Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-22T09:55:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexclare" created="2015-07-19T05:11:44Z" id="122631523">There are a few other places where this can be fixed: changing `doXContent` in the builder to avoid including `modifier` in the first place, or adding a conditional to the parsing logic. Not sure which you folks would prefer!
</comment><comment author="clintongormley" created="2015-07-19T11:29:09Z" id="122650666">Hi @alexclare 

Thanks for the PR - any chance you could add a test for this please?
</comment><comment author="alexclare" created="2015-07-19T16:43:09Z" id="122680058">Cool, here's that test.
</comment><comment author="clintongormley" created="2015-07-19T18:02:39Z" id="122689225">@brwe could you take a look please?
</comment><comment author="brwe" created="2015-07-22T09:55:24Z" id="123646776">&gt; Not sure which you folks would prefer!

I think this is the right place. 
</comment><comment author="clintongormley" created="2015-07-22T10:18:00Z" id="123652217">thanks @alexclare 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueTests.java</file></files><comments><comment>Merge pull request #12328 from alexclare/issue-12327</comment></comments></commit></commits></item><item><title>Using FieldValueFactorFunction.Modifier.NONE results in malformed queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12327</link><project id="" key="" /><description>When constructing field value factor functions, using `.modifier(FieldValueFactorFunction.Modifier.NONE)` yields malformed requests.

Sample expected output:

```
{
  "function_score" : {
    "query" : {
      "simple_query_string" : {
        "query" : "foo"
      }
    },
    "functions" : [ {
      "field_value_factor" : {
        "field" : "test",
        "modifier" : "none"
      }
    } ]
  }
}
```

Or alternatively, no modifier property should be added at all.

Actual output:

```
{
  function_score" : {
    "query" : {
      "simple_query_string" : {
        "query" : "foo"
      }
    },
    "functions" : [ {
      "field_value_factor" : {
        "field" : "test",
        "modifier" : ""
      }
    } ]
  }
}
```

This ultimately results in a parsing error: `IllegalArgumentException[No enum constant org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction.Modifier.]`
</description><key id="95887552">12327</key><summary>Using FieldValueFactorFunction.Modifier.NONE results in malformed queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexclare</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-07-19T05:06:03Z</created><updated>2015-07-22T10:13:25Z</updated><resolved>2015-07-22T10:13:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-07-22T10:13:25Z" id="123651462">fixed via #12328
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Intensity Highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12326</link><project id="" key="" /><description>## Introduction

On queries composed of many terms, for example when using More Like This (MLT) or an [Item Query](https://github.com/elastic/elasticsearch/issues/12293), it is often quite desirable to explain the results returned. This helps users form a mental model about the underlying matching algorithm, and subsequently removes some of the magic around these types of queries.

Currently, developers have two out of the box options in order to provide such a functionality: [explain](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-explain.html) and [highlighting](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-highlighting.html). Explain is useful for debugging, but has some unfortunate performance implications. Its output is also quite verbose. Highlighting is useful to show the matched keywords in context, but it only provides a partial understanding of scoring.

What we would need instead is something in between highlighting and explain. Something which would not only highlight the terms in context but also provide the level of importance that term had in scoring each document. This would provide a best of both worlds solution. In fact, the idea isn't new! Lucene already provides a GradientFormatter, though it is not used at the moment in Elasticsearch.

Let us further illustrate our point with an example.
## Example

In the example below, the user has asked for movies similar to the movie Terminator 2. Under the hood an MLT query was triggered which then generated a query composed of the salient terms in the requested document. The matching terms are then highlighted within each returned document, but with some additional hints as to their level of importance in scoring. The greater the intensity of the highlight, the more important that term is in computing the score of the document.

![itensity_highlighter2](https://cloud.githubusercontent.com/assets/43475/8762299/59560490-2d75-11e5-8ae8-16a89278e8d1.png)

In this example, we can see that the first movie has less terms matching the query than the second movie. However, these terms are highlighted with greater intensities, which explains why that movie is ranked higher in the result set. If the user did not have the intensity of the highlight, he could be puzzled as to why this is the case. For example, he could wrongly assume that more terms matching means greater scoring. Here we not only know why the document matched, but also how the score was probably computed (which terms participated in the scoring and to what extent). 
## API usage

We could return all the per-query-term scores, but maybe such an output would be too verbose, and too close to just using explain. Instead, and this is not set in stone, we could highlight the terms in context with style. In order to do so, we can introduce a `with_intensities` option which would take an integer representing the number of possible intensities.

Here is how the above example could have been generated:

``` json
GET /tmdb/movies/_search
{
  "query": {
    "more_like_this": {
      "like": {
        "_id": 280
      }
    }
  },
  "highlight": {
    "fields": {
      "plot": {
        "number_of_fragments": 0,
        "with_intensities": 4
      }
    }
  }
}
```

And the response:

``` json
{
  ...
  "hits": {
    "total": 3,
    "max_score": 10.2301,
    "hits": [
      {
        ...
        "highlight": {
          "plot": ["It's been 10 years since John &lt;em class=\"hlt4\"&gt;Connor&lt;/em&gt; saved Earth from
&lt;em class=\"hlt3\"&gt;Judgment Day&lt;/em&gt;, and he's now living under the radar, steering
clear of using anything &lt;em class=\"hlt4\"&gt;Skynet&lt;/em&gt; can trace. That is, until he
encounters T-X, a &lt;em class=\"hlt1\"&gt;robotic&lt;/em&gt; &lt;em class=\"hlt2\"&gt;assassin&lt;/em&gt; ordered
to finish what &lt;em class=\"hlt4\"&gt;T-1000&lt;/em&gt; started. Good thing &lt;em class=\"hlt4\"&gt;Connor&lt;/em&gt;'s
former nemesis, the &lt;em class=\"hlt4\"&gt;Terminator&lt;/em&gt;, is back to aid the now-adult
&lt;em class=\"hlt4\"&gt;Connor&lt;/em&gt; … just like he promised."]
        }
      },
      ...
    ]
  }
}
```
## Implementation Notes

As previously mentioned, Lucene does provide a GradientFormatter and a SpanGradientFormatter which can be used with the plain highlighter. Another possibility would include adding the option to retain the per-query-term scores to plain highlighter and/or to fast vector highlighter, that is retaining the scores which were used to compute the score given to each fragment. There are roughly the same term scores which were used to compute the score of the document before highlighting.
</description><key id="95840300">12326</key><summary>Intensity Highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Highlighting</label><label>feature</label><label>Meta</label></labels><created>2015-07-18T16:55:38Z</created><updated>2015-09-09T23:36:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-18T17:39:06Z" id="122571108">It's not a bad idea. I always dislike highlighter things in es now because
you have to do everything three times.
On Jul 18, 2015 12:55 PM, "Alex Ksikes" notifications@github.com wrote:

&gt; Introduction
&gt; 
&gt; On queries composed of many terms, for example when using More Like This
&gt; (MLT) or an Item Query
&gt; https://github.com/elastic/elasticsearch/issues/12293, it is often
&gt; quite desirable to explain the results returned. This helps users form a
&gt; mental model about the underlying matching algorithm, and subsequently
&gt; removes some of the magic around these types of queries.
&gt; 
&gt; Currently, developers have two out of the box options in order to provide
&gt; such a functionality: explain
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/master/search-explain.html
&gt; and highlighting
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-highlighting.html.
&gt; Explain is useful for debugging, but has some unfortunate performance
&gt; implications. Its output is also quite verbose. Highlighting is useful to
&gt; show the matched keywords in context, but it only provides a partial
&gt; understanding of scoring.
&gt; 
&gt; What we would need instead is something in between highlighting and
&gt; explain. Something which would not only highlight the terms in context but
&gt; also provide the level of importance that term had in scoring each
&gt; document. This would provide a best of both worlds solution. In fact, the
&gt; idea isn't new! Lucene already provides a GradientFormatter, though it is
&gt; not used at the moment in Elasticsearch.
&gt; 
&gt; Let us further illustrate our point with an example.
&gt; Example
&gt; 
&gt; In the example below, the user has asked for movies similar to the movie
&gt; Terminator 2. Under the hood an MLT query was triggered which then
&gt; generated a query composed of the salient terms in the requested document.
&gt; The matching terms are then highlighted within each returned document, but
&gt; with some additional hints as to their level of importance in scoring. The
&gt; greater the intensity of the highlight, the more important that term is in
&gt; computing the score of the document.
&gt; 
&gt; [image: itensity_highlighter2]
&gt; https://cloud.githubusercontent.com/assets/43475/8762299/59560490-2d75-11e5-8ae8-16a89278e8d1.png
&gt; 
&gt; In this example, we can see that the first movie has less terms matching
&gt; the query than the second movie. However, these terms are highlighted with
&gt; greater intensities, which explains why that movie is ranked higher in the
&gt; result set. If the user did not have the intensity of the highlight, he
&gt; could be puzzled as to why this is the case. For example, he could wrongly
&gt; assume that more terms matching means greater scoring. Here we not only
&gt; know why the document matched, but also how the score was probably computed
&gt; (which terms participated in the scoring and to what extent).
&gt; API usage
&gt; 
&gt; We could return all the per-query-term scores, but maybe such an output
&gt; would be too verbose, and too close to just using explain. Instead, and
&gt; this is not set in stone, we could highlight the terms in context with
&gt; style. In order to do so, we can introduce a with_intensities option
&gt; which would take an integer representing the number of possible intensities.
&gt; 
&gt; Here is how the above example could have been generated:
&gt; 
&gt; GET /tmdb/movies/_search
&gt; {
&gt;   "query": {
&gt;     "more_like_this": {
&gt;       "like": {
&gt;         "_id": 280
&gt;       }
&gt;     }
&gt;   },
&gt;   "highlight": {
&gt;     "fields": {
&gt;       "plot": {
&gt;         "number_of_fragments": 0,
&gt;         "with_intensities": 4
&gt;       }
&gt;     }
&gt;   }
&gt; }
&gt; 
&gt; And the response:
&gt; 
&gt; {
&gt;   ...
&gt;   "hits": {
&gt;     "total": 3,
&gt;     "max_score": 10.2301,
&gt;     "hits": [
&gt;       {
&gt;         ...
&gt;         "highlight": {
&gt;           "plot": [It's been 10 years since John &lt;em class="hlt4"&gt;Connor&lt;/em&gt; saved Earth from&lt;em class="hlt3"&gt;Judgment Day&lt;/em&gt;, and he's now living under the radar, steering
&gt; clear of using anything &lt;em class="hlt4"&gt;Skynet&lt;/em&gt; can trace. That is, until he
&gt; encounters T-X, a &lt;em class="hlt1"&gt;robotic&lt;/em&gt; &lt;em class="hlt2"&gt;assassin&lt;/em&gt; ordered
&gt; to finish what &lt;em class="hlt4"&gt;T-1000&lt;/em&gt; started. Good thing &lt;em class="hlt4"&gt;Connor&lt;/em&gt;'sformer nemesis, the &lt;em class="hlt4"&gt;Terminator&lt;/em&gt;, is back to aid the now-adult&lt;em class="hlt4"&gt;Connor&lt;/em&gt; … just like he promised.]        }      },      ...    ]  }}
&gt; 
&gt; Implementation Notes
&gt; 
&gt; As previously mentioned, Lucene does provide a GradientFormatter and a
&gt; SpanGradientFormatter which can be used with the plain highlighter. Another
&gt; possibility would include adding the option to retain the per-query-term
&gt; scores to plain highlighter and/or to fast vector highlighter, that is
&gt; retaining the scores which were used to compute the score given to each
&gt; fragment. There are roughly the same term scores which were used to compute
&gt; the score of the document before highlighting.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12326.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>geo_shape: getting IllegalArgumentException[Points of LinearRing do not form a closed linestring]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12325</link><project id="" key="" /><description>When trying to index a Polygon into a geo_shape, I incorrectly get IllegalArgumentException[Points of LinearRing do not form a closed linestring]:

```
Elasticsearch::Transport::Transport::Errors::InternalServerError: [500] {"error":"PercolatorException[[development-geoevents-20150708-090828] failed to parse query [chicago,South Chicago]]; nested: IllegalArgumentException[Points of LinearRing do not form a closed linestring]; ","status":500}
```

The Polygon in question is a neighborhood in Chicago taken from a reliable source (foursquare)
neighborhood: https://gist.github.com/dimroc/6ddb159afa0b4507cc0a
original: https://github.com/blackmad/neighborhoods/blob/master/chicago.geojson

I've run this through QGIS and it returns no errors:

![check_geometry_validity_and_qgis_2_8_2-wien](https://cloud.githubusercontent.com/assets/635121/8762478/d0e54f94-2d47-11e5-97ee-177ca057dbce.jpg)

I've also verified that the start and end coordinates match.

My goal is to have this successfully indexed with no problems :+1: . Thanks for any feedback.
</description><key id="95838056">12325</key><summary>geo_shape: getting IllegalArgumentException[Points of LinearRing do not form a closed linestring]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">dimroc</reporter><labels><label>:Geo</label><label>discuss</label><label>non-issue</label></labels><created>2015-07-18T16:25:53Z</created><updated>2016-11-02T11:21:29Z</updated><resolved>2016-01-26T14:36:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dimroc" created="2015-07-18T16:39:30Z" id="122563033">Bug Repro Steps:

```
curl -XPUT http://127.0.0.1:9200/repro-bug -d '{"settings":{"number_of_shards":3},"mappings":{"geoevent":{"properties":{"geojson":{"type":"geo_shape"}}}}}'

curl https://gist.githubusercontent.com/dimroc/51e20d6f0b309f3ddca7/raw/de3fc24504c6b37f5f9c63bb9f25a409ee2feb3f/repro-bug-payload.json &gt; /tmp/repro-bug.json

curl --data "@/tmp/repro-bug.json" -XPUT http://127.0.0.1:9200/repro-bug/geoevent/1
```

Result:

```
{"error":"MapperParsingException[failed to parse [geojson]]; nested: IllegalArgumentException[Points of LinearRing do not form a closed linestring]; ","status":400}
```
</comment><comment author="komarserjio" created="2015-10-08T13:02:28Z" id="146536069">@dimroc Looks like your polygon is not fully valid (by [ISO 19107:2003](http://www.iso.org/iso/catalogue_detail.htm?csnumber=26012) standard).
I had the same issue when I was importing [Flickr Shapefiles Dataset](http://code.flickr.net/2011/01/08/flickr-shapefiles-public-dataset-2-0/) files into elastic search.

You can repair/fix the polygon using this tool: https://github.com/tudelft3d/prepair
I did the trick with your repro-bug.json file and it helped - elastic search accepts fixed version.

Steps:

```
# fixed invalid polygon using tudelft3d/prepair tool
$ ./prepair --shpOut --ogr /tmp/repro-bug.json
# convert generated shape file back to geojson format
$ ogr2ogr -f GeoJSON  fixed-polygon.geojson out.shp

# ES accepts the fixed polygon now
$ curl --data "@/tmp/prepair/fixed-polygon.geojson" -XPUT http://localhost:9200/repro-bug/geoevent/1
{"_index":"repro-bug","_type":"geoevent","_id":"1","_version":1,"created":true}%
```

Hope it helps :)
</comment><comment author="dimroc" created="2015-10-08T14:49:49Z" id="146569107">Thanks for the heads up @komarserjio, I'll be sure to check it out.
</comment><comment author="bogensberger" created="2015-11-12T15:16:12Z" id="156134912">+1

I also verified that the shape is correct, using geojsonlint.com
This is the GeoShape i tried to index: https://gist.github.com/sfsheath/9592083/#file-rome-json
</comment><comment author="nknize" created="2015-11-12T17:30:47Z" id="156175971">@dimroc, hopefully you were able to resolve your issue. I was able to verify a closed loop at:

``` json
[-87.530045, 41.742051],
[-87.530044, 41.742051],
[-87.530045, 41.742051],
```

Which violates the OGC SFA spec.
</comment><comment author="nknize" created="2015-11-12T17:40:33Z" id="156179330">@bogensberger  

rome.json feature `"OBJECTID" : 1` also contains a closed loop at:

``` json
[ 24.041199999999662, 38.212069999999528 ], 
[ 24.093430000000922, 38.270619999999326 ], 
[ 24.07726, 38.317940000000931 ],
[ 24.04028, 38.354170000000465 ],
[ 24.009260000000666, 38.394249999999374 ],
[ 23.96148, 38.377899999999613 ],
[ 23.91187, 38.384640000000218 ],
[ 23.862670000000378, 38.39394 ],
[ 23.810740000000862, 38.386249999999677 ], 
[ 23.806060000000912, 38.317409999999711 ], 
[ 23.858089999999777, 38.314169999999706 ],
[ 23.907110000000159, 38.298380000000293 ], 
[ 23.95579, 38.275229999999397 ], 
[ 24.001120000000753, 38.244950000000244 ], 
[ 24.041199999999662, 38.212069999999528 ],
```

Try out the wonderful suggestion posted above by @komarserjio and let us know if you were able to clean up the geometry.
</comment><comment author="clintongormley" created="2016-01-26T14:36:41Z" id="175048303">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[build] Update to maven-shade-plugin 2.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12324</link><project id="" key="" /><description>``` xml
&lt;plugin&gt;
 &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
 &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
 &lt;version&gt;2.4.1&lt;/version&gt;
&lt;/plugin&gt;
```
## Release Notes - Maven Shade Plugin - Version 2.4.1

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317921&amp;version=12332978

Bugs:
- [MSHADE-148] - Shade Plugin gets stuck in infinite loop building dependency reduced POM
- [MSHADE-194] - Reporting uses maven-invoker-plugin version 1.9 instead of 1.10
</description><key id="95760395">12324</key><summary>[build] Update to maven-shade-plugin 2.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T22:56:04Z</created><updated>2015-08-11T14:00:22Z</updated><resolved>2015-07-27T16:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-27T13:06:57Z" id="125199209">Looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create a directory during repository verification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12323</link><project id="" key="" /><description>The repository verification process should create a subdirectory to make sure we check permission of newly created directories in case elasticsearch processes on different nodes are running using different uids and creating blobs with incompatible permissions.

Closes #11611
</description><key id="95738124">12323</key><summary>Create a directory during repository verification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T20:29:38Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-31T16:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-24T22:32:55Z" id="124749279">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception serialization checking causes tests to hang in 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12322</link><project id="" key="" /><description>It looks like 1.7's LocalTransportChannel has:

``` java
    @Override
    public void sendResponse(Throwable error) throws IOException {
        BytesStreamOutput stream = new BytesStreamOutput();
        if (ThrowableObjectOutputStream.canSerialize(error) == false) {
            assert false : "Can not serialize exception: " + error; // make sure tests fail
            error = new NotSerializableTransportException(error);
        }
...
```

This doesn't cause the tests to fail, at least not with the (probably outdated) way I run them in my plugin. It causes them to hang indefinitely.

For reference this is the way I run the integration tests that hang:

``` java
@ElasticsearchIntegrationTest.ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE, transportClientRatio = 0.0)
public class AbstractPluginIntegrationTest extends ElasticsearchIntegrationTest {
    /**
     * Enable plugin loading.
     */
    @Override
    protected Settings nodeSettings(int nodeOrdinal) {
        return ImmutableSettings.builder().put(super.nodeSettings(nodeOrdinal))
                .put("plugins." + PluginsService.LOAD_PLUGIN_FROM_CLASSPATH, true).build();
    }
}
```

Its old, 1.1 era stuff but it still worked fine util now.
</description><key id="95707067">12322</key><summary>Exception serialization checking causes tests to hang in 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Exceptions</label><label>bug</label></labels><created>2015-07-17T17:45:09Z</created><updated>2015-09-14T09:23:48Z</updated><resolved>2015-08-03T13:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-17T17:48:37Z" id="122355028">Looks to me like the assertion in LocalTransportChannel causes the response never to be sent at all.
</comment><comment author="nik9000" created="2015-07-17T19:43:48Z" id="122391259">I can fix the tests by throwing out the offending exceptions and throwing whole new, fully Serializable ones. Its what I'm doing with my extensions.

The hanging seems to be caused by that assertion. If I run the tests without assertions (`-ea:org.apache.lucene...` so the base classes don't barf) then they pass even without the changes.
</comment><comment author="nik9000" created="2015-07-17T19:44:09Z" id="122391322">I'm told I should ping @s1monw about this.
</comment><comment author="nik9000" created="2015-07-29T00:53:42Z" id="125793165">It doesn't look like @s1monw's picked this up and I filed this so I'm taking it.
</comment><comment author="nik9000" created="2015-07-29T13:11:31Z" id="125946126">Clearing myself as assignee - this doesn't look like it'll be an issue in 2.0 so its probably ok to just let it become obsolete when 2.0 is eventually released.
</comment><comment author="s1monw" created="2015-08-03T09:13:54Z" id="127173717">@nik9000 sorry for the late answer here. I think this assertion is ok and we should stick with it on 1.7 - it's there to tell you you are sending stuff across the network that will be wrapped in a weird `NotSerializableException` can you share what exception can't be serialized so we might be able to add this to the whitelist?
</comment><comment author="nik9000" created="2015-08-03T13:22:58Z" id="127232079">&gt; can you share what exception can't be serialized so we might be able to add this to the whitelist?

Its part of a plugin. It was simple enough to just make the exception serializable once I tracked down the breakage but it wasn't obvious where the breakage was because it looked like the test just hung - the assertion stopped elasticsearch from sending a response at all.
</comment><comment author="s1monw" created="2015-08-03T13:26:25Z" id="127233075">ok should we just close this and move on? I think it did was it was intended to do :)
</comment><comment author="nik9000" created="2015-08-03T13:27:25Z" id="127233279">Yeah - causing the test to hang wasn't great but its not worth fixing if its not a problem in 2.0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch, Logstash issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12321</link><project id="" key="" /><description>This is in relation to 
https://github.com/elastic/elasticsearch/issues/12276

We upgraded to ES 1.7 and same issue occurs. Shutdown Logstash server and ES locks - no connections are possible even though ES nodes report cluster state as green.
</description><key id="95700362">12321</key><summary>Elasticsearch, Logstash issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AGirin</reporter><labels /><created>2015-07-17T17:06:03Z</created><updated>2016-01-26T14:35:56Z</updated><resolved>2016-01-26T14:35:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T14:35:56Z" id="175048131">It sounds like you are using the node client in logstash. I'd advise moving to the http client instead.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`mvn package` doesn't handle dirty workspace correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12320</link><project id="" key="" /><description>Follow-up from #12307.

If I check out an older commit, clean and compile the workspace, then check out a newer commit and run `mvn package`, the workspace is not completely clean.

Here's a reproduction:

``` bash
git checkout c2c8956
mvn clean compile test-compile
git checkout pr/12307
mvn package -Drun -DskipTests
```

And then running the ES server will fail.

It looks like this is a known issue with Maven: https://cwiki.apache.org/confluence/display/MAVEN/Incremental+Builds but nevertheless, this issue can track any progress here.
</description><key id="95674387">12320</key><summary>`mvn package` doesn't handle dirty workspace correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2015-07-17T15:03:24Z</created><updated>2016-01-26T14:33:27Z</updated><resolved>2016-01-26T14:33:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-17T15:11:34Z" id="122307227">Note that also the `maven-shade-plugin` in many cases "replaces" the generated jar artifact and also has similar problems. @jaymode and I spent a good amount of time just the last few days tracking down issues with that in another project. 

The hack used there is only evil in that it will make things slow (https://github.com/elastic/securemock/commit/181f732ea082c67dc559edb9f1c7b7338c19c50c). But maybe its not so bad, packaging is mostly not slow because of compilation, its because of rpm/deb/shading which we are looking at moving out to separate modules in other issues.
</comment><comment author="clintongormley" created="2016-01-26T14:33:27Z" id="175046884">Closing as we've moved to gradle
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ThreadPools: schedule a timeout check after adding command to queue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12319</link><project id="" key="" /><description>Our thread pools have support for timeout on a task. To support this, a special background task is schedule to run at timeout. That background task fires and check if the main task is still in the executor queue and then cancels it if needed. Currently we schedule this background task before adding the main task to the queue. If the timeout is very small (in tests we often use numbers like 2 ms)  the background task can fire before the main one is added to the queue causing the timeout to be missed.

See http://build-us-00.elastic.co/job/es_g1gc_master_metal/11780/testReport/junit/org.elasticsearch.cluster/ClusterServiceTests/testTimeoutUpdateTask/
</description><key id="95661968">12319</key><summary>ThreadPools: schedule a timeout check after adding command to queue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Core</label><label>bug</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T14:02:50Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-20T08:50:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-17T14:03:00Z" id="122282393">@imotov can you take a look?
</comment><comment author="imotov" created="2015-07-17T14:52:03Z" id="122299856">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file></files><comments><comment>ThreadPools: schedule a timeout check after adding command to queue</comment></comments></commit></commits></item><item><title>Cluster ends up in unstable state after heavy indexing activity but does not recover</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12318</link><project id="" key="" /><description>Setup:
3 nodes of i2.xlarge on AWS (4 cores, 30G ram with 15G for java heap, 800G SSD instance store)
Elasticsearch version 1.6.0
Indexing is done from 3 storm worker nodes each doing bulk imports into the elasticsearch at frequencies of 2 seconds (averages around 2000 documents with each bulk import call)

State:
8 indices of 3 shards each, with async replication factor of 2
400 Million records in the store to a total of ~80GB of data

Tests
Run 1: indexing documents with a consistent rate of 1k documents per second for over 8 hours
           Observations:
                  - able to query aggregations without much latencies
                  - cluster status remained green all through
Run 2: indexing documents with consistent rate of 2k documents per second for over 6 hours
           Observations:
                  - able to query aggregations without much latencies
                  - cluster status remained green all through
Run 3: indexing documents with consistent rate of 3.5k documents
           Observations:
                  - after ~90 minutes first incident of cluster health moving to yellow
                  - traffic continued with varying rates of indexing for the next 5 hours, cluster state remained yellow, with nodes exiting and joining the cluster occasionally
                  - indexing was stopped after 5 hours since first yellow state
                  - Cluster throws IndexShardCreationException with cause as LockObtainFailedException: Can't lock shard [qwer3483987][0], timed out after 5000ms

The cluster remained in this state for hours after this - sometimes showing 3 nodes and sometimes only 2. The state was fluctuating between yellow and red.

```
[2015-07-17 02:06:32,482][WARN ][indices.cluster          ] [met-ds-1-es-tune] [[qwer3483987][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [qwer3483987][0] failed to create shard
        at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [qwer3483987][0], timed out after 5000ms
        at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
        at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
        at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
        ... 9 more
```
</description><key id="95656856">12318</key><summary>Cluster ends up in unstable state after heavy indexing activity but does not recover</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Srinathc</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2015-07-17T13:32:34Z</created><updated>2015-10-08T11:58:20Z</updated><resolved>2015-10-08T11:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Srinathc" created="2015-07-17T14:45:51Z" id="122298060">Update - the cluster recovered to green state with each node reporting consistent state (_cluster/state) after around 12 hours. Will resume bulk indexing at a lower rate to understand this better.

Is there any other info that will be helpful in looking into and resolving this issue?
</comment><comment author="clintongormley" created="2015-07-24T09:53:15Z" id="124459463">Possibly related to https://github.com/elastic/elasticsearch/issues/12011
</comment><comment author="clintongormley" created="2015-07-24T09:54:46Z" id="124459665">HI @Srinathc 

Can you tell us the reason that the cluster went yellow in the first place? What caused the shard failure?

Then could you also try to provide the info requested here please: https://github.com/elastic/elasticsearch/issues/12011#issuecomment-122241136

thanks
</comment><comment author="ClauditaO" created="2015-07-24T18:05:10Z" id="124601497">Hi,

any ideas on why this is happening? I have a similar problem, after indexing 500M docs, my cluster will not recover any index.

recovered [0] indices into cluster_state

It was showing all the indexes before the last indexing process.

Should I leave elasticsearch running to see if it manages to recover the indexes after a while?

Thank you
</comment><comment author="Srinathc" created="2015-07-26T04:33:47Z" id="124944749">@clintongormley not sure why it went into yellow but there was good amount of GC events happening. Will run up the tests again and share the hot_threads output if this happens again.
</comment><comment author="Srinathc" created="2015-07-27T05:09:59Z" id="125087914">@clintongormley we encountered this issue again during our overnight test.
The hot threads output is available here - https://gist.github.com/Srinathc/4726dfaa0e9ca314e666#file-hot_threads-txt.

Documents of around 2k were being indexed at the rate of 2500 per second and things seemed to be really calm with CPU ~15%. But we increased the index rate to around 3500-4000 per second and found that the cluster was in this state in the morning.

Hope it helps to resolve the issue. Let me know if you need anything else
</comment><comment author="clintongormley" created="2015-07-27T11:47:38Z" id="125176651">@martijnvg could you take a look at this please?
</comment><comment author="clintongormley" created="2015-10-08T11:58:20Z" id="146516467">This looks like it is related to this bug in Lucene: https://issues.apache.org/jira/browse/LUCENE-6670

Basically, an OOM exception is thrown on a merge thread which means the lock is never released.  This bug is fixed in Lucene 5.3 and backported to 2.0.  For 1.x, there is not much we can do expect advising you to keep heap usage as low as possible.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Time zone parser throws illegal argument exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12317</link><project id="" key="" /><description>```
DELETE t

POST /t/t/1
{
    "name": "Prince George",
   "event-date": {
      "timestamp": "2015-07-15T00:00:00.000Z"
   }
}

POST /t/t/_search
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "now/d",
        "lte": "now/d",
        "time_zone": "-5:00"
      }
    }
  }
}
```

returns:

```
{
   "error": {
      "root_cause": [
         {
            "type": "illegal_argument_exception",
            "reason": "Invalid format: \"-5:00\" is malformed at \"5:00\""
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "t",
            "node": "l78GF4wCT1W8r2U-LSOrpA",
            "reason": {
               "type": "illegal_argument_exception",
               "reason": "Invalid format: \"-5:00\" is malformed at \"5:00\""
            }
         }
      ]
   },
   "status": 400
}
```
</description><key id="95652480">12317</key><summary>Time zone parser throws illegal argument exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Dates</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T13:02:49Z</created><updated>2015-07-21T07:27:42Z</updated><resolved>2015-07-21T07:27:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-17T15:07:02Z" id="122304324">So this does not stem from the recent strict date changes, but from a refactoring far older in the `RangeQueryParser` which uses `DateTimeZone` instead of `DateMathParser`

``` java
    public void testThatDateMathParserAndDateTimeZoneActDifferently() throws Exception {
        System.out.println(DateMathParser.parseZone("-05:00"));
        System.out.println(DateMathParser.parseZone("-5:00"));

        System.out.println(DateTimeZone.forID("-05:00"));
        // this fails always
        System.out.println(DateTimeZone.forID("-5:00"));
    }
```

This was introduced by [this commit](https://github.com/elastic/elasticsearch/commit/37287284e64a0e91ab400a16b187e719b55c8929)
</comment><comment author="clintongormley" created="2015-07-17T15:55:34Z" id="122324041">thanks for investigating @spinscale 

@rjernst could you take a look please?
</comment><comment author="rjernst" created="2015-07-17T16:12:30Z" id="122329432">@clintongormley The timezone offset you provided is not valid, it must be of the for [+-]\d\d:\d\d.  
</comment><comment author="clintongormley" created="2015-07-17T16:27:05Z" id="122333064">OK - this worked before.  Does it really need to be that strict here?
</comment><comment author="clintongormley" created="2015-07-17T16:31:42Z" id="122333978">Plus the docs show an unsupported format currently:
- https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-range-query.html#_date_options
- https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-datehistogram-aggregation.html#_time_zone
</comment><comment author="rjernst" created="2015-07-17T16:50:48Z" id="122339942">I don't think we should have our own timezone parsing method (which was removed in the referenced commit) just so we can allow a lenient format.
</comment><comment author="rjernst" created="2015-07-17T16:51:19Z" id="122340255">This is a documented standard, let's stick to it.
</comment><comment author="clintongormley" created="2015-07-18T09:49:02Z" id="122523302">OK - so the docs need updating then
</comment><comment author="colings86" created="2015-07-20T07:31:44Z" id="122785988">Plus we should add a note to the braking changes in 2.0 doc
</comment><comment author="rjernst" created="2015-07-21T07:22:46Z" id="123197184">@clintongormley @colings86 Docs updated for your review: #12357
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update time_zone specification</comment></comments></commit></commits></item><item><title>Explore option of supporting more flexible search types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12316</link><project id="" key="" /><description>Today we have `query_then_fetch` and `query_and_fetch`. This imposes a limit on the types of search functionality we can support.  For instance, if you want to auto-adjust the bucket interval so that your documents fit neatly into 10 buckets, you first need to determine the min and max values in order to calculate the correct `interval` (eg see https://github.com/elastic/elasticsearch/issues/9572 and https://github.com/elastic/elasticsearch/issues/9531).

This requires two round trips:
- first determine the min/max values
- calculate the required `interval`
- do a second trip to bucket documents per interval

Or to improve term count accuracy in a terms agg, you could:
- retrieve eg the top 20 terms from each shard
- choose the top 10 overall
- do a second trip (if needed) to get accurate counts for all terms

Or to guarantee that you get the top 10 terms overall:
- first trip retrieves the top 20 terms per shard
- calculate the overall top 10
- take the doc count of the 10th term -&gt; `10th_count`
- second trip retrieves all terms that have at least `10th_count` / `num_shards`
- third trip calculates accurate counts for all the terms returned by the second trip

Multiple search phases would also help with clustering algorithms
</description><key id="95645939">12316</key><summary>Explore option of supporting more flexible search types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Aggregations</label><label>:Search</label><label>discuss</label><label>enhancement</label><label>high hanging fruit</label><label>Meta</label></labels><created>2015-07-17T12:17:42Z</created><updated>2016-11-28T12:10:11Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-24T10:11:07Z" id="124462387">https://github.com/elastic/elasticsearch/issues/10217 will be required before we do this as decisions on how many phases are required will need to be made on the coordinating node so the query needs to be parsed there before we can do this.

Also this could get very complex since term count accuracy would require re-running the parent aggregations to get the right context (right documents) for the terms aggregation to work on for the accuracy round and would also require running the sub-aggregations on the accuracy round (and not on the initial round) to get the right values for the sub-aggregations. This gets even more complex if multiple terms aggregations are nested all with accuracy set to true.
</comment><comment author="brettlyman" created="2016-06-06T16:16:27Z" id="224008465">We're seeing the same problem mentioned in https://github.com/elastic/elasticsearch/issues/1305 that was closed since facets were deprecated, and we're using terms aggregations. We have a pretty complex setup with multiple shards and replicas per index, and the field being aggregated is a nested document. 

When we do the terms aggregation we often see buckets with wrong counts, or even no buckets returned at all. If we change the terms aggregation to a filter aggregation looking for a specific value in the nested document that should result in a bucket, we get hits returned. Note that we're not looking for "top X" buckets, just returning all buckets and trying to get an accurate count. 

I believe our queries were fine up until a couple of weeks ago, so perhaps there's a shard/routing/etc. setting that causes this to happen? Otherwise, please add my +1 to the request for a parameter to force accurate results, even though execution would be slower. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search template render API returns 500 error on malformed template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12315</link><project id="" key="" /><description>see https://github.com/elastic/elasticsearch/issues/12292

This should return a 400 error instead
</description><key id="95635921">12315</key><summary>Search template render API returns 500 error on malformed template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Search Templates</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-07-17T11:00:38Z</created><updated>2017-03-15T06:01:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Shutting down a node stops the transport layer despite of ongoing indexing operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12314</link><project id="" key="" /><description>The current shutdown order is as follows:

```
        injector.getInstance(RestController.class).stop();
        injector.getInstance(TransportService.class).stop();

        for (Class&lt;? extends LifecycleComponent&gt; plugin : pluginsService.services()) {
            injector.getInstance(plugin).stop();
        }
        // we should stop this last since it waits for resources to get released
        // if we had scroll searchers etc or recovery going on we wait for to finish.
        injector.getInstance(IndicesService.class).stop();
        logger.info("stopped");
```

The means that the transport service is stopped before indices services, meaning that any ongoing write operation in IndicesService are blocked from the network and can't send requests. This can trigger #7572 . 

IMHO we should do a best effort in flushing on going operations in IndicesService, block new one from starting and then and only then close the transport service. These days we have mechanics to do so.  This does require careful thought. 
</description><key id="95635394">12314</key><summary>Shutting down a node stops the transport layer despite of ongoing indexing operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-07-17T10:57:34Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="joune" created="2015-07-17T13:23:13Z" id="122273812">For reference and test case, please see http://stackoverflow.com/questions/31294500/elasticsearch-1-6-seems-to-lose-documents-during-high-availability-test
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove securemock from repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12313</link><project id="" key="" /><description>The securemock has a new library, located at https://github.com/elastic/securemock
</description><key id="95633105">12313</key><summary>remove securemock from repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-07-17T10:43:51Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-17T16:44:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-17T10:44:32Z" id="122241521">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>securemock/src/main/java/org/elasticsearch/mock/Mockito.java</file></files><comments><comment>Merge pull request #12313 from jaymode/securemock</comment></comments></commit></commits></item><item><title>Make sure filter is correctly parsed for multi-term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12312</link><project id="" key="" /><description>This makes sure the `filter` parameter is correctly parsed in a multi-term
vector request when using `ids` and `parameters`.

Closes #12311
</description><key id="95629297">12312</key><summary>Make sure filter is correctly parsed for multi-term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T10:19:36Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-24T15:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-07-24T10:02:41Z" id="124461210">@jpountz can you take a quick look at this? Thank you.
</comment><comment author="jpountz" created="2015-07-24T12:17:02Z" id="124501344">I left one question.
</comment><comment author="jpountz" created="2015-07-24T15:08:26Z" id="124553277">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file></files><comments><comment>Make sure filter is correctly parsed for multi-term vectors</comment></comments></commit></commits></item><item><title>Ignored `filter` parameter in _mtermvectors REST request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12311</link><project id="" key="" /><description>This only happens when using `ids` with `parameters`. For example in a request such as this one:

``` javascript
GET index/type/_mtermvectors
{
  "ids": [ ... ],
  "parameters": {
    "fields": [ ... ],
    "filter": {
      "max_num_terms": 3
    }
  }
}
```
</description><key id="95628087">12311</key><summary>Ignored `filter` parameter in _mtermvectors REST request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:REST</label><label>:Term Vectors</label><label>bug</label></labels><created>2015-07-17T10:11:14Z</created><updated>2015-07-24T15:12:24Z</updated><resolved>2015-07-24T15:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file></files><comments><comment>Make sure filter is correctly parsed for multi-term vectors</comment></comments></commit></commits></item><item><title>Update Netty to version 3.10.3.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12310</link><project id="" key="" /><description>There a version mismatch between master (previously 3.10.0.Final) and 1.7 branch (3.10.3.Final)
</description><key id="95625250">12310</key><summary>Update Netty to version 3.10.3.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Network</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T09:52:08Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-07-17T10:28:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-17T09:57:23Z" id="122233225">Changelogs:
http://netty.io/news/2015/03/23/3-10-1-Final.html
http://netty.io/news/2015/05/08/3-9-8-Final-and-3.html
</comment><comment author="jpountz" created="2015-07-17T10:18:56Z" id="122237047">LGTM
</comment><comment author="tlrx" created="2015-07-17T10:29:12Z" id="122239211">@jpountz thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Significant_terms - select on basis of quantities other than doc count.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12309</link><project id="" key="" /><description>I was looking at government spending data with the significant-terms aggregation and found a strong relationship between a supplier and a government department:

![script manager-7](https://cloud.githubusercontent.com/assets/170925/8743535/947a2ee8-2c67-11e5-92e0-87d190c80a2e.jpg)

All of this supplier's purchases are from HMRC and no other dept and that was certainly [an interesting relationship](http://www.accountancyage.com/aa/news/1808456/mps-slam-hmrc-business-acumen-offshore-company-deal)

But this was only found due to the sheer volume of expense documents - not on the basis of their financial values.
This got me thinking - what if significant_terms aggs was able to count _values_ held on document fields rather than just the volume of documents? A user could select a choice of field as the basis for counting.

Offering this option brings a few challenges:

1) Speed - it could be much slower as background frequency checks would now have to also be able to compute background _counts_ which would involve iterating over termDocs (as background_filters do today) looking up doc values.
2) Accuracy - in a sharded system we live with the idea that, broadly speaking, the volumes of documents are equally distributed and each shard can make local decisions about term selections, however the _values_ held in these docs (e.g. the financial spend on an expense) may vary wildly between shards and skewed by a single document. For this reason, the reliability of this algorithm would be worse in a sharded system than the existing basis of counting.
3) API changes - counts would now have to work with floats instead of longs so the significance-heuristic classes would need changing. Some may not work well with the idea of floats.

Thoughts?
</description><key id="95617215">12309</key><summary>Significant_terms - select on basis of quantities other than doc count.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>discuss</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-07-17T08:59:28Z</created><updated>2016-01-18T21:02:08Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-08-10T15:47:18Z" id="129502089">+1 sounds interesting
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missed documentation update on default search thread pool size reduction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12308</link><project id="" key="" /><description>We made the change here: https://github.com/elastic/elasticsearch/pull/9165

But the doc still says 3x # of processors:  https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-threadpool.html?q=thread
Instead of the new default: (availableProcessors \* 3) / 2) + 1 (then round down)
</description><key id="95594763">12308</key><summary>Missed documentation update on default search thread pool size reduction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-07-17T06:10:00Z</created><updated>2015-07-17T14:06:17Z</updated><resolved>2015-07-17T14:06:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update threadpool.asciidoc</comment></comments></commit></commits></item><item><title>Remove broken `exec` build target, replace with something better.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12307</link><project id="" key="" /><description>Currently this target is "yet another way" to run elasticsearch, which we can't maintain since its an untested configuration. Besides the fact that even if we fixed it, it would break all the time since its untested, It also has the problem that it doesnt ensure its running on the latest source code, doesn't configure any scratch space properly, won't work with securitymanager, list goes on.

Instead, `mvn package -Drun -DskipTests` will run packaging, and then startup bin/elasticsearch(.bat) .. like integration tests, but in foreground).

It also enables debugger socket on port 8000, for people that like IDE debuggers and not system.out.println.

Its a little slower to execute because of all the shading/RPM/DEB building going on in `package` but that is just what it is right now until that stuff is moved out.

Logic is kind of in a wierd place, but thats also the point: its using our integration test logic that is in jenkins, so it is less likely to break.

Looks like this:

```
...
[INFO] Attaching created debian package /home/rmuir/workspace/elasticsearch/core/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.deb
[INFO] 
[INFO] --- maven-antrun-plugin:1.8:run (execute) @ elasticsearch ---
[INFO] Executing tasks
main:
stop-external-cluster:
start-foreground:
   [delete] Deleting directory /home/rmuir/workspace/elasticsearch/core/target/integ-tests
    [unzip] Expanding: /home/rmuir/workspace/elasticsearch/core/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.zip into /home/rmuir/workspace/elasticsearch/core/target/integ-tests
     [exec] Listening for transport dt_socket at address: 8000
     [exec] [2015-07-17 00:16:12,421][INFO ][node                     ] [smoke_tester] version[2.0.0-beta1-SNAPSHOT], pid[22803], build[48bc49c/2015-07-17T04:15:40Z]
     [exec] [2015-07-17 00:16:12,421][INFO ][node                     ] [smoke_tester] initializing ...
     [exec] [2015-07-17 00:16:12,423][INFO ][plugins                  ] [smoke_tester] loaded [], sites []
     [exec] [2015-07-17 00:16:12,493][INFO ][env                      ] [smoke_tester] using [1] data paths, mounts [[/ (/dev/disk/by-uuid/1b3a9e9c-3d5f-412e-be52-a7e64433e25d)]], net usable_space [205.5gb], net total_space [229.1gb], spins? [no], types [ext4]
     [exec] [2015-07-17 00:16:13,631][INFO ][node                     ] [smoke_tester] initialized
     [exec] [2015-07-17 00:16:13,631][INFO ][node                     ] [smoke_tester] starting ...
     [exec] [2015-07-17 00:16:13,710][INFO ][transport                ] [smoke_tester] bound_address {inet[/127.0.0.1:9300]}, publish_address {inet[/127.0.0.1:9300]}
     [exec] [2015-07-17 00:16:13,716][INFO ][discovery                ] [smoke_tester] prepare_release/yEiAMd9WTMmUMYBn1EUFtQ
     [exec] [2015-07-17 00:16:16,736][INFO ][cluster.service          ] [smoke_tester] new_master [smoke_tester][yEiAMd9WTMmUMYBn1EUFtQ][beast][inet[/127.0.0.1:9300]], reason: zen-disco-join(elected_as_master, [0] joins received)
     [exec] [2015-07-17 00:16:16,771][INFO ][http                     ] [smoke_tester] bound_address {inet[/127.0.0.1:9200]}, publish_address {inet[/127.0.0.1:9200]}
     [exec] [2015-07-17 00:16:16,772][INFO ][node                     ] [smoke_tester] started
     [exec] [2015-07-17 00:16:16,795][INFO ][gateway                  ] [smoke_tester] recovered [0] indices into cluster_state
     ....
```
</description><key id="95582531">12307</key><summary>Remove broken `exec` build target, replace with something better.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T04:21:00Z</created><updated>2015-07-17T19:13:30Z</updated><resolved>2015-07-17T19:13:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-17T05:04:17Z" id="122176613">LGTM, two questions:
- is it possible to hook it up to compilation or the `clean` target automatically? I accidentally ran it without a `clean` in front and got: https://gist.github.com/dakrone/3c3347e263cf2e474dc2
- I notice it's no longer possible to run multiple instances of ES out of the same checkout with this (I used to run `mvn exec:exec` 3 times in a row on a checkout to run 3 nodes), is that intended? (no worries either way, I can look at it myself if you don't want to)

Regardless, thanks for fixing the exec target, I will use this all the time!
</comment><comment author="rmuir" created="2015-07-17T10:23:27Z" id="122238117">&gt; is it possible to hook it up to compilation or the clean target automatically? I accidentally ran it without a clean in front and got: https://gist.github.com/dakrone/3c3347e263cf2e474dc2

Its hooked into compilation. You got something else going on...

&gt; I notice it's no longer possible to run multiple instances of ES out of the same checkout with this (I used to run mvn exec:exec 3 times in a row on a checkout to run 3 nodes), is that intended? (no worries either way, I can look at it myself if you don't want to)

not easily. currently this shares the integration test directory. I am getting really worried here honestly. I think if you want to run 3 nodes you should not use targets like this but run elasticsearch properly.

We gotta keep it simple...
</comment><comment author="jpountz" created="2015-07-17T10:23:41Z" id="122238175">LGTM
</comment><comment author="rmuir" created="2015-07-17T10:29:05Z" id="122239165">I am hostaging this pull request until we get consensus on the multiple nodes stuff. This is just not a direction i think we should take. 
</comment><comment author="dakrone" created="2015-07-17T14:19:51Z" id="122291519">&gt; I am hostaging this pull request until we get consensus on the multiple nodes stuff. This is just not a direction i think we should take.

My +1 is totally not contingent on either of the questions. **I'm +1 to push as-is**. I understand it's not something we should support, I just wondered if it was a byproduct.
</comment><comment author="dakrone" created="2015-07-17T14:40:29Z" id="122296752">&gt; Its hooked into compilation. You got something else going on...

This is reproducible for me:

``` bash
git checkout c2c8956
mvn clean compile test-compile
git checkout pr/12307
mvn package -Drun -DskipTests
```

Then - http://p.draines.com/1437144342682a61b674a.txt

(there's nothing special about c2c8956, I just picked an older commit from a week ago).

I guess I expect Maven to see that files have been changed and compile/clean accordingly? Is this an incorrect expectation? (I don't know Maven very well at all)

Again, not anything that should prevent this PR from being merged, I'm just trying to understand Maven's behavior.
</comment><comment author="rmuir" created="2015-07-17T14:57:06Z" id="122301848">&gt; I guess I expect Maven to see that files have been changed and compile/clean accordingly? Is this an incorrect expectation? (I don't know Maven very well at all

This pr depends on `mvn package`. In fact it is just running `mvn package` but with flags that say "ok also start a server afterwards".

so that problem you have, I mean it is a problem, but its unrelated to this PR and an existing problem in `mvn package`? 

The .zip file mvn package builds is broken in some cases if the workspace is dirty. If we were to release that, all users would get that exception.

Lets open a side issue for that one. It means that maven package isn't working well if the workspace is dirty. 
</comment><comment author="dakrone" created="2015-07-17T14:58:53Z" id="122302383">&gt; Lets open a side issue for that one. It means that maven package isn't working well if the workspace is dirty. 

Sounds good, I'll open an issue for this.
</comment><comment author="jpountz" created="2015-07-17T15:00:03Z" id="122302697">The issue might be in Maven itself? https://cwiki.apache.org/confluence/display/MAVEN/Incremental+Builds
</comment><comment author="rmuir" created="2015-07-17T15:00:10Z" id="122302724">&gt; I understand it's not something we should support, I just wondered if it was a byproduct.

I think its something we should not try to do, until the integration tests support multiple nodes and have jobs running in jenkins that do that. Indeed this is something we probably need, to hook in backwards compatibility tests nicely and other things. But we don't have it today.

The main point of this PR is to give a target that doesn't break every day for peoples IDEs/dev environments. The whole trick here to achieve this is to lean on stuff that is tested by jenkins. I think this is really important and if we start doing fancy stuff just for the dev environment only, then we are right back where we are today, no better than having the current broken `mvn exec:exec`.

So I'd rather tackle the tests first... and in general with all of this, we need to walk before we can run.
</comment><comment author="dakrone" created="2015-07-17T15:00:56Z" id="122302866">That sounds very reasonable, thanks for the explanation!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12307 from rmuir/maven_run</comment></comments></commit></commits></item><item><title>Correct default date format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12306</link><project id="" key="" /><description>`strictDateOptionalDate` =&gt; `strictDateOptionalTime`
</description><key id="95574409">12306</key><summary>Correct default date format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rodaine</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T03:02:12Z</created><updated>2015-07-17T10:44:33Z</updated><resolved>2015-07-17T10:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12306 from rodaine/patch-1</comment></comments></commit></commits></item><item><title>Allow Elasticsearch logging into Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12305</link><project id="" key="" /><description>We do this [for Shield](https://www.elastic.co/guide/en/shield/current/configuring-auditing.html#audit-index) and it'd be rad if we could also do it for ES itself.

I know users can leverage Logstash for this, but for support to be able to export an index containing the logs from all nodes by simply enabling this sort of thing in the ES config would be super dooper helpful.

This will probably be reliant on #8786.
</description><key id="95571159">12305</key><summary>Allow Elasticsearch logging into Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-07-17T02:30:53Z</created><updated>2017-07-15T04:13:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="khalighi" created="2015-09-24T22:04:55Z" id="143062409">+1
</comment><comment author="lshunter" created="2017-07-15T04:13:48Z" id="315508257">ES logging into Elasticsearch, logstash logging into Elasticsearch, kibana logging into Elasticsearch, all wanted. Manage logs, manage yourself first.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test failure: IndicesShardStoreRequestTests.testBasic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12304</link><project id="" key="" /><description>This reproduced:
mvn test -Pdev -Dtests.seed=C1D4270765B2B237 -Dtests.class=org.elasticsearch.action.admin.indices.segments.IndicesShardStoreRequestTests -Dtests.method="testBasic" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.heap.size=1024m -Dtests.locale=hu_HU -Dtests.timezone=America/Cancun

@areek can you take a look? I've disable the test for now.
</description><key id="95570633">12304</key><summary>Test failure: IndicesShardStoreRequestTests.testBasic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label></labels><created>2015-07-17T02:26:58Z</created><updated>2015-07-17T04:05:37Z</updated><resolved>2015-07-17T04:05:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-07-17T04:05:37Z" id="122164921">Thanks @rjernst. fixed https://github.com/elastic/elasticsearch/commit/98fd8d94fec3a376baf0bb3254cb928bc7c46951
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix REPRODUCE WITH for integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12303</link><project id="" key="" /><description>Currently this will give you a bogus commandline for integration tests that fail, which is no good.
</description><key id="95566419">12303</key><summary>fix REPRODUCE WITH for integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-17T01:46:56Z</created><updated>2015-07-17T02:00:20Z</updated><resolved>2015-07-17T02:00:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-17T01:48:03Z" id="122146365">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java</file></files><comments><comment>Merge pull request #12303 from rmuir/verify-reproduce-with</comment></comments></commit></commits></item><item><title>Use junit4 for running integration tests, too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12302</link><project id="" key="" /><description>failsafe uses surefire, which sucks. It also mean integ tests act alien right now.
I would rather have the consistency, e.g. things formatted the same way, running integ tests under security manager, etc.

@dweiss nicely added the summary file that failsafe expects to junit4. However it does have bug in the formatting, so we use an xslt hack as a temporary measure!
</description><key id="95554411">12302</key><summary>Use junit4 for running integration tests, too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T23:47:17Z</created><updated>2015-07-17T01:23:10Z</updated><resolved>2015-07-17T01:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-16T23:56:14Z" id="122133222">LGTM, love the second commit message.
</comment><comment author="rmuir" created="2015-07-17T00:56:39Z" id="122139631">I added another commit, full `mvn verify` passes. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12302 from rmuir/junit4_integ</comment></comments></commit></commits></item><item><title>fs stats empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12301</link><project id="" key="" /><description>Hi all

Looking at _cluster/stats?human&amp;pretty, I see stats for everything except for fs:

```
"fs" : { },
```

Can anyone tell me what command ES is using in the background to fetch filesystem information? Or anywhere to look to easily debug? Thanks
</description><key id="95547460">12301</key><summary>fs stats empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">azerton</reporter><labels /><created>2015-07-16T22:48:29Z</created><updated>2015-07-17T14:00:04Z</updated><resolved>2015-07-17T14:00:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-17T14:00:04Z" id="122281342">Hi @azerton 

This is probably because the OS you're using is not supported by libsigar. We've removed sigar in 2.0 and will be relying on stats available from the JVM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor geo_point validate_* and normalize_* for 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12300</link><project id="" key="" /><description>This PR deprecates validate_\* and normalize_\* options for `geo_point` field mapping in favor of  more straight-forward `validate` and `normalize` parameters. This PR is for 1.7 only. 2.0 will completely remove these parameters.
</description><key id="95537221">12300</key><summary>Refactor geo_point validate_* and normalize_* for 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>deprecation</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-07-16T21:43:40Z</created><updated>2015-09-14T17:15:58Z</updated><resolved>2015-09-01T22:49:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-07-20T16:10:01Z" id="122930910">@clintongormley made the suggested change from normalize/validate to coerce/ignore_malformed, respectively. This PR should be ready to go for 1.71. Will need to make 2.0 changes separately since the mapping logic is completely different.
</comment><comment author="clintongormley" created="2015-07-23T09:21:08Z" id="124031040">@nknize i think in 2.0 we still need to maintain bwc.  So we should accept the old settings when opening old indices, and translate them to the new settings.  New indices should only accept the new settings.

Also, once merged into master, could you open an issue to add a check for this to the [migration plugin](http://github.com/elastic/elasticsearch-migration/issues)

ta
</comment><comment author="clintongormley" created="2015-07-28T11:03:00Z" id="125558964">Closed in favour of #10248
</comment><comment author="nknize" created="2015-07-28T11:33:12Z" id="125566617">So we're keeping this change out of 1.7.x and going straight to 2.0, right?
</comment><comment author="clintongormley" created="2015-07-30T10:40:49Z" id="126270414">Ahh sorry @nknize - when i was doing the release, I saw that #10248 had been merged and this one hadn't.  I didn't look at the contents of the PR.  

OK - let's move this to 2.0. We'll need to support the old normalize\* and validate\* parameters for bwc, but we should automatically migrate the settings to coerce/ignore_malformed.
</comment><comment author="dakrone" created="2015-09-01T22:49:23Z" id="136884714">We discussed this today, and since these are deprecated already in the docs for 1.7 and already removed in 2.0, there's no need to refactor them for 1.7. Closing this for now, thanks @nknize!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use recently added allocation ids for shard started/failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12299</link><project id="" key="" /><description>#12242 introduced the notion of allocation id - a unique identifier that is changed every time a shard is assigned to a node. This new id is a much utility to do the shard matching needed when processing shard started/failed messages. No need to match index uuid, node id and shard state. This change simplifies the matching logic to use these allocation ids.

 On top of that:
 1) A relocation target shards' allocation id is changed to include the allocation id of the source shard under relocatingId (similar to shard routing semantics)
 2) The logic around state change for finalize shard relocation is simplified - one simple start the target shard (we previously had unused logic around relocating state)
</description><key id="95521147">12299</key><summary>Use recently added allocation ids for shard started/failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T20:31:42Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-20T08:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-16T20:31:53Z" id="122081276">@kimchy can you take a look?
</comment><comment author="kimchy" created="2015-07-17T10:50:12Z" id="122243321">@bleskes left some comments, it is so much cleaner now :)
</comment><comment author="bleskes" created="2015-07-17T13:38:49Z" id="122276692">@kimchy thx. Pushed another commit.
</comment><comment author="kimchy" created="2015-07-19T09:52:55Z" id="122644035">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTest.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java</file></files><comments><comment>Allocation: use recently added allocation ids for shard started/failed</comment></comments></commit></commits></item><item><title>Do not kill process on service shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12298</link><project id="" key="" /><description>When installed as a service with a DEB or RPM package, we should gently wait for elasticsearch to stop (flushing indices on closing can take some time) and never kill the process.

Closes #11248 
</description><key id="95511905">12298</key><summary>Do not kill process on service shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T19:44:38Z</created><updated>2015-08-10T08:10:37Z</updated><resolved>2015-08-10T08:06:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-16T19:47:15Z" id="122068234">@costin the Windows service manager accepts a `ES_STOP_TIMEOUT` variable which is set to `0`. I don't really find any documentation about this. I guess `procrun` is used but I'm not sure of how the process is terminated on Windows. Do you have any clue or pointer? Thanks.
</comment><comment author="costin" created="2015-07-16T20:16:01Z" id="122076993">From the (commons-daemon docs](
http://commons.apache.org/proper/commons-daemon/procrun.html):

--StopTimeoutNo TimeoutDefines the timeout in seconds that procrun waits
for service to exit gracefully.

On Thu, Jul 16, 2015 at 10:48 PM, Tanguy Leroux notifications@github.com
wrote:

&gt; @costin https://github.com/costin the Windows service manager accepts a
&gt; ES_STOP_TIMEOUT variable which is set to 0. I don't really find any
&gt; documentation about this. I guess procrun is used but I'm not sure of how
&gt; the process is terminated on Windows. Do you have any clue or pointer?
&gt; Thanks.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12298#issuecomment-122068234
&gt; .
</comment><comment author="tlrx" created="2015-07-16T20:19:31Z" id="122077996">@costin I supposed - wrongly - that `0` had a specific signification compared with no timeout at all. So it means that nothing need to be done in windows scripts. 

Thanks for your quick response.
</comment><comment author="nik9000" created="2015-08-03T14:09:47Z" id="127254679">So I'm right there with you that kill -9 is going to cause trouble. But maybe we should have some more documentation on what to do if the process doesn't die? When you hit a bug that eats all the memory nothing is going to stop elasticsearch but kill -9. Maybe a `--force` option or something. Its just that I've relied on this kill -9 behavior in the past when some nodes have filled their heap.
</comment><comment author="clintongormley" created="2015-08-05T12:27:23Z" id="127981054">I'm OK with this going in without a force option.  What does any sysadmin do if any process won't stop? They kill -9 it.  I don't think we need to document everything here.  I'd rather get the fix in.
</comment><comment author="tlrx" created="2015-08-07T08:37:36Z" id="128641931">@nik9000 I tend to agree with @clintongormley on this. I rebased the code, can I push this?
</comment><comment author="nik9000" created="2015-08-07T11:39:31Z" id="128680664">Sure
On Aug 7, 2015 4:37 AM, "Tanguy Leroux" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 I tend to agree with @clintongormley
&gt; https://github.com/clintongormley on this. I rebased the code, can I
&gt; push this?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12298#issuecomment-128641931
&gt; .
</comment><comment author="tlrx" created="2015-08-10T08:10:37Z" id="129353163">@nik9000 thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scheduled backups / Method for doing typical backup rotation weekly full, daily incremental, remove backups after a few months.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12297</link><project id="" key="" /><description>As an operations engineer, I want to be able to set up backups for my elasticsearch cluster and have them just work without causing any long term problems or huge bills from AWS S3 due to backups retained forever.

Is it a good practice to just do the initial full backup of an index, then do incremental snapshot ad infinitum? 

Is there some downside for doing all of those snapshots? 

If this is a constraint, I imagine one way to handle this is to create another repository every week and do the snapshots into that repository. One can then use S3 object expiry to get rid of old files and somehow use the API to delete the record of the old repository and the snapshots.

Is there anything on the roadmap to do something like that?
</description><key id="95511089">12297</key><summary>Scheduled backups / Method for doing typical backup rotation weekly full, daily incremental, remove backups after a few months.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jwojcik</reporter><labels /><created>2015-07-16T19:41:27Z</created><updated>2015-07-17T15:41:45Z</updated><resolved>2015-07-17T13:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-17T13:56:01Z" id="122280449">Hi @jwojcik 

Incremental backups just copy over the new segments, so there is no need to do a full backup on a regular basis.

&gt; Is it a good practice to just do the initial full backup of an index, then do incremental snapshot ad infinitum?

Yes

&gt; Is there some downside for doing all of those snapshots?

Only that your storage grows.  But you can just use a cronjob with curator to remove old snapshots.  See https://www.elastic.co/guide/en/elasticsearch/client/curator/current/snapshots-subcommand.html
</comment><comment author="jwojcik" created="2015-07-17T15:41:45Z" id="122317799">Will curator remove the actual files from S3 when it removes the snapshot, or will I need to set an expiry on the s3 bucket to do this?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consistently name Groovy scripts with the same content</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12296</link><project id="" key="" /><description>When adding a script to the Groovy classloader, the script name is used
as the class identifier in the classloader. This means that in order not
to break JVM Classloader convention, that script must always be
available by that name. As a result, modifying a script with the same
content over and over causes it to be loaded with a different name (due
to the incrementing integer).

This is particularly bad when something like chef or puppet replaces the
on-disk script file with the same content over and over every time a
machine is converged.

This change makes the script name the SHA1 hash of the script itself,
meaning that replacing a script with the same text will use the same
script name.

Resolves #12212 

As a test for this, I did the following:
- Configure the resource watcher to check for new scripts more frequently (every 100ms)

``` yaml
watcher.interval.low: 100ms
watcher.interval.medium: 100ms
watcher.interval.high: 100ms
resource.reload.interval.low: 100ms
resource.reload.interval.medium: 100ms
resource.reload.interval.high: 100ms
```
- Start ES with a 1.7 JVM (since 1.8 removed permgen)
- Run a script that constantly updated a script file with the same content over and over, causing it to be re-compiled by Elasticsearch:

``` bash
while true; do
  echo "ctx._source.counter += 1" &gt; config/scripts/myscript.groovy
  echo "ctx._source.counter += 2" &gt; config/scripts/myscript.groovy
done
```

Without this change, permgen grows linearly, then the ES node hits OOME in permgen:

![screenshot from 2015-07-16 12-42-15](https://cloud.githubusercontent.com/assets/19060/8732340/05647a40-2bba-11e5-8b08-b5c7a12156f9.png)

And with this change, the classes are able to be unloaded, because they share the same class name (SHA1 of the content):

![screenshot from 2015-07-16 12-34-20](https://cloud.githubusercontent.com/assets/19060/8732348/1851baa0-2bba-11e5-85f2-8b1740efe5da.png)
</description><key id="95502000">12296</key><summary>Consistently name Groovy scripts with the same content</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T18:57:19Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-16T21:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-07-16T19:14:48Z" id="122055099">Nice. LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>UpdateByQuery support AndFilterBuilder?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12295</link><project id="" key="" /><description>I need to update a document based on filter and query
# 
# Context 

I have many attributeId =100 and partnerId=10  in documents. 
I want to filter ( attributeId =100 AND  partnerId=10 ) then update all the documents based on the matched documents.

Example 1 : Works fine

The following code works fine based on filter ( andFilter and query ) combination.

```
String rule="attributeId: 100";
```

QueryStringQueryBuilder query = QueryBuilders.queryString(rule);
AndFilterBuilder andqueryFilters = FilterBuilders.andFilter(); 
andqueryFilters.add(FilterBuilders.termFilter("partnerId", partnerId));

SearchResponse scrollResp=client.
.prepareSearch(props.getProperty("index"))
.setTypes(props.getProperty("type"))

.setSearchType(SearchType.SCAN).setScroll(new TimeValue(60000))
.setQuery(query)
.setPostFilter(andqueryFilters) 
.addFields("UserId")
.setSize(10000).execute()
.actionGet();

Example 2 : UpdateByQuery support AndFilterBuilder ? 

how to use PostFilter on  UpdateByQueryResponse????

String rule="attributeId: 100";

QueryStringQueryBuilder query = QueryBuilders.queryString(rule);
AndFilterBuilder andqueryFilters = FilterBuilders.andFilter(); 
andqueryFilters.add(FilterBuilders.termFilter("partnerId", partnerId));

UpdateByQueryClient updateByQueryClient = new UpdateByQueryClientWrapper(client);

```
        UpdateByQueryResponse response = updateByQueryClient.prepareUpdateByQuery().setIndices("index").setTypes("type")
                .setIncludeBulkResponses(BulkResponseOption.ALL)
               .setScript("ctx._source.appId +="+appId).setScriptParams(scriptParams)

                .setPostFilter(andqueryFilters) --------- how to use PostFilter on  UpdateByQueryResponse
                 .setQuery(query)
                .execute()
                .actionGet();
```
</description><key id="95491890">12295</key><summary>UpdateByQuery support AndFilterBuilder?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Praveen82</reporter><labels /><created>2015-07-16T18:08:09Z</created><updated>2015-07-17T14:03:10Z</updated><resolved>2015-07-17T14:03:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Praveen82" created="2015-07-17T05:40:16Z" id="122181959">any update on this?
</comment><comment author="clintongormley" created="2015-07-17T14:03:10Z" id="122282476">Hi @Praveen82 

Please ask questions like this on the forum instead: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`multi_match` query applies boosts too many times.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12294</link><project id="" key="" /><description>The `multi_match` query groups terms that have the same analyzer together and
then applies the boost of the first query in each group. This is not necessary
given that boosts for each term are already applied another way.
</description><key id="95478178">12294</key><summary>`multi_match` query applies boosts too many times.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.6.3</label><label>v1.7.2</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T16:52:59Z</created><updated>2015-08-06T18:07:54Z</updated><resolved>2015-08-06T17:07:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-16T16:55:20Z" id="122021206">Interestingly, this was first reported due to the fact that the same query was returning different scores on different clusters. The reason is that since the query used to pick the boost of the first element in a group, we were relying on the iteration order of hash tables, so if you had two fields that had different boosts, any of these boosts could be applied to the whole query depending on the running JVM.
</comment><comment author="markharwood" created="2015-07-24T16:05:23Z" id="124568415">For practical purposes, the use of field boosts is incompatible with `cross_fields` mode.
The user and the engine are fighting over who knows best what to do.

Cross_fields will very subtly tweak the IDF (using the same doc count for scoring all fields but adding one to the "wrong" fields to ever so slightly bias towards the most likely field).
Any user-supplied field-level boosts are typically cruder attempts to boost and will invalidate the subtlety in this light-touch scoring tweak introduced by cross-fields.
</comment><comment author="jpountz" created="2015-07-27T09:31:10Z" id="125145066">@markharwood If the user is not interested in this IDF tweak, but just in the blended index statistics, wouldn't it still be reasonable to support per-field boosts? For instance, imagine you want to search for `smith` in both `first_name` and `last_name`, but with a higher weight on `last_name` (say boost=2). Then a `cross_fields` query could still be useful so that if someone happens to have `smith` as a `first_name` in the document collection, then the idf factor would not get higher than the boost on the `last_name` field and matches on `smith` in the `last_name` would still be ranked first?
</comment><comment author="clintongormley" created="2015-07-27T11:54:09Z" id="125178195">I agree with https://github.com/elastic/elasticsearch/pull/12294#issuecomment-125145066
</comment><comment author="markharwood" created="2015-07-27T21:47:50Z" id="125351627">```
could still be useful so that if someone happens to have smith as a first_name in the document
collection, then the idf factor would not get higher than the boost on the last_name
```

That is what cross_fields is designed to do without the need for the user's helping hand providing a boost as in your example (assuming normality and there are plenty of last_name smiths in the index)- it naturally favours the most likely field interpretation rather than the rarest interpretation . 
A better description for `cross_fields` mode might be `most_likely_field` and it really doesn't rely on user guidance to achieve this.
A better example here is where a user wants an unusual item of data e.g. a person with the first_name:smith and this would be a case where a boost would be required to overcome the "sensible" tendencies in cross_fields mode. As soon as a user decides to take control of scoring with field boosts it would seem to make sense to remove all notions of IDF from the choice of query mode to avoid the user having to guess what boost levels are required to overcome these default scoring biases.
</comment><comment author="jpountz" created="2015-07-28T07:01:20Z" id="125476516">@markharwood Indeed looking for `smith` with a higher boost in the `first_name` would have been a better example in that case. I agree that the stats are not useful for single-term queries but it might become useful again if you have multiple terms? We would ignore stats differences between fields but still take into account differences between terms? (this is essentially simulating what would happen if both fields were indexed in a single field)
</comment><comment author="markharwood" created="2015-07-28T09:51:22Z" id="125527200">```
this is essentially simulating what would happen if both fields were indexed in a single field
```

Searching for `john smith` in fields `first_name` and `last_name` with cross_fields is not exactly the same as searching an _all field. `john` will search both name fields but with a slight bias towards a match in the first_name. `smith` will do the same but with a slight bias towards matches in the last_name.

If the user supplies a field boost in this particular example it is a blunt tool because it doesn't say to which of the query terms it should apply (it will introduce a bias to all query terms).

The only example of a user-supplied field boost that makes sense to me with cross-fields is if the query terms are all targeted towards a single field (otherwise the boost choices are ambiguous), the interpretation of the terms is not the common one (eg looking for smith as a first name) but we also want to include as lower-ranked matches examples of the query terms in other fields (e.g. smith in the last_name). The only benefit in the choice of cross_field mode is that it evens out the IDF scores so that the user boosts supplied are more likely to achieve the desired ranking.
</comment><comment author="jpountz" created="2015-07-28T10:03:34Z" id="125537501">@markharwood Do I understand you correctly that we have the following options:
1. remove support for per-field boosts with the `cross_fields` type
2. change the `cross_fields` type to assume terms have exactly the same stats across fields instead of the tweaks it does today
3. rename `cross_fields` to a new `most_likely_fields` option which does not support boosts and change `cross_fields` as described in 2.?
</comment><comment author="markharwood" created="2015-07-28T22:15:39Z" id="125768807">Option 1) makes sense to me. The reference docs for cross_fields open by describing it as

```
"... particularly useful with structured documents where multiple fields should match"
```

As in: _"we are looking for a number of terms and we expect to find them a**cross** more than one field"_. We have subtle scoring to ensure we reward the right field for each term.
Why then would it make any sense to invite users to supply field-level boosts to favour a particular choice of field? This is a blunt tool as it is applied to _all_ terms provided - and being a cross_field query we fully expected the terms to come from _a variety_ of fields and so any boost attempt will actually be irrelevant/inappropriate for a percentage of the terms provided.

It would be clearer all-round if we rejected any user-attempts to boost fields in this mode as it is inappropriate.
</comment><comment author="jpountz" created="2015-07-29T08:29:18Z" id="125879989">&gt; We have subtle scoring to ensure we reward the right field for each term.

I would be good with documenting the fact that the query should give a slighly better weight to the most likely fields so boosts should generally not be required. However I'm not sure this is a good reason enough to remove boost support entirely. `cross_fields` gives a higher boost to the most likely fields, but this is a very slight difference, and then we still have the term frequency and the normalization factor that participate to the final score too. Keeping boost support would allow to thwart the effect of the norm and term freq up to a certain point?
</comment><comment author="markharwood" created="2015-07-29T17:25:00Z" id="126026416">&gt; we still have the term frequency and the normalization factor that participate to the final score too.

[The docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html#type-cross-fields) talk about use on "structured documents" with the example first_name/last_name fields - TF and norms don't typically play a part in these scenarios.

&gt; I would be good with documenting the fact that the query should give a slighly better weight to the most likely fields

The reference docs describe this but unfortunately the example given is for the wrong field:

```
 (Actually, first_name:smith is given a tiny advantage over last_name:smith, just to make 
the order of results more stable.)
```

This is wrong - last_name:smith is the sensible thing that is promoted highest.

I don't mind keeping the boosts if you still think there are cases where they are useful - the pessimist in me assumed that regardless of what advice we wrote in docs, in most cases user-provided boosts would be working against the good things cross_fields does rather than adding to them and throwing an error would encourage better user behaviour.
</comment><comment author="jpountz" created="2015-07-30T12:41:19Z" id="126309721">Docs are wrong indeed. I agree with you this query becomes less useful on actual text fields. I suggest that we wait for @s1monw 's opinion before doing anything since he implemented this query in the first place.
</comment><comment author="s1monw" created="2015-08-05T10:46:59Z" id="127953714">I think I agree with mark that boost are not useful in this context. I didn't have boosts in mind while I did this so it's purely a bug I guess.
</comment><comment author="jpountz" created="2015-08-05T11:56:59Z" id="127972137">Thanks @s1monw.

I tried to iterate on this PR to fail when boosts different from 1 are provided, but we would also need to reject analyzed string fields because of term freqs and length normalization, si I ended up just fixing the bug like the previous diff did, and documenting that `cross_fields` should only be used on `not_analyzed` string fields with boosts of 1. @markharwood would it work for you?
</comment><comment author="markharwood" created="2015-08-05T13:13:14Z" id="127993120">I'm not sure I agree with the `not_analyzed` restriction/advice. The example fields `first_name` and `last_name` I imagine would commonly be lowercased at the very least.
</comment><comment author="clintongormley" created="2015-08-05T17:28:41Z" id="128084157">I agree with Mark - `cross_fields` wasn't intended for not_analyzed fields at all.  It groups fields by analyzer (so blends terms that use the same analysis chain).  I think TF and length normalization effects are still perfectly valid here.
</comment><comment author="markharwood" created="2015-08-05T17:46:33Z" id="128088433">So I think the general heuristic here is that structured forms generally capture short fields (first_name, city, address, postcode..) and when attempts to structure fail there's a single catch all big-free-text field ("comments" or "description"). Basic lower-casing is typically applied to both these field types to aid search-ability but TF and norms only generally apply to the big-free-text field. 
My assumption was cross-fields mode was about helping search on those shorter, targetted fields where norms and TF are irrelevant.
</comment><comment author="jpountz" created="2015-08-06T10:30:02Z" id="128318789">Thanks, I think I've been a bit confused by the whole discussion. I just updated the docs to say "short" instead of "not_analyzed", is it better?
</comment><comment author="markharwood" created="2015-08-06T12:29:13Z" id="128349270">Works for me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #12294 from jpountz/fix/multi_match_boost</comment></comments></commit><commit><files /><comments><comment>Updated multi-match-query.asciidoc</comment></comments></commit></commits></item><item><title>Item Query (the API)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12293</link><project id="" key="" /><description>## Introduction

In this issue we will present the API for [Item Query](https://github.com/elastic/elasticsearch/issues/11814). Item Query is a different implementation of More Like This (MLT) which is purely based on the [term vectors API](https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-termvectors.html). It could be described as a more powerful and generalized MLT. Item Query performs similarity search in three distinct steps. First, the user specifies a set of document input(s). Second, the documents are translated into vectors that best describe each of these documents. We call these vectors characteristic vectors, because they are sought to characterize each item. Third, a query is generated from the terms in the characteristic vectors and is executed. So to summarize, Item Query goes from documents (or items) to terms, and from terms to a query. The API gives the user complete control over each of these steps:

``` javascript
{
  "item_query": {
    "items": {
      ... how items are specified ...
    },
    "terms": {
      ... how terms are selected for every item ...
    }
    "query": {
      ... how the query is formed from the terms ...
    }
  }
}
```

We will describe each of these parameters in more detail in the following sections. But first let us mention that only `items`, also shorthanded `item` if there is only one item, is required. All the other parameters use sensible defaults. Following is an example query with some explanations.

``` javascript
{
  "item_query": {
    "items": {
      "item_1": {
        "_index": "index",
        "_type": "type",
        "_id": "1"
      },
      "item_2": {
        "_index": "index",
        "_type": "type",
        "_id": "2",
      }
    },
    "query": {
      "combination": "2*item_1 + item_2",
      "min_should_match": "30%"
    }
  }
}
```

Here the user has specified two items from which he would like to find similar items (or documents). To do so he has specified the `_index`, `_type` and `_id` of each item. He has additionally given them the name item_1 and item_2 respectively. From these items a characteristic vector is generated and combined according the rule "2*item_1 + item_2". We will describe the `combination` rule in more detail. But basically this lets us write any linear combination of the characteristic vectors, so we could model things like: "I like item_1 twice more than item_2", or "I like item_1 but dislike stuff from item_2". From the result of this operation, only the best terms are kept and then used to generate a boolean `OR` query with the given `min_should_match`. This query is then executed and the top matching documents are returned to the user as usual.

We now describe in more detail the `items`, `terms` and `query` parameters.
## The `items` parameter

The `items` parameter can be specified as an array of document inputs, or as an object of named document inputs to be used for combination. We can also use the shorthand notation `item` followed by a single document specification if there is only one item. Each document input correspond to the parameters of the [term vectors API](https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-termvectors.htm). The main parameters are:

| Parameter | Description | Required | Default |
| --- | --- | --- | --- |
| _index | index where to fetch item | if not in context | context |
| _type | type where to fetch or parse item | if not in context | context |
| _id | item id | if no `doc` | none |
| doc | artificial doc | if no `_id` | none |

Something important to mention is that like text queries (`like_text` in MLT) are supported with [artificial documents](https://www.elastic.co/guide/en/elasticsearch/reference/1.6/docs-termvectors.html#docs-termvectors-artificial-doc). Other per item parameters include `fields`, `filter`, `dfs` or `per_field_analyzer` and are used to override the settings in the `terms` parameter if need be.
## The `terms` parameter

The `terms` parameter is optional. It specifies top level parameters as to how to generate the characteristic vectors of each item. It corresponds to the `parameters` of the [multi-term vectors API](https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-multi-termvectors.html). The main parameters are:

| Parameter | Description | Default |
| --- | --- | --- |
| fields | fields to fetch the terms from | * |
| filter | terms filtering parameter | MLT like |
| per_field_analyzer | overrides analyzer at the field | analyzer at the field |
| dfs | whether to use dfs for exact scoring | false |

By default the `filter` parameter selects the best term in a similar fashion to MLT, that is using the same defaults. Under the hood this uses the [terms filtering feature](https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-termvectors.html#_terms_filtering) of the term vectors API.
## The `query` parameter

The `query` parameter is optional. It corresponds to the same [query formation parameters](https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-mlt-query.html#_query_formation_parameters) as MLT, with the same defaults.

| Parameter | Description | Default |
| --- | --- | --- |
| minimum_should_match | number of terms that must match | 30% |
| boost_terms | boost each term by score \* boost factor | 0 (deactivated) |
| include | include input documents | false |
| combination | linear combination of the characteristic vectors | sum of the vectors |

All the parameters are straight forward. The `combination` parameter requires more explanation and is probably best described with an example. Something to note though is that, like all the parameters of `query`, this parameter is optional and defaults to simply summing the entries of the resulting characteristic vectors. Let us go through an example to better explain this parameter.

From the previous query, suppose the characteristic vectors for item_1 and item_2 would look like this:

Vector for item_1:

| term_a | term_b | term_c |
| --- | --- | --- |
| 1 | 2.5 | 0.5 |

Vector for item_2:

| term_a | term_c | term_d |
| --- | --- | --- |
| 1 | 3 | 7 |

Under the combination rule "2*item_1 + item_2" this would give us:

| term_a | term_b | term_c | term_d |
| --- | --- | --- | --- |
| 2*1 + 1 = 3 | 2*2.5 = 5 | 2*0.5 + 3 = 4 | 7 |

Which would then subsequently result in the following final characteristic vector being chosen:

| term_b | term_c | term_d |
| --- | --- | --- |
| 5 | 4 | 7 |

We could also have written something like "2*item_1 - item_2" to model dislikes. In this case, the possibly negative entries are just ignored, and therefore will not be part of the final generated query.
## Conclusion

To summarize an item query could be as simple as:

``` javascript
{
  "item_query": {
    "item": {
      "_index": "index",
      "_type": "type",
      "_id": "1"
    }
  }
}
```

Or as complicated as:

``` javascript
{
  "item_query": {
    "items": {
      "item_1": {
        "_index": "index",
        "_type": "type",
        "_id": "1"
      },
      "item_2": {
        "_index": "index",
        "_type": "type",
        "_id": "2"
      },
      "item_3": {
        "_index": "index",
        "_type": "type",
        "doc": {
          "plot": "A human-looking indestructible cyborg is sent from 2029 to 1984 to assassinate a waitress"
        }
      }
    },
    "terms": {
      "fields": [
        "plot",
        "title"
      ],
      "filter": {
        "max_num_terms": 100,
        "min_doc_freq": 1
      }
    },
    "query": {
      "combination": "item_1 + item_2 - 3*item_3",
      "min_should_match": "30%"
    }
  }
}
```

The API gives complete control over each step from item specification to query generation. It is as simple to use as MLT, while at the same time powerful enough to perform sophisticated nearest neighbor type of searches.
</description><key id="95461584">12293</key><summary>Item Query (the API)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>feature</label><label>Meta</label></labels><created>2015-07-16T15:36:26Z</created><updated>2015-08-11T09:14:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-16T17:06:07Z" id="122023578">This looks easier to understand to me than MLT for multiple documents. However I think there are too many parameters here. I know we have them on MLT as well but I'm concerned we are investing time and energy in supporting these parameters while few people are using them. I would rather keep as few parameters as possible and invest time into making it work better out of the box?
</comment><comment author="alexksikes" created="2015-07-16T17:21:15Z" id="122026808">Thanks for the quick feedback. 

The `items` and `terms` parameters are nothing more than the same parameters as the mTVs API. So in terms of support we are re-using these parsers and then simply making a call to the mTVs API. The only new code to be written is for the `query` parameter, but then it seems fairly straight forward to write the code to select the best terms from the characteristic vectors and generate a boolean OR query from these terms. The `combination` sub-parameter might be a little more tricky though, but we might not need to support this at first. 

If you think there are two many options, which can confuse the user, it is also possible to restrict them and keep it simple. But under the hood, it is term vectors code with terms filtering.
</comment><comment author="rjernst" created="2015-07-16T17:37:29Z" id="122031132">Can we call it "documents" query instead of "items"? We index documents, documents are what the items query refers to (with _id and such) and we don't use the terminology "items" anywhere I know of? Since we are aiming for simplicity, I don't think we should reuse terminology from MLT where it doesn't make sense.
</comment><comment author="mikemccand" created="2015-07-18T10:42:26Z" id="122528429">This looks like a nice generalization of MLT!  I agree "documents" seems better since it's a known name users all understand already.

Can you give some example real-world use cases?  I know you have a cool screen shot somewhere showing how to find similar movies :)
</comment><comment author="alexksikes" created="2015-07-18T17:04:01Z" id="122564139">@mikemccand Thanks for the feedback.

There is a good reason why this is called item query as opposed to document query, but I still need to write the proposal on Features. Right now it is best to think of this as a generalized MLT, just more powerful and easier to understand.

Concerning the screen shot, I think that was for the [intensity highlighter](https://github.com/elastic/elasticsearch/issues/12326) which I think would be much needed in the context of MLT or Item Query.
</comment><comment author="rjernst" created="2015-07-18T19:11:37Z" id="122581330">&gt; There is a good reason why this is called item query as opposed to document query, but I still need to write the proposal on Features. Right now it is best to think of this as a generalized MLT, just more powerful and easier to understand.

Can you explain the reasoning for item query without referencing MLT? The point I was making is we shouldn't be stuck in naming simply because this is a "generalized MLT". Think of someone who has never used MLT: what makes sense to them, a query that you pass "documents" (a term already used throughout), or a query that you pass "items" (a term they haven't seen at all in our documentation)?
</comment><comment author="clintongormley" created="2015-07-19T13:06:02Z" id="122660134">&gt;  I would rather keep as few parameters as possible and invest time into making it work better out of the box?

I agree - I'd like to hear about the variety of use cases for this feature.  If there are one or two use cases which make up 95%, (eg query expansion) then perhaps we can provide something more specific which just works out of the box.
</comment><comment author="alexksikes" created="2015-07-20T11:20:33Z" id="122853471">Sure let's take an example in the context of image search and computational biology. The [YRC Public Image Repository](http://images.yeastrc.org/imagerepo/searchImageRepoInit.do) is a large image collection of yeasts which have been genetically modified and studied. If you wanted to provide a "more like this" feature for that system, the first thing you may want to try is reduce each image to an histogram of intensities, turn these histograms into vectors and perform nearest neighbor search in that space. The hope is that images with similar histograms are probably related. 

The picture below shows more precisely how intensity histograms are computed. Here we simply go over each pixel, or neighborhood around that pixel, and count the intensity values.

![intensity_histograms](https://cloud.githubusercontent.com/assets/43475/8773190/a12af5ba-2ed2-11e5-9184-584e2a0b562e.png)

This just one type of features for images, but there are many others for all types of applications. For example, we could have further considered texture features or shape features. However, no matter what types of features we choose, the same process could be applied. The feature values are extracted from the item, and then put into a vector so that we can perform nearest neighbor search in that space.

How is this related to Item Query?

Our feature space is no longer tf-idf based, but rather intensity counts based. If we want to have decent results, we would need to change the default Lucene similarity to take these counts into account. We would also need a custom way to generate the characteristic vector for the image, which is no longer based of tf-idf. This later correspond to the `terms` parameter of item query which in the future should allow for scripting. 

I haven't mentioned how these feature vectors would be stored. There are different approaches, but we could for example imagine storing them as unique tokens with payloads corresponding to the intensity values, all in one field.

So the process has been to go from a document, which is not necessarily textual, that we call item, to a characteristic vector of terms or features, not necessarily based of tf-idf, and finally to an actual textual query.
</comment><comment author="jpountz" created="2015-07-21T07:34:42Z" id="123199626">&gt; Can we call it "documents" query instead of "items"?

+1 to documents, this looks more consistent to me
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Invalid root_cause for mustache_exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12292</link><project id="" key="" /><description>In the JavaScript tests I'm trying to validate the error output when specifying an invalid mustache template, and while the correct error is produced, it is not sent in the API response as the root cause.

@colings86 mentioned this is likely a bug in the exception XContent rendering. (whatever that means :smile_cat:)

Here is a trace of the request:

```
  -&gt; POST http://localhost:9406/_render/template
  {
    "inline": "{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }",
    "params": {
      "my_value": "bar",
      "my_size": 100
    }
  }
  &lt;- 500
  {
    "error": {
      "root_cause": [
        {
          "type": "script_exception",
          "reason": "Failed to compile inline script [{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }] using lang [mustache]"
        }
      ],
      "type": "script_exception",
      "reason": "Failed to compile inline script [{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }] using lang [mustache]",
      "caused_by": {
        "type": "mustache_exception",
        "reason": "Improperly closed variable in query-template:1"
      }
    },
    "status": 500
  }
```

And the test that produced it: https://github.com/elastic/elasticsearch/blob/d4b94a9be2f0f519083c79c61dc9b6bbe67b7387/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml#L52-L55
</description><key id="95457917">12292</key><summary>Invalid root_cause for mustache_exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels><label>:Exceptions</label><label>bug</label><label>discuss</label></labels><created>2015-07-16T15:20:45Z</created><updated>2015-07-17T17:27:27Z</updated><resolved>2015-07-17T17:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-16T15:37:15Z" id="121995671">From talking to @clintongormley and @kimchy, the `root_cause` is actually correct here as it is supposed to stop at the `script_exception` since the `mustache_exception` is a implementation detail. The problem here is that there is a difference in implementation between the exception checking on the Java client REST tests and the JS REST tests (and possibly other clients too). The Java client REST tests implementation presumably gets an Exception object (rather than a JSON representation) and will check the messages for all causes (maybe @javanna can confirm that?) whereas the JS Client REST tests implementation checks the rendered JSON and looks only at the `root_cause.reason` field. So the question is which implementation to change. Should the Java implementation only check the root cause as defined by the ExceptionsHelper? or should the JS implementation (and other clients potentially) be changed to check all `reason` fields under `error`?
</comment><comment author="colings86" created="2015-07-16T15:41:25Z" id="121997005">Looking at [DoSection](https://github.com/elastic/elasticsearch/blob/4c981ff4bfc250080d521af105b5e8589c9fc517/core/src/test/java/org/elasticsearch/test/rest/section/DoSection.java#L105) it looks like we take the string value of the `error` object in the JSON response and try to match it to the regex.
</comment><comment author="clintongormley" created="2015-07-17T11:16:46Z" id="122249324">@spalger Shouldn't you just be including the rest of the structured data in the exception object that youc reate? and rendering it as part of stringification?
</comment><comment author="spalger" created="2015-07-17T17:27:27Z" id="122348520">Yeah @clintongormley, I'm going to revamp the exception message a bit to include the related issue information so that the test will pass and the errors will be more informative. Thanks all :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport Tracer should exclude "cluster:monitor/nodes/liveness" by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12291</link><project id="" key="" /><description>This action is a liveness test added in #8763 . It should be excluded, just like the fault detection logic or things become overly chatty.
</description><key id="95456888">12291</key><summary>Transport Tracer should exclude "cluster:monitor/nodes/liveness" by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Network</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T15:16:09Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-16T15:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-16T15:19:43Z" id="121990438">you could use `TransportLivenessAction.NAME` instead of a string

LGTM otherwise
</comment><comment author="bleskes" created="2015-07-16T15:20:13Z" id="121990551">good one. will change
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file></files><comments><comment>Transport: Tracer should exclude "cluster:monitor/nodes/liveness" by default</comment></comments></commit></commits></item><item><title>CLITool: Port PluginManager to use CLITool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12290</link><project id="" key="" /><description>In order to unify the handling and reuse the CLITool infrastructure
the plugin manager should make use of this as well.

This obsoletes the getopt options for the `install`, `remove` and `list` commands
options but requires the user to use `install` as the first argument of the CLI.

This is basically just a port of the existing functionality, which
is also the reason why this is not a refactoring of the plugin manager,
which will come in a separate commit.
</description><key id="95443146">12290</key><summary>CLITool: Port PluginManager to use CLITool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T14:17:45Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-21T12:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-16T17:25:21Z" id="122027938">@spinscale I left a couple questions.
</comment><comment author="spinscale" created="2015-07-21T09:36:40Z" id="123239780">@rjernst if you have time, can you take another look?
</comment><comment author="rjernst" created="2015-07-21T09:48:49Z" id="123244390">LGTM, but can you open an issue to add mutually exclusive options support to CliTool?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>During an upgrade one of the newly upgraded nodes restarted all on its own</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12289</link><project id="" key="" /><description>The very last machine to be upgraded from 1.3 to 1.6 restarted all on its own:
https://phabricator.wikimedia.org/P994

Looks like a copy failed and then it restarted. Grand mystery or something that can be explained?
</description><key id="95430838">12289</key><summary>During an upgrade one of the newly upgraded nodes restarted all on its own</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-07-16T13:15:58Z</created><updated>2016-01-18T21:01:21Z</updated><resolved>2016-01-18T21:01:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T21:01:21Z" id="172652881">Nothing more in 7 months - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade groovy from 2.4.0 to 2.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12288</link><project id="" key="" /><description /><key id="95418313">12288</key><summary>Upgrade groovy from 2.4.0 to 2.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Scripting</label><label>upgrade</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T11:57:12Z</created><updated>2016-12-21T07:30:36Z</updated><resolved>2015-07-16T12:40:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-16T12:22:09Z" id="121942234">LGTM

I'm attaching changelogs for reference:
- http://groovy-lang.org/changelogs/changelog-2.4.1.html
- http://groovy-lang.org/changelogs/changelog-2.4.2.html
- http://groovy-lang.org/changelogs/changelog-2.4.3.html
- http://groovy-lang.org/changelogs/changelog-2.4.4.html
</comment><comment author="clintongormley" created="2015-07-16T12:39:33Z" id="121945740">LGTm
</comment><comment author="lup7in" created="2015-08-04T06:18:48Z" id="127491579">I noticed that this problem occurred in the transport protocol when it deserialize the special serialized object,but in the changed files I cannot find code changes,I want to know which Class  is Affected.
</comment><comment author="smartcat315" created="2016-12-21T05:27:13Z" id="268438811">How to upgrade。I mean not changing the version of elasticsearch , just upgrade groovy。My elasticsearch version is 2.3.3</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file></files><comments><comment>Merge pull request #12288 from martijnvg/upgrade_groovy</comment></comments></commit></commits></item><item><title>Searching in associated models using elasticsearch query dsl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12287</link><project id="" key="" /><description>After indexing some child records using 'include' inside as_indexed_json method in the user model, what is the correct way to refer the fields of the child record. If a user has_many phone_numbers with number as a field, I am using the following query but it returns a syntax error on phone_numbers.number. Is there any known way of referring to the number field?
response = User.search query: {
        filtered: {
            filter: {
                bool: {
                    must:[
                        {term: {phone_numbers.number: "123-456-7890"}}
                    ]
                }
            }
        }
    }
</description><key id="95411849">12287</key><summary>Searching in associated models using elasticsearch query dsl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pushkargarg</reporter><labels /><created>2015-07-16T11:20:31Z</created><updated>2015-07-17T13:36:39Z</updated><resolved>2015-07-17T13:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-17T13:36:39Z" id="122276139">Hi @pushkargarg 

Please ask questions like these on the forum instead: http://discuss.elastic.co/

the issues list is for bug reports and feature requests

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Packaging: Split packages into maven sub-modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12286</link><project id="" key="" /><description>This includes a couple of fixes for #11523 and thus supercedes it. Once we get this in, #11523 will be closed as well.

@tlrx can you take a look, if I missed something, I will test this under an RPM based distro using chkconfig based setup now.
</description><key id="95399535">12286</key><summary>Packaging: Split packages into maven sub-modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-07-16T10:04:01Z</created><updated>2015-07-28T08:24:20Z</updated><resolved>2015-07-27T15:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-16T11:05:23Z" id="121927026">tested on centos6.5 (init.d), centos 7 (systemd), ubuntu precise (inits), ubuntu vivid (systemd).. looks good
</comment><comment author="rjernst" created="2015-07-16T17:03:44Z" id="122023102">Why are the licenses/sha's under the tar distribution?
</comment><comment author="spinscale" created="2015-07-17T13:14:43Z" id="122272332">maybe @dadoonet can shed some light here, as the maven portion has not been ported, I do not see an obvious reason
</comment><comment author="rmuir" created="2015-07-17T15:07:51Z" id="122304825">I think the license checker just looks to inspect certain extensions?

I haven't done a in-depth picky review of this, but at a glance this PR looks wonderful. 

We can even think in the future about brainstorming how to test the different packages or something in some simple automated way...

I would love to see a followup issue where we make `distribution/shaded-jar` or something and give that tests as well, because currently thats yet another untested artifact. But if we separate it out, we could easily test that it works too.
</comment><comment author="clintongormley" created="2015-07-17T16:03:49Z" id="122327778">I don't understand why the licenses are under `distribution/tar` either.  Also, the license checker will only run if there is a `$project/licenses` directory, so moving the licenses disables this check.

see: https://github.com/elastic/elasticsearch/blob/master/pom.xml#L1126
</comment><comment author="dadoonet" created="2015-07-17T22:38:43Z" id="122437262">IIRC, when I created the original PR, I first decided to create 2 modules to generate `tar` and `zip` (https://github.com/spinscale/elasticsearch/commit/84680eb6fef7d603a8ab0137503e1f2212e1cbe1).

But I found then that the license checker was looking at both distributions tar and zip and was comparing some stuff. So I removed zip module and move it within tar module (https://github.com/spinscale/elasticsearch/commit/2a043c17e2528f17e6e0854915d6cb05760c8f31).

That's why all licenses are in `distribution/tar` dir.

I think we should check for each distribution (tar, zip, rpm, deb, exe)  that we have all the needed licences packaged within each artifact. Unless I totally misunderstand the purpose of the licence checker.

I did not want to touch the licence checker perl script (don't have perl knowledge TBH) but I think that licenses should be moved to `distribution/src/main/resources` so they would be shared for each package if it makes sense.

So the perl script should check if there is a `.zip` or `.tar.gz` in `release` dir and then check if all required license files are packaged. I don't really know how to do it with `rpm` or `deb` packages though.

I hope this makes sense.
</comment><comment author="clintongormley" created="2015-07-19T10:56:34Z" id="122647686">At the moment, the core and plugin modules all produce a zip file, so the licence checker uses the zip file as the canonical source of truth and compares the contents of the zip file with the contents of the licenses directory.  

The core module also produces a .tar.gz.  So if a .tar.gz file exists, it is unpacked and its contents compared to the .zip. Similar functionality could be added to unpack and check the .rpm and .deb.

Is your intention that each package should be buildable independently of the others?  Or that we will always build all 4 packages? If the latter, then the license checker should just run after the packages are built, and so can use the .zip as the source of truth, no?

I don't see the reason for moving licenses into distribution/tar.
</comment><comment author="dadoonet" created="2015-07-19T14:34:09Z" id="122666462">I understand. 

I think we should not have any dependency between tar module and zip module.

So tar can check if everything is correct even if we did not build yet the zip.

If you want to build the zip first, then you most likely need to add a new dependency to the zip artifact in the tar module to make sure that Maven build first the zip then others.
That said, this zip must not be included itself in packages. So may be it should be declared as a test dependency or excluded but TBH I don't like trying to hack Maven.

IMO I would simply add all licenses in distribution/src/main/resources then each distribution (which depends on this distribution parent) should use them as the source of truth.

Why did we compare the zip and tar contents instead of comparing tar and zip against licenses dir. Am I missing something?
</comment><comment author="clintongormley" created="2015-07-20T08:37:00Z" id="122813559">&gt; I think we should not have any dependency between tar module and zip module.

OK - I can modify the script to look for .zip/.tar.gz/.rpm/.deb and unpack and check whatever it finds.  This will make them independent.

&gt; Why did we compare the zip and tar contents instead of comparing tar and zip against licenses dir. Am I missing something?

All modules build a zip.  Only core builds .tar.gz.  Comparing the contents of .tar.gz to the .zip was just easier than to the licenses directory, but easy to change that.
</comment><comment author="tlrx" created="2015-07-20T14:36:23Z" id="122904350">@spinscale sorry for the time it takes me to review this.

Note that I had to change `&lt;version&gt;2.0.0.beta1-SNAPSHOT&lt;/version&gt;` by `&lt;version&gt;2.0.0-beta1-SNAPSHOT&lt;/version&gt;` to be able to build the packages, but once done I checked the content of every package and everything seems OK.

I tested the TAR and DEB packages on Debian 7.8, Debian 8, Ubuntu 12.04, Ubuntu 14.04 and Ubuntu 15.04: it works like a charm.

The RPM package seems to have a wrong name (see `Name` field):
- on master branch:

```
rpm -qip  elasticsearch-2.0.0-beta1_SNAPSHOT20150720132724.noarch.rpm
Name        : elasticsearch
Version     : 2.0.0
Release     : beta1_SNAPSHOT20150720132724
...
```
- on 1.7.0:

```
rpm -qip /home/tanguy/Logiciels/elasticsearch/elasticsearch-1.7.0.noarch.rpm 
Name        : elasticsearch
Version     : 1.7.0
Release     : 1
...
```
- on this pull request:

```
rpm -qip rpm/target/releases/elasticsearch-2.0.0-beta1-SNAPSHOT.rpm 
Name        : elasticsearch-2.0.0-beta1-SNAPSHOT.rpm
Version     : 2.0.0
Release     : beta1_SNAPSHOT20150720143334
...
```

Sadly it prevents me from running automated tests for RPM package.
</comment><comment author="spinscale" created="2015-07-21T10:16:20Z" id="123251279">@tlrx fixed the naming issue
</comment><comment author="tlrx" created="2015-07-21T14:51:16Z" id="123355373">I still had to change `&lt;version&gt;2.0.0.beta1-SNAPSHOT&lt;/version&gt;` by `&lt;version&gt;2.0.0-beta1-SNAPSHOT&lt;/version&gt;` to build the packages, otherwise LGTM.

Tested on CentOS6.6, CentOS7, Fedora21 and OpenSUSE13
</comment><comment author="spinscale" created="2015-07-22T07:21:11Z" id="123588418">@tlrx what error did you get if you did not change the version?

**Update**: Scratch that, it is only wrong in the `distribution` directory poms.. will fix
</comment><comment author="tlrx" created="2015-07-22T07:25:28Z" id="123589328">@spinscale Maven cannot resolve the `elasticsearch-parent` project in version `&lt;version&gt;2.0.0.beta1-SNAPSHOT&lt;/version&gt;`. Anyway the current master is set to [2.0.0-beta1-SNAPSHOT](https://github.com/elastic/elasticsearch/blob/master/pom.xml#L8-9) so it has to be changed before merging.
</comment><comment author="spinscale" created="2015-07-22T15:35:13Z" id="123760348">added the license checks to all distributions...

there is one last blocker, where my maven skills &amp; understanding leave me. Right now the `mvn verify` in the core needs access to the `.zip` disitribution as it is used in the integration tests. However the generation of the zip is not yet done. Two work arounds where
- Just put the zip generation back into the core
- Move the tests ending with `IT` into the zip file distribution and run the tests from there. However there needs more to be done than just moving the class, and I am not a hundred percent sure what and how, as I dont understand what maven is doing/requiring here - maybe we just need a certain dependency here like the unpacked test jar?

@dadoonet @rmuir any pointers are appreciated
</comment><comment author="rmuir" created="2015-07-22T16:15:40Z" id="123776731">Integration tests test these artifacts, that is the point of them... we take the zip of es core, unzip it, bin/plugin install any plugins, and start up bin/elasticsearch in the background and run tests against this external cluster. afterwards we shut it down.

So to me, ideally these tests would run from the .zip module. Off the top of my head:
- look in /pom.xml: maven-dependency-plugin is used to bring in the es-core.zip for the pre-integration-test phase. This dependency needs to be org.elasticsearch.distribution.
- look in /core/pom.xml: the pre-integration-test and post-integration-test logic here needs to be moved to the .zip module. now add `&lt;skip.integ.tests&gt;true&lt;/skip.integ.tests&gt;` to the core module as it doesnt have any integration tests (if it is to have any, it should be against the artifacts it produces, e.g. its shaded jar) 
- other distribution formats probably need `&lt;skip.integ.tests&gt;true&lt;/skip.integ.tests&gt;` at the moment too, because they are also missing them. we can give them similar tests to the zip one in the future.

Let me know if you have trouble, I can take a look at the branch too.

PS: this PR changes a lot of things like groupids. i would use grep to look for old artifacts and see if they are mentioned anywhere else in the build. otherwise it can be hell to debug when you are bringing in stale shit.
</comment><comment author="spinscale" created="2015-07-23T06:36:56Z" id="123996041">I may be misunderstanding maven again here, but referring to the distribution in `/pom.xml` means, a distribution build must have been finished, which in turn also requires a a successful run integration test in the `distribution/` directory?

Would be awesome if you could take another look at the branch, I am getting lost in maven translation here (like when do you access the maven repository to get a dependency vs. when are local files used, which I think should always happen, but does not).

I will recheck the groupids,  thanks for the pointer.
</comment><comment author="rmuir" created="2015-07-23T11:18:41Z" id="124061954">&gt; I may be misunderstanding maven again here, but referring to the distribution in /pom.xml means, a distribution build must have been finished, which in turn also requires a a successful run integration test in the distribution/ directory?

It works the same as having a dependency for our purposes here. That means it comes from reactor in a multi-module build, or will download if needed. Note that pom builds don't do the copy because they have no integration tests.
</comment><comment author="rmuir" created="2015-07-23T13:11:21Z" id="124092344">I pushed fixes for integration tests. But this just shows packaging is broken!

for example bin/plugin does not work from the zip.

The only reason we know this is because of integration tests. I think this is complicated enough that we need integration tests for each packaging type, that needs to block the change. otherwise its too scary to touch anything.

I am not familiar enough to know what is happening and why bin/plugin no longer works due to this change, but you can mvn install -DskipTests, then cd plugins/analysis-kuromoji and do 'mvn verify -Dskip.unit.tests' to quickly see.
</comment><comment author="spinscale" created="2015-07-23T21:47:48Z" id="124248394">fixed the `/bin/plugin` issue (the shell script was duplicated), removed the wrong one and fixed the used one... `mvn clean verify` now passes when run in `/`
</comment><comment author="spinscale" created="2015-07-24T10:00:23Z" id="124460667">I also moved the bats scripts around (those still need to get into CI, they are also some part of integrations tests)... 

Tested installation of the packages on ubuntu/centos and looks good

@clint from my point of view this looks ready - another review would be nice
</comment><comment author="spinscale" created="2015-07-27T15:53:08Z" id="125252516">Closed by https://github.com/elastic/elasticsearch/commit/9628d2632fa39fb8aa03f5eb008a65fc4948bb1a
</comment><comment author="dadoonet" created="2015-07-27T16:55:38Z" id="125270921">Thank you @spinscale for moving this PR forward. really excited to see this merged in! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>allow dynamic template deletion by defining an empty template (based on master)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12285</link><project id="" key="" /><description>allow dynamic template deletion by defining an empty template - TESTS FAIL due to improper handling of mergeresult simulation. See https://github.com/elastic/elasticsearch/pull/11677#issuecomment-121566538
</description><key id="95392639">12285</key><summary>allow dynamic template deletion by defining an empty template (based on master)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">salyh</reporter><labels><label>:Mapping</label><label>feature</label><label>feedback_needed</label></labels><created>2015-07-16T09:35:58Z</created><updated>2016-09-12T21:20:20Z</updated><resolved>2016-09-12T21:20:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-17T08:14:14Z" id="122212478">Hi @salyh , I just had a look and removal does not work because empty templates are lost during serialization. I tried to comment out the `nonEmptyTemplatePresent` logic in `RootObjectMapper.doXContent` and it makes removal work but now `testAddEmtpy` fails. :)
</comment><comment author="jpountz" created="2015-07-17T10:03:26Z" id="122234061">I guess the issue here is that we should not handle mappings and mappings updates differently. So maybe we should merge empty templates like any regular template in the merge method and just make sure to consider empty templates as non-existing in `findTemplate`. Said otherwise, putting an empty template would not remove existing templates but just deactivate them?
</comment><comment author="clintongormley" created="2016-03-10T11:20:36Z" id="194798507">Hi @salyh 

Are you still interested in working on this?
</comment><comment author="salyh" created="2016-03-10T11:28:05Z" id="194800244">if its still valueable of course
</comment><comment author="dakrone" created="2016-04-06T20:58:08Z" id="206564902">Hi @salyh, would you be willing to make the changes that @jpountz suggested and update this PR?
</comment><comment author="dakrone" created="2016-09-12T21:20:19Z" id="246497580">Closing this with no feedback
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not assert that all shards were successful in tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12284</link><project id="" key="" /><description>Most of our tests call assertSearchResponse which checks whether all shards
were successful. However this is usually not necessary given that all shards
that received documents should be available given the way our indexing works.
The only shards that might not be available are those that did not index
any documents. So removing this assertion would allow us to remove most
ensureGreen/ensureYellow calls while still being able to assert on the content
of the search response since all data have been taken into account.

For now I only removed the ensureYellow/Green calls from SearchQueryTests in
order to not de-stabilize the build, but eventually we should remove most of
them.
</description><key id="95390901">12284</key><summary>Do not assert that all shards were successful in tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-16T09:25:17Z</created><updated>2015-08-13T14:27:52Z</updated><resolved>2015-07-16T12:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-16T09:26:24Z" id="121898574">LGTM. Thx for picking it up.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Merge pull request #12284 from jpountz/fix/failure_assertions</comment></comments></commit></commits></item><item><title>WIP: GeoDistanceQueryBuilder/-Parser refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12283</link><project id="" key="" /><description>This is pretty much work in progress (e.g. misses validation). It comes with two questions/issues I ran into and need help with/ feedback for:
1. In order to make JSON (de-)serialization roundtrip I believe in QueryBuilder:doXContent I need to be able to generate JSON that can subsequently be parsed here: https://github.com/elastic/elasticsearch/blob/9fa69bd663206c0d7800d866acd7afe40bb6d9ce/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java#L128 - we seem to parse two consecutive boolean values stored as one value in a field. I didn't find a way of generating valid JSON that can be parsed by this line though.
2. When checking the code for query generation I ran into the same issue as with #11969 - except this time (if I'm reading things correctly) we always need a valid indexService.
</description><key id="95382847">12283</key><summary>WIP: GeoDistanceQueryBuilder/-Parser refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-16T08:50:02Z</created><updated>2015-09-10T19:19:52Z</updated><resolved>2015-09-10T19:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-07-20T09:51:23Z" id="122829211">Wrt. to parsing consecutive boolean values out of json - this might become obsolete anyway as soon as #12300 is in.
</comment><comment author="cbuescher" created="2015-07-21T13:46:38Z" id="123311225">@MaineC did a first quick round of review. Not sure what to do regarding the question if we need a valid indexService. If this is related to the "optimizeBbox" option, I saw that theres also a "none" option in the resulting Lucene query, maybe that would help? Don't properly understand what that option does to be honest. 
The other question regarding the "normalize" parameter seems to be solved or at least deprecated, right? 
</comment><comment author="MaineC" created="2015-07-21T19:32:16Z" id="123455349">Thanks for your comments - will incorporate them into the PR tomorrow.

&gt; Not sure what to do regarding the question if we need a valid indexService. If this is related to the
&gt; "optimizeBbox" option, 

... it is - but only partially: Even if setting this option to the value that worked for the GeoBoundingBoxQueryBuilder tests it's still impossible to run the doQuery method. The problem is in the following line:

https://github.com/elastic/elasticsearch/pull/12283/files#diff-07ff0f9821841e51d05e48d81d599582R230

which runs into an AssertionError:

java.lang.AssertionError
    at __randomizedtesting.SeedInfo.seed([C187522A567CA44F:367C501427FF61A5]:0)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.&lt;init&gt;(IndicesFieldDataCache.java:155)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache.buildIndexFieldDataCache(IndicesFieldDataCache.java:103)
    at org.elasticsearch.index.fielddata.IndexFieldDataService.getForField(IndexFieldDataService.java:268)
    at org.elasticsearch.index.query.QueryParseContext.getForField(QueryParseContext.java:181)
...

I've had the same issue with the GeoBoundingBoxQuery test, except that there was at least one code path that didn't run into the problem above. Maybe this is one of those queries that really need at least a single node pulled up to be tested?

&gt; The other question regarding the "normalize" parameter seems to be solved or at least deprecated, 
&gt; right?

s/seems to be solved/will be solved as soon as the PR I reference above is in, so far it's still open, no?
</comment><comment author="MaineC" created="2015-09-10T19:01:52Z" id="139345906">Needs another rebase, also needs to have unit test fixed.

Caveat: As validation and some tests will rely on what was already done for GeoBoundingBox the latter changes were merged into the branch this PR is based on. This should be merged after the bounding box PR was merged.
</comment><comment author="MaineC" created="2015-09-10T19:19:52Z" id="139352246">Will be continued in a separate PR by @cbuescher 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticsearchIntegrationTest.flushAndRefresh() not flushing (committing?) dynamic mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12282</link><project id="" key="" /><description>Perhaps an odd scenario here, but it's affecting my test stability.  Basically, the problem is that when I do, in sequence:
- bulkRequest of a dynamic document
- flushAndRefresh()
- bulkRequest of a second dynamic document
- flushAndRefresh()
- GetMappingsRequest to find all the new keys that have now appeared

Depending on the `@Seed` I've chosen, or whether I put a `Thread.sleep(1000)` after the second `flushAndRefresh()`, I may or may not find the two new keys from my two bulkRequests.
- Expected:  flushAndRefresh() would commit the dynamic documents in the bulkRequest and update the mappings
- Actual:  the documents are committed but the mappings are not

The code:

```
        Client localClient = ElasticsearchIntegrationTest.client();

        BulkRequestBuilder bulkRequest = localClient.prepareBulk();

        IndexRequestBuilder indexRequest = localClient.prepareIndex(DEFAULT_INDEX, "logs")
                .setSource("{mykey1:\"value\"}").setOpType(OpType.CREATE);

        bulkRequest.add(indexRequest).execute().get();

        flushAndRefresh(DEFAULT_INDEX);

        IndexRequestBuilder indexRequest2 = localClient.prepareIndex(DEFAULT_INDEX, "logs")
                .setSource("{mykey2:\"value\"}").setOpType(OpType.CREATE);

        bulkRequest.add(indexRequest2).execute().get();

        flushAndRefresh(DEFAULT_INDEX);

        // Get the dynamically created mappings for the custom charts
        GetMappingsRequest getMappingsRequest = new GetMappingsRequest();
        getMappingsRequest.indices(DEFAULT_INDEX).types("logs");
        GetMappingsResponse getMappingsResponse = localClient.admin().indices().getMappings(getMappingsRequest).actionGet();
        ImmutableOpenMap&lt;String, ImmutableOpenMap&lt;String, MappingMetaData&gt;&gt; mappingsByIndex = getMappingsResponse.getMappings();

        // it's a dynamic mapping, so we should find a property named 'mykey1' and 'mykey2'
        int foundCount = 0;

        // Loop is necessary due to elasticsearch API, but we're always only searching one index
        for (ObjectObjectCursor&lt;String, ImmutableOpenMap&lt;String, MappingMetaData&gt;&gt; indexEntry : mappingsByIndex) {

            if (indexEntry.value.isEmpty()) {
                continue;
            }

            // Again, the loop is necessary because of the API, but we're only looping through the custom data mapping
            for (ObjectObjectCursor&lt;String, MappingMetaData&gt; typeEntry : indexEntry.value) {
                JSONObject allMappings = JSONObject.parse(typeEntry.value.source().toString());
                JSONObject currentMapping = (JSONObject)allMappings.get("logs");
                JSONObject mappingProperties = (JSONObject)currentMapping.get("properties");

                for (Object mappingKey : mappingProperties.keySet()) {
                    String mappingKeyString = (String)mappingKey;
                    if (mappingKeyString.equals("mykey1") || mappingKeyString.equals("mykey2")) {
                        foundCount++;
                    }
                }
            }
        }

        assertEquals("property keys were not found by looking at the mappings even though I called flushAndRefresh()", 2, foundCount);
```

Recreate project here:  https://github.com/mikerott/eb_issue_12282
</description><key id="95332443">12282</key><summary>ElasticsearchIntegrationTest.flushAndRefresh() not flushing (committing?) dynamic mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikerott</reporter><labels /><created>2015-07-16T02:21:40Z</created><updated>2015-07-16T06:53:51Z</updated><resolved>2015-07-16T06:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-16T06:53:51Z" id="121851579">Mapping changes are sent asynchronously to the master in the 1.x branch. This means that they are not guaranteed to immediately be exposed once an indexing operation is done. To make sure we take this into account the testing infrastructure explicitly delays those updates on a random basis. 

If you need to check for anything that is async in nature you can use the assertBusy utility method (check the code base for example).

Note that in 2.0 this is no longer the case and mapping updates are guaranteed to be processed before indexing is started. I'm going to therefore close this. Thx for reporting!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch crashing on Windows because of problem with sigar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12281</link><project id="" key="" /><description>On Elasticsearch 1.4.4 on Windows Server 2008, the elasticsearch process for the master node crashes. The follow error is showing up. This has happened several times and consistently sigar in the Java frames. I understand in Elasticsearch 2.0 sigar is being removed but what workaround is there for Elasticsearch 1.4.4?

```
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x00000000202c7cf5, pid=11224, tid=15288
#
# JRE version: Java(TM) SE Runtime Environment (8.0_11-b12) (build 1.8.0_11-b12)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.11-b03 mixed mode windows-amd64 compressed oops)
# Problematic frame:
# C  [sigar-amd64-winnt.dll+0x7cf5]
#
# Core dump written. Default location: D:\ElasticCluster-9200\Engine\hs_err_pid11224.mdmp
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x000000001f0f3000):  JavaThread "elasticsearch[fastprods13.production.online.dell.com][management][T#4]" daemon [_thread_in_native, id=15288, stack(0x0000000026960000,0x00000000269a0000)]

siginfo: ExceptionCode=0xc0000005, reading address 0x0000000025d2aca8

Registers:
RAX=0x0000000025d2aca8, RBX=0x000000002699f378, RCX=0x000000000129c000, RDX=0x0000000024a8ebc0
RSP=0x000000002699f0a0, RBP=0x000000002699f290, RSI=0x0000000000000040, RDI=0x000000068b56ce28
R8 =0x0000000000000000, R9 =0x0000000000000000, R10=0x0000000000000002, R11=0x0000000000000000
R12=0x0000000000000000, R13=0x00000000cc9f182e, R14=0x000000068b4a2d68, R15=0x000000001f0f3000
RIP=0x00000000202c7cf5, EFLAGS=0x0000000000010213

Top of Stack: (sp=0x000000002699f0a0)
0x000000002699f0a0:   0000000021ff7c80 0000000020318b88
0x000000002699f0b0:   0000000000000001 000000002699f130
0x000000002699f0c0:   00000007c01e14f0 000000001489aa20
0x000000002699f0d0:   000000002699f0e8 000000001f0f3000
0x000000002699f0e0:   0000000700000001 0000000021ff80d0
0x000000002699f0f0:   0000000024a8ebc0 0000000400000000
0x000000002699f100:   0000000000000000 0000000000000000
0x000000002699f110:   0000000000000000 0000000000000000
0x000000002699f120:   0000000025d2aca8 0000000055a6dd25
0x000000002699f130:   0000000000000000 0000000024a8e5e0
0x000000002699f140:   00000000000000d8 0000000024a8eca8
0x000000002699f150:   0000000000000000 0000000000000694
0x000000002699f160:   000000002699f290 00000000202d5193
0x000000002699f170:   0000000022e90f48 0000000000000032
0x000000002699f180:   000000001f0f31e0 00000000202c867e
0x000000002699f190:   0000000021ff7c80 0000000000002bd8 

Instructions: (pc=0x00000000202c7cf5)
0x00000000202c7cd5:   00 00 00 48 8b 84 24 98 00 00 00 8b 40 28 39 44
0x00000000202c7ce5:   24 40 0f 83 7f 02 00 00 48 8b 84 24 80 00 00 00
0x00000000202c7cf5:   8b 00 48 8b 8c 24 80 00 00 00 48 03 c8 48 8b c1
0x00000000202c7d05:   48 89 84 24 a8 00 00 00 83 7c 24 70 00 74 18 8b 


Register to memory mapping:

RAX=0x0000000025d2aca8 is pointing into the stack for thread: 0x0000000021b58800
RBX=0x000000002699f378 is pointing into the stack for thread: 0x000000001f0f3000
RCX=0x000000000129c000 is pointing into the stack for thread: 0x000000001d832000
RDX=0x0000000024a8ebc0 is an unknown value
RSP=0x000000002699f0a0 is pointing into the stack for thread: 0x000000001f0f3000
RBP=0x000000002699f290 is pointing into the stack for thread: 0x000000001f0f3000
RSI=0x0000000000000040 is an unknown value
RDI=0x000000068b56ce28 is an oop
org.elasticsearch.monitor.process.ProcessService 
 - klass: 'org/elasticsearch/monitor/process/ProcessService'
R8 =0x0000000000000000 is an unknown value
R9 =0x0000000000000000 is an unknown value
R10=0x0000000000000002 is an unknown value
R11=0x0000000000000000 is an unknown value
R12=0x0000000000000000 is an unknown value
R13=0x00000000cc9f182e is an unknown value
R14=0x000000068b4a2d68 is an oop
[Ljava.lang.Object; 
 - klass: 'java/lang/Object'[]
 - length: 64
R15=0x000000001f0f3000 is a thread


Stack: [0x0000000026960000,0x00000000269a0000],  sp=0x000000002699f0a0,  free space=252k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [sigar-amd64-winnt.dll+0x7cf5]
C  [sigar-amd64-winnt.dll+0x867e]
C  [sigar-amd64-winnt.dll+0x1fc55]
C  0x0000000002811d4d

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
J 4271  org.hyperic.sigar.ProcFd.gather(Lorg/hyperic/sigar/Sigar;J)V (0 bytes) @ 0x0000000002811ccf [0x0000000002811c80+0x4f]
J 4841 C2 org.elasticsearch.node.service.NodeService.stats(Lorg/elasticsearch/action/admin/indices/stats/CommonStatsFlags;ZZZZZZZZZ)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats; (210 bytes) @ 0x000000000293aaa8 [0x0000000002939be0+0xec8]
J 4160 C1 org.elasticsearch.action.admin.cluster.stats.TransportClusterStatsAction.nodeOperation(Lorg/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction$ClusterStatsNodeRequest;)Lorg/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse; (380 bytes) @ 0x000000000279db4c [0x000000000279d9e0+0x16c]
J 4840 C2 org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run()V (113 bytes) @ 0x0000000002163de0 [0x0000000002161380+0x2a60]
J 3880 C1 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00000000026ba034 [0x00000000026b9180+0xeb4]
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
```
</description><key id="95314635">12281</key><summary>Elasticsearch crashing on Windows because of problem with sigar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msimos</reporter><labels /><created>2015-07-15T23:57:27Z</created><updated>2016-05-13T08:18:31Z</updated><resolved>2015-07-17T13:32:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-16T07:24:28Z" id="121857592">@msimos thanks for reporting. This is a known problem and this is one of the reason we removed sigar from elasticsearch 2.0.

Starting 1.6, the setting `bootstrap.sigar` can be set to `false` to disable sigar. For previous versions, you have to shutdown the node, manually remove sigar libraries (located in `lib/sigar` directory), and restart the node.
</comment><comment author="javadevmtl" created="2015-09-17T21:03:53Z" id="141227881">So if you remove sigar, then will you no longer offer os stats? I guess we have to rely on os specific monitoring?
</comment><comment author="clintongormley" created="2015-09-18T17:16:59Z" id="141511199">@javadevmtl we'll only provide stats exposed by the JVM for the foreseeable future.
</comment><comment author="kbhute-ibm" created="2016-05-13T06:48:47Z" id="218964989">I was installing Elasticsearch 1.7.3 on Ubuntu16.04 VM where I got "/usr/bin/java: symbol lookup error: /usr/lib/jni/libsigar.so: undefined symbol: sigar_skip_token" issue while starting the service. I downloaded required sigar libraries from https://sourceforge.net/projects/sigar/files/sigar/1.6/hyperic-sigar-1.6.4.zip and set LD_LIBRARY_PATH env variable to point to /hyperic-sigar-1.6.4/sigar-bin/lib. I could successfully start elasticsearch service.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Add better validation of moving_avg model settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12280</link><project id="" key="" /><description>Adds proper validation of the settings passed to a moving average model.  You will now get an exception if you try to pass an unknown, non-whitelisted parameter to a model, instead of silently dropping it on the floor. 

The only point of contention is that I added a `settings()` method to the MovingAvgBuilder, so that this could be tested, and perhaps allow users to specify all the parameters in one pass.  This is opposed to specifying all the parameters on the model builder itself, which only allows you to specify applicable params.

So on one hand the `settings()` param more closely mimics the REST api and allows convenience, on the other hand it provides a small foot gun.  

@colings86 If you could take a peek at this when you get time, it'd be swell.  No rush, pretty low priority :heart: 
</description><key id="95304660">12280</key><summary>Aggregations: Add better validation of moving_avg model settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T22:31:58Z</created><updated>2015-07-17T13:39:41Z</updated><resolved>2015-07-16T15:44:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-16T08:13:12Z" id="121872857">@polyfractal left a small comment but otherwise it looks good
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Merge pull request #12280 from polyfractal/bugfix/movavg_validation</comment></comments></commit></commits></item><item><title>Shard Allocation Activity Balancing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12279</link><project id="" key="" /><description>Now that we have a sense of recovery priority (#11787), it may make sense to use that priority for allocation weight when all things are equal.
## Problem

The use case that I am thinking of is, during a cluster restart, you can end up with lopsided primaries even with allocation disabled.

![Recovery Hotspot](https://cloud.githubusercontent.com/assets/1501235/8709244/faf885fc-2b0e-11e5-94f4-c0c1e2519258.png)

The normal distribution can be observed along the top (minus the lopsided primary distribution). This was on 1.6.0 following a full cluster restart with allocation disabled, and it was only reenabled after everyone was up. This particularly recovery lead to three very real problems:
1. All of the indexing was hot spotting both nodes.
2. All of the replicas were appearing on both nodes.
3. All merging was effectively limited to both nodes.

![Hotspotted Machines](https://cloud.githubusercontent.com/assets/1501235/8709487/836f57fc-2b10-11e5-916b-0ee1fd897b81.png)

This was _without_ searching, but, being today's index, it would also be receiving the brunt of the search load as well. Obviously that would have a very negative impact on these two nodes that the rest of the cluster would simply shrug off.
## Solution

The concept of primary balancing has been discussed (and removed), but this type of hot spotting is clearly a non-trivial problem. It's easy to spot it, but it's not easy to prevent it.

Given that the cluster maintains index writers for shards that are sized based on their activity level and we have sync_ids, segment counts, and index readability, then we should be able to come up with some estimate of activity to balance against. New shards should be assumed to be as active as the most active shards.

Ideally we can come up with a way to "guess" activity with that. With or without it, we could either use the `index.priority` or some `index.activity` as a separate mechanism to allow the user to control it. A readonly index could still get the brunt of the requests. The nice thing about a separate setting is that it could be curated over time separate from priority.

From there, we need to modify the allocator equation to weight significantly based on activity to avoid getting the picture above in normal circumstances. If we go purely based on a number, then we can only do this for advanced use cases because all normal indices would have an equal--defaulted--activity value.
## Workaround

Manually rerouting shards can help to prevent this in current situations when you unluckily come across it. It's only really a problem once those shards become large and therefore movement is expensive.
</description><key id="95293089">12279</key><summary>Shard Allocation Activity Balancing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-07-15T21:20:31Z</created><updated>2015-10-07T15:28:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-17T11:47:02Z" id="122253640">&gt; The concept of primary balancing has been discussed (and removed), but this type of hot spotting is clearly a non-trivial problem. It's easy to spot it, but it's not easy to prevent it.

The idea that having 5 primaries on one node and 5 replicas on the other node is a bad situation is (for the most part) incorrect.  Primaries and replicas do the same amount of work, both at index time and at search time.  I say "for the most part" because there are two exceptions:
- updates have a get-modify-put process, and the get and modify steps happen only on the primary today. Heavy scripts can make primaries a hotspot.  There is an open issue for making the get &amp; modify steps distributed as well, which should solve this issue (https://github.com/elastic/elasticsearch/issues/8369)
- with shadow replicas, only the primary writes, while the shadow replicas just handle search - clearly this can result in primaries being a hotspot

That said, having most of the shards of your most active index sitting on only two nodes can lead to load imbalance.  The `cluster.routing.allocation.balance.index` setting tries to force shards from the same node apart, but of course this is only one of the factors taken into account (see https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-cluster.html).

Also, the cost of the hotspot needs to be weighed against the cost of copying GBs of data around your cluster in a shard shuffle. Finding the sweet spot for these values is HARD, and so we've left it up to the operator to make these decisions.

&gt; Ideally we can come up with a way to "guess" activity with that. With or without it, we could either use the index.priority or some index.activity as a separate mechanism to allow the user to control it. A readonly index could still get the brunt of the requests. The nice thing about a separate setting is that it could be curated over time separate from priority.

I agree that something more automated would be of value, and perhaps the `index.priority` setting is a nice way of doing that, eg the higher the priority, the more impact the `cluster.routing.allocation.balance.index` setting has.

I'd like us to make improvement here, but it is a hard problem.  Looking forward to hearing what ideas others have.
</comment><comment author="pickypg" created="2015-08-17T20:14:49Z" id="131947173">I definitely agree that, in general, the difference between a replica and a primary shard is practically none (minus the fact that the primary must also wait for a response). In the above screenshot, there are zero updates taking place and there are no shadow replicas in play.

&gt; Also, the cost of the hotspot needs to be weighed against the cost of copying GBs of data around your cluster in a shard shuffle. Finding the sweet spot for these values is HARD, and so we've left it up to the operator to make these decisions.

I completely agree, especially about it being a hard problem. Unfortunately, in the above picture, it was doing the recovery onto the same two nodes with _all_ of the indexing activity, thus doubling the load on them rather than spreading it around the cluster.

Annoyingly, the cluster figures out that it should rebalance _after_ it eventually recovers the replicas, but the damage is generally done at that point (it also adds even more load with rebalancing).

Having spent some time away from this issue, I think that we should look at `index.activity` with some defaulted value, then use the standard priority details (`index.priority &gt; index.creation_date &gt; index.name (descending)`) to weigh the activity for equivalent activity values.

This allows for new indices to be created with high `index.activity` values that can be curated as time marches forward.
</comment><comment author="vvcephei" created="2015-10-07T15:28:55Z" id="146231127">This looks like the most relevant discussion to ask my related question on.

I totally agree with @clintongormley that building a bunch of smart heuristics into the balanced allocator is high risk. Like you said, it's hard to get right in general, and it's expensive to shuffle data around the cluster.

I think this is a good opportunity for exploring the issue with plugins. Assuming I understand the code, anyone can provide their own Allocator. The problem is that currently, Allocators are only invoked when the cluster changes (routing table changes, cluster settings updates, etc.).

Allocating based on dynamic properties like index activity, search activity, or whatever else requires the allocator to be invoked on an interval. I can simulate this by updating the settings periodically, but I wonder if anyone has better advice on how to get the cluster to rebalance periodically.

Thanks,
-John
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date Histogram Aggregations w/ `extended_bounds` and `time_zone`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12278</link><project id="" key="" /><description>Hey all, 

I'm performing a date histogram aggregation over the past day (`'now/d'` -&gt; `'now/d'`), and would like to get results into hourly buckets. I am using the `extended_bounds` of the aggregation because I would still like to get empty buckets back (as long as they are within my time range).

Everything is working _almost_ as expected... except it seems like `extended_bounds` is not respecting the time zone. 

My query returns the UTC "midnight" bucket which becomes 7pm (on the previous day) when I adjust for my timezone (CST). I would expect that `extended_bounds` with `time_zone` would return that timezone's midnight.

So if I were performing a query from `'now/d'` to `'now/d'` in CST (which is GMT-5 currently), I would expect the first bucket to be `'2015-07-15T05:00:00.000Z'` and _not_ `'2015-07-15T00:00:00.000Z'`.

My question is: Is `extended_bounds` not respecting `time_zone`?

I'm having trouble succinctly describing this in English; hopefully my test case below helps to explain:
#### Test Case:

Index a new document with timestamp of 00:00 GMT:

```
curl -XPOST "http://localhost:9200/analytics/event" -d'
{
    "name": "Prince George",
   "event-date": {
      "timestamp": "2015-07-15T00:00:00.000Z"
   }
}'
```

Index a new document with timestamp of 08:00 GMT:

```
curl -XPOST "http://localhost:9200/analytics/event" -d'
{
    "name": "James Madison",
    "event-date": {
        "timestamp": "2015-07-15T08:00:00.000Z"
    }
}'
```

Search!

```
curl -XPOST "http://localhost:9200/analytics/event/_search" -d'
{
   "query": {
      "filtered": {
         "query": {
            "match_all": {}
         },
         "filter": {
            "bool": {
               "must": [
                  {
                     "range": {
                        "timestamp": {
                           "from": "now/d",
                           "to": "now/d",
                           "time_zone": "-5:00",
                           "include_lower": true,
                           "include_upper": true
                        }
                     }
                  }
               ]
            }
         }
      }
   },
   "aggs": {
      "dateagg": {
         "date_histogram": {
            "field": "event-date.timestamp",
            "interval": "1h",
            "time_zone": "-5:00",
            "min_doc_count": 0,
            "extended_bounds": {
               "min": "now/d",
               "max": "now/d"
            }
         }
      }
   }
}'
```

**Result**:

```
{
   "took": 92,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "analytics",
            "_type": "event",
            "_id": "AU6S1QrhLJ3RT4VnGs-M",
            "_score": 1,
            "_source": {
               "name": "James Madison",
               "event-date": {
                  "timestamp": "2015-07-15T08:00:00.000Z"
               }
            }
         }
      ]
   },
   "aggregations": {
      "dateagg": {
         "buckets": [
            {
               "key_as_string": "2015-07-15T00:00:00.000Z",
               "key": 1436918400000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T01:00:00.000Z",
               "key": 1436922000000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T02:00:00.000Z",
               "key": 1436925600000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T03:00:00.000Z",
               "key": 1436929200000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T04:00:00.000Z",
               "key": 1436932800000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T05:00:00.000Z",
               "key": 1436936400000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T06:00:00.000Z",
               "key": 1436940000000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T07:00:00.000Z",
               "key": 1436943600000,
               "doc_count": 0
            },
            {
               "key_as_string": "2015-07-15T08:00:00.000Z",
               "key": 1436947200000,
               "doc_count": 1
            }
         ]
      }
   }
}
```

As you can see, we are getting the expected hits ("James Madison"), and the document is in the correct bucket. I would prefer the first bucket to start at the beginning of the day in the timezone I specified in the query ("-05:00"), and not at the beginning of the day in UTC.

Thanks for your review, and thanks your hard work!
</description><key id="95285591">12278</key><summary>Date Histogram Aggregations w/ `extended_bounds` and `time_zone`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">feltnerm</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-07-15T20:40:50Z</created><updated>2015-08-19T16:42:19Z</updated><resolved>2015-08-19T16:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-17T13:20:51Z" id="122273387">@cbuescher could you take a look at this one?  I think the docs need some updating - I'm confused as to best way to do something like this.

thanks
</comment><comment author="cbuescher" created="2015-07-21T12:23:45Z" id="123288315">I just verified this on 1.7. and the result still looks like @feltnerm describes it above. Might be a bug, but looking at the code, at first glance it looks like timezone should already be applied to `extended_bounds`. Will have to look into this more deeply.
</comment><comment author="cbuescher" created="2015-07-21T12:24:38Z" id="123288460">@feltnerm which version of ES were you using for the output you described above?
</comment><comment author="feltnerm" created="2015-07-21T15:09:01Z" id="123364390">@cbuescher the above is from 1.5.2, but I have tried with 1.6.x as well. Can try on 1.6.x or 1.7.x if needed.
</comment><comment author="cbuescher" created="2015-07-28T13:43:16Z" id="125615089">Some preliminary findings: I was able to dig a bit deeper into this, and could reproduce the behaviour on master, haven't found a way to fix this though. 
The root problem seems to be that the extended bounds datemath expression is first evaluated without considering the timezone (so the "/d" day rounding that comes with the datemath expression results in UTC day changes). Later on, the time zone rounding of the aggregation is applied, but since we have specified one hour intervals here, the min/max of the extended bounds doesn't change. Before jumping to a quick fix here we need to rethink what the timezone parameter in this type of aggregation should apply to, if it should also affect date math expressions like "now/d" or if this has undesired side effects. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file></files><comments><comment>Aggregations: Make ValueParser.DateMath aware of timezone setting</comment></comments></commit></commits></item><item><title>Fix serialization of IndexFormatTooNewException and IndexFormatTooOldException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12277</link><project id="" key="" /><description>This is essentially an ugly hack to get us by until a proper solution is possible with Lucene 5.3.

The current serialization of IndexFormatTooNewException and IndexFormatTooOldException exceptions is causing them to change the message every time they get serialized. So, while exception starts on the data node as 

`Format version is not supported (resource Format version is not supported (resource BufferedChecksumIndexInput(MockIndexInputWrapper(NIOFSIndexInput(path=\"/home/jenkins/workspace/es_core_master_metal/core/target/J2/temp/org.elasticsearch.index.store.CorruptedFileTest_40DD3FE2206C616D-001/tempDir-001/data/SUITE-slave19.build.ci.hetz.es.io-CHILD_VM=[2]-CLUSTER_SEED=[-5959755590901272649]-HASH=[6F0DDBF15A26B2]-cluster/nodes/0/indices/test/0/index/_0.si\")))): 257 (needs to be between 0 and 1)`

when it reaches the master and master publishes it to all the nodes it becomes:

`Format version is not supported (resource Format version is not supported (resource Format version is not supported (resource Format version is not supported (resource BufferedChecksumIndexInput(MockIndexInputWrapper(NIOFSIndexInput(path=\"/home/jenkins/workspace/es_core_master_metal/core/target/J2/temp/org.elasticsearch.index.store.CorruptedFileTest_40DD3FE2206C616D-001/tempDir-001/data/SUITE-slave19.build.ci.hetz.es.io-CHILD_VM=[2]-CLUSTER_SEED=[-5959755590901272649]-HASH=[6F0DDBF15A26B2]-cluster/nodes/0/indices/test/0/index/_0.si\")))): 257 (needs to be between 0 and 1)): -1 (needs to be between -1 and -1)): -1 (needs to be between -1 and -1)): -1 (needs to be between -1 and -1)];`

Basically on each iteration it gets wrapped into `Format version is not supported (` and `): -1 (needs to be between -1 and -1)`. When this exception becomes a part of a cluster state it trips the cluster state consistency check since data nodes have one extra  `Format version is not supported (` comparing to the data node. An example of such failure can be found in http://build-us-00.elastic.co/job/es_core_master_metal/10331/ .
</description><key id="95284605">12277</key><summary>Fix serialization of IndexFormatTooNewException and IndexFormatTooOldException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T20:36:00Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-16T17:16:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-15T20:45:50Z" id="121742238">Can you also attach a link to the build failure this cause(s)?
</comment><comment author="dakrone" created="2015-07-15T20:47:30Z" id="121742702">I also think this needs more documentation so after the upgrade someone can undo it, as-is, there's no explanation for what exactly is happening (multiple wrapping of the exception)
</comment><comment author="imotov" created="2015-07-15T20:50:48Z" id="121743419">It causes http://build-us-00.elastic.co/job/es_core_master_metal/10331/ to fail. Hmm, I thought "Lucene 5.3 will have getters for all these" is explaining enough, I guess not. Will try to come up with more elaborate explanation. 
</comment><comment author="imotov" created="2015-07-15T21:37:28Z" id="121757708">@dakrone I added more detailed explanation of what needs to be done when we upgrade to lucene 5.3, added static check for the lucene version and updated PR comment to include description of the build failure this issue was causing. Could you take another look? Thanks!

@s1monw I trusted your comment in the original code that lucene 5.3 will get getters, but I cannot find the LUCENE ticket I could attach to the issue. Do you know where this work is getting tracked?
</comment><comment author="dakrone" created="2015-07-15T21:59:30Z" id="121762058">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>core/src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file></files><comments><comment>Fix serialization of IndexFormatTooNewException and IndexFormatTooOldException</comment></comments></commit></commits></item><item><title>Elasticsearch, Logstash and AWS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12276</link><project id="" key="" /><description>We are having interesting issues with ES, Logstash on AWS. 

When we stop Logstash instances and they disconnect from ES, ES cluster goes into a weird state where it reports that it is healthy but HEAD or Bigdesk plugins cannot connect to it. We are getting a lot of this messages in the logs:

[2015-07-15 15:01:04,010][DEBUG][action.admin.cluster.node.info] [NODE001] failed to execute on node [r9kifgqTQN2bufj3JMGRSA]
org.elasticsearch.transport.NodeDisconnectedException: [logstash-NODE012-1377-4420][inet[/xxx.xxx.xxx.xxx:9306]][cluster/nodes/info/n] disconnected
[2015-07-15 15:01:04,014][DEBUG][action.admin.cluster.node.info] [NODE001] failed to execute on node [r9kifgqTQN2bufj3JMGRSA]
org.elasticsearch.transport.NodeDisconnectedException: [logstash-NODE012-1377-4420][inet[/xxx.xxx.xxx.xxx:9306]][cluster/nodes/info/n] disconnected
[2015-07-15 15:01:04,014][DEBUG][action.admin.cluster.node.stats] [NODE001] failed to execute on node [r9kifgqTQN2bufj3JMGRSA]
org.elasticsearch.transport.NodeDisconnectedException: [logstash-NODE012-1377-4420][inet[/xxx.xxx.xxx.xxx:9306]][cluster/nodes/stats/n] disconnected
[2015-07-15 15:01:04,013][DEBUG][action.admin.cluster.node.info] [NODE001] failed to execute on node [r9kifgqTQN2bufj3JMGRSA]
org.elasticsearch.transport.NodeDisconnectedException: [logstash-NODE012-1377-4420][inet[/xxx.xxx.xxx.xxx:9306]][cluster/nodes/info/n] disconnected

Kibana also stops working since it cannot connect to Elasticsearch cluster. 

It takes about 10 minutes for everything to go back to normal without any intervention on our part. Does anybody have any ideas why would this be happening?

Thanks!

Andrew
</description><key id="95281019">12276</key><summary>Elasticsearch, Logstash and AWS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AGirin</reporter><labels /><created>2015-07-15T20:19:20Z</created><updated>2015-07-17T17:04:31Z</updated><resolved>2015-07-17T13:26:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AGirin" created="2015-07-15T21:22:39Z" id="121753626">This actually happens in our own environment - not in AWS - as well. Shutdown Logstash node and Elasticsearch goes into strange state. Comes back after 10 min. 

ES version is 1.2.2
</comment><comment author="clintongormley" created="2015-07-17T13:26:50Z" id="122274494">Hi @AGirin 

1.2.2 is very old now, and numerous connection bugs have been fixed.  Please upgrade and reopen this ticket if you see the same thing in a recent version.  

thanks
</comment><comment author="AGirin" created="2015-07-17T17:04:31Z" id="122344191">Same happens with ES 1.7. Nothing really changed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Projection and De-normalization of nested relationships</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12275</link><project id="" key="" /><description>I find that it is quite often that we need to de-normalize (produce a cartesian join) on nested data and apply filter on the resulting data set (on both to parent and nested fields). We also need to support a case of multiple unrelated nested fields being de-normalized. It would need to go together with projections API because we would only want to include necessary parts of data avoiding repetition of complete parent or other nested (in case of multiple independent nested fields) data.

One of the main use cases is export of subsets of data by end users from our systems for further analysis and reporting. We have an engine that provides basic de-normalization and projection functionality but filtering of nested data is really challenging as we would have to replicate elastic filtering logic for that. So I was wondering if elastic team would find it to be a potentially interesting feature.

As an aspect of projection functionality (apart from denormalization) it would be nice to be able to apply nested filters to returned sources nested elements collection. That would go long way towards achieving complete goal as denormalization could be still done on client as long as filtering of nested collections in source is done in elastic
</description><key id="95275453">12275</key><summary>Projection and De-normalization of nested relationships</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>feedback_needed</label></labels><created>2015-07-15T19:52:23Z</created><updated>2015-07-20T13:42:43Z</updated><resolved>2015-07-19T10:46:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-17T12:25:37Z" id="122260225">I'm sorry @roytmana but I don't have a clue what you're talking about from this issue description.  It is way too abstract.  Actual JSON examples would go a long way to making this comprehensible.

I think what you're describing may already be handled by the inner-hits functionality (https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html) possible combined with response filtering (https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering)
</comment><comment author="roytmana" created="2015-07-17T20:56:44Z" id="122413943">What I am describing is essentially SQL like joins between parent table and its nested collections allowing joining of the parent with one or multiple nested fields at once. 
Multiple nested fields joining will produce all permutations of members of the nested fields. Think of nested fields as SQL tables implicetly connected on their parent object(s) and how SQL will return result

You can call it SQL Over Elastic but just "selection" and "joining" part without SQL query support

Say we have meeting between Customers and Staff

``` json
{
  "meetingId": 123,
  "meetingDate": "2015-07-03",
  "customers": [
    {
      "id": 10,
      "name": "John Doe"
    },
    {
      "id": 20,
      "name": "Jane Smith"
    }
  ],
  "staff": [
    {
      "id": 10,
      "name": "Joe Brown"
    },
    {
      "id": 20,
      "name": "Kate Davis"
    }
  ]
}
```

Say I want return metting info and customer info in denormolazed manner to feed to my reporting or any orger tool that is geared towards relational data:
Here is the result

``` json
[
  {
    "meetingId": 10,
    "meetingDate": "2015-07-03",
    "customers.id": 10,
    "customers.name": "John Doe"
  },
  {
    "meetingId": 10,
    "meetingDate": "2015-07-03",
    "customers.id": 20,
    "customers.name": "Jane Smith"
  }
]
```

"select statement" of the query will identify which parts to include and which nested collections to denormalize

if I want to return all my meeting data in denormalized form it'll be :

``` json
[
  {
    "meetingId": 10,
    "meetingDate": "2015-07-03",
    "customers.id": 10,
    "customers.name": "John Doe",
    "staff.id": 10,
    "staff.name": "Joe Brown"
  },
  {
    "meetingId": 10,
    "meetingDate": "2015-07-03",
    "customers.id": 20,
    "customers.name": "Jane Smith",
    "staff.id": 10,
    "staff.name": "Joe Brown"
  },
  {
    "meetingId": 10,
    "meetingDate": "2015-07-03",
    "customers.id": 10,
    "customers.name": "John Doe",
    "staff.id": 20,
    "staff.name": "Kate Davis"
  },
  {
    "meetingId": 10,
    "meetingDate": "2015-07-03",
    "customers.id": 20,
    "customers.name": "Jane Smith",
    "staff.id": 20,
    "staff.name": "Kate Davis"
  }
]
```
</comment><comment author="clintongormley" created="2015-07-19T10:46:17Z" id="122646720">OK - that's clearer, and that looks like something which is quite use case specific and can be done easily on the client side (using `inner_hits` to do the join filtering on the Elasticsearch side).

thanks for the idea @roytmana, but I don't think we should build a new API specifically for this.
</comment><comment author="roytmana" created="2015-07-19T21:46:25Z" id="122708785">What about denormalization @clintongormley?
that could be rather useful for integration with millions of reporting, statistical, productivity and other packages built to work with relational databases. As it is there are very few, if any end user products capable reading elastic data.if elastic were capable of providing tabular data practically any product could read it
</comment><comment author="kimchy" created="2015-07-20T10:50:16Z" id="122847467">there are 2 aspects here. Denormalize the JSON response to _another_ json response that is denormalized for nested docs. I don't think ES should do it, as it is very use case dependent and the client app can easily denormalize the response using it's own logic.

There is another aspect of returning responses in a "tabular" format, that applies both for aggregations and nested docs within hits. This is an interesting question and requires thinking about how best to answer that, but we should definitely not implement like how it is purposed here, since it is somewhere "in the middle" and will cause too much confusion.
</comment><comment author="roytmana" created="2015-07-20T11:58:33Z" id="122862930">@kimchy, while denormalizatuon itself can be done generically on an arbitrary json the combination of arbitrary query fultering and denormalizatuon is lot more complex on client side. also what's the usfulness of tabulating if you do not denormalize 1-N relations?  Flattening 1-1 nested json witin each collection is a trivial op.

Btw building an entirely generic data mart like solution on top of elastic made me wish nesting and filtering in aggs were smarter and did not have to introduce all this extra nodes in the outpit tree. I have quite a framework to automatically nest aggs when using nested aggs or filters and then un nest results to prrsent logical grouping to consumer apps entirely automatically. It should not be so labor intensive. Another thing is ability to flatten aggs results capturing bucket keys and metrics as you recurse the result tree
</comment><comment author="kimchy" created="2015-07-20T12:00:48Z" id="122863569">but you are talking here about solving a point solution of flattening hits documents with nested elements in them, this is a point based "solution", it will complicate things without really solving anything
</comment><comment author="roytmana" created="2015-07-20T12:50:52Z" id="122873486">I do not dispute it. It is really your call. I feel it is pretty generic and provides interface for millions of products that were developed with relational data in mind. I think it is huge for opening elastic to wider ad hoc and end user use.
</comment><comment author="kimchy" created="2015-07-20T12:58:16Z" id="122875961">@roytmana I am 100% with you, I just think that if we want to provide this "tabular" format of response, we need to think about it in a broader context then just hits, or just aggregations. It is trickier, but I think it is the right way forward as it will incur less point solutions...
</comment><comment author="roytmana" created="2015-07-20T13:11:12Z" id="122878829">That would be great! That and a smarter aggs that are not so assembly language like :-)
they could understand nesting relatuonships for a start :-)
</comment><comment author="clintongormley" created="2015-07-20T13:42:43Z" id="122888746">@roytmana if you can think of a way to improve the API, please do suggest it (with JSON examples)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve JVM Arch Detection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12274</link><project id="" key="" /><description>PR for #12256 Notify the user if the JVM architecture could not be detected, instead of silently defaulting to 32-bit. Also add a max heap parameter. On machines with RAM sizes in the order of 64GB plus, the initial heap could be multi-gigabyte. This can lead to an out of memory error when checking for the version.
</description><key id="95270424">12274</key><summary>Improve JVM Arch Detection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smflorentino</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T19:22:14Z</created><updated>2015-08-11T14:17:48Z</updated><resolved>2015-08-11T14:17:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="smflorentino" created="2015-07-15T20:02:27Z" id="121730045">I just signed the CLA, perhaps I need to recreate the PR? 
</comment><comment author="jpountz" created="2015-07-17T11:02:09Z" id="122246430">Did you sign the CLA with a different name, I can't find your name in the list?

Regarding your PR, I know very little about windows but it looks like find will return an error level of 1 whenever it can't find the provided string. So this error message would be displayed for every person who is not running a 64-bits system. Doesn't it feel wrong?
</comment><comment author="smflorentino" created="2015-07-17T18:02:14Z" id="122359069">@jpountz You're right. I forgot one small change which I've just pushed to the branch and squashed. I signed the CLA yesterday, and signed it again this morning, with my name/username...even got the email confirmation with the document. Should I forward you a copy? 
</comment><comment author="clintongormley" created="2015-07-19T10:40:20Z" id="122646241">@smflorentino did you sign the personal CLA or the corp CLA? We can only check the personal CLAs automatically, the corp ones we have to do a manual search on :(
</comment><comment author="smflorentino" created="2015-07-19T16:50:25Z" id="122680877">The corp CLA. Should I also sign the personal one? 
</comment><comment author="clintongormley" created="2015-07-20T13:03:19Z" id="122876955">no worries @smflorentino - we've found the cla, thanks 
</comment><comment author="Mpdreamz" created="2015-08-10T13:28:08Z" id="129451614">LGTM :+1: small nag change from `-Xmx64M` to `-Xmx2M` just to signify that 64M has nothing to do with `x64`
</comment><comment author="smflorentino" created="2015-08-10T19:05:32Z" id="129568128">@Mpdreamz do you think 2M is enough? I wouldn't want to cause a VM initialization failure because the heap was set too small. Just to be safe, maybe 50M or 128M (stated as the Java RAM requirement per the JVM docs). Otherwise I can change to 2M 
</comment><comment author="Mpdreamz" created="2015-08-10T19:33:00Z" id="129577205">50 works too, I read in the java docs it had to be a multiple of 1024kb but maybe 50 is a safer assumption to work over different JVM's :+1
</comment><comment author="smflorentino" created="2015-08-10T19:44:37Z" id="129581177">I set it to 50M and tested successfully. Per http://docs.oracle.com/javase/7/docs/technotes/guides/vm/gc-ergonomics.html I think using Xmx arguments in megabytes will ensure that the value, in bytes, is a multiple of 1024. Do you think we are good to merge?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12274 from smflorentino/smflorentino-fix-winjvm-detect</comment></comments></commit></commits></item><item><title>ES doesn't exhaust options for routing when shards are unassigned.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12273</link><project id="" key="" /><description>Shared 5 will not get assigned after an upgrade from 1.5.0 to 1.6.0.

```
[root@ls2-es-lb ~]# curl -XGET "http://localhost:9200/_cluster/state/routing_table,routing_nodes/logstash-cdr-2015.05.18" | jq '.'
{
  "allocations": [],
  "routing_nodes": {
    "nodes": {
      "Ts0HJNFvSGy2JVd31VlotQ": [
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 1,
          "relocating_node": null,
          "node": "Ts0HJNFvSGy2JVd31VlotQ",
          "primary": false,
          "state": "STARTED"
        },
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 2,
          "relocating_node": null,
          "node": "Ts0HJNFvSGy2JVd31VlotQ",
          "primary": false,
          "state": "STARTED"
        }
      ],
      "6AS8BMZKQkivehCUWANRdQ": [
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 3,
          "relocating_node": null,
          "node": "6AS8BMZKQkivehCUWANRdQ",
          "primary": true,
          "state": "STARTED"
        },
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 1,
          "relocating_node": null,
          "node": "6AS8BMZKQkivehCUWANRdQ",
          "primary": true,
          "state": "STARTED"
        }
      ],
      "6fs0j8RWQ2esU7wgvAPcdg": [
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 4,
          "relocating_node": null,
          "node": "6fs0j8RWQ2esU7wgvAPcdg",
          "primary": false,
          "state": "STARTED"
        },
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 2,
          "relocating_node": null,
          "node": "6fs0j8RWQ2esU7wgvAPcdg",
          "primary": true,
          "state": "STARTED"
        }
      ],
      "srLX4NZDTIaHq9qBVsxcZw": [
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 0,
          "relocating_node": null,
          "node": "srLX4NZDTIaHq9qBVsxcZw",
          "primary": true,
          "state": "STARTED"
        },
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 3,
          "relocating_node": null,
          "node": "srLX4NZDTIaHq9qBVsxcZw",
          "primary": false,
          "state": "STARTED"
        }
      ],
      "DnCwjImuRFOsranelYuOaw": [
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 5,
          "relocating_node": null,
          "node": "DnCwjImuRFOsranelYuOaw",
          "primary": true,
          "state": "STARTED"
        }
      ],
      "3ZOu2V5xSX-BxL2Osd5l7A": [
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 4,
          "relocating_node": null,
          "node": "3ZOu2V5xSX-BxL2Osd5l7A",
          "primary": true,
          "state": "STARTED"
        },
        {
          "index": "logstash-cdr-2015.05.18",
          "shard": 0,
          "relocating_node": null,
          "node": "3ZOu2V5xSX-BxL2Osd5l7A",
          "primary": false,
          "state": "STARTED"
        }
      ]
    },
    "unassigned": [
      {
        "index": "logstash-cdr-2015.05.18",
        "shard": 5,
        "relocating_node": null,
        "node": null,
        "primary": false,
        "state": "UNASSIGNED"
      }
    ]
  },
  "routing_table": {
    "indices": {
      "logstash-cdr-2015.05.18": {
        "shards": {
          "2": [
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 2,
              "relocating_node": null,
              "node": "6fs0j8RWQ2esU7wgvAPcdg",
              "primary": true,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 2,
              "relocating_node": null,
              "node": "Ts0HJNFvSGy2JVd31VlotQ",
              "primary": false,
              "state": "STARTED"
            }
          ],
          "5": [
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 5,
              "relocating_node": null,
              "node": "DnCwjImuRFOsranelYuOaw",
              "primary": true,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 5,
              "relocating_node": null,
              "node": null,
              "primary": false,
              "state": "UNASSIGNED"
            }
          ],
          "1": [
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 1,
              "relocating_node": null,
              "node": "6AS8BMZKQkivehCUWANRdQ",
              "primary": true,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 1,
              "relocating_node": null,
              "node": "Ts0HJNFvSGy2JVd31VlotQ",
              "primary": false,
              "state": "STARTED"
            }
          ],
          "3": [
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 3,
              "relocating_node": null,
              "node": "srLX4NZDTIaHq9qBVsxcZw",
              "primary": false,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 3,
              "relocating_node": null,
              "node": "6AS8BMZKQkivehCUWANRdQ",
              "primary": true,
              "state": "STARTED"
            }
          ],
          "0": [
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 0,
              "relocating_node": null,
              "node": "3ZOu2V5xSX-BxL2Osd5l7A",
              "primary": false,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 0,
              "relocating_node": null,
              "node": "srLX4NZDTIaHq9qBVsxcZw",
              "primary": true,
              "state": "STARTED"
            }
          ],
          "4": [
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 4,
              "relocating_node": null,
              "node": "3ZOu2V5xSX-BxL2Osd5l7A",
              "primary": true,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 4,
              "relocating_node": null,
              "node": "6fs0j8RWQ2esU7wgvAPcdg",
              "primary": false,
              "state": "STARTED"
            }
          ]
        }
      }
    }
  },
  "cluster_name": "tropo-es"
}
```

I tried to force a re-route w/ the following script but it didnt work

```
for h in 3ZOu2V5xSX-BxL2Osd5l7A srLX4NZDTIaHq9qBVsxcZw 6fs0j8RWQ2esU7wgvAPcdg 6AS8BMZKQkivehCUWANRdQ DnCwjImuRFOsranelYuOaw Ts0HJNFvSGy2JVd31VlotQ; do
  curl -sw "%{http_code}" -XPOST -d '{ "commands" : [ { "allocate" : { "shard": 5, "index": "logstash-cdr-2015.05.18", "node" : "'"$h"'"  } } ] }'   'http://ls2-es-lb.int.tropo.com:9200/_cluster/reroute?pretty'  | jq '.'

# jdyer at JOHNDYE-M-F9G6 in ~/Projects/logstash-input-stomp on git:master o [13:37:32]
$ for h in 3ZOu2V5xSX-BxL2Osd5l7A srLX4NZDTIaHq9qBVsxcZw 6fs0j8RWQ2esU7wgvAPcdg 6AS8BMZKQkivehCUWANRdQ DnCwjImuRFOsranelYuOaw Ts0HJNFvSGy2JVd31VlotQ; do
for&gt;   curl -sw "%{http_code}" -XPOST -d '{ "commands" : [ { "allocate" : { "shard": 5, "index": "logstash-cdr-2015.05.18", "node" : "'"$h"'"  } } ] }'   'http://ls2-es-lb.int.tropo.com:9200/_cluster/reroute?pretty'  | jq '.'
for&gt; done
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es1.int.tropo.com][3ZOu2V5xSX-BxL2Osd5l7A][ls2-es1][inet[/10.1.0.103:9300]]{master=false} is not allowed, reason: [YES(shard is not allocated to same node or host)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][NO(too many shards for this index on node [2], limit: [2])][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [468.2gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
400
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es2.int.tropo.com][srLX4NZDTIaHq9qBVsxcZw][ls2-es2][inet[/10.1.0.102:9300]]{master=false} is not allowed, reason: [YES(shard is not allocated to same node or host)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][NO(too many shards for this index on node [2], limit: [2])][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [469.7gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
400
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es3.int.tropo.com][6fs0j8RWQ2esU7wgvAPcdg][ls2-es3][inet[/10.1.0.101:9300]]{master=false} is not allowed, reason: [YES(shard is not allocated to same node or host)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][NO(too many shards for this index on node [2], limit: [2])][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [472.2gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
400
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es4.int.tropo.com][6AS8BMZKQkivehCUWANRdQ][ls2-es4][inet[/10.1.0.104:9300]]{master=false} is not allowed, reason: [YES(shard is not allocated to same node or host)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][NO(too many shards for this index on node [2], limit: [2])][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [481gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
400
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es5.int.tropo.com][DnCwjImuRFOsranelYuOaw][ls2-es5][inet[/10.1.0.55:9300]]{master=false} is not allowed, reason: [NO(shard cannot be allocated on same node [DnCwjImuRFOsranelYuOaw] it already exists on)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][YES(shard count under limit [2] of total shards per node)][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [466.9gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
400
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es6.int.tropo.com][Ts0HJNFvSGy2JVd31VlotQ][ls2-es6.int.tropo.com][inet[/10.1.0.106:9300]]{master=false} is not allowed, reason: [YES(shard is not allocated to same node or host)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][NO(too many shards for this index on node [2], limit: [2])][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [483.3gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
400
```

this is the only unassigned shared since the restart and I am not sure how to get it back to green.  Any advice ? 

Thanks
</description><key id="95249385">12273</key><summary>ES doesn't exhaust options for routing when shards are unassigned.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johntdyer</reporter><labels><label>:Allocation</label><label>bug</label><label>discuss</label></labels><created>2015-07-15T17:38:33Z</created><updated>2016-01-18T21:00:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-15T17:57:00Z" id="121693380">@johntdyer you can see why the shard won't allocate to the node in the output of the reroute command, specifically this line:

```
NO(too many shards for this index on node [2], limit: [2])
```

This comes from the `index.routing.allocation.total_shards_per_node` setting (looks like it's been set to 2).

In the future however, issues like this should be opened on the [discussion forums](http://discuss.elastic.co) instead of as issues here.
</comment><comment author="johntdyer" created="2015-07-15T18:01:49Z" id="121694502">Lee,

```
 Why is this only effecting this shard?   All the other shards reassigned after the rolling restart.  My problem seems limited to just this single shard of this single index.
```

John

Sent from my iPhone

&gt; On Jul 15, 2015, at 1:57 PM, Lee Hinman notifications@github.com wrote:
&gt; 
&gt; @johntdyer you can see why the shard won't allocate to the node in the output of the reroute command, specifically this line:
&gt; 
&gt; NO(too many shards for this index on node [2], limit: [2])
&gt; This comes from the index.routing.allocation.total_shards_per_node setting (looks like it's been set to 2).
&gt; 
&gt; In the future however, issues like this should be opened on the discussion forums instead of as issues here.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="dakrone" created="2015-07-15T18:05:18Z" id="121695635">&gt; Why is this only effecting this shard? All the other shards reassigned after the rolling restart.

It looks like each of your nodes already has the maximum 2 shards for this index (the setting from above). This shard happened to be last and thus won't be allocated. You need to increase the total_shards_per_node setting, or add another node.
</comment><comment author="johntdyer" created="2015-07-15T20:50:58Z" id="121743462">@dakrone -  I am sorry for my naivety on this but I am still confused....  I can see under `routing_table.indices.logstash-cdr-2015.05.18.shards`  that shard 5 is not assigned

```
      {
              "index": "logstash-cdr-2015.05.18",
              "shard": 5,
              "relocating_node": null,
              "node": "DnCwjImuRFOsranelYuOaw",
              "primary": true,
              "state": "STARTED"
            },
            {
              "index": "logstash-cdr-2015.05.18",
              "shard": 5,
              "relocating_node": null,
              "node": null,
              "primary": false,
              "state": "UNASSIGNED"
            }
``

and it is still not clear to me why this is only happening with this one index, and only happened after an upgrade from 1.5.0 to 1.6.0... 

```
</comment><comment author="dakrone" created="2015-07-15T21:19:42Z" id="121752440">Digging a little deeper here:

# Explanation&lt;a id="sec-1" name="sec-1"&gt;&lt;/a&gt;

You have six nodes:
-   Ts0HJNFvSGy2JVd31VlotQ
-   3ZOu2V5xSX-BxL2Osd5l7A
-   srLX4NZDTIaHq9qBVsxcZw
-   6fs0j8RWQ2esU7wgvAPcdg
-   6AS8BMZKQkivehCUWANRdQ
-   DnCwjImuRFOsranelYuOaw

Each of these nodes has two shards on it, except for one:

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;
&lt;colgroup&gt;
&lt;col  class="left" /&gt;
&lt;col  class="left" /&gt;
&lt;col  class="left" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="left"&gt;Node&lt;/th&gt;
&lt;th scope="col" class="left"&gt;shard 1&lt;/th&gt;
&lt;th scope="col" class="left"&gt;shard 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="left"&gt;Ts0HJNFvSGy2JVd31VlotQ&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[1][r]`&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[2][r]`&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="left"&gt;6AS8BMZKQkivehCUWANRdQ&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[3][p]`&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[1][p]`&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="left"&gt;6fs0j8RWQ2esU7wgvAPcdg&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[4][r]`&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[2][p]`&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="left"&gt;srLX4NZDTIaHq9qBVsxcZw&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[0][p]`&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[3][r]`&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="left"&gt;DnCwjImuRFOsranelYuOaw&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[5][p]`&lt;/td&gt;
&lt;td class="left"&gt;&amp;#xa0;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="left"&gt;3ZOu2V5xSX-BxL2Osd5l7A&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[4][p]`&lt;/td&gt;
&lt;td class="left"&gt;`logstash-cdr-2015.05.18[0][r]`&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

The unassigned shard is `logstash-cdr-2015.05.18[5][r]`

Usually, this should would be assigned to `DnCwjImuRFOsranelYuOaw`, however, you
can see in the output of the reroute why it is not:

```
{
  "error": "ElasticsearchIllegalArgumentException[[allocate] allocation of [logstash-cdr-2015.05.18][5] on node [ls2-es5.int.tropo.com][DnCwjImuRFOsranelYuOaw][ls2-es5][inet[/10.1.0.55:9300]]{master=false} is not allowed, reason: [NO(shard cannot be allocated on same node [DnCwjImuRFOsranelYuOaw] it already exists on)][YES(node passes include/exclude/require filters)][YES(primary is already active)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][YES(shard count under limit [2] of total shards per node)][YES(target node version [1.6.0] is same or newer than source node version [1.6.0])][YES(enough disk for shard on node, free: [466.9gb])][YES(shard not primary or relocation disabled)]]",
  "status": 400
}
```

Specifically this line:

```
NO(shard cannot be allocated on same node [DnCwjImuRFOsranelYuOaw] it already exists on)
```

This is because the primary for that shard already exists on the "DnCw" node, so
ES cannot assign the replica to the node.

Additionally, Elasticsearch will not rebalance the shards on the other nodes
until all `UNASSIGNED` shards are assigned, so they will not move.

***_Elasticsearch is stuck in a state waiting for space to allocate the unassigned
shard because it cannot assign it to the only node with space.**_\*  So from the
perspective of Elasticsearch, the shard cannot be allocated anywhere, which is
why it is unassigned.

---

# Workarounds&lt;a id="sec-2" name="sec-2"&gt;&lt;/a&gt;

For a temporary workaround, there are two options:
-   Temporarily set `index.routing.allocation.total_shards_per_node` to `3` for
  this index, then set it back to `2` after this shard has been allocated.

Elasticsearch should be able to assign this to one of the other nodes if you
increment this to 3, then re-balance to equalize the number of shards per node.
Once it's been allocated, you can lower it back to `2`
-   Manually swap the `logstash-cdr-2015.05.18[5][p]` shard on the `DnCw...` node
  with another node using the reroute API

If you swap it with another shard, ES will be able to allocate the unassigned
shard because it is not the same exact shard being allocated on the same node.

# Why?&lt;a id="sec-3" name="sec-3"&gt;&lt;/a&gt;

Why did this happen with the 1.6.0 upgrade? It is a by-product of the nodes
being restarted, and bad luck for the allocation of shard here.

# Further action&lt;a id="sec-4" name="sec-4"&gt;&lt;/a&gt;

I think we can consider this ticket a bug report for this behavior, as we should
try as hard as possible to prevent it!
</comment><comment author="clintongormley" created="2015-07-17T11:52:44Z" id="122254883">The `total_shards_per_node` setting is documented (in master: https://www.elastic.co/guide/en/elasticsearch/reference/master/allocation-total-shards.html) to sometimes cause unassigned shards.

It's a hard limit in a process which, for the most part, relies on heuristics and, as such, is a bit of a hack. I'd prefer to remove the setting and instead solve the problem by trying harder to spread out shards from the same index.  See #12279 for more on this.
</comment><comment author="nik9000" created="2015-07-17T12:06:38Z" id="122257244">That is a dangerous one to remove. It's one of the most important parts of
keeping the Wikimedia cluster up and running smoothly. The hard limit it
provides is useful because putting two enwiki shards next to each other
will bring the node down.
On Jul 17, 2015 7:52 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; The total_shards_per_node setting is documented (in master:
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/master/allocation-total-shards.html)
&gt; to sometimes cause unassigned shards.
&gt; 
&gt; It's a hard limit in a process which, for the most part, relies on
&gt; heuristics and, as such, is a bit of a hack. I'd prefer to remove the
&gt; setting and instead solve the problem by trying harder to spread out shards
&gt; from the same index. See #12279
&gt; https://github.com/elastic/elasticsearch/issues/12279 for more on this.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12273#issuecomment-122254883
&gt; .
</comment><comment author="clintongormley" created="2015-07-17T14:19:13Z" id="122291281">@nik9000 i'm only proposing removing it if we support a better option that doesn't suffer from the same issues.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Documented index prioritization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12272</link><project id="" key="" /><description>Relates to #11975
</description><key id="95230769">12272</key><summary>Docs: Documented index prioritization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T16:06:41Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-15T16:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-15T16:08:54Z" id="121663507">LGTM - this needs to go to 1.7 as well
</comment><comment author="clintongormley" created="2015-07-15T16:16:13Z" id="121665983">Backported to 1.7 in https://github.com/elastic/elasticsearch/commit/aa1b2ac41035fb50e6ce806a6a460b0fb71f8465
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12272 from clintongormley/index_priority_docs</comment></comments></commit></commits></item><item><title>restrict the test unicast zen discovery to the port range of the JVM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12271</link><project id="" key="" /><description>Today, the unicast zen test configuration will try to find a open port starting at the internal test cluster's
base port and continuing for 1000 ports. The internal test cluster class assigns a port range of 100 ports
to each JVM. This means that the unicast zen test configuration will try ports in the range for another JVM
and can lead to port conflicts. This change uses the same value for both so that the unicast configuration
does not go into another JVM's port range.
</description><key id="95226642">12271</key><summary>restrict the test unicast zen discovery to the port range of the JVM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T15:44:48Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-15T16:38:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-15T16:06:26Z" id="121662980">LGTM. Any concrete failures explained by this?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java</file></files><comments><comment>Merge pull request #12271 from jaymode/test_zen_port_range</comment></comments></commit></commits></item><item><title>Deploy artifacts to S3 as well as sonatype when running a release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12270</link><project id="" key="" /><description>This change allows when running a release build with:

```
 mvn -Prelease deploy
```

To deploy all artifacts to S3 as well as to sonatype at the same time.
Both sources will be consistent in terms of content and no further action
is required to publish our artifacts including .rpm, .deb, .zip, .tar.gz, .jar
etc. on to out S3 download service. Albeit this changes the structure of our
downloads to pretty much matching the maven repository layout, this makes
releaseing core as well as the plugins extremely simple. This will allow to
remove most of our python script used for release and it will automatically
allow to release and integrate new modules without further interaction.

This also allows us to bascially streamline our release process on CI such that
CI builds can simply run maven deploy which is all we do during a release.

With this commit only the git related modifications like tagging, version bumping
on our pom files and publishing RPM and .deb in their dedicated repository is left
for the python script.

With this change our artifact are available as follows:

```
http://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/elasticsearch/2.0.0-beta1/elasticsearch-2.0.0-beta1.deb
http://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/elasticsearch/2.0.0-beta1/elasticsearch-2.0.0-beta1.zip
http://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/elasticsearch/2.0.0-beta1/elasticsearch-2.0.0-beta1.rpm
```

Plugins are deployed to URLs like this:

```
http://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/plugin/elasticsearch-analysis-icu/2.0.0-beta1/elasticsearch-analysis-icu-2.0.0-beta1.zip
```

All artifacts like .jar as well as their checksums and gpg signatures are also available next to it.
</description><key id="95220315">12270</key><summary>Deploy artifacts to S3 as well as sonatype when running a release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T15:15:11Z</created><updated>2015-08-13T14:27:57Z</updated><resolved>2015-07-15T16:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-15T15:15:46Z" id="121648441">@drewr @dadoonet @spinscale FYI - I think this would simplify our deployment process quite a bit
</comment><comment author="dadoonet" created="2015-07-15T16:00:13Z" id="121661333">w00t! Even simplier than what I imagined.

LGTM
</comment><comment author="drewr" created="2015-07-15T16:05:17Z" id="121662696">I like the direction. The only hangup I see is a fair amount of internal auditing work that will result from changing the artifact URLs. If it's not possible to keep the same URLs, then LGTM.
</comment><comment author="rjernst" created="2015-07-15T16:06:27Z" id="121662983">LGTM too.

I assume this requires aws keys to be located somewhere in a file? Is this documented somewhere?
</comment><comment author="s1monw" created="2015-07-15T16:11:03Z" id="121664110">&gt; I assume this requires aws keys to be located somewhere in a file? Is this documented somewhere?

ENV vars or settings.xml see --&gt; https://github.com/spring-projects/aws-maven

&gt; I like the direction. The only hangup I see is a fair amount of internal auditing work that will result from changing the artifact URLs. If it's not possible to keep the same URLs, then LGTM.

it would bring us back to what we did before so no I don't think we can!
</comment><comment author="tlrx" created="2015-07-16T07:51:15Z" id="121869527">Nice!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file></files><comments><comment>PluginManager: Fix elastic.co download URLs, add snapshot ones</comment></comments></commit><commit><files /><comments><comment>Merge pull request #12270 from s1monw/publish_s3</comment></comments></commit></commits></item><item><title>QueryString ignores maxDeterminizedStates when creating a WildcardQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12269</link><project id="" key="" /><description>This patch backports https://issues.apache.org/jira/browse/LUCENE-6677 in MapperQueryParser

Closes #12266
</description><key id="95198050">12269</key><summary>QueryString ignores maxDeterminizedStates when creating a WildcardQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">nomoa</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T13:46:26Z</created><updated>2015-07-17T13:40:10Z</updated><resolved>2015-07-16T15:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nomoa" created="2015-07-15T13:51:33Z" id="121622713">I've just signed the CLA should I recreate the PR?
</comment><comment author="mikemccand" created="2015-07-15T14:03:55Z" id="121625331">Thanks @nomoa the patch looks good, no need to recreate the PR, your CLA signing should be reflected here soon ... I'll check and and then push for 1.6.1 and 1.7.0.
</comment><comment author="mikemccand" created="2015-07-15T14:05:18Z" id="121625638">I'll do this for 2.0.0 as well, since it's (likely) based on 5.2.1 which doesn't have LUCENE-6677 fix.
</comment><comment author="nomoa" created="2015-07-15T15:41:34Z" id="121655770">Thanks!
</comment><comment author="martijnvg" created="2015-07-16T09:54:16Z" id="121912378">Bumping the version up to 1.7.1 for the release today.
</comment><comment author="nik9000" created="2015-07-16T16:07:43Z" id="122004393">Thanks @nomoa and @mikemccand!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Invalid cluster settings allowed to be persisted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12268</link><project id="" key="" /><description>Invalid cluster settings are allowed to be persisted and causing below exception at each node restart, not sure if further impact on the allocation decider:

```
[2015-07-15 15:21:42,675][WARN ][node.settings            ] [The Wink] failed to refresh settings for [org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider@55a88417]
org.elasticsearch.ElasticsearchIllegalArgumentException: Illegal allocation.enable value []
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider$Allocation.parse(EnableAllocationDecider.java:122)
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.onRefreshSettings(EnableAllocationDecider.java:100)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Repro

1) install elasticsearch 1.6.0

```
Antonios-MacBook-Air-2:bin abonuccelli$ ./elasticsearch
[2015-07-15 15:20:14,248][INFO ][node                     ] [Giant-Man] version[1.6.0], pid[14041], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-15 15:20:14,249][INFO ][node                     ] [Giant-Man] initializing ...
[2015-07-15 15:20:14,262][INFO ][plugins                  ] [Giant-Man] loaded [marvel], sites [marvel]
[2015-07-15 15:20:14,338][INFO ][env                      ] [Giant-Man] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [169.1gb], net total_space [465.1gb], types [hfs]
[2015-07-15 15:20:18,222][INFO ][node                     ] [Giant-Man] initialized
[2015-07-15 15:20:18,225][INFO ][node                     ] [Giant-Man] starting ...
[2015-07-15 15:20:18,548][INFO ][transport                ] [Giant-Man] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.43.141:9300]}
[2015-07-15 15:20:18,587][INFO ][discovery                ] [Giant-Man] elasticsearch/7V8IhiMqSvGdQywjbja7fw
[2015-07-15 15:20:22,377][INFO ][cluster.service          ] [Giant-Man] new_master [Giant-Man][7V8IhiMqSvGdQywjbja7fw][Antonios-MacBook-Air-2.local][inet[/192.168.43.141:9300]], reason: zen-disco-join (elected_as_master)
[2015-07-15 15:20:22,426][INFO ][http                     ] [Giant-Man] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.43.141:9200]}
[2015-07-15 15:20:22,427][INFO ][node                     ] [Giant-Man] started
[2015-07-15 15:20:22,462][INFO ][gateway                  ] [Giant-Man] recovered [0] indices into cluster_state
[2015-07-15 15:20:29,292][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] creating index, cause [auto(bulk api)], templates [marvel], shards [1]/[1], mappings [_default_, shard_event, index_event, index_stats, node_event, routing_event, cluster_event, cluster_state, cluster_stats, node_stats, indices_stats]
[2015-07-15 15:20:30,207][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [node_stats] (dynamic)
[2015-07-15 15:20:30,273][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [routing_event] (dynamic)
[2015-07-15 15:20:30,299][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [cluster_state] (dynamic)
[2015-07-15 15:20:30,302][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [node_event] (dynamic)
[2015-07-15 15:20:30,317][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [cluster_event] (dynamic)
[2015-07-15 15:20:30,329][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [index_event] (dynamic)
[2015-07-15 15:20:30,369][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [index_stats] (dynamic)
[2015-07-15 15:20:30,399][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [indices_stats] (dynamic)
[2015-07-15 15:20:30,456][INFO ][cluster.metadata         ] [Giant-Man] [.marvel-2015.07.15] update_mapping [cluster_stats] (dynamic)
[2015-07-15 15:20:37,274][DEBUG][river.cluster            ] [Giant-Man] processing [reroute_rivers_node_changed]: execute
[2015-07-15 15:20:37,275][DEBUG][river.cluster            ] [Giant-Man] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-07-15 15:20:37,275][DEBUG][cluster.service          ] [Giant-Man] processing [cluster_update_settings]: took 2ms done applying updated cluster_state (version: 12)
[2015-07-15 15:20:37,275][DEBUG][cluster.service          ] [Giant-Man] processing [reroute_after_cluster_update_settings]: execute
[2015-07-15 15:20:37,276][DEBUG][cluster.service          ] [Giant-Man] processing [reroute_after_cluster_update_settings]: took 0s no change in cluster_state
```

2) Set invalid empty string

```
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "routing": {
        "allocation": {
          "enable": ""
        }
      }
    }
  }
}
```

3) validate this is let through and persisted

```
GET _cluster/settings



{
   "persistent": {
      "cluster": {
         "routing": {
            "allocation": {
               "enable": ""
            }
         }
      }
   },
   "transient": {}
}
```

3) In the logs

```
[2015-07-15 15:20:39,635][DEBUG][cluster.service          ] [Giant-Man] processing [cluster_update_settings]: execute
[2015-07-15 15:20:39,636][DEBUG][cluster.service          ] [Giant-Man] cluster state updated, version [13], source [cluster_update_settings]
[2015-07-15 15:20:39,636][DEBUG][cluster.service          ] [Giant-Man] publishing cluster state version 13
[2015-07-15 15:20:39,636][DEBUG][cluster.service          ] [Giant-Man] set local cluster state to version 13
[2015-07-15 15:20:39,638][WARN ][node.settings            ] [Giant-Man] failed to refresh settings for [org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider@55a88417]
org.elasticsearch.ElasticsearchIllegalArgumentException: Illegal allocation.enable value []
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider$Allocation.parse(EnableAllocationDecider.java:122)
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.onRefreshSettings(EnableAllocationDecider.java:100)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-15 15:20:39,642][DEBUG][river.cluster            ] [Giant-Man] processing [reroute_rivers_node_changed]: execute
[2015-07-15 15:20:39,642][DEBUG][river.cluster            ] [Giant-Man] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-07-15 15:20:39,650][DEBUG][cluster.service          ] [Giant-Man] processing [cluster_update_settings]: took 14ms done applying updated cluster_state (version: 13)
[2015-07-15 15:20:39,651][DEBUG][cluster.service          ] [Giant-Man] processing [reroute_after_cluster_update_settings]: execute
[2015-07-15 15:20:39,651][DEBUG][cluster.service          ] [Giant-Man] processing [reroute_after_cluster_update_settings]: took 0s no change in cluster_state
[2015-07-15 15:20:48,242][DEBUG][indices.memory           ] [Giant-Man] recalculating shard indexing buffer (reason=[[ADDED]]), total is [99mb] with [1] active shards, each shard set to indexing=[99mb], translog=[64kb]
[2015-07-15 15:20:48,242][DEBUG][index.shard              ] [Giant-Man] [.marvel-2015.07.15][0] updating index_buffer_size from [64mb] to [99mb]
```

4) stop node

```
^C[2015-07-15 15:21:32,381][INFO ][node                     ] [Giant-Man] stopping ...
[2015-07-15 15:21:32,401][DEBUG][marvel.agent             ] [Giant-Man] shutting down worker, exporting pending event
[2015-07-15 15:21:32,401][DEBUG][marvel.agent             ] [Giant-Man] worker shutdown
[2015-07-15 15:21:32,405][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing ... (reason [shutdown])
[2015-07-15 15:21:32,406][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing index service (reason [shutdown])
[2015-07-15 15:21:32,412][DEBUG][index                    ] [Giant-Man] [.marvel-2015.07.15] [0] closing... (reason: [shutdown])
[2015-07-15 15:21:32,423][DEBUG][index.shard              ] [Giant-Man] [.marvel-2015.07.15][0] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[2015-07-15 15:21:32,423][DEBUG][index.shard              ] [Giant-Man] [.marvel-2015.07.15][0] operations counter reached 0, will not accept any further writes
[2015-07-15 15:21:32,423][DEBUG][index.engine             ] [Giant-Man] [.marvel-2015.07.15][0] flushing shard on close - this might take some time to sync files to disk
[2015-07-15 15:21:32,440][DEBUG][index.engine             ] [Giant-Man] [.marvel-2015.07.15][0] close now acquiring writeLock
[2015-07-15 15:21:32,441][DEBUG][index.engine             ] [Giant-Man] [.marvel-2015.07.15][0] close acquired writeLock
[2015-07-15 15:21:32,456][DEBUG][index.engine             ] [Giant-Man] [.marvel-2015.07.15][0] engine closed [api]
[2015-07-15 15:21:32,457][DEBUG][index                    ] [Giant-Man] [.marvel-2015.07.15] [0] closed (reason: [shutdown])
[2015-07-15 15:21:32,457][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing index cache (reason [shutdown])
[2015-07-15 15:21:32,457][DEBUG][index.cache.filter.weighted] [Giant-Man] [.marvel-2015.07.15] full cache clear, reason [close]
[2015-07-15 15:21:32,457][DEBUG][index.cache.fixedbitset  ] [Giant-Man] [.marvel-2015.07.15] clearing all bitsets because [close]
[2015-07-15 15:21:32,457][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] clearing index field data (reason [shutdown])
[2015-07-15 15:21:32,457][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing analysis service (reason [shutdown])
[2015-07-15 15:21:32,457][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing index engine (reason [shutdown])
[2015-07-15 15:21:32,457][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing index gateway (reason [shutdown])
[2015-07-15 15:21:32,458][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing mapper service (reason [shutdown])
[2015-07-15 15:21:32,458][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing index query parser service (reason [shutdown])
[2015-07-15 15:21:32,459][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closing index service (reason [shutdown])
[2015-07-15 15:21:32,459][DEBUG][indices                  ] [Giant-Man] [.marvel-2015.07.15] closed... (reason [shutdown])
[2015-07-15 15:21:32,467][INFO ][node                     ] [Giant-Man] stopped
[2015-07-15 15:21:32,467][INFO ][node                     ] [Giant-Man] closing ...
[2015-07-15 15:21:32,485][INFO ][node                     ] [Giant-Man] closed
```

5) restart node

```
Antonios-MacBook-Air-2:bin abonuccelli$ ./elasticsearch
[2015-07-15 15:21:34,636][INFO ][node                     ] [The Wink] version[1.6.0], pid[14054], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-15 15:21:34,637][INFO ][node                     ] [The Wink] initializing ...
[2015-07-15 15:21:34,649][INFO ][plugins                  ] [The Wink] loaded [marvel], sites [marvel]
[2015-07-15 15:21:34,710][INFO ][env                      ] [The Wink] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [169.1gb], net total_space [465.1gb], types [hfs]
[2015-07-15 15:21:38,459][INFO ][node                     ] [The Wink] initialized
[2015-07-15 15:21:38,460][INFO ][node                     ] [The Wink] starting ...
[2015-07-15 15:21:38,684][INFO ][transport                ] [The Wink] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.43.141:9300]}
[2015-07-15 15:21:38,738][INFO ][discovery                ] [The Wink] elasticsearch/SSoVsPgpTMSCn3DLoxkJIw
[2015-07-15 15:21:42,519][INFO ][cluster.service          ] [The Wink] new_master [The Wink][SSoVsPgpTMSCn3DLoxkJIw][Antonios-MacBook-Air-2.local][inet[/192.168.43.141:9300]], reason: zen-disco-join (elected_as_master)
[2015-07-15 15:21:42,671][INFO ][http                     ] [The Wink] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.43.141:9200]}
[2015-07-15 15:21:42,672][INFO ][node                     ] [The Wink] started
[2015-07-15 15:21:42,675][WARN ][node.settings            ] [The Wink] failed to refresh settings for [org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider@55a88417]
org.elasticsearch.ElasticsearchIllegalArgumentException: Illegal allocation.enable value []
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider$Allocation.parse(EnableAllocationDecider.java:122)
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.onRefreshSettings(EnableAllocationDecider.java:100)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-15 15:21:42,735][INFO ][gateway                  ] [The Wink] recovered [1] indices into cluster_state
```
</description><key id="95195484">12268</key><summary>Invalid cluster settings allowed to be persisted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels><label>:Settings</label><label>bug</label></labels><created>2015-07-15T13:32:54Z</created><updated>2015-07-15T14:26:58Z</updated><resolved>2015-07-15T14:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T14:26:58Z" id="121633214">Hi @nellicus 

Yes this is currently the behaviour.  Unfortunately it is not easily fixable - it is a project we're planning on tackling in the 2.x series (see #6732 for more).

The way to fix it is to reset this setting to its default value (which you have to set explicitly) as in:

```
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "routing": {
        "allocation": {
          "enable": "all"
        }
      }
    }
  }
}
```

It'll remain in the cluster settings though - there's currently no way to delete settings.

Closing in favour of #6732
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding more information to scaling threadpools documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12267</link><project id="" key="" /><description>This PR adds explanation around `scaling` threadpools listed on on https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-threadpool.html.

I originally got this explanation from @bleskes about the `warmer` threadpool. I have based the explanations for the other two `scaling` threadpools listed on this documentation page — `snapshot` and `refresh` — on this source: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L124-L126
</description><key id="95182273">12267</key><summary>Adding more information to scaling threadpools documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels><label>docs</label></labels><created>2015-07-15T12:39:52Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-15T14:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-15T14:06:27Z" id="121625897">LGTM. thx.
</comment><comment author="clintongormley" created="2015-07-15T14:53:46Z" id="121642171">thanks @ycombinator - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12267 from ycombinator/threadpool-doc</comment></comments></commit></commits></item><item><title>QueryString ignores maxDeterminizedStates when creating a WildcardQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12266</link><project id="" key="" /><description>This is a bug that has been fixed in lucene (https://issues.apache.org/jira/browse/LUCENE-6677) recently.
I don't know if it will be backported to lucene 4.10.x and thus have chance to get into elasticsearch 1.6.x.
Note that this fix could be backported in MapperQueryParser by overriding the newWildcardQuery method.
</description><key id="95181678">12266</key><summary>QueryString ignores maxDeterminizedStates when creating a WildcardQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nomoa</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T12:36:31Z</created><updated>2015-07-16T16:23:13Z</updated><resolved>2015-07-16T16:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T14:52:16Z" id="121641252">@nomoa This fix won't make it into 1.6.1 or 1.7.0 - too short notice.  I think it'll have to wait for 2.0
</comment><comment author="clintongormley" created="2015-07-15T14:52:28Z" id="121641386">Will be fixed by the upgrade to Lucene 5.3
</comment><comment author="clintongormley" created="2015-07-15T14:55:40Z" id="121642914">hah - just seen your PR and that mike's on it - you're in luck :)
</comment><comment author="nomoa" created="2015-07-16T09:06:09Z" id="121889616">Yes, I guess it's my lucky day :)
This will be of great help to us to protect a cluster that was killed by a nasty wildcard query.
</comment><comment author="mikemccand" created="2015-07-16T16:23:13Z" id="122009977">I merged to 1.6.2, 1.7.1, 2.0.  Thank you for fixing in both Lucene (5.3) and ES @nomoa!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Spurious type created when initialising other types while using index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12265</link><project id="" key="" /><description>Two types are created when using index templates and one document is inserted:

```
curl -XPUT 'localhost:9200/_template/first-template' -d '
{
    "template": "myindex",
    "mappings": {
        "wtfirst": {
            "properties": {
                "name": { "type": "string" }
            }
        }
    }
}
'
curl -XPUT 'localhost:9200/myindex/first-type/first' -d '
{
    "name": "first"
}
'
```

This creates a `myindex` index and `first-type` type with a single document, but a `wtfirst` type is also created in the index. This unexpected type also has a dynamically derived explicit(?) mapping which is also unexpected.

If you add a more templates and documents in the same manner (using `wtsecond` and `second-type` etc.), the same thing continues to happen. However if you add a third or more in the same manner and `index.mapper.dynamic` is `false`, then a `TypeMissingException` is thrown on the third document creation.

The effect on mappings for types becomes unclear when later adding documents for the pre-created/not-created types. This problem appears on tested versions 1.4.0, 1.5.1 and 1.6.0.
</description><key id="95168190">12265</key><summary>Spurious type created when initialising other types while using index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MrHash</reporter><labels /><created>2015-07-15T11:18:27Z</created><updated>2015-07-15T11:31:56Z</updated><resolved>2015-07-15T11:31:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T11:31:56Z" id="121587133">&gt; This creates a myindex index and first-type type with a single document, but a wtfirst type is also created in the index. This unexpected type also has a dynamically derived explicit(?) mapping which is also unexpected.

This isn't unexpected at all.  Elasticsearch is doing exactly what you told it to do: "when you see an index called `myindex`, create it with the following mapping"

&gt; index.mapper.dynamic is false

This setting turns off dynamic mappings, so you have to add mappings manually, hence the TypeMissingException.

I think you've misunderstood how dynamic mapping works.  I suggest asking about what you're trying to achieve in the forums: https://discuss.elastic.co
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update match-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12264</link><project id="" key="" /><description>fix typo
</description><key id="95168003">12264</key><summary>Update match-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels><label>docs</label></labels><created>2015-07-15T11:16:54Z</created><updated>2015-07-15T11:22:47Z</updated><resolved>2015-07-15T11:18:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T11:22:47Z" id="121585843">thanks @peschlowp 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12264 from peschlowp/patch-6</comment></comments></commit></commits></item><item><title>Carry over shard exception failure to master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12263</link><project id="" key="" /><description>Don't loose the shard exception failure when sending a shard failrue to the master node
</description><key id="95156826">12263</key><summary>Carry over shard exception failure to master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T10:14:59Z</created><updated>2015-08-07T10:07:17Z</updated><resolved>2015-07-15T13:00:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-15T11:15:58Z" id="121585029">+1 to getting more structured info to the master. Left some minor comments. LGTM.
</comment><comment author="kimchy" created="2015-07-15T12:55:31Z" id="121606020">pushed changes on the review comments, will push soonish
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs fix- added performance note about plain highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12262</link><project id="" key="" /><description>Closes #11442
</description><key id="95152497">12262</key><summary>Docs fix- added performance note about plain highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>docs</label></labels><created>2015-07-15T09:54:27Z</created><updated>2015-07-15T13:31:51Z</updated><resolved>2015-07-15T13:31:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T11:16:59Z" id="121585164">LGTM
</comment><comment author="colings86" created="2015-07-15T12:39:55Z" id="121603672">LGTM
</comment><comment author="nik9000" created="2015-07-15T13:18:39Z" id="121612258">+1
</comment><comment author="markharwood" created="2015-07-15T13:31:50Z" id="121617923">Pushed to master https://github.com/elastic/elasticsearch/commit/52fb3c3a09d7f5574cd5bcff912df005466107f0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reset the `ShardTargetType` after serializing inner hits.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12261</link><project id="" key="" /><description>This fixes a bug where only the first top level search hit has a shard target and any subsequent search hits don't.

This bug was reported in the forum: https://discuss.elastic.co/t/elasticsearch-innerhits-java-api-causing-npe/25532
</description><key id="95151996">12261</key><summary>Reset the `ShardTargetType` after serializing inner hits.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T09:51:23Z</created><updated>2015-08-18T16:55:59Z</updated><resolved>2015-08-14T20:39:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-08-14T09:04:14Z" id="131032182">Left a minor comment but otherwise it LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/internal/InternalSearchHitTests.java</file></files><comments><comment>test: added a unit test for #12261</comment></comments></commit></commits></item><item><title>Include stacktrace in rendered exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12260</link><project id="" key="" /><description>This commit includes the stacktrace into the structured exception rendering
to ensure we can find the reason / cause for certain things quicker. This
is enabled by default and is very verbose. Users can disable it via `rest.exception.stacktrace.skip = true|false`

Closes #12239
</description><key id="95135780">12260</key><summary>Include stacktrace in rendered exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T08:35:54Z</created><updated>2015-08-11T10:55:36Z</updated><resolved>2015-08-11T10:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-15T08:37:53Z" id="121531400">here is an example:

``` JSON
GET /test/_search?sort=foo&amp;pretty=true
{
  "error" : {
    "root_cause" : [ {
      "type" : "index_not_found_exception",
      "reason" : "no such index",
      "resource.type" : "index_or_alias",
      "resource.id" : "test",
      "index" : "test",
      "stack_trace" : "[test] IndexNotFoundException[no such index]\n\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:547)\n\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:96)\n\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:76)\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.&lt;init&gt;(TransportSearchTypeAction.java:118)\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:72)\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:66)\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:63)\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:52)\n\tat org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)\n\tat org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99)\n\tat org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:44)\n\tat org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)\n\tat org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)\n\tat org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:343)\n\tat org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)\n\tat org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)\n\tat org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:343)\n\tat org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:566)\n\tat 
...
```

while when skipped:

``` JSON
GET /test/_search?sort=foo&amp;pretty=true&amp;rest.exception.stacktrace.skip=true
{
  "error" : {
    "root_cause" : [ {
      "type" : "index_not_found_exception",
      "reason" : "no such index",
      "resource.type" : "index_or_alias",
      "resource.id" : "test",
      "index" : "test"
    } ],
    "type" : "index_not_found_exception",
    "reason" : "no such index",
    "resource.type" : "index_or_alias",
    "resource.id" : "test",
    "index" : "test"
  },
  "status" : 404
}
```
</comment><comment author="colings86" created="2015-07-15T13:09:44Z" id="121609926">LGTM
</comment><comment author="jpountz" created="2015-07-15T22:33:53Z" id="121768109">LGTM this will certainly prove useful!
</comment><comment author="nik9000" created="2015-08-04T19:48:29Z" id="127733271">Does this need any documentation? Otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/test/java/org/elasticsearch/ESExceptionTests.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java</file></files><comments><comment>Merge pull request #12260 from s1monw/issues/12239</comment></comments></commit></commits></item><item><title>index migration problem from elasticsearch1.5.2 to elasticsearch1.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12259</link><project id="" key="" /><description>we have already generated many large index using elasticsearch1.5.2.
To use some new features (e.g. JSON response body filtering), we have to migrate to elasticsearch1.6.0. Are the index compatible in version 1.6.0 ?
Thanks
</description><key id="95128929">12259</key><summary>index migration problem from elasticsearch1.5.2 to elasticsearch1.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JoshuaZe</reporter><labels /><created>2015-07-15T07:52:54Z</created><updated>2015-07-15T08:01:34Z</updated><resolved>2015-07-15T07:59:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-15T07:59:15Z" id="121522358">please use the discuss forum for these kind of questions https://discuss.elastic.co/  in the future. The answer is YES it's compatible you can do a rolling restart.
</comment><comment author="JoshuaZe" created="2015-07-15T08:01:34Z" id="121522703">Thank you very much.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Concurrent snapshot restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12258</link><project id="" key="" /><description>Working towards concurrent snapshot restoration.

Related to #11148
</description><key id="95109604">12258</key><summary>Concurrent snapshot restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">4ydx</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label></labels><created>2015-07-15T05:24:59Z</created><updated>2016-01-19T08:36:52Z</updated><resolved>2016-01-19T08:36:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T19:19:28Z" id="172627266">@ywelsch this is a PR in response to your issue here https://github.com/elastic/elasticsearch/issues/11148

Would you take a look (and apologies to @4ydx for slow response)
</comment><comment author="ywelsch" created="2016-01-19T08:36:52Z" id="172775793">@4ydx are you still interested in working on this? If so, can you make a new PR against master branch? I'll close this one as it targets 1.6.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12257</link><project id="" key="" /><description>Looking at adding concurrent restoration across differing indices.
</description><key id="95107513">12257</key><summary>1.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">4ydx</reporter><labels /><created>2015-07-15T05:09:28Z</created><updated>2015-07-15T05:23:57Z</updated><resolved>2015-07-15T05:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Service.bat Windows Java version detection improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12256</link><project id="" key="" /><description>I have a PowerShell script to deploy Elasticsearch via PowerShell, I'm registering the service using service.bat during a PowerShell session. This is on a machine with 64GB of RAM. It was always defaulting to a 32 bit service, although no 32-bit service was installed on the machine. I traced the problem to be the fact that the default Java heap size with my JVM is 2GB. Here's the problem. The default memory limit for a PowerShell session is 1GB - so the JVM can't even start. But the service.bat file defaults to detecting a 32 bit JVM when in fact none exists. I'd like to add preferably two things - (1) a check to see that the JVM was able to initialize for the version check and (2) a reasonable heap maximum size for the check (-Xmx 2M) to avoid this type of problem.

I know this is probably a real specific problem, but what thoughts do you guys have? I have made changes locally and the script works perfectly now, and am more than willing to submit a PR.
</description><key id="95104013">12256</key><summary>Service.bat Windows Java version detection improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smflorentino</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-07-15T04:30:44Z</created><updated>2015-08-11T17:37:07Z</updated><resolved>2015-08-11T17:37:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T11:08:11Z" id="121583732">Hi @smflorentino 

Probably the easiest way to move forward is for you to submit the PR?  Easier to follow what's going on when we can see the code changes.

thanks
</comment><comment author="smflorentino" created="2015-07-15T19:26:29Z" id="121720704">I've just attached the PR. I can repro this on machines with large amounts of RAM, when service.bat is called within PowerShell. 
</comment><comment author="smflorentino" created="2015-08-11T17:37:06Z" id="129983383">PR #12274 merged to master.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Closing #12256 Update service.bat with a safer RAM value</comment></comments></commit><commit><files /><comments><comment> #12256 Improve JVM Arch Detection</comment></comments></commit></commits></item><item><title>ElasticSearch memory leaks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12255</link><project id="" key="" /><description>We have that pattern (below) long enough that guessing memory leaks in ElasticSearch seem to be the most likely explanation.
Summary: after running our ElasticSearch for about 5 days we start to get ~exponentially more timeouts with every passing day.
These timeouts go away after restarting ElasticSearch service... for another 5 days.

Details:
1) We saw that pattern in both Windows (Windows Server 2012 R2) and Linux (CentOs 7) environments.
We currently use ElasticSearch 1.6.0, but we observed similar "~5-days timebomb" pattern on previous versions such as ElasticSearch 1.4.3 on Windows).
2) We run about 200,000 ElasticSearch queries per day
3) We have timeouts at 100s mark.
4) Normally failing queries should execute in well below 1s (e.g. ~50ms).
5) Timeouts we are getting usually do not show on slowlog:
https://github.com/elastic/elasticsearch/issues/11919
</description><key id="95101619">12255</key><summary>ElasticSearch memory leaks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dennisgorelik</reporter><labels /><created>2015-07-15T04:12:46Z</created><updated>2015-07-17T13:29:44Z</updated><resolved>2015-07-15T11:05:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T11:05:31Z" id="121583397">@dennisgorelik please don't open another issue for this.  Continue the discussion in the previous issue.
</comment><comment author="dennisgorelik" created="2015-07-16T05:20:28Z" id="121831945">These are two separate issues.
"Memory leaks" and "Timeouts missing in slowlog" may have the same or different underlying causes.

"Memory leaks" issue is also much more important than missing timeouts in slowlog.
</comment><comment author="clintongormley" created="2015-07-17T13:29:44Z" id="122274985">I think they are more than likely related, and i very much doubt there are memory leaks.  Let's continue on the other issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index warmer does not execute function score script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12254</link><project id="" key="" /><description>Repro here (https://gist.github.com/ppf2/63e19e4ede49a6ff89ba).

In short, when a function score query with a script is added to the index warmer, the warmer itself fires (I can confirm via index.warmer at trace level) but it does not execute the script.  If I run the same query directly, it does fire the script.  Script filters' script does fire when added to the index warmer.  

From a scoring point of view, it may not make sense to run it via the warmer, though I have the impression that we also try to cache the script for faster subsequent executions which makes me wonder if it will still help to run the script as part of the warmer.  Either way, would be nice to know more about how it works :)

Not sure if the behavior is expected (if so, can we document it?) 
</description><key id="95101300">12254</key><summary>Index warmer does not execute function score script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Warmers</label><label>discuss</label></labels><created>2015-07-15T04:09:39Z</created><updated>2016-01-10T19:05:52Z</updated><resolved>2016-01-10T19:05:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T11:03:35Z" id="121583139">@martijnvg any ideas here?

@ppf2 as a side note, this is a particularly poor way to use warmers.  Just sorting on `listprice` would have the same effect, or updating the mapping for the field to (eg) load global ordinals eagerly.
</comment><comment author="ppf2" created="2015-07-15T16:22:23Z" id="121667323">@clintongormley btw, the repro is really just for quick demonstration purposes using groovy equivalent, the end user actually has a more elaborate native script being used which is not getting executed by the warmer.
</comment><comment author="martijnvg" created="2015-07-17T10:22:38Z" id="122237818">warmers queries are executed with search_type=count if no sort is specified and therefor scores are never computed. The function_score scripts are only invoked if scores are computed, so that is why the script doesn't get invoked during warming.
</comment><comment author="clintongormley" created="2015-07-17T10:23:52Z" id="122238206">thanks @martijnvg - sounds like this should be documented
</comment><comment author="ppf2" created="2015-07-17T18:28:53Z" id="122366276">```
  "sort": [
    {
      "_score": {
        "order": "desc"
      }
    }
  ]
```

Thanks for the feedback.  Per comments above, I attempted a fix by adding an explicit sort to the _score.

The results are:
1.  At the time the put warmer is called with the sort in place, it fires the script successfully
2.  But when the node is restarted, the warmer still does not execute the script.

I discussed with @martijnvg , it sounds like there is actually a bug here.
</comment><comment author="masaruh" created="2015-12-24T04:09:43Z" id="167038763">I think I found the cause. While it's parsing `sort` part, it omits sorting if query has one sort criteria and if it's sorting on `_score` in descending order (https://github.com/elastic/elasticsearch/blob/v2.1.1/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java#L101-L117). Then, SearchService thinks it doesn't have sort and changes query to size=0 (https://github.com/elastic/elasticsearch/blob/v2.1.1/core/src/main/java/org/elasticsearch/search/SearchService.java#L1148-L1153).

So, if you change the sort part from `desc` to `asc`, the script will be executed.
</comment><comment author="clintongormley" created="2016-01-10T19:05:52Z" id="170382385">Warmers have been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_default_ mapping not created in most randomizedtesting cases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12253</link><project id="" key="" /><description>I have a class that extends `ElasticsearchIntegrationTest` as part of our test infrastructure.  I've overridden `nodeSettings` and `indexSettings` methods in order to take back some control from the `RandomizedTest` class up the class hierarchy.  This worked well when we ran against ES 1.1.1.  Now, against 1.5.2, I'm hitting a blocker.  First, I had to explicitly add `plugin.types` to the `Settings` returned from `nodeSettings`, but that problem was only 30 minutes to find!  The blocker for me is that ES won't create the `_default_` mapping in roughly 90% of the times I run my unit test.

At this point, I've found which `@Seed` consistently passes, and which consistently fails, so it's clearly some randomization that's causing it.  Problem is, I don't know how to find which setting is causing the difference in behavior.  If I could only see what values the randomizer picked under each seed, I surely could spot the difference.

How can I make `RandomizedTest` be more verbose?  I can pause my test in a debugger and go poke around on port 9200, or use the Java API.  Anything will do.  In which case(s) will ES _not_ create the `_default_` mapping?

I have a full (easy) re-create scenario for this that I can upload somewhere.
</description><key id="95099464">12253</key><summary>_default_ mapping not created in most randomizedtesting cases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikerott</reporter><labels /><created>2015-07-15T03:55:04Z</created><updated>2015-07-15T12:45:58Z</updated><resolved>2015-07-15T12:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-15T11:22:51Z" id="121585854">I'm not sure exactly what you mean, but the testing infra will indeed randomly add a _default_ mapping. See https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java#L369 for the decision and  https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java#L450 for logging of it. 

the randomDynamicTemplates parameter to the ClusterScope annotation allows you to turn off the random behavior , giving you the option to add what ever template/default mapping you want.

Does this answer you question?

Out of curiosity  - what does it matter to you if the `_default_` mapping is added?
</comment><comment author="mikerott" created="2015-07-15T12:45:46Z" id="121604513">That parameter does (mostly) answer my question.  I was also curious about how to print out the decisions RandomizedTest makes, but I can find that some other time.

As for the parameter, yes that helps!  I guess I missed all the params for ClusterScope annotation.

As for why it matters, it's actually not such a big deal, except that we want strict control over the mappings that are created into ES, and in 1.1.1 everything was perfectly deterministic.  In 1.5.2, it _appeared_ initially like some nondeterministic behavior.  Of course, it ultimately just came down to the (intentional) randomized behavior of the tests.

I think all is well now.  Thank you for the help!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12252</link><project id="" key="" /><description>1. tests don't have a bogus test dependency on zips anymore,
   instead we handle this in pre-integration-test. This reduces
   lots of confusion for e.g. mvn clean test.
2. refactor integ logic so that core/ and plugin/ share it.
   previously they were duplicates but the above change simplifies life.
   it also makes it easier for doing more interesting stuff
</description><key id="95098694">12252</key><summary>Refactor integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T03:45:34Z</created><updated>2015-07-15T04:18:37Z</updated><resolved>2015-07-15T04:18:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-15T04:17:22Z" id="121483021">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12252 from rmuir/refactor_integ</comment></comments></commit></commits></item><item><title>Why do I still need @ThreadLeakFilters for threads named "elasticsearch"?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12251</link><project id="" key="" /><description>I recently updated from 1.1.1 (yeah, I know) to 1.5.2.  The easy parts were changing some of the values in `@ClusterScope` annotation and a few method signature changes here and there.  But then I hit a blocker.  I'm using a class that `extends ElasticsearchIntegrationTest`.  Against 1.1.1, I had no need for `@ThreadLeakFilters` annotation, but now against 1.5.2, I find that I _do_ need it so I can explicitly ignore threads named `elasticsearch`.

Without the annotation, I see the following failures during test execution:

```
com.carrotsearch.randomizedtesting.ThreadLeakError: 65 threads leaked from SUITE scope at com.mike.IndexManagerTest: 
   1) Thread[id=124, name=elasticsearch[Cypher][http_server_worker][T#14]{New I/O worker #74}, state=RUNNABLE, group=TGRP-IndexManagerTest]
        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:159)
        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
        at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
... [63 more]
```

So I added the annotation to my class that `extends ElasticsearchIntegrationTest`:

```
@ThreadLeakFilters(defaultFilters = true, filters = {TestRunnerThreadsFilter.class})
```

And the TestRunnerThreadsFilter class is:

```
import com.carrotsearch.randomizedtesting.ThreadFilter;

public class TestRunnerThreadsFilter implements ThreadFilter {

    @Override
    public boolean reject(Thread thread) {
        // randomizedtesting shouldn't bother checking for zombie threads for these:
        String threadName = thread.getName();

        if (threadName.startsWith("elasticsearch")) {
            return true;
        }

        return false;
    }

}
```

Why do I need to do this?  Do I have a more fundamental problem I need to investigate?
</description><key id="95098529">12251</key><summary>Why do I still need @ThreadLeakFilters for threads named "elasticsearch"?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikerott</reporter><labels><label>discuss</label><label>test</label></labels><created>2015-07-15T03:43:50Z</created><updated>2016-04-25T14:27:46Z</updated><resolved>2016-04-25T14:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-15T07:12:51Z" id="121512092">It seems to imply cluster / nodes are not properly shut down after the suite ends.   Any idea why that would happen? do you override any of the setup / teardown methods?
</comment><comment author="s1monw" created="2015-07-15T07:32:50Z" id="121515168">in `1.1.x` we did filter out all those threads ourself. Now in 1.5 we shutdown stuff properly. Can you provide one test that leaks threads so we can take a look?
</comment><comment author="mikerott" created="2015-07-15T12:35:53Z" id="121602795">I pushed a recreate to https://github.com/mikerott/es_issue_12251.

Also to answer the question, yes I do override setup / teardown.  You'll see in the github project that some of them are empty and do not call super(), or super.teardown() or whatever.  That's only the case because I was using the legacy project to produce the recreate scenario.  However, that's a good observation, and I will be playing with changes in there.
</comment><comment author="mikerott" created="2015-07-15T13:36:07Z" id="121619417">Adding super() (or equivalent) to all `@Before`, `@BeforeClass`, `@After`, `@AfterClass` does not resolve the need for `@ThreadLeakFilters`.
</comment><comment author="clintongormley" created="2016-01-18T20:59:40Z" id="172652548">Sorry it has been a while @mikerott - is this still an issue?
</comment><comment author="clintongormley" created="2016-04-25T14:27:46Z" id="214360089">Closed by #17921
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test Failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12250</link><project id="" key="" /><description>In branch 1.6 running the following in ubuntu 14.04 (running on parallels desktop):

ES_TEST_LOCAL=true
mvn clean test

I am seeing errors like:

```
1&gt; REPRODUCE WITH  : mvn clean test -Dtests.seed=FC2ED73E5703D99E -Dtests.class=org.elasticsearch.cluster.MinimumMasterNodesTests -Dtests.method="multipleNodesShutdownNonMasterNodes" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.locale=sv_SE -Dtests.timezone=US/Alaska -Dtests.processors=2
1&gt; Throwable:
1&gt; org.elasticsearch.discovery.MasterNotDiscoveredException: waited for [30s]
1&gt;     org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$4.onTimeout(TransportMasterNodeOperationAction.java:164)
1&gt;     org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:231)
1&gt;     org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:560)
1&gt;     java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
1&gt;     java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
1&gt;     java.lang.Thread.run(Thread.java:745)
```

I am wondering if this is expected or if I should be doing something differently when testing?
</description><key id="95093182">12250</key><summary>Test Failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">4ydx</reporter><labels /><created>2015-07-15T03:01:06Z</created><updated>2015-08-26T20:04:05Z</updated><resolved>2015-08-26T20:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T20:04:05Z" id="135155571">Closing as there have been lots of changes since then, but if you have such issues again can you please open a ticket with the name of the test that failed as the title? Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify docs for transpositions setting in completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12249</link><project id="" key="" /><description>closes #12228
</description><key id="95090811">12249</key><summary>Clarify docs for transpositions setting in completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T02:41:14Z</created><updated>2015-07-15T19:45:52Z</updated><resolved>2015-07-15T19:45:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-15T07:33:25Z" id="121515264">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12249 from areek/fix/12228</comment></comments></commit></commits></item><item><title>enable system assertions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12248</link><project id="" key="" /><description>This has found problems before with lucene tests at least, when something goes horribly wrong inside jdk. might as well enable any checks we can get in unit tests.
</description><key id="95080869">12248</key><summary>enable system assertions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-15T01:26:30Z</created><updated>2015-07-15T01:35:42Z</updated><resolved>2015-07-15T01:35:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-15T01:27:19Z" id="121448546">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12248 from rmuir/systemassertions</comment></comments></commit></commits></item><item><title>remove shadow eclipse build</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12247</link><project id="" key="" /><description>we can't maintain this m2e crap, and its surely not working. On the other hand it causes a lot of noise and warnings.

just use mvn eclipse:eclipse

we need to make this build simpler.
</description><key id="95072970">12247</key><summary>remove shadow eclipse build</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-15T00:32:22Z</created><updated>2015-07-15T00:44:31Z</updated><resolved>2015-07-15T00:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-15T00:34:33Z" id="121435832">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12247 from rmuir/remove_shadow_eclipse_build</comment></comments></commit></commits></item><item><title>allow inner_hits to work on `array type` properties of document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12246</link><project id="" key="" /><description>It would be awesome if `inner_hits` could also apply to properties of a document that are not `nested type` but simple `array type`. Currently in order to achieve similar thing a document attribute has to be "nested_type".

Currently following mapping won't work

``` json
{
      "mappings": {
         "products": {
            "properties": {
               "keywords": {
                  "type": "string"
               },
               "name": {
                  "type": "string"
               }
            }
         }
      }
   }
```

for document of format

``` json
{
  "name":"Samsung 42\" LED 4200UHD",
  "keywords":["tv","led","samsing"]
}
```

currently the only trick to get matching keywords is to transform the document into format that would artificially create nested type

``` json
{
  "name":"Samsung 42\" LED 4200UHD",
  "keywords":[{"keyword":"tv"},{"keyword":"led"},{"keyword:":"Samsung"}]
}
```
</description><key id="95072783">12246</key><summary>allow inner_hits to work on `array type` properties of document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">g00fy-</reporter><labels /><created>2015-07-15T00:29:51Z</created><updated>2015-07-15T10:45:14Z</updated><resolved>2015-07-15T10:45:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T10:45:14Z" id="121572927">Hi @g00fy- 

These work in a completely different way from nested documents.  To figure out which keywords matched, either use highlighting, or use a term query for each keyword and specify a `_name` for each term query.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Should 'from' parameter be changed from 'int' to 'long'?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12245</link><project id="" key="" /><description>For paginating results we have a 'from' and 'size' parameter in SearchRequestBuilder. These are both 'int' fields. However, the total hits returned from a search, e.g. SearchHits.totalHits() is defined as a 'long'. 

Maybe I'm missing something, but it seems that it should be impossible to paginate an entire result set if the total hits is greater than Integer.MAX_VALUE. Those hits in the range (Integer.MAX_VALUE, Long.MAX_VALUE) would not be addressable. 
</description><key id="95072094">12245</key><summary>Should 'from' parameter be changed from 'int' to 'long'?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>bug</label></labels><created>2015-07-15T00:24:41Z</created><updated>2015-08-13T13:52:09Z</updated><resolved>2015-07-15T10:43:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-15T10:43:15Z" id="121572194">Your system will die long before you get to the "long" range.  Either use scrolling for deep retrieval, or use a filter to exclude previously shown results.  Deep pagination using from/size is a bad idea.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _replica and _replica_first as search preference.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12244</link><project id="" key="" /><description>Just like specifying `?preference=_primary`, this adds the ability to
specify `?preference=_replica` or `?preference=_replica_first` on
requests that support it.

Resolves #12222
</description><key id="95013745">12244</key><summary>Add _replica and _replica_first as search preference.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T18:57:30Z</created><updated>2015-08-25T22:29:05Z</updated><resolved>2015-07-16T15:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-07-14T19:04:00Z" id="121343695">left a couple of comments, can we also add unit tests to `RoutingIteratorTests`? I think in that case, we don't really need to have an integration test
</comment><comment author="dakrone" created="2015-07-15T19:05:08Z" id="121715609">@kimchy thanks for taking a look! I think I misunderstood what the iterators were doing previously, but I think I have fixed it now. (and added a unit test to make sure)
</comment><comment author="kimchy" created="2015-07-16T08:50:30Z" id="121882255">left few minor comments, LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use enforcer:display-info to print version info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12243</link><project id="" key="" /><description>Currently this is done with ant tasks, but this is simpler and designed for this purpose.

```
[INFO] --- maven-enforcer-plugin:1.0:display-info (print-versions) @ elasticsearch ---
[INFO] Maven Version: 3.2.1
[INFO] JDK Version: 1.8.0_45 normalized as: 1.8.0-45
[INFO] OS Info: Arch: amd64 Family: unix Name: linux Version: 3.19.0-15-generic
```
</description><key id="95012156">12243</key><summary>Use enforcer:display-info to print version info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T18:50:00Z</created><updated>2015-07-14T19:01:14Z</updated><resolved>2015-07-14T19:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-14T18:55:19Z" id="121340749">+1
</comment><comment author="rjernst" created="2015-07-14T18:57:25Z" id="121341622">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12243 from rmuir/enforcer</comment></comments></commit></commits></item><item><title>Unique allocation id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12242</link><project id="" key="" /><description>Add a unique allocation id for a shard, helping to uniquely identify a specific allocation taking place to a node.
A special case is relocation, where a transient relocationId is kept around to make sure the target initializing shard (when using RoutingNodes) is using it for its id, and when relocation is done, the transient relocationId becomes the actual id of it.
</description><key id="94991099">12242</key><summary>Unique allocation id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T16:58:55Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-15T16:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-07-14T17:00:14Z" id="121308576">@bleskes WDYT? once it is in, I think we can use it to identify unique allocations in both IndicesClusterStateService and when sending started/failed actions
</comment><comment author="bleskes" created="2015-07-15T09:31:15Z" id="121548612">LGTM. left some minor comments
</comment><comment author="s1monw" created="2015-08-03T13:44:05Z" id="127238659">/me likes this a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/AllocationIdTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file></files><comments><comment>Unique allocation id</comment><comment>Add a unique allocation id for a shard, helping to uniquely identify a specific allocation taking place to a node.</comment><comment>A special case is relocation, where a transient relocationId is kept around to make sure the target initializing shard (when using RoutingNodes) is using it for its id, and when relocation is done, the transient relocationId becomes the actual id of it.</comment><comment>closes #12242</comment></comments></commit></commits></item><item><title>Add url repository whitelist - backport of #11687 to 1.6 and 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12241</link><project id="" key="" /><description>Require urls for URL repository to be listed in repositories.url.allowed_urls setting. This change ensures that only authorized URLs can be accessed by elasticsearch.
</description><key id="94977840">12241</key><summary>Add url repository whitelist - backport of #11687 to 1.6 and 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-07-14T15:55:47Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-15T02:13:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-14T15:59:53Z" id="121290136">@rmuir this is backport of #11687 to 1.6/1.7 the only significant changes are in [Environment](https://github.com/elastic/elasticsearch/pull/12241/files#diff-f06b176696959d1967c63d5b74fd58acR197) due to difference in handling paths. Could you take another look? Thanks!
</comment><comment author="rmuir" created="2015-07-14T21:15:01Z" id="121388951">looks good.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Render structured exceptions in mget / mpercolate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12240</link><project id="" key="" /><description>Instead of rendering only the exception message this commit
adds structured exception rendering to mget and mpercolate
</description><key id="94977613">12240</key><summary>Render structured exceptions in mget / mpercolate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T15:54:48Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-14T20:03:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-14T16:24:54Z" id="121296520">Left one minor comment, otherwise LGTM
</comment><comment author="martijnvg" created="2015-07-14T19:57:15Z" id="121363401">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/MultiPercolatorTests.java</file></files><comments><comment>Merge pull request #12240 from s1monw/m_structured_response</comment></comments></commit></commits></item><item><title>Structured exceptions should render the stacktrace of it's cause for debugging purposes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12239</link><project id="" key="" /><description>without the stacktrace the exception is pretty useless for debugging. we should by default render also the stacktrace of the cause.
</description><key id="94974596">12239</key><summary>Structured exceptions should render the stacktrace of it's cause for debugging purposes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T15:39:03Z</created><updated>2015-08-11T10:55:33Z</updated><resolved>2015-08-11T10:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/test/java/org/elasticsearch/ESExceptionTests.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/rest/BytesRestResponseTests.java</file></files><comments><comment>Include stacktrace in rendered exceptions</comment></comments></commit></commits></item><item><title>ElasticsearchException tests failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12238</link><project id="" key="" /><description>E.g. http://build-us-00.elastic.co/job/es_core_master_metal/10311/testReport/junit/org.elasticsearch/ElasticsearchExceptionTests/testToXContent/
</description><key id="94968142">12238</key><summary>ElasticsearchException tests failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T15:07:40Z</created><updated>2015-07-14T15:09:17Z</updated><resolved>2015-07-14T15:09:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file></files><comments><comment>[TEST] Don't rely on iteration oder in tests</comment></comments></commit></commits></item><item><title>Allow shards to be allocated if leftover shard from different index exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12237</link><project id="" key="" /><description>If an index name is reused but a leftover shard still exists on any node
we fail repeatedly to allocate the shard since we now check the index UUID
before reusing data. This commit allows to recover even if there is such a
leftover shard by deleting the leftover shard.

Closes #10677
</description><key id="94965007">12237</key><summary>Allow shards to be allocated if leftover shard from different index exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T14:56:16Z</created><updated>2015-08-13T14:28:07Z</updated><resolved>2015-07-15T07:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-14T17:37:21Z" id="121318126">LGTM
</comment><comment author="bleskes" created="2015-07-15T08:18:09Z" id="121526163">LGTM2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregation: Removed Old Script Java API from metrics aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12236</link><project id="" key="" /><description>The old script syntax has been removed from the Java API but the metrics aggregations were missed. This change removes the old script API from the ValuesSourceMetricsAggregationBuilder and removes the relevant test methods for the metrics aggregations.
</description><key id="94957553">12236</key><summary>Aggregation: Removed Old Script Java API from metrics aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>breaking</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T14:30:14Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-14T14:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-14T14:37:51Z" id="121257290">LGTM - nice stats
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify assignToNode to only do initializing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12235</link><project id="" key="" /><description>The method really only should do the move from unassigned to initializing, all the other moves have explicit methods like relocate
</description><key id="94948681">12235</key><summary>Simplify assignToNode to only do initializing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T13:48:02Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-14T14:06:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-14T13:52:38Z" id="121244562">LGTM. Left a tiny comment. No need for another cycle.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>why native plugin use 'docFieldLongs' get empty result?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12234</link><project id="" key="" /><description>I extends AbstractSearchScript to write native plugin. some code as below.

public Object run() {
            logger.info("vv: "+docFieldLongs("user.user_id").getValue());
            ScriptDocValues.Longs fieldData = docFieldLongs("user_id");
            Long id = fieldData.get(0);
            logger.info(user_field + " = " + id);
            logger.info(fieldData.toString());
            return score();
}

I found it prints 0, and the fieldData is empty. However "user_id" is a field in my doc. 

{"user_id":33895,"name":"qq","name_en":["qq"],"university":""}

Could you tell me how to fetch the user_id?I use es-1.6.0, thx.
</description><key id="94943063">12234</key><summary>why native plugin use 'docFieldLongs' get empty result?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ishare</reporter><labels /><created>2015-07-14T13:16:25Z</created><updated>2015-07-17T05:09:15Z</updated><resolved>2015-07-17T05:09:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Preparing ValuesSourceAggregatorFactory/Parser for refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12233</link><project id="" key="" /><description>This change adds AbstractValuesSourceParser which will be the new class used to create ValuesSourceAggregatorFactory objects. AbstractValuesSourceParser parses all the parameters required for ValuesSource and passes to the sub-class to parse any other (implementation specific) parameters. After parsing is complete it will call createFactory on the implementing class to create the AggregatorFactory object and then set the ValuesSource specific parameters before returning it.

ValuesSourceAggregatorFactory also now has setter methods so that it can be used as the 'builder' object in the future.
</description><key id="94931860">12233</key><summary>Preparing ValuesSourceAggregatorFactory/Parser for refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label></labels><created>2015-07-14T12:13:05Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-15T07:50:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-14T12:13:26Z" id="121218047">@jpountz when you are back would you mind reviewing this?
</comment><comment author="jpountz" created="2015-07-15T06:41:13Z" id="121508335">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recreate /var/run/elasticsearch/ if it doesn't exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12232</link><project id="" key="" /><description>Since /var/run/ is on a tmpfs in Ubuntu 14.04, the PID_DIR needs to be recreated if it doesn't exist, e.g. after a reboot.
</description><key id="94921362">12232</key><summary>Recreate /var/run/elasticsearch/ if it doesn't exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nobse</reporter><labels /><created>2015-07-14T11:08:46Z</created><updated>2015-07-14T11:30:14Z</updated><resolved>2015-07-14T11:30:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add old script API use to the deprecations logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12231</link><project id="" key="" /><description>We should add logs to the deprecations logging when users use the old script API syntax. The tricky bit here is not overloading the logs since a user could bulk ingest thousands of update requests per second and fill the deprecations log files very quickly, not only making the log files very hard to digest but also possibly quickly filling the disk
</description><key id="94920513">12231</key><summary>Add old script API use to the deprecations logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-14T11:02:37Z</created><updated>2016-03-16T19:36:51Z</updated><resolved>2016-01-18T20:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T20:58:25Z" id="172652317">Given that we've moved to the new format a few versions ago, I'm going to close
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate [0..1] similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12230</link><project id="" key="" /><description>@clintongormley can you take a look
</description><key id="94920215">12230</key><summary>Deprecate [0..1] similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>deprecation</label><label>docs</label></labels><created>2015-07-14T10:59:52Z</created><updated>2015-08-07T10:07:33Z</updated><resolved>2015-07-14T12:42:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-14T12:42:50Z" id="121228824">fixed formatting and merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't allow fuzziness specified as a % and require edits [0,2] instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12229</link><project id="" key="" /><description>Lucene deprecated this in 4.0 and we only try best effort to support it.
Folks should only use edit distance rather than some length based
similarity. Yet the formular is simple enough such that users can
still do it in the client if they really need to.

Closes #10638
</description><key id="94916598">12229</key><summary>Don't allow fuzziness specified as a % and require edits [0,2] instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T10:38:31Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-14T15:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-14T10:41:49Z" id="121197453">I guess we have to deprecated this in 1.7
</comment><comment author="colings86" created="2015-07-14T14:54:32Z" id="121269357">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ambiguous docs for completion suggester "transpositions"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12228</link><project id="" key="" /><description>The docs says

&gt; **transpositions** _Sets if transpositions should be counted as one or two changes, defaults to true_

It's not really clear whether or not setting it to `true` will make transpositions be counted as one or two changes.

Ref:
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html &amp; 
https://github.com/elastic/elasticsearch/blob/master/docs/reference/search/suggesters/completion-suggest.asciidoc
</description><key id="94911823">12228</key><summary>Ambiguous docs for completion suggester "transpositions"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">ankr</reporter><labels><label>docs</label></labels><created>2015-07-14T10:09:52Z</created><updated>2015-07-15T19:45:52Z</updated><resolved>2015-07-15T19:45:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-14T12:39:25Z" id="121228343">thanks @ankr 

@areek could you clarify in the docs please?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12249 from areek/fix/12228</comment></comments></commit><commit><files /><comments><comment>Clarify docs for transpositions setting in completion suggester</comment></comments></commit></commits></item><item><title>Add a release profile to the parent pom.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12227</link><project id="" key="" /><description>This commit adds a release profile that runs additional checks like:
- check for `norelease` in the source
- check for `AwaitsFix` annotations
- deploys only if all reactor builds where successful
- signs all artifacts with the GPG key provided (required for release)
- builds the RPM together with the zip, tar.gz and .deb during package phase

Note this is WIP of replacing major functionality of the release pythong script but
won't work by itself.
</description><key id="94911095">12227</key><summary>Add a release profile to the parent pom.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T10:04:52Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-14T19:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-14T10:05:02Z" id="121189815">@spinscale can you take a look
</comment><comment author="rjernst" created="2015-07-14T17:43:44Z" id="121320058">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12227 from s1monw/release_profile</comment></comments></commit></commits></item><item><title>How processes 'C|D' in elasticsearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12226</link><project id="" key="" /><description>I meet special requirements when I migrate elasticsearch into project,there is the SQL statement in my business that looks like "A='XX' and B='XX' and (C|D) ",for example,it will be "A='1' and B='2' and (0000032768|000000001)",when "C|D" is 1,it represents true ,otherwise ,it will be false,so,how to deal it in elasticsearch?
I am very grateful!
</description><key id="94898475">12226</key><summary>How processes 'C|D' in elasticsearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mlc0202</reporter><labels /><created>2015-07-14T08:53:57Z</created><updated>2015-07-14T12:07:32Z</updated><resolved>2015-07-14T12:07:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-14T12:07:31Z" id="121216595">As stated in https://github.com/elastic/elasticsearch/issues/12140 please ask questions on discuss.elastic.co. We can help there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Debian init script should create pid directory on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12225</link><project id="" key="" /><description>Elasticsearch 1.6.0 creates a pidfile in /var/run/elasticsearch/, but does not make sure that this directory exists. On Ubuntu 14.04, and probably quite a few other OSes, the /var/run/ directory is re-created every boot, which causes elasticsearch to fail to start with `touch: cannot touch ‘/var/run/elasticsearch/elasticsearch.pid’: No such file or directory`.

This can probably be fixed by adding `$PID_DIR` to [this](https://github.com/elastic/elasticsearch/blob/master/core/src/packaging/deb/init.d/elasticsearch#L153) line.
</description><key id="94894966">12225</key><summary>Debian init script should create pid directory on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Crote</reporter><labels /><created>2015-07-14T08:34:31Z</created><updated>2015-07-14T11:49:12Z</updated><resolved>2015-07-14T11:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-14T11:49:11Z" id="121213395">@Crote thanks for reporting.

This issue has been reported in #11594 and fixed in #11674, so I close this one. Feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix github source link in resiliency page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12224</link><project id="" key="" /><description /><key id="94891585">12224</key><summary>Fix github source link in resiliency page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awislowski</reporter><labels><label>docs</label></labels><created>2015-07-14T08:14:22Z</created><updated>2015-07-14T09:15:57Z</updated><resolved>2015-07-14T09:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-14T09:15:11Z" id="121175152">Thx @awislowski . I'll merge it right in.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12224 from awislowski/patch-2</comment></comments></commit><commit><files /><comments><comment>Merge pull request #12224 from awislowski/patch-2</comment></comments></commit></commits></item><item><title>Update index.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12223</link><project id="" key="" /><description /><key id="94885948">12223</key><summary>Update index.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awislowski</reporter><labels><label>docs</label></labels><created>2015-07-14T07:36:24Z</created><updated>2015-07-14T12:36:03Z</updated><resolved>2015-07-14T12:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="awislowski" created="2015-07-14T07:37:08Z" id="121153664">https://github.com/elastic/elasticsearch/issues/8688 is closed
</comment><comment author="clintongormley" created="2015-07-14T12:36:00Z" id="121226990">thanks @awislowski - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12223 from awislowski/patch-1</comment></comments></commit></commits></item><item><title>adding _replicas_only preference for Shadow Replica indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12222</link><project id="" key="" /><description>For Shadow Replica indices stored in shared storage we want to delegate queries to the shadow replica nodes only. This is to maximize the ingestion rates by preventing primary shard nodes from both indexing and running queries. The preference settings (see: https://www.elastic.co/guide/en/elasticsearch/reference/1.6/search-request-preference.html ) do not include an option to delegate queries to replicas only.

This is a request to add _replicas_only preference to the current set of preferences (or _shadow_replicas_only, if we limit the feature to Shadow Replica indices).
</description><key id="94864081">12222</key><summary>adding _replicas_only preference for Shadow Replica indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kozaczynski</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-07-14T04:16:01Z</created><updated>2015-07-16T15:30:44Z</updated><resolved>2015-07-16T15:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/Preference.java</file><file>core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>core/src/test/java/org/elasticsearch/search/preference/SearchPreferenceTests.java</file></files><comments><comment>Add _replica and _replica_first as search preference.</comment></comments></commit></commits></item><item><title>fail plugins on version mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12221</link><project id="" key="" /><description>Fail with a clean error if jars were compiled against a different elasticsearch version.

Also fix tests here, they didn't ensure they hit expected exceptions, and they weren't working correctly because they wrote invalid manifests in the test jars.
</description><key id="94844438">12221</key><summary>fail plugins on version mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-14T01:03:07Z</created><updated>2015-07-14T12:32:11Z</updated><resolved>2015-07-14T01:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-14T01:22:58Z" id="121105652">LGTM
</comment><comment author="clintongormley" created="2015-07-14T12:32:11Z" id="121225348">nice!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java</file></files><comments><comment>Merge pull request #12221 from rmuir/version_hell</comment></comments></commit></commits></item><item><title>Record applied _templates when creating indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12220</link><project id="" key="" /><description>It'd be nice if we have a meta field (optional?) that stored what _templates were applied when creating an index.

It's simple enough to figure this out when there are a few templates and indices, but becomes difficult if there are multiple ordered templates.
</description><key id="94829146">12220</key><summary>Record applied _templates when creating indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2015-07-13T22:48:12Z</created><updated>2015-07-14T12:31:27Z</updated><resolved>2015-07-14T12:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-14T12:31:27Z" id="121225187">We already log this information https://github.com/elastic/elasticsearch/pull/8646 - i definitely don't think we should store it.  Not least of all because templates can be changed, and so the stored value may have nothing to do with the old value.

If you need to debug this, then it is easy to just create a new index with the current template settings and see what comes out in the logs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: Explicitly use newer junit runner for maven integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12219</link><project id="" key="" /><description>The older runner (junit 4.0?) hides exceptions in static initialization
(and runs the tests anyways!). With this change, the actual cause
of the recent windows integ test failures with security manager
are shown.
</description><key id="94827291">12219</key><summary>Tests: Explicitly use newer junit runner for maven integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T22:35:18Z</created><updated>2015-07-13T22:37:36Z</updated><resolved>2015-07-13T22:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-13T22:36:07Z" id="121081284">looks good. thanks for debugging this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12219 from rjernst/fix/integ-test-exceptions</comment></comments></commit></commits></item><item><title>Fix maven integration tests to work with security manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12218</link><project id="" key="" /><description>Maven integration tests currently do not work with security manager. There are a couple easy to fix issues like passing the right system properties to give access to .class files. However, a larger issue is the forked classloader the maven failsafe plugin uses. We can disable this isolated classloader with `useManifestOnlyJar=false`. However, the jars from maven itself then have jar hell (the failsafe api jar and failsafe junit jar have duplicate classes). We might be able to exclude the api jar (assuming the junit jar is a superset of the classes).
</description><key id="94822696">12218</key><summary>Fix maven integration tests to work with security manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2015-07-13T22:03:44Z</created><updated>2015-07-17T00:59:57Z</updated><resolved>2015-07-14T17:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-13T23:19:34Z" id="121088455">I got a little farther in removing jar hell:
https://gist.github.com/rjernst/32f1666b8a6fd955e085

However, this surefire/failsafe plugin is just messy. This is the current failure:

```
Caused by: java.lang.IllegalStateException: jar hell!
class: org.apache.maven.shared.utils.StringUtils
jar1: /Users/rjernst/.m2/repository/org/apache/maven/surefire/common-java5/2.18.1/common-java5-2.18.1.jar
jar2: /Users/rjernst/.m2/repository/org/apache/maven/shared/maven-shared-utils/0.4/maven-shared-utils-0.4.jar
```

I'm not sure it is worthwhile to fix this. Security manager is already running for the actual ES server for the tests. This is only for rest test itself (ie the client). One thing to try might be using junit4 as we do for unit tests.
</comment><comment author="s1monw" created="2015-07-14T08:37:17Z" id="121166893">&gt; I'm not sure it is worthwhile to fix this. Security manager is already running for the actual ES server for the tests. This is only for rest test itself (ie the client). One thing to try might be using junit4 as we do for unit tests.

I agree - the actual service that we are testing (the unzipped elasticsearch server) starts up with sec manager enabled so I think we should be just fine here?
</comment><comment author="rmuir" created="2015-07-17T00:59:57Z" id="122140288">By the way, this is fixed with #12302

Although its true in this case, its unimportant for testing (since the external cluster runs with security manager and thats the important part), i feel better with integ tests not running unrestrained.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use the same Randomized Runner as used by the elasticsearch-parent project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12217</link><project id="" key="" /><description>Closes #12216 
</description><key id="94816058">12217</key><summary>Use the same Randomized Runner as used by the elasticsearch-parent project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>build</label><label>test</label><label>v1.7.0</label></labels><created>2015-07-13T21:30:06Z</created><updated>2015-07-16T15:02:58Z</updated><resolved>2015-07-14T08:35:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Use Randomized Runner 2.1.14 in 1.7 branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12216</link><project id="" key="" /><description>The 1.7 branch should use the same randomized runner as the `elasticsearch-parent` project for the matching release.

Currently it uses `2.1.11`, but plugins are using `2.1.14`.
</description><key id="94815726">12216</key><summary>Use Randomized Runner 2.1.14 in 1.7 branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>build</label><label>low hanging fruit</label><label>v1.7.0</label></labels><created>2015-07-13T21:28:20Z</created><updated>2015-07-16T10:00:34Z</updated><resolved>2015-07-16T10:00:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-16T10:00:34Z" id="121914173">The associated PR got merged, so closing this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support filtering percolator queries by date using `now`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12215</link><project id="" key="" /><description>PR for #12185
</description><key id="94813340">12215</key><summary>Support filtering percolator queries by date using `now`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T21:14:06Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-14T12:35:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-14T10:21:44Z" id="121193020">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Merge pull request #12215 from martijnvg/percolator/filter_by_now</comment></comments></commit></commits></item><item><title>Fix NPE when percolating a document that has a _parent field configured in its mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12214</link><project id="" key="" /><description>PR for #12192
</description><key id="94805259">12214</key><summary>Fix NPE when percolating a document that has a _parent field configured in its mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T20:34:14Z</created><updated>2015-07-14T08:31:13Z</updated><resolved>2015-07-14T08:31:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-13T20:43:59Z" id="121051619">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Merge pull request #12214 from martijnvg/percolator/_parent_field/npe</comment></comments></commit></commits></item><item><title>Add index name to the upgrade exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12213</link><project id="" key="" /><description /><key id="94793534">12213</key><summary>Add index name to the upgrade exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T19:30:07Z</created><updated>2015-08-25T13:16:50Z</updated><resolved>2015-07-14T23:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-13T19:37:32Z" id="121033511">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PermGen OutOfMemoryError when reloading scripts too often</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12212</link><project id="" key="" /><description>ENV:
- ES = 1.4.4
- OS = SmartOS
- JDK = OpenJDK 7
- RAM = 16GB

We are running into a system failure because the PermGen cache is filling up.  Our systems are deployed with Chef and the chef-client runs about every thirty minutes.  The errors are happening after sitting for a few days.  Every time Chef runs it will copy the scripts from a repo to the scripts folder, changed or not.  ES will always recompile these scripts because the timestamp on the files has changed.  Eventually the PermGen will fill up and crash the system (even with no indexing or searching happening).

```
[2015-07-12 02:31:33,382][INFO ][script                   ] [S-10.129.22.15] compiling script file [/opt/local/etc/elasticsearch/scripts/dateFormat.groovy]
[2015-07-12 02:31:33,385][INFO ][script                   ] [S-10.129.22.15] compiling script file [/opt/local/etc/elasticsearch/scripts/inbound_group_duration.groovy]
[2015-07-12 02:31:33,388][INFO ][script                   ] [S-10.129.22.15] compiling script file [/opt/local/etc/elasticsearch/scripts/inbound_group_duration_doesNotEqualFilter.groovy]
[2015-07-12 02:31:33,391][INFO ][script                   ] [S-10.129.22.15] compiling script file [/opt/local/etc/elasticsearch/scripts/inbound_group_duration_equalsFilter.groovy]
[2015-07-12 02:31:33,394][INFO ][script                   ] [S-10.129.22.15] compiling script file [/opt/local/etc/elasticsearch/scripts/inbound_group_duration_greaterThanEqualFilter.groovy]
[2015-07-12 02:31:35,718][INFO ][script                   ] [S-10.129.22.15] compiling script file [/opt/local/etc/elasticsearch/scripts/inbound_group_duration_greaterThanFilter.groovy]
[2015-07-12 02:32:12,053][WARN ][script                   ] [S-10.129.22.15] failed to load/compile script [inbound_group_duration_greaterThanFilter]
org.elasticsearch.script.groovy.GroovyScriptCompilationException: OutOfMemoryError[PermGen space]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:153)
        at org.elasticsearch.script.ScriptService$ScriptChangesListener.onFileInit(ScriptService.java:588)
        at org.elasticsearch.script.ScriptService$ScriptChangesListener.onFileChanged(ScriptService.java:621)
        at org.elasticsearch.watcher.FileWatcher$FileObserver.onFileChanged(FileWatcher.java:271)
        at org.elasticsearch.watcher.FileWatcher$FileObserver.checkAndNotify(FileWatcher.java:122)
        at org.elasticsearch.watcher.FileWatcher$FileObserver.updateChildren(FileWatcher.java:207)
        at org.elasticsearch.watcher.FileWatcher$FileObserver.checkAndNotify(FileWatcher.java:108)
        at org.elasticsearch.watcher.FileWatcher.doCheckAndNotify(FileWatcher.java:62)
        at org.elasticsearch.watcher.AbstractResourceWatcher.checkAndNotify(AbstractResourceWatcher.java:43)
        at org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor.run(ResourceWatcherService.java:180)
        at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:490)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

This is the GC that was performed just before the crash:

```
[2015-07-12 02:00:18,361][INFO ][monitor.jvm              ] [S-10.129.22.15] [gc][old][204306][1] duration [5.2s], collections [1]/[6s], total [5.2s]/[5.2s], memory [5.2gb]-&gt;[1.8g
b]/[7.6gb], all_pools {[young] [2.5gb]-&gt;[20.2mb]/[2.5gb]}{[survivor] [2mb]-&gt;[0b]/[42.5mb]}{[old] [2.6gb]-&gt;[1.8gb]/[5.3gb]}
```

If we stop copying the scripts with Chef then the issue doesn't happen.  Somewhere there is an issue with the JVM not letting go of memory related to the scripts being recompiled.
</description><key id="94778747">12212</key><summary>PermGen OutOfMemoryError when reloading scripts too often</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">hulu1522</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-07-13T18:13:10Z</created><updated>2015-07-16T22:16:10Z</updated><resolved>2015-07-16T21:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-14T11:57:17Z" id="121215232">Hmm this is odd.  This PR should have fixed it https://github.com/elastic/elasticsearch/pull/8062 but you're using 1.4.4, which should include the fix.

@dakrone any ideas?
</comment><comment author="hulu1522" created="2015-07-14T17:10:25Z" id="121311163">Here is a more precise version of my ES but let me know if you need any other information.

```
=&gt; curl 10.129.22.15:9200

{
  "status" : 200,
  "name" : "S-10.129.22.15",
  "cluster_name" : "&lt;cluster-name-here&gt;",
  "version" : {
    "number" : "1.4.4",
    "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
    "build_timestamp" : "2015-02-19T13:05:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
  "tagline" : "You Know, for Search"
}
```

and the JAVA version:

```
=&gt; java -version

openjdk version "1.7.0-internal"
OpenJDK Runtime Environment (build 1.7.0-internal-pkgsrc_2015_02_22_06_07-b00)
OpenJDK 64-Bit Server VM (build 24.71-b01, mixed mode)
```

Thanks.
</comment><comment author="dakrone" created="2015-07-16T18:29:44Z" id="122042218">@hulu1522 thanks for reporting this, I'm working on a fix for this.
</comment><comment author="hulu1522" created="2015-07-16T19:50:25Z" id="122069286">@dakrone Thanks!  The fix looks good.  Any estimate for 1.7.1 release?
</comment><comment author="dakrone" created="2015-07-16T22:16:10Z" id="122119782">@hulu1522 unfortunately I'm not sure, since 1.7.0 was just released today it will probably be a couple of weeks at the least, maybe @clintongormley knows more?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file></files><comments><comment>Consistently name Groovy scripts with the same content</comment></comments></commit></commits></item><item><title>Add global search timeout setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12211</link><project id="" key="" /><description>This commit adds a dynamically updatable cluster-level search timeout setting. Additionally, some other search settings were moved to
org.elasticsearch.search.SearchSettings.

Closes #12149
</description><key id="94776617">12211</key><summary>Add global search timeout setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T18:01:18Z</created><updated>2015-07-17T13:54:59Z</updated><resolved>2015-07-17T13:23:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-13T20:44:30Z" id="121051873">I left some comments, thanks @jasontedor 
</comment><comment author="jasontedor" created="2015-07-16T01:25:05Z" id="121795999">@s1monw @jpountz I've updated the pull request to reflect the comments regarding introducing a new class.
</comment><comment author="jpountz" created="2015-07-17T11:13:07Z" id="122248901">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #15831 from jasontedor/global-search-timeout-docs</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/search/MockSearchService.java</file></files><comments><comment>Merge pull request #12211 from jasontedor/features/12149</comment></comments></commit></commits></item><item><title>updated the elasticsearch versioning format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12210</link><project id="" key="" /><description>Moving to from `X.Y.Z.beta1`/`X.Y.Z.RC1` to `X.Y.Z-beta1`/`X.Y.Z-rc1`
</description><key id="94774438">12210</key><summary>updated the elasticsearch versioning format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T17:50:17Z</created><updated>2015-07-14T12:04:53Z</updated><resolved>2015-07-13T18:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-13T18:07:49Z" id="121009128">LGTM
</comment><comment author="rmuir" created="2015-07-13T18:24:24Z" id="121014736">+1: i tried out the branch and didn't see any missing occurences ; all packaging and tests pass.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add date math support in index names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12209</link><project id="" key="" /><description>Date math index name resolution enables you to search a range of time-series indices, rather than searching all of your time-series indices and filtering the the results or maintaining aliases. Limiting the number of indices that are searched reduces the load on the cluster and improves execution performance. For example, if you are searching for errors in your daily logs, you can use a date math name template to restrict the search to the past two days.

The added `ExpressionResolver` implementation that is responsible for resolving date math expressions in index names. This resolver is evaluated before wildcard expressions are evaluated.

The supported format: `&lt;static_name{date_math_expr{date_format|timezone_id}}&gt;` and the date math expressions must be enclosed within angle brackets. The `date_format` is optional and defaults to `YYYY.MM.dd`. The `timezone_id` id is optional too and defaults to `utc`.

The `{` character can be escaped by places `\\` before it.

The following index names can be specified: (assuming now is 2024.03.22 noon utc)
- `&lt;logstash-{now/d}&gt;` translates into `logstash-2024.03.22`
- `&lt;logstash-{now/M}&gt;` translates into `logstash-2024.03.01`
- `&lt;logstash-{now/M{YYYY.MM}}&gt;` translates into `logstash-2024.03`
- `&lt;logstash-{now/M-1M{YYYY.MM}}&gt;` translates into `logstash-2024.02`
- `&lt;logstash-{now/d{YYYY.MM.dd|+12:00}}&gt;` translates into `logstash-2024.03.23`

The following example shows a search request that searches the Logstash indices for the past
three days:

``` bash
curl -XGET 'localhost:9200/&lt;logstash-{now/d-2d}&gt;,&lt;logstash-{now/d-1d}&gt;,&lt;logstash-{now/d}&gt;/_search' {
  "query" : {
    ...
  }
}
```

PR for #12059
</description><key id="94765868">12209</key><summary>Add date math support in index names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T16:58:14Z</created><updated>2015-07-30T13:55:47Z</updated><resolved>2015-07-29T15:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-14T15:04:43Z" id="121275398">Updated the pr to fix @clintongormley's docs feedback.
</comment><comment author="javanna" created="2015-07-27T14:45:01Z" id="125230964">left a few comments
</comment><comment author="martijnvg" created="2015-07-28T06:23:56Z" id="125465667">@javanna I rebased the PR to master and applied your comments.
</comment><comment author="javanna" created="2015-07-28T15:03:44Z" id="125640745">did another round, left a couple minor comments
</comment><comment author="martijnvg" created="2015-07-28T15:42:44Z" id="125658125">@javanna I updated the PR.
</comment><comment author="javanna" created="2015-07-29T09:09:25Z" id="125887401">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>High memory usage and stop working every often</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12208</link><project id="" key="" /><description>System info:
- Ubuntu 14.04.2 LTS
- ElasticSearch 1.6.0 from Elastic repository
- 32Gb RAM, 8 CPUs @ 2GHz

I noticed that from time to time, ElasticSearch just stops working and Kibana and Logstash cannot connect anymore to it. It happens every few days (or maybe every day).
When having a closer look at the issue, I noticed that ElasticSearch was still running but stopped listening (no more listening socket) and was using about 1.2Gb of RAM (checked using ps_mem.py). Restarting it fix the issue but it is just a workaround.

Looking at the logs in /var/log/elasticsearch didn't show anything wrong or any warning. There was nothing in system logs either.

Is there a way I can debug that issue? Any flag that I can add to figure out what's going on?
</description><key id="94765131">12208</key><summary>High memory usage and stop working every often</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ThomasdOtreppe</reporter><labels /><created>2015-07-13T16:53:53Z</created><updated>2016-01-18T20:57:55Z</updated><resolved>2016-01-18T20:57:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-14T12:33:55Z" id="121225993">Hi @ThomasdOtreppe 

This sounds like a configuration issue.  I suggest you have a read through the setup docs (https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html) and ask any questions you have in the forum https://discuss.elastic.co/.

If, after that, you think you've found a bug, then feel free to open an issue with the details here.
</comment><comment author="ThomasdOtreppe" created="2015-07-14T15:26:33Z" id="121281674">I already went through the docs. @clintongormley, if I understand correctly, you mean that the listening socket stopping is a configuration option? Weird option.

I'll ask in the forum as you request.
</comment><comment author="clintongormley" created="2015-07-14T15:28:19Z" id="121282049">No, I'm suggesting that you are filling up your heap and probably don't have swap disabled, which is slowing down GCs and making nodes unresponsive.  This is the likeliest condition.  
</comment><comment author="ThomasdOtreppe" created="2015-07-14T15:40:08Z" id="121284886">No, the listening socket was gone. netstat -ln didn't show it anymore. It reappeared when I restarted the service.

Have you seen the specs of the system? 32Gb of RAM, 8 CPU @ 2GHz.
</comment><comment author="clintongormley" created="2015-07-14T15:57:27Z" id="121289040">OK I paid more attention to the memory filling up and becoming unresponsive.  I wonder if the socket times out because of slow GCs.  

&gt; Have you seen the specs of the system? 32Gb of RAM, 8 CPU @ 2GHz.

You can fill up 32GB with garbage quite happily, then if you have any heap swapped out, slow GCs will kill responsiveness.  Are you seeing slow GC messages in the logs?  I don't understand why you said "high memory usage" in the title, then say it was only using 1.2 GB?
</comment><comment author="ThomasdOtreppe" created="2015-07-14T18:02:50Z" id="121324737">It wasn't filled. There was maybe a total 2Gb of RAM used according to ps_mem.py. I don't remember seeing any swap used (according to htop) but I could be wrong about that.

A total of 1.7Gb for 2 java processes (Logstash and ElasticSearch). After noticing the issues with ES and killing it, it went down to ~500Mb.

1.2Gb of memory usage for a single program not doing much seemed excessive especially considering I probably hadn't sent more than a few Mb of logs since the last time I had restarted it.
It's now been running for one day (I restarted it yesterday) and it's using about 700Mb.

Feel free to change the title.
</comment><comment author="clintongormley" created="2015-07-14T18:19:15Z" id="121328952">I wonder if it is related to this futex bug https://github.com/elastic/elasticsearch/issues/11526, https://access.redhat.com/articles/3078

Some other random ideas: 
- what do you set `$ES_HEAP` to?
- If you do `GET /_nodes` do you see `mlockall` set to `true`?
- Are you using the transport client to connect logstash to ES. If so, do you see the same issue if you switch to using the `http` client?
- What do you see in the logs?
</comment><comment author="ThomasdOtreppe" created="2015-07-14T19:04:04Z" id="121343716">I haven't changed the settings in the init.d file which says it defaults to 256Mb, 1Gb max.

I don't see any reference to mlockall when querying /_nodes:

```
{
    "cluster_name": "elasticsearch",
    "nodes": {
        "gmxTUsBZQzCtZU4lpO5x2A": {
            "name": "Doctor Glitternight",
            "transport_address": "local[1]",
            "host": "REDACTED",
            "ip": "PRIVATE_LAN_IP",
            "version": "1.6.0",
            "build": "cdd3ac4",
            "http_address": "inet[/127.0.0.1:9200]",
            "attributes": {
                "local": "true"
            },
            "settings": {
                "node": {
                    "local": "true"
                },
                "config.ignore_system_properties": "true",
                "client": {
                    "type": "node"
                },
                "name": "Doctor Glitternight",
                "pidfile": "/var/run/elasticsearch/elasticsearch.pid",
                "path": {
                    "data": "/var/lib/elasticsearch",
                    "work": "/tmp/elasticsearch",
                    "home": "/usr/share/elasticsearch",
                    "conf": "/etc/elasticsearch",
                    "logs": "/var/log/elasticsearch"
                },
                "config": "/etc/elasticsearch/elasticsearch.yml",
                "cluster": {
                    "name": "elasticsearch"
                },
                "discovery": {
                    "zen": {
                        "ping": {
                            "multicast": {
                                "enabled": "false"
                            }
                        }
                    }
                },
                "network": {
                    "host": "127.0.0.1"
                }
            },
            "os": {
                "refresh_interval_in_millis": 1000,
                "available_processors": 8,
                "cpu": {
                    "vendor": "Intel",
                    "model": "Xeon",
                    "mhz": 2000,
                    "total_cores": 8,
                    "total_sockets": 4,
                    "cores_per_socket": 2,
                    "cache_size_in_bytes": 20480
                },
                "mem": {
                    "total_in_bytes": 33738743808
                },
                "swap": {
                    "total_in_bytes": 34357637120
                }
            },
            "process": {
                "refresh_interval_in_millis": 1000,
                "id": 1912,
                "max_file_descriptors": 65535,
                "mlockall": false
            },
            "jvm": {
                "pid": 1912,
                "version": "1.7.0_79",
                "vm_name": "OpenJDK 64-Bit Server VM",
                "vm_version": "24.79-b02",
                "vm_vendor": "Oracle Corporation",
                "start_time_in_millis": 1436807943854,
                "mem": {
                    "heap_init_in_bytes": 268435456,
                    "heap_max_in_bytes": 1037959168,
                    "non_heap_init_in_bytes": 24313856,
                    "non_heap_max_in_bytes": 224395264,
                    "direct_max_in_bytes": 1037959168
                },
                "gc_collectors": ["ParNew", "ConcurrentMarkSweep"],
                "memory_pools": ["Code Cache", "Par Eden Space", "Par Survivor Space", "CMS Old Gen", "CMS Perm Gen"]
            },
            "thread_pool": {
                "generic": {
                    "type": "cached",
                    "keep_alive": "30s",
                    "queue_size": -1
                },
                "index": {
                    "type": "fixed",
                    "min": 8,
                    "max": 8,
                    "queue_size": "200"
                },
                "fetch_shard_store": {
                    "type": "scaling",
                    "min": 1,
                    "max": 16,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "get": {
                    "type": "fixed",
                    "min": 8,
                    "max": 8,
                    "queue_size": "1k"
                },
                "snapshot": {
                    "type": "scaling",
                    "min": 1,
                    "max": 4,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "merge": {
                    "type": "scaling",
                    "min": 1,
                    "max": 4,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "suggest": {
                    "type": "fixed",
                    "min": 8,
                    "max": 8,
                    "queue_size": "1k"
                },
                "bulk": {
                    "type": "fixed",
                    "min": 8,
                    "max": 8,
                    "queue_size": "50"
                },
                "optimize": {
                    "type": "fixed",
                    "min": 1,
                    "max": 1,
                    "queue_size": -1
                },
                "warmer": {
                    "type": "scaling",
                    "min": 1,
                    "max": 4,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "flush": {
                    "type": "scaling",
                    "min": 1,
                    "max": 4,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "search": {
                    "type": "fixed",
                    "min": 13,
                    "max": 13,
                    "queue_size": "1k"
                },
                "fetch_shard_started": {
                    "type": "scaling",
                    "min": 1,
                    "max": 16,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "listener": {
                    "type": "fixed",
                    "min": 4,
                    "max": 4,
                    "queue_size": -1
                },
                "percolate": {
                    "type": "fixed",
                    "min": 8,
                    "max": 8,
                    "queue_size": "1k"
                },
                "management": {
                    "type": "scaling",
                    "min": 1,
                    "max": 5,
                    "keep_alive": "5m",
                    "queue_size": -1
                },
                "refresh": {
                    "type": "scaling",
                    "min": 1,
                    "max": 4,
                    "keep_alive": "5m",
                    "queue_size": -1
                }
            },
            "network": {
                "refresh_interval_in_millis": 5000,
                "primary_interface": {
                    "address": "PRIVATE_LAN_IP",
                    "name": "eth0",
                    "mac_address": "RE:DA:CT:ED:XX:XX"
                }
            },
            "transport": {
                "bound_address": "local[1]",
                "publish_address": "local[1]",
                "profiles": {}
            },
            "http": {
                "bound_address": "inet[/127.0.0.1:9200]",
                "publish_address": "inet[/127.0.0.1:9200]",
                "max_content_length_in_bytes": 104857600
            },
            "plugins": []
        }
    }
}
```

Here is what I use with logstash:

```
output {
        elasticsearch {
                host =&gt; localhost
                protocol =&gt; 'http'
        }
}
```

I will try to switch to the http client

As I said, there is nothing unusual in the logs and no warnings or errors. I haven't kept them unfortunately but I'll post them next time it happens.
</comment><comment author="clintongormley" created="2015-07-14T19:12:44Z" id="121345513">It looks like you're already using the http client for logstash.   

You have mlockall turned off (look under the `process` section). Try turning on mlockall (or disabling swap) (see https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory for how) and see if that solves it

Also, are you using a Haswell CPU and can you find out whether your kernel version is susceptible to the futex bug?
</comment><comment author="ThomasdOtreppe" created="2015-07-14T19:26:13Z" id="121348554">How can I find that out? It's a kernel shipped by Ubuntu (3.19.0-22-generic) and even if the base kernel might be susceptible, they might have patched it.

Quick question: is there any good reason using swap is enabled by default? Shouldn't it be disabled by default since it's causing issues?
</comment><comment author="clintongormley" created="2015-07-14T20:48:15Z" id="121380028">&gt; How can I find that out? It's a kernel shipped by Ubuntu (3.19.0-22-generic) and even if the base kernel might be susceptible, they might have patched it.

Then I suggest reading the kernel change logs?  Certainly from my googling attempt, it seems that &gt; 3.18 is OK, but I'm not sure about that.  I'd also check if you have a Haswell processor.

&gt; Quick question: is there any good reason using swap is enabled by default? Shouldn't it be disabled by default since it's causing issues?

Because that is OS level - we can't set that for you.
</comment><comment author="ThomasdOtreppe" created="2015-07-14T21:02:11Z" id="121385835">It's not Haswell according to http://ark.intel.com/products/codename/42174/Haswell#@All : We have Intel(R) Xeon(R) CPU E5-2640 v2 @ 2.00GHz.

Let me rephrase the question. Is there any good reason not to have  'bootstrap.mlockall: true' in the elasticsearch configuration by default since swap causes issues?
</comment><comment author="ThomasdOtreppe" created="2015-07-14T22:41:32Z" id="121414257">Should I increase ES_HEAP_SIZE and if yes, how much?
</comment><comment author="clintongormley" created="2015-07-15T10:41:07Z" id="121571127">&gt; Let me rephrase the question. Is there any good reason not to have 'bootstrap.mlockall: true' in the elasticsearch configuration by default since swap causes issues?

Yes, because we could lock too much memory into RAM, which could bring down your system or starve other processes.  This is not a decision we can take on your behalf.

&gt; Should I increase ES_HEAP_SIZE and if yes, how much?

I refer you to the docs and to the forum again.
</comment><comment author="clintongormley" created="2016-01-18T20:57:55Z" id="172652147">Nothing further - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rewrite set twice in WildcardQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12207</link><project id="" key="" /><description>Relates #12110
</description><key id="94749544">12207</key><summary>Rewrite set twice in WildcardQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T15:35:29Z</created><updated>2015-08-13T13:54:43Z</updated><resolved>2015-07-13T15:40:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file></files><comments><comment>Fix rewrite set twice in WildcardQueryParser</comment></comments></commit></commits></item><item><title>1.6.0's init scripts don't kill some older versions of Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12206</link><project id="" key="" /><description>Elasticsearch 1.6.0's init scripts don't shut down Elasticsearch 1.3.x and that caused me trouble.

I'm posting this here more for posterity than anything. Feel free to close if its not in scope.

My usual (pre 1.6.0) upgrade procedure for Elasticsearch is:
0. Install new plugin versions on all nodes
1. Install new version
2. Shut down allocation
3. Shut down Elasticsearch
4. Start Elasticsearch
5. Wait for Elasticsearch to reply to _cat/health
6. Reenable allocation
7. Wait a minute
8. Wait for the cluster to go green
9. Repeat 1-8 per node

We have steps 2-8 scripted. But since the init script doesn't shut down Elasticsearch 1.3.x we can't use the script. Sadness.
</description><key id="94745666">12206</key><summary>1.6.0's init scripts don't kill some older versions of Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2015-07-13T15:20:02Z</created><updated>2015-08-10T09:45:35Z</updated><resolved>2015-08-10T09:45:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-13T20:29:37Z" id="121047095">Hey nik,

I assume, that you did not stop Elasticsearch, install the .deb, start Elasticsearch, but replaced the debian package first?
</comment><comment author="nik9000" created="2015-07-13T20:34:50Z" id="121048181">&gt; I assume, that you did not stop Elasticsearch, install the .deb, start Elasticsearch, but replaced the debian package first?

Yes. If you stop before you replace it works like it ought to.
</comment><comment author="nik9000" created="2015-08-10T09:45:34Z" id="129384958">Getting irrelevant -closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>UpdateRequest after ES 1.5.0 doesn’t work function script()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12205</link><project id="" key="" /><description>i use kafka(input) and logstash (1.5.2) with (output) elasticsearch (ES 1.5.2 or 1.6.0)
- i change the code in plugin output elasticsearch to be able to update documents and for that i use the UpdateRequest API.
  https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/188/files
- until Elasticsearch 1.5.0 the function request.script(args[:_script]) works without problem but after in versions 1.5.2 or 1.6.0 this function doesn't work anymore 

Even a simple action in the script like update a integer field fail : 

```
{"document_id":"2754","script":"ctx._source.country = 76","action":"update","retry_on_conflict":3}
```

error log : 

```
    failedactionwithresponseof400,
droppingaction: [
    "update",
    {
        : _id=&gt;"21-2-1",
        : _index=&gt;"drilling_m02_2014_test",
        : _type=&gt;"detailed",
        : _routing=&gt;nil,
        : _script=&gt;" if (ctx._source.edt == null) { ctx._source.edt = eventData; ctx._source.vdu = []; ctx._source.pdt = []; } else { ctx._source.edt += eventData; } ",
        : _scriptParams=&gt;{
            "label"=&gt;"eventData",
            "params"=&gt;&lt;Java: : JavaUtil: : ArrayList: -610349907[
                {
                    "filter"=&gt;"city",
                    "value"=&gt;"New York",
                    "vid"=&gt;156442965,
                    "eid"=&gt;16928
                },
                {
                    "filter"=&gt;"image_id",
                    "value"=&gt;"34eerts7",
                    "vid"=&gt;156442965,
                    "eid"=&gt;16928
                }
            ]&gt;
        },
        : _retry_on_conflict=&gt;5
    },
    #&lt;LogStash: : Event: 0x3a60323f@metadata={
        "retry_count"=&gt;0
    },
    @accessors=#&lt;LogStash: : Util: : Accessors: 0x38494d4c@store={
        "action"=&gt;"update",
        "document_id"=&gt;"21-2-1",
        "index_date"=&gt;"drilling_m02_2014_test",
        "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
    },
    @lut={
        "@version"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "@version"
        ],
        "index_date"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "index_date"
        ],
        "document_id"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "document_id"
        ],
        "action"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "action"
        ],
        "script"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "script"
        ],
        "scriptParams"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "scriptParams"
        ],
        "retry_on_conflict"=&gt;[
            {
                "action"=&gt;"update",
                "document_id"=&gt;"21-2-1",
                "index_date"=&gt;"drilling_m02_2014_test",
                "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
            },
            "retry_on_conflict"
        ]
    }&gt;,
    @data={
        "action"=&gt;"update",
        "document_id"=&gt;"21-2-1",
        "index_date"=&gt;"drilling_m02_2014_test",
        "@timestamp"=&gt;"2015-07-13T09:58:13.297Z"
    },
    @metadata_accessors=#&lt;LogStash: : Util: : Accessors: 0x3dc05ec4@store={
        "retry_count"=&gt;0
    },
    @lut={

    }&gt;,
    @cancelled=false&gt;
]{
    : level=&gt;: warn
}
```
</description><key id="94731077">12205</key><summary>UpdateRequest after ES 1.5.0 doesn’t work function script()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CamiloSierraH</reporter><labels /><created>2015-07-13T14:16:30Z</created><updated>2015-07-13T16:05:13Z</updated><resolved>2015-07-13T14:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-13T14:21:11Z" id="120944414">You haven't included the actual error that Elasticsearch would return, but I'm guessing the problem comes either from the changes that the script API have undergone (we've made more changes subsequently, this has finally stabilised in master), or the fact that dynamic groovy scripts are disabled by default.

Check your Elasticsearch logs - the answer will be in there.
</comment><comment author="CamiloSierraH" created="2015-07-13T15:09:31Z" id="120962030">hello @clintongormley i start my elasticsearch with the command bin/elasticsearch and the only logs that i have when execute the unit test is : (i already tested in my local install and pre-prod install)

[2015-07-13 17:07:32,254][INFO ][cluster.service          ] [William Baker] added {[logstash-debian-6334-11622][Ho8P9-6qRtmUPDNhr-lpyw][debian][inet[/10.0.0.71:9302]]{client=true, data=false},}, reason: zen-disco-receive(from master [[Equinox][yeH-4ZFaRuSTEnSv89a5tg][debian][inet[/10.0.0.71:9300]]])
[2015-07-13 17:07:32,435][DEBUG][discovery.zen.publish    ] [William Baker] received cluster state version 57
</comment><comment author="clintongormley" created="2015-07-13T16:05:13Z" id="120981093">just checked and you're right - this exception isn't logged with the default settings.  It looks like the exception is being swallowed by logstash.

However, if you try to execute the same command against elasticsearch directly, you'll probably see the following exception: "ElasticsearchIllegalArgumentException[failed to execute script]; nested: ScriptException[dynamic scripting for [groovy] disabled]; "

So you either need to : 
- use the Lucene expressions scripting language 
- save your scripts as files in the config directory on each node
- enable dynamic scripting for groovy (and make sure that Elasticsearch is not accessible by anybody else)

Best place to find out more about these options is in the docs, or in the forum: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PrefixQueryParser takes a String as value like its Builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12204</link><project id="" key="" /><description>Relates #12032
</description><key id="94713587">12204</key><summary>PrefixQueryParser takes a String as value like its Builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T12:48:16Z</created><updated>2015-08-13T14:05:41Z</updated><resolved>2015-07-16T16:25:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-07-15T10:49:24Z" id="121574104">@jpountz can you please have a look at this?
</comment><comment author="jpountz" created="2015-07-15T22:36:43Z" id="121768473">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file></files><comments><comment>PrefixQueryParser takes a String as value like its Builder</comment></comments></commit></commits></item><item><title>Throw LockObtainFailedException exception when we can't lock index directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12203</link><project id="" key="" /><description>Today we throw ElasticsearchException if we can't lock the index. This can cause
problems since some places where we have logic to deal with IOException on shard
deletion won't schedule a retry if we can't lock the index dir for removal. This
is the case on shadow replicas for instance if a shared FS is used. The result
of this is that the delete of an index is never acked.

here is a failure caused by this:

http://build-us-00.elastic.co/job/es_core_16_window-2012/13/consoleFull
</description><key id="94686691">12203</key><summary>Throw LockObtainFailedException exception when we can't lock index directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T10:03:19Z</created><updated>2015-07-14T13:45:18Z</updated><resolved>2015-07-13T15:51:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-13T10:29:01Z" id="120886414">@dakrone can you take a look at this?
</comment><comment author="dakrone" created="2015-07-13T15:34:08Z" id="120970228">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup the data structures used in MetaData class for alias and index lookups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12202</link><project id="" key="" /><description>Major changes:
- Replaced the two maps in MetaData (`aliases` and `aliasAndIndexToIndexMap`) with a single map for alias and index resolving / wildcard expansion (MetaData#aliasAndIndexLookup). This has a positive impact on the used heap memory for building the the alias / index name lookup and the memory ES takes just having this.
- Instead of using a normal map this change use a TreeMap instead. This significantly speeds up wildcard suffix expansions of index/alias names.
- Moved the building of the alias / index lookup to the metadata builder.
</description><key id="94685630">12202</key><summary>Cleanup the data structures used in MetaData class for alias and index lookups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T09:58:58Z</created><updated>2015-07-24T20:20:01Z</updated><resolved>2015-07-24T20:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-22T14:47:50Z" id="123746782">@javanna @jpountz I've updated the PR.
</comment><comment author="javanna" created="2015-07-23T10:03:21Z" id="124039037">I left a few comments, mainly questions for my better understanding of the change...
</comment><comment author="martijnvg" created="2015-07-23T10:38:32Z" id="124052399">@javanna I answered the questions.
</comment><comment author="javanna" created="2015-07-23T12:34:43Z" id="124081760">LGTM besides the minor documentation comment I left, maybe @jpountz wants to have another look too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Performance Issue when using ~ operator with Query String Query - ES 1.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12201</link><project id="" key="" /><description>Hello,

When running a query like:

``` json
POST /my-index/_search
{
  "from" : 0,
  "size" : 5,
  "query" : {
      "query_string" : {
      "fields" : ["field1","field2",...,"fieldn"],
      "query" : "flight~",
      "use_dis_max" : true
    }
  },
  "highlight" : {
    "pre_tags" : [ "&lt;span class=\"mark\"&gt;" ],
    "post_tags" : [ "&lt;/span&gt;" ],
    "order" : "score",
    "encoder" : "html",
    "require_field_match" : true,
    "fields" : {    
      "*" : {}
    }
  }
}
```

query_string contains n fields (in my case about 40 fields), and query contains ~ operator (in the example flight~), takes about 6 seconds.
If I run the same query (flight~), but remove the fields section from query_string section, it takes 10 milliseconds (no time).

Why does it happen?
Is there a solution for this performance penalty (I have to determine specific fields into _query_string_ in order to solve highlight issue, see below)?

**Background:**
Using elasticsearch 1.6.
Run it on index that contains 1000 documents.
Determine specific fields into _query_string_ in order to solve follow highlight issue:
https://discuss.elastic.co/t/highlight-when-query-contains-fields-and-text-es-1-6-0/24311/5

Thanks,
Effi
</description><key id="94665875">12201</key><summary>Performance Issue when using ~ operator with Query String Query - ES 1.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">effoeffi</reporter><labels /><created>2015-07-13T08:29:16Z</created><updated>2015-07-13T08:44:50Z</updated><resolved>2015-07-13T08:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-13T08:44:50Z" id="120852262">Please keep questions on the discuss forum, you already have a post for this question on there (https://discuss.elastic.co/t/query-string-query-performance-issue-when-using-operator-es-1-6/25126), the Github issues list is used for bug tracking only. Also on your post on discuss you indicated that your issue was solved
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RegexpQueryParser takes a String as value like its Builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12200</link><project id="" key="" /><description>Relates #11896 
</description><key id="94665036">12200</key><summary>RegexpQueryParser takes a String as value like its Builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T08:23:49Z</created><updated>2015-08-13T14:00:27Z</updated><resolved>2015-07-16T16:09:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-13T08:25:57Z" id="120846980">don't we want to change the mappers as well to accept a String instead of an Object?
</comment><comment author="alexksikes" created="2015-07-13T09:46:09Z" id="120875042">@javanna Thanks for the review. I changed the mapper as well.
</comment><comment author="javanna" created="2015-07-13T11:43:11Z" id="120899255">looks good, @jpountz can you please have a look too? I had a question above ;)
</comment><comment author="jpountz" created="2015-07-15T22:37:44Z" id="121768623">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file></files><comments><comment>RegexpQueryParser takes a String as value like its Builder</comment></comments></commit></commits></item><item><title>Only clear open search ctx if the index is delete or closed via API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12199</link><project id="" key="" /><description>A change in #12116 introduces closing / cleaning of search ctx even if
the index service was closed due to a relocation of it's last shard. This
is not desired since in that case it's fine to serve the pending requests from
the relocated shard. This commit adds an extra check to ensure that the index is
either removed (delete) or closed via API.

there where some failures related to this on CI here:

http://build-us-00.elastic.co/job/es_core_17_centos/50/
</description><key id="94664833">12199</key><summary>Only clear open search ctx if the index is delete or closed via API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-13T08:22:40Z</created><updated>2015-07-13T11:35:13Z</updated><resolved>2015-07-13T09:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-13T08:42:31Z" id="120851881">@martijnvg can you take a look at this 
</comment><comment author="martijnvg" created="2015-07-13T09:33:36Z" id="120870820">@s1monw that was a sneaky failure. checking the MetaData to see if the index got closed via close api is the right way imo. LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can some let know about the use of ~ &amp; ^ in below syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12198</link><project id="" key="" /><description>TITLE:"same old song and dance"^100 
 OR (
 TITLE:same~0.8 AND TITLE:old~0.8 AND TITLE:song~0.8 AND TITLE:and~0.8 AND TITLE:dance~0.8
 )^50 

Please explain about ^ &amp; ~ in above query.
</description><key id="94633155">12198</key><summary>Can some let know about the use of ~ &amp; ^ in below syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vaniaravinda</reporter><labels /><created>2015-07-13T03:55:42Z</created><updated>2015-07-13T11:32:25Z</updated><resolved>2015-07-13T04:16:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-13T04:16:15Z" id="120805376">Please ask your questions on discuss.elastic.co.

We can help there.
</comment><comment author="clintongormley" created="2015-07-13T11:32:25Z" id="120897637">And see the docs here: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Serial Diff aggregator problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12197</link><project id="" key="" /><description>Hello!

I've been experimenting with the new [`serial_diff`](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline-serialdiff-aggregation.html) aggregator in master branch and get an error. Is this an error, or am I doing it wrong?

``` sh
#!/bin/sh
curl -s -XPUT 'localhost:9200/test-es20' -d '{
  "mappings": {
    "order": {
      "properties": {
        "article": { "type": "string" },
        "price": { "type": "float" },
                "time": { "type":"date", "format": "YYYY-MM-dd" }
      }
    }
  }
}'

# Add some documents
curl -s -XPUT 'localhost:9200/test-es20/order/1' -d '{"article":"MacBook","price":1200,"time":"2015-01-02"}'
curl -s -XPUT 'localhost:9200/test-es20/order/2' -d '{"article":"Dell XPS","price":1600,"time":"2015-01-08"}'
curl -s -XPUT 'localhost:9200/test-es20/order/3' -d '{"article":"iPad","price":600,"time":"2015-02-01"}'

# Serial diff
curl -s -XGET 'localhost:9200/test-es20/_search?pretty' -d '{
  "aggregations": {
    "sales_per_month": {
      "aggregations": {
        "sales": {
          "sum": {
            "field": "price"
          }
        },
        "the_diff": {
          "serial_diff": {
            "buckets_path": "sales",
            "lag": 1
          }
        }
      },
      "date_histogram": {
        "field": "time",
        "interval": "month"
      }
    }
  },
  "query": {
    "match_all": {}
  }
}'
```

Result:

``` json
{
  "error" : {
    "root_cause" : [ {
      "type" : "search_parse_exception",
      "reason" : "Could not find aggregator type [serial_diff] in [the_diff]",
      "line" : 11,
      "col" : 11
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "test-es20",
      "node" : "SvAbVOMMSp23QbSCyHFGQA",
      "reason" : {
        "type" : "search_parse_exception",
        "reason" : "Could not find aggregator type [serial_diff] in [the_diff]",
        "line" : 11,
        "col" : 11
      }
    } ]
  },
  "status" : 400
}
```
</description><key id="94564603">12197</key><summary>Serial Diff aggregator problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">olivere</reporter><labels /><created>2015-07-12T13:13:42Z</created><updated>2015-07-13T09:30:45Z</updated><resolved>2015-07-13T08:06:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-13T07:12:49Z" id="120834712">Your script works for me on the latest master. Is it possible you weren't on the latest master when you tried this? The `serial_diff` aggregation was only merged to master 2-3 days ago.
</comment><comment author="olivere" created="2015-07-13T08:06:11Z" id="120843897">@colings86 Ha! Works fine now. I was inspecting the Java source for my Go client and `SerialDiffBuilder` was already there, so I thought that it might not be glued together appropriately. ;-)  Thanks!
</comment><comment author="colings86" created="2015-07-13T08:07:33Z" id="120844071">@olivere no worries. By the way, I really like your issue descriptions, putting a reproduction script in all your issues is super helpful. Thanks.
</comment><comment author="olivere" created="2015-07-13T08:09:57Z" id="120844363">@colings86 You guys should really have something like `play.elastic.co` (like http://play.golang.org/). It would make sharing scripts even easier :-)
</comment><comment author="colings86" created="2015-07-13T09:24:19Z" id="120867351">@olivere you can use https://found.no/play to create these kinds of scripts, there is an export feature that will produce a shell script for you to be able to run. The Found team are looking to keep developing this to add features.

There is also the Sense tool in Marvel which lets you create scripts of commands in a easily readable format.
</comment><comment author="olivere" created="2015-07-13T09:30:45Z" id="120869741">@colings86 Oh, didn't know about https://found.no/play. Exactly what I had in mind. Thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch (2.0 master) not starting after os restart.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12196</link><project id="" key="" /><description>OS: Mac os 10.10.4
Java: 1.7.0_07-b10

It was running fine till the os restart. After that this log is thrown whenever i try to start elasticsearch.

```
{2.0.0-SNAPSHOT}: Initialization Failed ...
- IncompatibleClassChangeError[Found class org.elasticsearch.common.settings.Settings, but interface was expected]
Exception in thread "main" java.lang.SecurityException: exit(3) not allowed by system policy
    at org.elasticsearch.bootstrap.Security$1.checkExit(Security.java:67)
    at java.lang.Runtime.exit(Runtime.java:105)
    at java.lang.System.exit(System.java:960)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:304)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:28)
```
</description><key id="94561560">12196</key><summary>Elasticsearch (2.0 master) not starting after os restart.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msathis</reporter><labels><label>feedback_needed</label></labels><created>2015-07-12T12:24:24Z</created><updated>2015-07-13T14:40:12Z</updated><resolved>2015-07-13T14:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-13T09:56:40Z" id="120878195">Sounds like you need to do a mvn clean before rebuilding.  
</comment><comment author="msathis" created="2015-07-13T11:04:28Z" id="120891381">@clintongormley I copied the master snapshot zip to different location and was running from there. Not directly from checkout code. The only suspicious thing was, while elasticsearch was running, the os crashed and rebooted. After that i couldn't start elasticsearch at all. I will try reproducing and post you the zip if i am able to.
</comment><comment author="clintongormley" created="2015-07-13T11:16:25Z" id="120893542">It sounds like you have different versions in the classpath.
</comment><comment author="s1monw" created="2015-07-13T13:21:57Z" id="120925005">do you have a plugin installed that is outdated? can you list the content of your plugins directory?
</comment><comment author="msathis" created="2015-07-13T13:33:23Z" id="120927695">@s1monw I had marvel installed at that point of time. No other plugins.
</comment><comment author="clintongormley" created="2015-07-13T13:40:56Z" id="120931576">marvel is not master compatible
</comment><comment author="s1monw" created="2015-07-13T13:40:59Z" id="120931598">@msathis I guess that's the reason did you get a marvel snapshot build somehow for this? can you remove marvel and try again
</comment><comment author="msathis" created="2015-07-13T14:33:45Z" id="120950234">@s1monw Yep. That is the issue. After removing marvel it works fine. Sorry for wasting your time. Please close the issue. :(
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Term Query: Be more strict during parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12195</link><project id="" key="" /><description>The term query parser was too lenient during parsing and allowed to specify
more than one field, even though this expected to filter only for a single field.

This commit returns an exception if a query has been specified more than once.

Closes #12184
</description><key id="94543069">12195</key><summary>Term Query: Be more strict during parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-12T07:16:20Z</created><updated>2015-07-13T08:03:39Z</updated><resolved>2015-07-13T08:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-13T07:17:05Z" id="120835261">LGTM
</comment><comment author="javanna" created="2015-07-13T07:54:46Z" id="120841827">LGTM besides a minor comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>jsr166e was left out of shaded jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12194</link><project id="" key="" /><description>The classes in com.twitter.jsr166e were not getting included in the
shaded jar due to a missing configuration line.

Closes #12193
</description><key id="94512020">12194</key><summary>jsr166e was left out of shaded jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Packaging</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-11T22:15:59Z</created><updated>2015-07-13T07:19:49Z</updated><resolved>2015-07-12T23:49:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-11T22:19:12Z" id="120665764">LGTM
</comment><comment author="s1monw" created="2015-07-13T07:19:49Z" id="120835666">thanks @aleph-zero 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12194 from aleph-zero/fix/12193</comment></comments></commit></commits></item><item><title>NoClassDefFoundError w/shaded 2.0.0.beta1 jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12193</link><project id="" key="" /><description>I am consistently getting the following stack trace when attempting to create a node from a simple application:

```
--- Start Example ---
Jul 11, 2015 1:37:24 PM org.elasticsearch.node.Node &lt;init&gt;
INFO: [The Blank] version[2.0.0.beta1-SNAPSHOT], pid[18006], build[0b27ded/2015-07-11T20:22:54Z]
Jul 11, 2015 1:37:24 PM org.elasticsearch.node.Node &lt;init&gt;
INFO: [The Blank] initializing ...
Jul 11, 2015 1:37:24 PM org.elasticsearch.plugins.PluginsService &lt;init&gt;
INFO: [The Blank] loaded [], sites []
Jul 11, 2015 1:37:24 PM org.elasticsearch.env.NodeEnvironment maybeLogPathDetails
INFO: [The Blank] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [79.6gb], net total_space [464.7gb], spins? [unknown], types [hfs]
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.337 sec &lt;&lt;&lt; FAILURE!
x(org.primer.stupid.TestFoo)  Time elapsed: 0.301 sec  &lt;&lt;&lt; ERROR!
java.lang.NoClassDefFoundError: org/elasticsearch/common/util/concurrent/jsr166e/LongAdder
    at org.elasticsearch.common.metrics.CounterMetric.&lt;init&gt;(CounterMetric.java:28)
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.&lt;init&gt;(EsAbortPolicy.java:31)
    at org.elasticsearch.common.util.concurrent.EsExecutors.newCached(EsExecutors.java:70)
    at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:339)
    at org.elasticsearch.threadpool.ThreadPool.build(ThreadPool.java:296)
    at org.elasticsearch.threadpool.ThreadPool.&lt;init&gt;(ThreadPool.java:134)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:159)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:157)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:164)
    at org.primer.stupid.TestFoo.x(TestFoo.java:50)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
    at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
    at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.util.concurrent.jsr166e.LongAdder
    at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 39 more
```

Note that this _only_ happens when using the shaded jar. 

A simple test program to reproduce is:

``` java
public class TestFoo {

    @Test
    public void x() throws Exception {

        System.out.println("--- Start Example ---");

        File home = Files.createTempDir();
        home.deleteOnExit();
        Settings settings = Settings.builder()
                .put("path.home", home.getAbsolutePath()).build();
        Node node = nodeBuilder().settings(settings).local(false).data(true).clusterName("test-cluster").node();
        node.close();

        System.out.println("--- Finished ---");
    }
}
```

From what I can tell, the jsr166e classes are not getting shaded to the correct location. If I look at the shaded jar I can see the actual location of the jsr166e classes:

```
[ gnocchi 2.0.0.beta1-SNAPSHOT ] [ 01:42:39 ] &gt; jar tvf elasticsearch-2.0.0.beta1-SNAPSHOT-shaded.jar | grep LongAdder
  3422 Sat Jul 11 13:23:58 PDT 2015 org/elasticsearch/common/cache/LongAdder.class
```

However, the pom.xml for core has this:

``` xml
&lt;relocation&gt;
    &lt;pattern&gt;com.twitter.jsr166e&lt;/pattern&gt;
    &lt;shadedPattern&gt;org.elasticsearch.common.util.concurrent.jsr166e&lt;/shadedPattern&gt;
&lt;/relocation&gt;
```
</description><key id="94506498">12193</key><summary>NoClassDefFoundError w/shaded 2.0.0.beta1 jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-11T20:47:01Z</created><updated>2015-07-12T23:49:01Z</updated><resolved>2015-07-12T23:49:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2015-07-11T22:11:31Z" id="120665474">This was simply a matter of a missing line in the core/pom.xml. Adding `&lt;include&gt;com.twitter:jsr166e&lt;/include&gt;` to the shade configuration fixed it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>jsr166e was left out of shaded jar</comment></comments></commit></commits></item><item><title>Percolator NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12192</link><project id="" key="" /><description>Hi there!

I'm testing the percolator API with the current master. I'm getting a (gracefully handled) null-pointer exception. Here's what I'm trying to do:

``` sh
#!/bin/sh
curl -s -XPUT 'localhost:9200/test-es20' -d '{
  "mappings": {
    "tweet": {
      "properties": {
        "user": { "type": "string" },
        "message": { "type": "string" },
        "created": { "type": "date" }
      }
    },
    "comment": {
      "_parent": { "type": "tweet" },
      "properties": {
        "user": { "type": "string" },
        "message": { "type": "string" }
      }
    }
  }
}'

# Register query in percolator
curl -s -XPUT 'localhost:9200/test-es20/.percolator/1' -d '{
  "query": {
    "match": {
      "message" : "Elasticsearch"
    }
  }
}'

# Match document with percolator
curl -s -XGET 'localhost:9200/test-es20/tweet/_percolate?pretty' -d '{
  "doc": {
    "user": "olivere",
    "message": "Welcome to Elasticsearch"
  }
}'
```

Which yields:

```
{
  "took" : 6,
  "_shards" : {
    "total" : 5,
    "successful" : 0,
    "failed" : 5,
    "failures" : [ {
      "shard" : 0,
      "index" : "test-es20",
      "status" : "BAD_REQUEST",
      "reason" : {
        "type" : "parse_exception",
        "reason" : "failed to parse request",
        "caused_by" : {
          "type" : "mapper_parsing_exception",
          "reason" : "failed to parse [_parent]",
          "caused_by" : {
            "type" : "null_pointer_exception",
            "reason" : null
          }
        }
      }
    } ]
  },
  "total" : 0,
  "matches" : [ ]
}
```

If you remove the `_parent` from the comment mapping, everything is fine.

Is this supposed to work, or is it the same issue mentioned in #2960?
</description><key id="94469551">12192</key><summary>Percolator NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">olivere</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-11T12:32:44Z</created><updated>2015-07-14T08:31:26Z</updated><resolved>2015-07-14T08:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="olivere" created="2015-07-11T12:34:56Z" id="120614027">BTW: Just tested 1.6 and it's working fine (with the `_parent` in the comment type).
</comment><comment author="dadoonet" created="2015-07-11T12:36:21Z" id="120614069">So we can close the issue, right?
</comment><comment author="olivere" created="2015-07-11T12:38:29Z" id="120614122">Ehm, no. The error e.g. occurs with 2.0-BETA1. Works fine with 1.6.
</comment><comment author="dadoonet" created="2015-07-11T13:07:19Z" id="120615315">Ha thanks! I missed you were testing master branch. o_O

&gt; I'm testing the percolator API with the current master.

Sorry :)
</comment><comment author="olivere" created="2015-07-11T13:20:19Z" id="120616112">@dadoonet No problem. And thanks for your time. :-)
</comment><comment author="martijnvg" created="2015-07-13T20:35:02Z" id="121048235">@olivere thanks for testing out master! I've opened #12214 so it should be fixed soon.
</comment><comment author="olivere" created="2015-07-13T21:11:59Z" id="121060963">@martijnvg My pleasure. And thanks for fixing :-)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>percolator: Don't throw NPE when percolating a document that has a _parent field configured in its mapping</comment></comments></commit></commits></item><item><title>error to start elasticsearch as a service with elasticsearch-servicewapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12191</link><project id="" key="" /><description>mac os 10.10.1
➜  ~  java -version
java version "1.7.0_71"
Java(TM) SE Runtime Environment (build 1.7.0_71-b14)
Java HotSpot(TM) 64-Bit Server VM (build 24.71-b01, mixed mode)

error information:
STATUS | wrapper  | 2015/07/11 14:10:52 | Launching a JVM...
ERROR  | wrapper  | 2015/07/11 14:10:52 | JVM exited while loading the application.
INFO   | jvm 1    | 2015/07/11 14:10:52 | Error: This Java instance does not support a 32-bit JVM.
INFO   | jvm 1    | 2015/07/11 14:10:52 | Please install the desired version.
</description><key id="94442797">12191</key><summary>error to start elasticsearch as a service with elasticsearch-servicewapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JasonBian</reporter><labels /><created>2015-07-11T06:14:43Z</created><updated>2015-07-11T06:24:53Z</updated><resolved>2015-07-11T06:24:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-11T06:24:52Z" id="120585382">Please use the mailing list on discuss.elastic.co

The service wrapper is deprecated and not maintained, many of the features have been added to Elastisearch itself, and purpose built deb, rpm packages, as well as windows service, are avaialble.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>enforce plugins are installed with correct name, and install correctly in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12190</link><project id="" key="" /><description>Integration tests install plugins with outdated syntax, so when they run they install kuromoji like this:

```
plugins/2.0.0.beta1-SNAPSHOT
```

Instead we should install the correct way, and check that the result is what we expect:

```
plugins/analysis-kuromoji
```
</description><key id="94412120">12190</key><summary>enforce plugins are installed with correct name, and install correctly in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-10T23:22:55Z</created><updated>2015-07-11T05:12:03Z</updated><resolved>2015-07-11T05:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-11T00:42:06Z" id="120556693">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12190 from rmuir/enforce_plugin_name</comment></comments></commit></commits></item><item><title>Defective deb package for 1.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12189</link><project id="" key="" /><description>Nice deb package:

 # apt-get install elasticsearch
Reading package lists... Done
Building dependency tree  
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.
Need to get 0 B/27.2 MB of archives.
After this operation, 31.2 MB of additional disk space will be used.
dpkg: unrecoverable fatal error, aborting:
 unknown user 'elasticsearch' in statoverride file
E: Sub-process /usr/bin/dpkg returned an error code (2)

ls -altr /var/cache/apt/archives/ | grep -i elast
-rw-r--r-- 1 root root 27225316 Jun  9 13:51 elasticsearch_1.6.0_all.deb
</description><key id="94412060">12189</key><summary>Defective deb package for 1.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mabushey</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2015-07-10T23:22:15Z</created><updated>2016-09-04T07:12:34Z</updated><resolved>2015-07-14T13:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-14T13:25:34Z" id="121236920">Hi @mabushey 

We're unable to replicate this.  Any more info you can provide? What version of debian or ubuntu are you using?
</comment><comment author="clintongormley" created="2015-07-14T13:26:02Z" id="121236993">Have you customised something? Not sure what the statoverride line is about?
</comment><comment author="nik9000" created="2015-07-14T13:45:18Z" id="121241538">&gt; Have you customised something? Not sure what the statoverride line is about?

Looks like it as something to do shroot or virtualization. The deb package is fine if you aren't in whatever situation is triggering this. We've used the 1.6.0 version dozens of times by now and never seen this.
</comment><comment author="clintongormley" created="2015-07-14T13:49:56Z" id="121242533">thanks @nik9000 - closing as it sounds like a local issue
</comment><comment author="GlenRSmith" created="2015-07-14T14:07:16Z" id="121248655">'statoverride' is /var/lib/dpkg/statoverride.
dpkg (and apt) will throw this error when 
1. a user is created by a package installer
2. the user is deleted via the CLI
3. another dpkg (or apt) install/update is run

The second step removes the user from /etc/password, but not from /var/lib/dpkg/statoverride.
dpkg doesn't like that arrangement.
One fix seems to be doing an apt-get purge on the package that created the user.
</comment><comment author="spinscale" created="2015-07-21T09:18:20Z" id="123230976">@GlenRSmith thx for the heads up! Is there anything we can do inside of our package to improve this use-case?
</comment><comment author="miltonh26" created="2016-09-04T07:12:34Z" id="244587960">I know this issue has been closed, but here is a solution that worked for me: 
$ sudo sed -i '/elasticsearch/d' /var/lib/dpkg/statoverride
Reference: http://askubuntu.com/questions/506043/how-do-i-remove-a-user-from-the-dpkg-statoverride-file
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Proposal: Streaming results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12188</link><project id="" key="" /><description>Maybe this is a crazy idea but I've spent some time working with another system ([blazegraph](http://www.blazegraph.com/bigdata)) that supports sending large result sets over its API using HTTP 1.1's chunked encoding. The results stream back to the client and the client can close the tcp connection when it has enough results and the server stops producing results. I was wondering if it might make sense to do something similar to Elasticsearch. The advantage it'd have over scan/scroll is that its simpler to reason about when server side resources are in use - only as long as the tcp connection is open to the client.

I don't know enough about the overhead of scan/scroll to know if its worth doing. It doesn't solve the infinite scroll problem either - for that you need an efficient way for clients to poll deeply and this just isn't it.
</description><key id="94378741">12188</key><summary>Proposal: Streaming results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-07-10T19:47:36Z</created><updated>2016-04-12T15:48:01Z</updated><resolved>2016-01-18T21:37:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2015-07-10T22:16:04Z" id="120540583">Not sure how blocking chunked transfer can solve challenges like back pressure, but it should be possible to write RxJava https://github.com/ReactiveX/RxJava based code that implements reactive streams http://www.reactive-streams.org/ similar to http://mongodb.github.io/mongo-java-driver-reactivestreams/
This would be more easier if Observer pattern for actions and Java 8 lambdas could be used. Example of what can be done is "HTTP tail" for JVM https://github.com/myfreeweb/rxjava-http-tail
</comment><comment author="nik9000" created="2015-07-20T13:36:44Z" id="122886413">&gt; Not sure how blocking chunked transfer can solve challenges like back pressure

It _kind of_ can.  There isn't anything that I know of like the triple-ack of tcp but there are buffers and you could in theory check how full they are and only try to fill them when they get below a certain point.
</comment><comment author="clintongormley" created="2016-01-18T20:57:08Z" id="172651816">@nik9000 is this still something you want to investigate?
</comment><comment author="nik9000" created="2016-01-18T21:37:10Z" id="172660779">&gt; @nik9000 is this still something you want to investigate?

I think its a neat idea and might be useful for something someday but it just doesn't have the crazy +1 train that some other proposals have accumulated. I'm going to close it. Maybe someone can revive it when they have some super awesome use case.

Honestly the flip side might be more useful: implement bulk indexing using chunked uploads. That has really simple back pressure on the uploading thread and would be simpler to implement. @mikemccand and I talked about it many months ago. The neat thing about it is that Elasticsearch can better manage its memory if the user is uploading using chunks - they can continue sending chunks until they want to make sure the translog has fsynced - then they send the last chunk and we consider the bulk request complete and run the fsync. Rather than having to load the whole bulk request we get to rely on tcp's back pressure to slow the client down so we can have as much of the bulk request "in flight" as we think is appropriate.

Its a neat idea but I dunno if its actually worth implementing.
</comment><comment author="mikemccand" created="2016-01-18T23:33:41Z" id="172681576">&gt; implement bulk indexing using chunked uploads.

+1

&gt; The neat thing about it is that Elasticsearch can better manage its memory if the user is uploading using chunks

Manage its memory and also manage appropriate concurrency to bring to bear.  Plus the client gets much simpler, not having to play games with proper item count per bulk request, how many client threads to use, dealing w/ rejected exceptions, etc.

@HonzaKral recently added some nice sugar to the ES python client APIs that does some of this for the user, so the user feels like they're using a single streaming bulk indexing API, and under the hood the Python ES client breaks it into chunks using N threads ...
</comment><comment author="Bargs" created="2016-04-12T15:45:30Z" id="208970272">+1 on chunked uploads. This would be a huge benefit to the CSV upload functionality I'm building into Kibana. Right now I have to make educated guesses about what bulk size will be the best for the largest number of users, and it just won't be a good experience for some people. If ES supported chunked uploads the entire thing could be implemented as one big stream from the user's browser, to Kibana's node backend, to ES and back.

(https://github.com/elastic/kibana/issues/6541 and https://github.com/elastic/kibana/pull/6844)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `_top/searches` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12187</link><project id="" key="" /><description>Admins faced with busy nodes have no way of knowing what bad queries users are sending to their cluster.  The `_top/searches` API should provide a list of all currently executing queries, how long they have been executing, and the ability to kill a query (where possible).

This could be implemented as follows:
- a coordinating node adds an ID to each search, and keeps the search request in some data structure until it is complete
- a `GET _top/search` request will reach out to all nodes to retrieve currently running requests, their elapsed execution time, and which nodes they are running on
- `POST _top/search/_kill/[searchid]` will cause the coordinating node to update the timeout for the request to 0, killing the request as soon as possible (if possible)

NOTE: a script like `while (1) {...}` is not killable without restarting affected nodes.  We can't use thread interrupts because they are buggy.

Inspired by #4329 
</description><key id="94336242">12187</key><summary>Add `_top/searches` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>discuss</label><label>feature</label></labels><created>2015-07-10T16:03:41Z</created><updated>2016-09-23T15:09:15Z</updated><resolved>2016-09-23T15:09:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-10T16:43:30Z" id="120455600">It's a fine proposal I'd love to try my hand at!  Unless someone else wants
it. I won't be able to be 100% on it for a few weeks and don't want to
cookie lick it but I do care deeply about the issue.
On Jul 10, 2015 12:03 PM, "Clinton Gormley" notifications@github.com
wrote:

&gt; Admins faced with busy nodes have no way of knowing what bad queries users
&gt; are sending to their cluster. The _top/searches API should provide a list
&gt; of all currently executing queries, how long they have been executing, and
&gt; the ability to kill a query (where possible).
&gt; 
&gt; This could be implemented as follows:
&gt; - a coordinating node adds an ID to each search, and keeps the search
&gt;   request in some data structure until it is complete
&gt; - a GET _top/search request will reach out to all nodes to retrieve
&gt;   currently running requests, their elapsed execution time, and which nodes
&gt;   they are running on
&gt; - POST _top/search/_kill/[searchid] will cause the coordinating node
&gt;   to update the timeout for the request to 0, killing the request as soon as
&gt;   possible (if possible)
&gt; 
&gt; NOTE: a script like while (1) {...} is not killable without restarting
&gt; affected nodes. We can't use thread interrupts because they are buggy.
&gt; 
&gt; Inspired by #4329 https://github.com/elastic/elasticsearch/issues/4329
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12187.
</comment><comment author="jtharpla" created="2015-07-20T07:11:22Z" id="122783257">This sounds very much like MongoDB's db.currentOp() and db.killOp(), both of which are immensely useful in the MongoDB world.  Can't wait to see this functionality in Elasticsearch as well.
</comment><comment author="eskibars" created="2015-09-15T20:16:10Z" id="140524694">From a HTTP verb perspective, does it make sense to use DELETE rather than POST to kill the query?
</comment><comment author="srikanthbirada" created="2015-09-25T07:27:54Z" id="143148809">Can we expect this feature for the coming elasticsearch-2.0 version ?
</comment><comment author="clintongormley" created="2016-01-18T20:55:09Z" id="172651030">It looks like the task management API will fulfill this need. Closing in favour of https://github.com/elastic/elasticsearch/issues/15117
</comment><comment author="nik9000" created="2016-01-18T22:16:45Z" id="172668737">It might be useful to keep this around as an explicit use case for task management. It may be we get all these "for free" using stuff built into task management but it'd be nice to talk about this feature explicitly in docs, maybe have tests for it, etc.
</comment><comment author="pickypg" created="2016-03-25T20:32:20Z" id="201483436">Reopening based on @nik9000's comment.
</comment><comment author="ppf2" created="2016-03-25T20:34:59Z" id="201484784">+1 This is a fairly common request so it will be nice to track it separately, or close it once we have confirmed that it has been released with the task management api. thx!
</comment><comment author="evanv" created="2016-09-22T15:31:58Z" id="248938796">Being able to list running queries would be helpful; being able to kill bad ones if needed would be really helpful. 

&gt; Admins faced with busy nodes have no way of knowing what bad queries users are sending to their cluster.

The key thing that stands out to me about this statement is that it has to do with logging queries at the cluster level. The slow query log doesn't do this. Without a view of queries being sent to the cluster, it's impossible to do any kind of reasoning about where you can get the most bang for your buck helping devs rewrite their queries and/or revising schemas to support the queries they need to run. 

If the `_top/searches` API also had the option to log queries to ES, you could also start to do something like Percona does for MySQL with their Percona Query Digest tool (xref https://github.com/elastic/elasticsearch/issues/9172). 

IMO, adding a  `_top/searches` API (or perhaps an `active/_searches` API would make more sense, naming wise?) would be a godsend. If we can log those searches to Elastic when they complete, that would be ever better and would remove a tremendous blindspot that ops people have when trying to help devs reason about their queries and schemas. I really really really like what @clintongormley is suggesting here. 
</comment><comment author="clintongormley" created="2016-09-23T15:09:14Z" id="249218547">This feature is being implemented as part of the task management API here https://github.com/elastic/elasticsearch/pull/20405

&gt; The key thing that stands out to me about this statement is that it has to do with logging queries at the cluster level. The slow query log doesn't do this. 

Agreed - this will be tackled separately.

Closing in favour of #20405
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Timestamp isn't generated with mapping file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12186</link><project id="" key="" /><description>Hi,

I'm trying to create a timestamp thanks to the mapping.
It seems not working. I will try to explain what I did:

From a fresh ElasticSearch 1.6.0 on Windows, I created a file for the mapping:
_config\mappings\_default\tweet.json_

```
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "path" : "post_date"
        }
    }
}
```

From my understanding, at the first document creation in tweet type, ES will create a timestamp date inside the field post_date.
So let's try to put something in tweet:

```
curl -XPOST http://localhost:9200/foo/tweet?pretty -d '
{
    "Message": "Hello"
}'
```

And the result is:

```
{
  "_index" : "foo",
  "_type" : "tweet",
  "_id" : "AU54jUpV_Q4fyYYVxweD",
  "_version" : 1,
  "created" : true
}
```

On ES, no error:  _[Moonstone] [foo] update_mapping [tweet](dynamic)_

Let's check the mapping for tweet:
curl -XGET 'http://localhost:9200/foo/_mapping/tweet' 

```
{"foo":{"mappings":{"tweet":{"_timestamp":{"enabled":true,"path":"post_date"},"properties":{"Message":{"type":"string"}}}}}}
```

So it seems the timestamp is enabled for tweet type.

I will GET the first document:
curl -XGET http://localhost:9200/foo/tweet/AU54jUpV_Q4fyYYVxweD?pretty

```
{
  "_index" : "foo",
  "_type" : "tweet",
  "_id" : "AU54jUpV_Q4fyYYVxweD",
  "_version" : 1,
  "found" : true,
  "_source":
{
"Message": "Hello"
}
}
```

So finally, the field 'post_date' isn't generated. Maybe it's my bad and I did something wrong, but I've got no clue what.

Yohann 
</description><key id="94330642">12186</key><summary>Timestamp isn't generated with mapping file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ymartel06</reporter><labels /><created>2015-07-10T15:35:36Z</created><updated>2015-07-10T16:08:42Z</updated><resolved>2015-07-10T16:08:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-10T16:08:40Z" id="120446501">`_source` is never modified by elasticsearch. It does not mean that your `_timestamp` does not exist. Just that you don't see it in the source you provided.

But you'd better ask this on discuss.elastic.co.

Closing as I don't think it's an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date parsing (now) bug when filtering in addition to percolation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12185</link><project id="" key="" /><description>This seems similar (albeit different) to #12179: there is an issue with "now" and percolator queries: when .percolator documents are filtered in addition to percolation. 

``` json
POST /idx/type/_percolate
{
  "doc": {},
  "filter": {
    "range": {
      "$context.created": {
        "lte": "now"
      }
    }
  }
}
```

for .percolator:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "match_all": []
      }
    }
  },
  "$context": {
    "name": "test",
    "created": "2015-07-10T14:41:54+0000",
  }
}
```

The response is:

```
BroadcastShardOperationFailedException[[idx][0] ]; 
 nested: PercolateException[failed to percolate];  
 nested: ElasticsearchParseException[failed to parse request]; 
 nested: ElasticsearchParseException[Could not read the current timestamp];
 nested: UnsupportedOperationException;
```
</description><key id="94326020">12185</key><summary>Date parsing (now) bug when filtering in addition to percolation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">zergin</reporter><labels><label>:Percolator</label><label>bug</label></labels><created>2015-07-10T15:12:22Z</created><updated>2015-07-14T14:22:35Z</updated><resolved>2015-07-14T12:35:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-14T07:47:10Z" id="121155664">@zergin Thanks for reporting this. There is a PR open for the missing support of resolving `now` in the percolate api: #12215
</comment><comment author="zergin" created="2015-07-14T10:18:57Z" id="121192591">Splendid, thanks. Any ETA for this going live? I see it's scheduled for 1.6.1?
</comment><comment author="martijnvg" created="2015-07-14T10:22:37Z" id="121193191">when the PR gets reviewed it will get merged and then I'll try to back port
it to 1.7 and 1.6 branches. So it should get into the 1.6.1 release.

On 14 July 2015 at 12:19, zergin notifications@github.com wrote:

&gt; Splendid, thanks. Any ETA for this going live? I see it's scheduled for
&gt; 1.6.1?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12185#issuecomment-121192591
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="martijnvg" created="2015-07-14T14:22:35Z" id="121252072">@zergin The change got backported to 1.6, so it is scheduled for 1.6.1 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>percolator: Support filtering percolator queries by date using `now`</comment></comments></commit></commits></item><item><title>Term filter parser too lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12184</link><project id="" key="" /><description>The term filter parser does allow specifying more than one field without barfing and throwing an exception, resulting in a last one wins scenario.

Sample to reproduce

```
GET /foo/bar/_search
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "term": {
          "field2": "bar",
          "field1": "foo"
        }
      }
    }
  }
}
```

This should throw an exception, once `field2` has been found and is tried to be overwritten.
</description><key id="94313451">12184</key><summary>Term filter parser too lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Search</label><label>low hanging fruit</label></labels><created>2015-07-10T14:09:55Z</created><updated>2015-07-13T08:03:24Z</updated><resolved>2015-07-13T08:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T17:18:46Z" id="120464013">@javanna is this still relevant with the query refactoring branch?
</comment><comment author="javanna" created="2015-07-10T17:44:47Z" id="120475619">yes it is still relevant, it's a fix that could be applied on master independently
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Term Query: Be more strict during parsing</comment></comments></commit></commits></item><item><title>Fix documentation typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12183</link><project id="" key="" /><description /><key id="94308262">12183</key><summary>Fix documentation typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erichard</reporter><labels><label>docs</label></labels><created>2015-07-10T13:32:59Z</created><updated>2015-07-10T17:15:27Z</updated><resolved>2015-07-10T17:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T17:15:26Z" id="120463401">thanks @erichard 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12183 from erichard/patch-1</comment></comments></commit></commits></item><item><title>Query refactoring: SpanMultiTermQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12182</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217
</description><key id="94305462">12182</key><summary>Query refactoring: SpanMultiTermQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-10T13:17:39Z</created><updated>2016-03-11T11:51:11Z</updated><resolved>2015-07-22T08:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-10T13:26:23Z" id="120410654">left a minor comment, LGTM though
</comment><comment author="cbuescher" created="2015-07-10T14:08:20Z" id="120417923">Thanks, I still wonder if it makes sense at all to allow Date/Numeric range queries inside a SpanMultiTerm query or if we should simply not allow this case (see https://github.com/elastic/elasticsearch/pull/12123#issuecomment-120416155). In this case I would rather change the test slightly here than have this rewrite.
</comment><comment author="cbuescher" created="2015-07-10T15:03:35Z" id="120430767">@javanna removed the check for the LateParsingQuery case, Date Ranges will (and should) not work inside this type of query, so cast will not work in this case. Only problem now is we don't detect this until we generate the lucene query, since we need to check the field mapping if it is a Range query (possibly also for other nested MultiTermQueries like Fuzzy/Regex/... where field doesn't have positions). I don't know if we can do this already in the parser (coordinating node) later, for now I changed the test setup so it only uses String ranges for the nested queries. 
</comment><comment author="cbuescher" created="2015-07-10T15:41:50Z" id="120439275">Added type check for inner query and test that we throw UnsupportedOperationException instead of ClassCastException when trying to use nested query that doesn't resolve to MultiTermQuery.
</comment><comment author="cbuescher" created="2015-07-20T13:37:46Z" id="122886580">Adressed the last comment regarding error message and rebased on master.
</comment><comment author="cbuescher" created="2015-07-21T14:20:41Z" id="123330854">Added changes considering last round of comments.
</comment><comment author="cbuescher" created="2015-07-21T15:14:17Z" id="123365958">Removed calling RangeQueryBuilder from RandomQueryBuilder utility class, instead creating simple string field range query there now.
</comment><comment author="cbuescher" created="2015-07-21T15:52:50Z" id="123382164">Last minor cleanup, rebased once again on current 
</comment><comment author="javanna" created="2015-07-21T15:52:50Z" id="123382171">did another round, left a few comments
</comment><comment author="cbuescher" created="2015-07-21T15:53:09Z" id="123382249">Sorry, didn't mean to close this.
</comment><comment author="cbuescher" created="2015-07-21T17:23:59Z" id="123408110">Added some clarifying comment around edge test case.
</comment><comment author="javanna" created="2015-07-22T07:22:55Z" id="123588810">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12182 from cbuescher/feature/query-refactoring-spanmultiterm</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java</file></files><comments><comment>Query refactoring: SpanMultiTermQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Allow rpm to be build as part of package phase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12181</link><project id="" key="" /><description>This allows the creation of the RPM artifact as part of the
maven package phase. The result of this is that we get checksum and
name correction for-free as it's all build an installed into the m2
repository. This also publishes the RPM together with .deb to the mvn
mirror.

Note: this will only build the RPM as part of the package phase if
`-Dpackage.rpm=true` since the binaries to build the RPM are not
availabel on all platforms.
</description><key id="94297350">12181</key><summary>Allow rpm to be build as part of package phase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-10T12:26:44Z</created><updated>2015-07-14T08:35:27Z</updated><resolved>2015-07-10T12:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-10T12:38:36Z" id="120400424">Seems like a good idea. I don't know anything about RPM or understnad when/how RPMs are currently built, but I agree today its very strange the .deb is a maven artifact and not the RPM!
</comment><comment author="dadoonet" created="2015-07-10T12:48:46Z" id="120402018">Actually it's because we did not use maven extensions.
If you look at my pending PR (https://github.com/elastic/elasticsearch/pull/11523), here is what I did: 
- activate extension: https://github.com/dadoonet/elasticsearch/blob/maven/distribution/pom.xml#L876
- define the artifact as a rpm package https://github.com/dadoonet/elasticsearch/blob/maven/distribution/distribution/rpm/pom.xml#L14
</comment><comment author="dadoonet" created="2015-07-10T12:53:46Z" id="120403170">BTW I think I solved this rpm thing on other platform which don't have `rpmbuild` installed.

https://github.com/dadoonet/elasticsearch/blob/maven/distribution/distribution/pom.xml#L154-180

With this patch, the `rpm` module is added to the reactor only if you can build it. It means that on Linux and MacOS with `brew install rpm`, `mvn install` will create the RPM and put it in `~/.m2`. No need to provide anymore a `-Dpackage.rpm=true`.
</comment><comment author="s1monw" created="2015-07-10T13:03:03Z" id="120405159">ok @dadoonet I appreciate it - we can fix your issue once beta1 is out
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Free all pending search contexts if index is closed or removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12180</link><project id="" key="" /><description>Today we only clear search contexts for deleted indies. Yet, we should
do the same for closed indices to ensure they can be reopened quickly.

Closes #12116
</description><key id="94286640">12180</key><summary>Free all pending search contexts if index is closed or removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-10T11:11:19Z</created><updated>2015-07-10T17:08:06Z</updated><resolved>2015-07-10T12:37:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-10T11:11:30Z" id="120386818">@martijnvg can you take a look
</comment><comment author="martijnvg" created="2015-07-10T11:34:37Z" id="120390068">@s1monw that is quick! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix time sensitive percolator queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12179</link><project id="" key="" /><description>Percolator queries are parsed to a Lucene query at percolator query creation time. It matter what fields are in the mapping at percolator creation time. This is why ES requires fields to exist in the mapping when a percolator query is added. 

However some percolator queries are even more sensitive to when they are parsed:
- If a range query contains `now`.
- If a field in some queries contain wildcards. (depending of what the fields are resolved at percolator query parse time)

For the first case there is a workaround, but there is an issue with it (#12122), so we can do better.

With the query refactoring, queries are going to be parsed on the coordination node and the transformed in an intermediate format that gets send to nodes actually executing the queries. On these nodes this intermediate format is transformed into a Lucene query.

The percolator could for some queries that are time sensitive store the intermediate format instead of the Lucene query. When the a percolator evaluates a query in an intermediate format it just transforms it into a Lucene query before percolating a document.
</description><key id="94271858">12179</key><summary>Fix time sensitive percolator queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>:Query Refactoring</label></labels><created>2015-07-10T10:08:35Z</created><updated>2015-09-29T11:48:56Z</updated><resolved>2015-09-29T11:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-10T16:22:35Z" id="120450426">Does this late binding mean errors (missing fields) would not be surfaced until documents are percolated?
</comment><comment author="martijnvg" created="2015-07-10T18:30:13Z" id="120488216">yes, the errors would be known when documents are percolated, but because the the percolator queries haven't been parsed into their final form, it should be possible to add the missing field to the mapping.
</comment><comment author="jpountz" created="2015-09-27T22:23:03Z" id="143598628">Is it something we can move forward with now that query refactoring landed?
</comment><comment author="javanna" created="2015-09-28T06:20:33Z" id="143650645">&gt; Is it something we can move forward with now that query refactoring landed?

yes now that we have the infra for parsing into our own intermediate objects, we can look into migrating the percolator to actually make use of it.
</comment><comment author="javanna" created="2015-09-29T08:40:38Z" id="143988833">I chatted with @martijnvg about this. Using our own intermediate query objects would make it possible to only parse the json of percolator queries at index time (`QueryParser#fromXContent`) and delay the rest to the percolation time e.g. mappings, wildcards and now resolution (`QueryBuilder#toQuery`). This would allow us to potentially remove the need for strict field resolution in the percolator, similarly to what we did for aliases when we moved parsing to search time rather index time.

On the other hand, #13646 relies on calling `Query#extractTerms` in lucene so that we can index the terms extracted out of each query, and that requires calling `QueryBuilder#toQuery` at index time, which also requires again strict field resolution. It seems like these two are conflicting changes and #13646 is much more important as it brings bigger benefits, so maybe we should simply close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>It is not possible to use indexed_shape if shape is not in the _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12178</link><project id="" key="" /><description>```
{
    "filtered": {
        "query": {
            "match_all": {}
        },
        "filter": {
            "geo_shape": {
                "location": {
                    "indexed_shape": {
                        "id": "DEU",
                        "type": "countries",
                        "index": "shapes",
                        "path": "location"
                    }
                }
            }
        }
    }
}
```

This query will not work if location field excluded from _source. In 'shapes' mapping:

```
_source: {
  excludes : ['location']
}
```

An error will be returned: Shape found but missing field
</description><key id="94252643">12178</key><summary>It is not possible to use indexed_shape if shape is not in the _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">igorlesnenko</reporter><labels /><created>2015-07-10T08:43:25Z</created><updated>2015-07-10T13:24:33Z</updated><resolved>2015-07-10T13:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T13:24:32Z" id="120410137">Correct.  You shouldn't exclude the shape from `_source`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[test] list of plugins should have well-defined order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12177</link><project id="" key="" /><description>We now return list of plugins sorted by name.

Closes #12174.
</description><key id="94244504">12177</key><summary>[test] list of plugins should have well-defined order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-10T08:04:31Z</created><updated>2015-07-10T08:27:02Z</updated><resolved>2015-07-10T08:26:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-10T08:22:14Z" id="120290736">@jpountz Thanks for the review. I pushed a new commit.
</comment><comment author="jpountz" created="2015-07-10T08:22:42Z" id="120291127">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Simple patch to make creating aliases with glob patterns clearer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12176</link><project id="" key="" /><description>This PR is a simple doc patch to explicitly mention with an example of
how to create an alias using a glob pattern.  This comes up from
time-to-time with our customers and in the community and although
mentioned in the documentation already, is not obvious.

Also mention that the alias will not auto-update as indices matching the
glob change.

Closes #12175
</description><key id="94216234">12176</key><summary>[DOCS] Simple patch to make creating aliases with glob patterns clearer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>docs</label></labels><created>2015-07-10T05:20:15Z</created><updated>2015-07-10T16:59:33Z</updated><resolved>2015-07-10T16:59:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-10T07:42:37Z" id="120270253">+1 thanks @joshuar 
</comment><comment author="clintongormley" created="2015-07-10T16:59:27Z" id="120460052">thanks @joshuar - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Simple patch to make creating aliases with glob patterns clearer</comment></comments></commit></commits></item><item><title>Add example on using glob pattern to define alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12175</link><project id="" key="" /><description>We should show an example of creating an alias that points to a glob pattern of indices.  People often ask if this is possible and although it is mentioned in the documentation, it isn't obvious you can do this. 
</description><key id="94214505">12175</key><summary>Add example on using glob pattern to define alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/joshuar/following{/other_user}', u'events_url': u'https://api.github.com/users/joshuar/events{/privacy}', u'organizations_url': u'https://api.github.com/users/joshuar/orgs', u'url': u'https://api.github.com/users/joshuar', u'gists_url': u'https://api.github.com/users/joshuar/gists{/gist_id}', u'html_url': u'https://github.com/joshuar', u'subscriptions_url': u'https://api.github.com/users/joshuar/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/7401?v=4', u'repos_url': u'https://api.github.com/users/joshuar/repos', u'received_events_url': u'https://api.github.com/users/joshuar/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/joshuar/starred{/owner}{/repo}', u'site_admin': False, u'login': u'joshuar', u'type': u'User', u'id': 7401, u'followers_url': u'https://api.github.com/users/joshuar/followers'}</assignee><reporter username="">joshuar</reporter><labels><label>docs</label></labels><created>2015-07-10T05:07:34Z</created><updated>2015-07-10T16:59:33Z</updated><resolved>2015-07-10T16:59:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Simple patch to make creating aliases with glob patterns clearer</comment></comments></commit></commits></item><item><title>list of plugins (e.g. from nodes info) should have well-defined order.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12174</link><project id="" key="" /><description>Can we alphasort this?

I think today its sorted by filesystem order, which makes for hell in tests because many developers have macs. So me and jenkins get the short end of the stick since we run linux and tests fail sporatically.
</description><key id="94179627">12174</key><summary>list of plugins (e.g. from nodes info) should have well-defined order.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>PITA</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-10T00:11:15Z</created><updated>2015-07-10T17:02:12Z</updated><resolved>2015-07-10T08:26:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-10T06:36:43Z" id="120249466">+1 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginInfo.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfo.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/info/PluginsInfoTest.java</file></files><comments><comment>[test] list of plugins should have well-defined order</comment></comments></commit></commits></item><item><title>Lookup concrete indices and alias information via a single lookup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12173</link><project id="" key="" /><description>If a request is accepted by the coordination node we lookup the concrete indices, potential aliases and whether an alias has a filter up via several lookups from the cluster metadata. Also when checking for cluster blocks we do a lookup too.

All this can be done via a single lookup and if we change the transport action api we pass this information too all the places that need it.

Doing this via a single lookup cleans things up (no need for MetaData#filteringAliases(...) and MetaData#resolveSearchRouting(...) methods) but in cases where there are many aliases and indices this can speed things up too.

Follow up issue for #12058
</description><key id="94166843">12173</key><summary>Lookup concrete indices and alias information via a single lookup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2015-07-09T22:30:33Z</created><updated>2016-01-18T20:54:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: index.compound_format uses Lucene's default 0.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12172</link><project id="" key="" /><description>Just correcting the docs for 1.7.0 ...
</description><key id="94147199">12172</key><summary>Docs: index.compound_format uses Lucene's default 0.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>docs</label><label>v1.7.0</label></labels><created>2015-07-09T20:38:13Z</created><updated>2015-07-09T20:46:09Z</updated><resolved>2015-07-09T20:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-09T20:39:05Z" id="120138477">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[test] Remove httpclient dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12171</link><project id="" key="" /><description>We can provide our own implementation of a simple HTTP client to run all our tests (REST or others) so we don't really need to depend on httpclient and its dependencies.

It will help to avoid also maven dependency issues as reported in https://github.com/elastic/elasticsearch/pull/12036#issuecomment-119591775

We can remove httpclient as well from cloud plugins and let maven find the right transitive version.

Funny stuff:

Cloud-gce is expecting httpclient 4.0.1 but as it was defined in parent pom.xml, version 4.3.6 was used.
Same for azure 4.3.1 vs 4.3.6.

We also add `http.method_override.enabled` parameter:
- `http.method_override.enabled` enables support for `X-HTTP-Method-Override`, `X-HTTP-Method` and `X-METHOD-OVERRIDE` headers.

Defaults to `false`.
</description><key id="94144358">12171</key><summary>[test] Remove httpclient dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>stalled</label><label>test</label></labels><created>2015-07-09T20:22:46Z</created><updated>2016-06-24T11:04:25Z</updated><resolved>2016-06-24T11:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-10T08:09:19Z" id="120282258">this looks good but when I look at it I get lost on why we need to remove the http-client dependency. Seems odd that we can't use the library we want in our test framework otherwise some plugins may break, so this seems more like a workaround to me. That said if we want to move to HttpUrlConnection  and remove this dependency regardless I don't have any problem with that.
</comment><comment author="rmuir" created="2015-07-10T12:54:45Z" id="120403502">Its not a workaround. You simply cannot pollute your test classpath with dependencies like this, or it will hide bugs.

Thats because if something has a _real_ dependency on it, and the classpath is fucked up, nothing will catch you. 

Its just a standard thing with java development, keep the test classpath limited to testing jars. 

Adding commonly used jars like httpclient to the test classpath hides bugs, like cloud plugins not working at all.
</comment><comment author="dadoonet" created="2015-07-10T14:00:53Z" id="120416061">@jaymode Thanks! I added a new commit to disable by default this feature
</comment><comment author="rjernst" created="2015-07-10T16:41:48Z" id="120455293">Do we really want to make hacks in production code just to fix a potential problem with classpath in tests? I'm all for fixing the issue (Robert is right, we need to not pollute the test classpath with non testy stuff), but can we try the netty route he suggested? I would hate for a user to find this hack, start _using_ it on purpose, and then someone make the argument later that we can't remove because it is a "feature".
</comment><comment author="rmuir" created="2015-07-10T17:32:04Z" id="120468233">very simple, don't document it. then it can be removed when we require java 8 or its eliminated via another mechanism. 
</comment><comment author="nik9000" created="2015-07-28T19:38:19Z" id="125731879">&gt; very simple, don't document it. then it can be removed when we require java 8 or its eliminated via another mechanism.

That doesn't really work when people can and will read the source code.
</comment><comment author="rmuir" created="2015-07-28T19:48:50Z" id="125734072">It totally works. we add an unsupported api/hack, and then we remove it. Pretty much 100% of what I have added to elasticsearch sits in this category.

If someone builds shit that relies upon that unsupported API, their bad, they will pay the piper. Not my problem.
</comment><comment author="dadoonet" created="2015-08-05T08:39:22Z" id="127915861">Guys? What is the consensus here if any? Should I try to rebase that and merge it in or simply close the PR?

Thanks!
</comment><comment author="clintongormley" created="2015-08-05T16:52:01Z" id="128068555">@dadoonet let's merge this and turn it on only for testing.

thanks
</comment><comment author="dadoonet" created="2015-08-05T21:36:45Z" id="128156946">I rebased the PR so I had to fix some stuff.
For example, I needed to define `System.setProperty("sun.net.http.allowRestrictedHeaders", "true");` in the HTTPClient (test package).

I ran a full `mvn install` and everything looks fine but I'd love to get a final review.

Thanks!
</comment><comment author="dadoonet" created="2015-08-13T13:33:00Z" id="130673197">@rmuir Do you agree with the latest commit I added?
</comment><comment author="dadoonet" created="2015-08-28T15:33:59Z" id="135807926">I'm closing this one. I don't think we will merge it.
Thinking about it more, it actually does not really make sense to forbid the usage of any HttpClient lib in our tests and try to rewrite one from scratch.

We need to solve the dependency issues we might hit in another way IMHO. 

For now, the only "side effect" I see by keeping the current code is with GCE and Azure plugins:
- GCE is using `httpclient 4.0.1`.
- Azure is using `httpclient 4.3.1`.

We defined version `httpclient 4.3.6` and this one is packaged within our GCE/Azure plugins.

I don't see it as an issue for now for Azure. Might be problematic for GCE though.

Related GCE issues:
- https://github.com/google/google-http-java-client/issues/250
- https://github.com/google/google-api-java-client/issues/928
</comment><comment author="rjernst" created="2015-08-28T18:28:38Z" id="135854409">&gt; Thinking about it more, it actually does not really make sense to forbid the usage of any HttpClient lib in our tests and try to rewrite one from scratch.

Why? The reasoning was well laid out. Tests shouldn't be adding non test libs; that just leads to errors down the road. The state of things with different httpclient versions for GCE and Azure is a perfect example of why this needs to happen: we aren't testing the right thing. 
</comment><comment author="kimchy" created="2015-08-28T20:58:32Z" id="135886386">I read though this issue and change, I like this change and I think we should pull it in. The less dependencies we have in general, the better, and this change gets us in the right place.

I would put a javadoc on the new setting, saying it will be removed in the future, and not document it. It is the best out of all the options I can think about (having this setting in the first place). We can then remove this setting once we move to Java 8 as a core dependency.
</comment><comment author="jaymode" created="2015-08-28T21:03:06Z" id="135887134">+1 to keeping this going. @dadoonet let me know if you'd like me to help with the truststore usage (looking at the last commit)
</comment><comment author="dadoonet" created="2015-08-28T21:29:41Z" id="135892833">Ok. Reopening.
</comment><comment author="dadoonet" created="2015-08-31T09:08:06Z" id="136308879">@jaymode Yes! I'd love to have your help with the truststore part.
Do you want me to open a public branch in elasticsearch repo so you could push new commits or do you prefer to fork my repo and send PR on it?
</comment><comment author="jaymode" created="2015-08-31T15:01:17Z" id="136399223">@dadoonet sent a PR on your fork, if the change is good with you, the we can just cherry-pick it into your branch.
</comment><comment author="rmuir" created="2015-11-18T20:57:12Z" id="157861150">Can we bring this guy back to life? We require java 8 now, so we should not need the controversial part right? and this would be a huge win for master. It avoids not just dragging in httpclient dependency, but that itself also drags in 2 commons-\* dependencies.
</comment><comment author="dadoonet" created="2016-02-22T00:47:47Z" id="186954358">Now that we [decided to build an elasticsearch HTTP client](https://github.com/elastic/elasticsearch/issues/7743#issuecomment-186951568):
- does it makes sense to keep this PR?
- if it still makes sense, I think we can probably reduce its surface. Most of the challenges I had to face were for REST tests passing. As REST tests will now use the elasticsearch HTTP client, I think we could reduce the complexity of this PR.

Thoughts?
</comment><comment author="javanna" created="2016-02-22T01:24:37Z" id="186960450">I think we should re-evaluate once the http client is ready and not spend on time on this, especially given that our http client will most likely use the dependency that we were trying to remove here? Then it will raise a point of whether we want to use our http client to run rest tests, do we want to test the client or the server? Seems like we should do both but not at the same time?
</comment><comment author="javanna" created="2016-06-24T11:04:25Z" id="228318042">the new low level RestClient is in, the REST tests infrastructure is using it now. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Symlink support on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12170</link><project id="" key="" /><description>We currently support symlinks with SecurityManager, and have tests. However, the symlink test always fails on windows. While the permission for the link is correctly setup, on windows, permission is not automatically granted for the target of the link.

We could possibly solve this by explicitly adding permission for the target, whenever we add a link to the policy.
</description><key id="94135612">12170</key><summary>Symlink support on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2015-07-09T19:40:49Z</created><updated>2016-03-16T19:38:04Z</updated><resolved>2016-01-18T21:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T20:53:36Z" id="172650463">@rjernst do you know if this has been resolved?
</comment><comment author="rjernst" created="2016-01-18T20:59:49Z" id="172652569">AFAIK, this has not been fixed. We still have an "assume not windows" in the test `EvilSecurityTests.testSymlinkPermissions()`.
</comment><comment author="rmuir" created="2016-01-18T21:04:51Z" id="172653501">Er, but that was placed here by you for this issue. 

keep in mind that "symlinks" on windows are really a different thing than unix ones. If you have an issue with how filepermission works for the windows case there, I strongly suggest opening an openjdk issue. We can't work around it here, it needs to be fixed in the right place.
</comment><comment author="rjernst" created="2016-01-18T21:09:49Z" id="172654464">&gt; Er, but that was placed here by you for this issue.

Yes, I simply created this issue after investigating a build failure on windows. I have no problem with closing saying this is an issue with java+windows.  We should probably document this limitation, although I'm not sure where/if we currently document what "symlink support" we have.
</comment><comment author="rmuir" created="2016-01-18T21:11:45Z" id="172654871">Was there actually a build system running with permissions to create the links? FYI that is probably indicative of a bigger problem: it means it was running code as administrator.

See thats the thing with symlinks on windows, they work differently, and you need special permissions that ordinary users dont have. Definitely more of an esoteric thing...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12169</link><project id="" key="" /><description>Using a dynamic template, I have a need to create multiple indexed fields on the same string field using different analysers , however I wish these fields to be siblings of the matched field as opposed to children.

So, if I am matching on a field called 'value' at the moment I can currently specify the output (using a dynamic template) as (for example) value.text and value.string - so in the case where value is a child of description I would get

description.value.text
description.value.string (for example)

what i wish is to be able to obtain is

description.value
description.value_text

This means that I can add alternate indexed fields in future without breaking any existing searches.

Thanks

Rob

PS - I also think it has a **slight** tie in with the unwieldy need to specify the full path on a copy-to, where a shorthand 'copy-as-sibling' type feature would help greatly.
</description><key id="94125645">12169</key><summary>Dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rob-tice</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-07-09T18:54:22Z</created><updated>2017-02-17T13:58:12Z</updated><resolved>2017-02-17T13:58:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T16:42:04Z" id="120455330">Hi @rob-tice 

There are no plans to support this.  Why do you need this feature? What's the problem with `description.value.text` instead of `description.value_text`?
</comment><comment author="rob-tice" created="2015-07-10T16:56:41Z" id="120459486">I need it because otherwise I need to structure my indexes as though every possible string value has another level of hierarchy just in case I want to index it twice in the future. I already have an index structure which is highly nested and this makes it more  unwieldy and seems like a hack : ) . It just seemed  a logical extension to me… with behaviour like copy-to.

Cheers

R

From: Clinton Gormley [mailto:notifications@github.com] 
Sent: 10 July 2015 17:43
To: elastic/elasticsearch
Cc: rob-tice
Subject: Re: [elasticsearch] Dynamic templates (#12169)

Hi @rob-tice https://github.com/rob-tice  

There are no plans to support this. Why do you need this feature? What's the problem with description.value.text instead of description.value_text?

—
Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/12169#issuecomment-120455330 .  https://github.com/notifications/beacon/ADLulBE-GSxRgJCqm-HM5KGBl-rZLDueks5ob-2UgaJpZM4FVhXP.gif 
</comment><comment author="clintongormley" created="2015-07-10T17:41:01Z" id="120474709">I don't really understand what you mean by the index structuring comment or breaking searches by adding future fields.  However, I think we should support the `{path}` parameter, which would work like the `{name}` parameter. Then you can do:

```
PUT t
{
  "mappings": {
    "t": {
      "dynamic_templates": [
        {
          "foo": {
            "match": "foo*",
            "mapping": {
              "type": "string",
              "copy_to": [
                "{path}.bar-{name}"
              ]
            }
          }
        },
        {
          "bar": {
            "match": "bar*",
            "mapping": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      ]
    }
  }
}

PUT t/t/1
{
  "stuff": {
    "foo": "bar"
  }
}
```
</comment><comment author="rob-tice" created="2015-07-11T09:50:41Z" id="120600172">Hi Clinton

That was what I was hoping you would say : ). It would also fix my existing issues with the unwieldy existing copy-to.

From my perspective if the path was the path minus the actual field name then it would work for my use case and yours.

e.g.

“match”:”foo*”

So in that case I could choose to use:

copy-to:{path}.name_text

or  for your example it would be

copy-to:{path}.{name}.bar

as a concrete example:

a.b.c.foo=”test”

```
      "foo": {
        "match": "foo*",
        "mapping": {
          "type": "string",
          "copy_to": [
            "{path}.{name}-bar"
          ]
        }
      }
```

This would result in the content appearing in

a.b.c.foo=”test”
a.b.c.foo-bar=”test”

whereas

"foo": {
            "match": "foo*",
            "mapping": {
              "type": "string",
              "copy_to": [
                "{path}.{name}.original","{path}.{name}.bar",
              ]
            }
          }

This would result in the content appearing in

a.b.c.foo.original=”test”
a.b.c.foo.bar=”test”

Does that make sense : )

Cheers

Rob

From: Clinton Gormley [mailto:notifications@github.com] 
Sent: 10 July 2015 18:42
To: elastic/elasticsearch
Cc: rob-tice
Subject: Re: [elasticsearch] Dynamic templates (#12169)

I don't really understand what you mean by the index structuring comment or breaking searches by adding future fields. However, I think we should support the {path} parameter, which would work like the {name} parameter. Then you can do:

PUT t
{
  "mappings": {
    "t": {
      "dynamic_templates": [
        {
          "foo": {
            "match": "foo_",
            "mapping": {
              "type": "string",
              "copy_to": [
                "{path}.bar-{name}"
              ]
            }
          }
        },
        {
          "bar": {
            "match": "bar_",
            "mapping": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      ]
    }
  }
}

PUT t/t/1
{
  "stuff": {
    "foo": "bar"
  }
}

—
Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/12169#issuecomment-120474709 .
</comment><comment author="jpountz" created="2015-12-23T16:55:51Z" id="166943797">The example confuses me a bit: it seems that we are trying to use `copy_to` for a `fields` use-case?
</comment><comment author="rob-tice" created="2015-12-23T22:12:42Z" id="167000496">For clarification:

I wish to be able to say the following (for example - pseudo mapping):

wherever the json object 'note' appears

apply the following template

"note":
        {
            "properties": 
            {
                "type":{"type":"string","index":"not_analyzed","include_in_all" : false},
                "value":{"type":"string","index":"analyzed","analyzer":"standard": "copy_to":"value_text"},
                                "value_text":{"type":"string","index":"analyzed","analyzer":"autocomplete"},

```
        }
    }
```

I wish the 'copy to' to use the appropriate path so it copies to the sibling of value. Hence access to the path element so you could say copy_to: {$path}.value_text

I have not upgraded to 2.1 yet so I don;t know if this is already available

Cheers
Rob
</comment><comment author="pk-picturepark" created="2016-04-20T01:23:05Z" id="212196256">+1 

It would be nice to use a {path} variable within copy_to just to avoid the necessity to provide full field paths. 

It would even be nicer to configure a relative path. See sample below. So in order to copy the values from Person.Name to Person.Names I won't have to provide the full path _Person.Names_ in the copy_to field, but can use a relative syntax, e.g _/Names_.

`PUT /my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "Person": {
            "properties": {
                "Name": {
                  "type": "string",
                  "copy_to": "/Names"
                },
                "Names": {
                  "type": "string"
                }
              }
            }
          }
        }
    }
}`
</comment><comment author="jpountz" created="2017-02-17T13:58:12Z" id="280656210">We discussed it in FixitFriday. While we understand how it could help some use-cases, it also makes it too easy to generate unnecessarily complex mappings, which is something we do not want to encourage. Moreoven, dynamic mappings already have some complexity that we would rather not increase.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Persist all engine failure exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12168</link><project id="" key="" /><description>Currently when an engine fails the shard state file is no longer deleted (see #11933)
and the underlying store is only marked as corrupted for index corruption exceptions.
This means that the store can be opened, even after it failed with IOE, OOM exceptions.

It would be useful to persist the engine failures that are not due to corruption for
inspection, these can be exposed later through #11545
</description><key id="94121974">12168</key><summary>Persist all engine failure exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Engine</label><label>enhancement</label><label>review</label></labels><created>2015-07-09T18:41:37Z</created><updated>2016-03-16T10:03:20Z</updated><resolved>2016-03-16T05:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-07-09T18:48:31Z" id="120104728">@s1monw @kimchy @bleskes thoughts? This is a spin-off from https://github.com/elastic/elasticsearch/pull/11933. We keep all the failures around but only mark a store as corrupted on index corruption. 
</comment><comment author="bleskes" created="2015-07-12T07:12:38Z" id="120698004">@areek it's greet you are picking it up - I'm on the move now so excuse for me for being brief. I think that non-corruption issue shoulnd't be stored on the store but rather in the shard state file (or some other file next to it). The reason is that the store it self can be shared across machines (shadow replicas FTW), so it feels wrong to mark it as "failed" where it didn't really. Take the case of an OOM - it's the node that failed, not the data. The state file is local to the node, so it's different there.
</comment><comment author="areek" created="2016-03-16T05:04:19Z" id="197154684">Closing as now we have `UnassignedInfo` (eg: through `_cat/shards`) for any failed and unassigned shards
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete unused mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12167</link><project id="" key="" /><description>I want to delete the mappings for a type when all the elements of that type have been deleted, but I found no way of knowing if a type is being used or not. Do I need to keep track of its usage myself to then delete the mapping (so I can reuse the same type name for something else) or is there a way to do so within ES?

Also, I've read that in 2.0 you will remove the ability to delete mappings. If that's true, it would be awesome if you automatically delete mappings for unused types then :) Is that in the plans?

Thanks a ton!
</description><key id="94121356">12167</key><summary>Delete unused mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">albertvaka</reporter><labels /><created>2015-07-09T18:39:08Z</created><updated>2015-07-10T16:39:38Z</updated><resolved>2015-07-10T16:39:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T16:39:38Z" id="120454897">Hi @albertvaka 

No, it is not in the plans, and we don't allow you to delete mappings because it can result in index corruption later on.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default delayed allocation timeout to 1m from 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12166</link><project id="" key="" /><description>Chagne the default delayed allocation timeout from 0 (no delayed allocation) to 1m. The value came from a test of having a node with 50 shards being indexed into (so beefy translog requiring flush on shutdown), then shutting it down and starting it back up and waiting for it to join the cluster. This took, on a slow machine, about 30s.
The value is conservatively low and does not try to address a virtual machine / OS restart for now, in order to not have the affect of node going away and users being concerned that shards are not being allocated to the rest of the cluster as a result of that. The setting can always be changed in order to increase the delayed allocation if needed.
</description><key id="94109857">12166</key><summary>Default delayed allocation timeout to 1m from 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>breaking</label><label>feature</label><label>release highlight</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T17:38:52Z</created><updated>2015-07-14T10:14:03Z</updated><resolved>2015-07-14T10:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-09T18:08:32Z" id="120089688">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Default delayed allocation timeout to 1m from 0</comment><comment>Change the default delayed allocation timeout from 0 (no delayed allocation) to 1m. The value came from a test of having a node with 50 shards being indexed into (so beefy translog requiring flush on shutdown), then shutting it down and starting it back up and waiting for it to join the cluster. This took, on a slow machine, about 30s.</comment><comment>The value is conservatively low and does not try to address a virtual machine / OS restart for now, in order to not have the affect of node going away and users being concerned that shards are not being allocated to the rest of the cluster as a result of that. The setting can always be changed in order to increase the delayed allocation if needed.</comment><comment>closes #12166</comment></comments></commit></commits></item><item><title>Startup: Remove getopt parsing in shell script, use java CLITool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12165</link><project id="" key="" /><description>In order to ensure, we have the same experience across operating systems
and shells, this commit uses the java CLI parser instead of the shell
getopt parsing to parse arguments.

This also allows for support for paths, which contain spaces.

Also commons-cli depdency was upgraded to 1.3.1 and tests have been added.

Changes
- new exit code, OK_AND_EXIT, allowing to tell the caller to exit, as everything
  went as expected (e.g. when running a version output)

BWC breaking:
- execute() returns an ExitStatus instead of an integer, otherwise there is no
  possibility to signal by a command, if the JVM should be exited after a run.
  This affects plugins, that have command line tools
- -v used to be version, but is a verbose flag by default in the current CLI infra,
  must be -V or --version now
- -X has been removed - the current implementation was useless anyway, as
  it prefixed those properties with "es.". You should use
  ES_JAVA_OPTS/JAVA_OPTS for JVM configuration

TODO
- Can we fork into the background with java code, so we do not need that shell script hack?
- Should we rename the `-v` parameter and only support `--verbose` to resolve confusion?
</description><key id="94101221">12165</key><summary>Startup: Remove getopt parsing in shell script, use java CLITool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T16:58:04Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-30T13:07:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-07-09T19:01:14Z" id="120108785">Wrt. to forking into the background: Would https://commons.apache.org/proper/commons-daemon maybe be helpful here?
</comment><comment author="spinscale" created="2015-07-10T08:25:59Z" id="120293901">@MaineC looks platform specific after a quick look - would love to not have that. I need to dully understand first, what we do in in `Bootstrap()` constructor, where a non-daemonized keep-alive thread is started. Pointers appreciated :)
</comment><comment author="colings86" created="2015-07-16T15:21:05Z" id="121990742">@spinscale left some comments but I think in general it looks pretty good.
</comment><comment author="spinscale" created="2015-07-16T16:34:44Z" id="122013276">pushed a new commit, addressing the commetns
</comment><comment author="colings86" created="2015-07-16T20:31:42Z" id="122081211">Is there a test to check the `lenient`setting? If not, I think we should add one. Otherwise LGTM
</comment><comment author="spinscale" created="2015-07-17T11:26:17Z" id="122250451">@colings86 I actually decided to overwrite `-v` for `--verbose` and keep using `-v` for `--version` in order to be bwc compatible. You think that makes sense? This means I can also remove the `breaking` label...

I also renamed the `lenient` option to the name of the CLI parser and fixed the documentation so it is less confusing and resembles the functionality of the commons parser

Last but not least I fixed `bin/elasticsearch` to better find out when to daemonize
</comment><comment author="colings86" created="2015-07-17T11:29:14Z" id="122251115">@spinscale I don't have a strong opinion on the `-v` option so happy to go with what you think is best but this will probably be our best chance of changing to be in line with other CLI tools, so now is a good opportunity if we want to change it at some point.
</comment><comment author="colings86" created="2015-07-17T11:29:56Z" id="122251237">LGTM though
</comment><comment author="clintongormley" created="2015-07-17T14:17:48Z" id="122290578">Personally, I think I'd expect `-v` to be for verbose, as it is more common requirement than version.
</comment><comment author="spinscale" created="2015-07-20T06:43:26Z" id="122776813">ok, made `-v` for verbose again and '-V' for the version, added notes in the migration file
</comment><comment author="colings86" created="2015-07-21T08:15:28Z" id="123209101">LGTM
</comment><comment author="s1monw" created="2015-08-03T13:05:38Z" id="127224105">this is so good to see! thanks @spinscale you are my hero!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Regression with parent-child, object type and aggregations?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12164</link><project id="" key="" /><description>I am experiencing a regression when using parent-child relationships and aggregations of 
inner objects in Elasticsearch versions newer than 1.4.4.

Here is a simple made-up mapping that can demonstrate my issue:

```
curl -XDELETE 'http://localhost:9200/test'
curl -XPOST 'localhost:9200/test' -d '
{
  "mappings": {
  "Gym": {
      "properties": {
        "GymUID": {
          "type": "string",
          "include_in_all": true,
          "index": "not_analyzed"
        },
        "Short Description": {
          "type": "string"      
        }
       }
    },
    "Member": {
      "_parent": {
        "type": "Gym"
      },
      "properties": {
        "MemberUID": {
          "type": "string",
          "include_in_all": true,
          "index": "not_analyzed"
        },
        "Age": {
          "type": "integer",
          "null_value": 0
        },
        "Gender": {
          "type": "string",
          "include_in_all": true,
          "index": "not_analyzed"
        }
     }
     },
     "Visit": {
      "_parent": {
        "type": "Member"
      },
      "properties": {
        "Location": {
          "properties": {
            "Name": {
              "type": "string",
              "include_in_all": true,
              "index": "not_analyzed"
            }
          }
        },
        "Member": {
          "properties": {
            "MemberUID": {
              "type": "string",
              "include_in_all": true,
              "index": "not_analyzed"
            },
            "Age": {
              "type": "integer",
              "null_value": 0
            },
            "Gender": {
              "type": "string",
              "include_in_all": true,
              "index": "not_analyzed"
            }
          }
        },
        "VisitUID": {
          "type": "string",
          "include_in_all": true,
          "index": "not_analyzed"
        }
      }
    }
  }
}
'
```

We can now post some data

```
curl -XPOST 'http://localhost:9200/test/Gym/Gym1' -d '{"GymUID":1,"Short Description":"Sport center"}'
curl -XPOST 'http://localhost:9200/test/Member/1?parent=Gym1&amp;routing=Gym1' -d '{"MemberUID":"1","Gender":"Male","Age":65}'
curl -XPOST 'http://localhost:9200/test/Visit/1?parent=1&amp;routing=Gym1' -d '{"VisitUID":"1","Location":{"Name":"Paolo Ciccarese house"},"Member":{"MemberUID":"1","Gender":"Male","Age":65}}'
```

If now we query for visits of Male members and aggregate by age:

```
curl -XGET 'http://localhost:9200/test/Visit/_search?pretty=true&amp;size=1' -d '
{
  "query":{
     "match":{"Member.Gender":"Male"} 
  },
  "aggs":{
     "members":{"terms":{"field":"Member.Age"}}
  }
}'
```

The query returns one result and an empty aggregation for versions after 1.4.4 (I've tried 1.5.2 and 1.6.0).
It returns one result and one item in the aggregation bucket with version 1.4.4.

Notice that if I use the same mapping for Visit but without the parents:

```
curl -XDELETE 'http://localhost:9200/test'
curl -XPOST 'localhost:9200/test' -d '
{
  "mappings": {
     "Visit": {
      "properties": {
        "Location": {
          "properties": {
            "Name": {
              "type": "string",
              "include_in_all": true,
              "index": "not_analyzed"
            }
          }
        },
        "Member": {
          "properties": {
            "MemberUID": {
              "type": "string",
              "include_in_all": true,
              "index": "not_analyzed"
            },
            "Age": {
              "type": "integer",
              "null_value": 0
            },
            "Gender": {
              "type": "string",
              "include_in_all": true,
              "index": "not_analyzed"
            }
          }
        },
        "VisitUID": {
          "type": "string",
          "include_in_all": true,
          "index": "not_analyzed"
        }
      }
    }
  }
}
'
```

And I push the same data

```
curl -XPOST 'http://localhost:9200/test/Visit/1' -d '{"VisitUID":"1","Location":{"Name":"Paolo Ciccarese house"},"Member":{"MemberUID":"1","Gender":"Male","Age":65}}'
```

The same query works beautifully:

```
curl -XGET 'http://localhost:9200/test/Visit/_search?pretty=true&amp;size=1' -d '
{
  "query":{
     "match":{"Member.Gender":"Male"} 
  },
  "aggs":{
     "members":{"terms":{"field":"Member.Age"}}
  }
}'
```

and returns one item in the aggregation bucket no matter which version I use.

Any idea?
</description><key id="94095035">12164</key><summary>Regression with parent-child, object type and aggregations?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paolociccarese</reporter><labels /><created>2015-07-09T16:31:12Z</created><updated>2015-07-10T16:50:35Z</updated><resolved>2015-07-10T16:34:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T16:34:51Z" id="120453617">Hi @paolociccarese 

You're being bitten by ambiguous field resolution.  It is using the `Age` field in the `Member` type, instead of the `Member.Age` field in the `Visit` type.

We have solved these issues in 2.0 with #8871 

However in 1.6 you can do the following:

```
GET /test/Visit/_search?pretty=true&amp;size=1
{
  "query":{
     "match":{"Member.Gender":"Male"} 
  },
  "aggs":{
     "members":{"terms":{"field":"Visit.Member.Age"}}
  }
}
```
</comment><comment author="pciccarese" created="2015-07-10T16:50:35Z" id="120457211">Thank you @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix REST test for cat.nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12163</link><project id="" key="" /><description>Windows load average OS stats returns `-1.00`.
</description><key id="94091699">12163</key><summary>Fix REST test for cat.nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T16:17:44Z</created><updated>2015-07-20T12:08:35Z</updated><resolved>2015-07-09T17:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-09T16:18:21Z" id="120055521">@jaymode Can you have a look please? thanks.
</comment><comment author="jaymode" created="2015-07-09T16:44:41Z" id="120065265">LGTM. Thanks @tlrx 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Backport to 1.7 - Snapshot info should contain version of elasticsearch that created the snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12162</link><project id="" key="" /><description>Backport of #11985 to 1.7

This information was stored with the snapshot but wasn't available on the interface. Knowing the version of elasticsearch that created the snapshot can be useful to determine the minimal version of the cluster that is required in order to restore this snapshot.

Closes #11980
</description><key id="94091154">12162</key><summary>Backport to 1.7 - Snapshot info should contain version of elasticsearch that created the snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.7.0</label></labels><created>2015-07-09T16:16:20Z</created><updated>2015-07-09T18:46:12Z</updated><resolved>2015-07-09T18:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-09T17:42:23Z" id="120083340">LGTM, left two bikeshed comments.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wait on incoming joins before electing local node as master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12161</link><project id="" key="" /><description>During master election each node pings in order to discover other nodes and validate the liveness of existing nodes. Based on this information the node either discovers an existing master or, if enough nodes are found (based on `discovery.zen.minimum_master_nodes&gt;&gt;) a new master will be elected.  

Currently, the node that is elected as master will currently update it the cluster state to indicate the result of the election. Other nodes will submit a join request to the newly elected master node. Instead of immediately processing the election result, the elected master
node should wait for the incoming joins from other nodes, thus validating the elections result is properly applied. As soon as enough nodes have sent their joins request (based on the `minimum_master_nodes` settings) the cluster state is modified.

Note that if `minimum_master_nodes` is not set, this change has no effect.
</description><key id="94086411">12161</key><summary>Wait on incoming joins before electing local node as master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T15:56:08Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-15T06:23:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-09T15:56:30Z" id="120047182">@kimchy @martijnvg can you take a look ? thx...
</comment><comment author="martijnvg" created="2015-07-10T08:40:25Z" id="120302056">@bleskes Left a couple of questions. This is a good improvement to the zen disco!
</comment><comment author="bleskes" created="2015-07-12T07:01:55Z" id="120697252">@martijnvg Thx. Pushed a new commit.
</comment><comment author="martijnvg" created="2015-07-13T13:02:39Z" id="120921510">@bleskes LTGM
</comment><comment author="kimchy" created="2015-07-14T13:07:40Z" id="121233565">LGTM, two minor changes: more javadoc on node controller, and break the test for 0 expected and more to 2 tests
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTest.java</file><file>core/src/test/java/org/elasticsearch/test/cluster/TestClusterService.java</file></files><comments><comment>Discovery: wait on incoming joins before electing local node as master</comment></comments></commit></commits></item><item><title>strip elasticsearch- and es- from any plugin name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12160</link><project id="" key="" /><description>#12158 only partially fixed #12143 by removing the prefix from official plugin names. This change

removes the prefixes from any plugin names.
</description><key id="94085158">12160</key><summary>strip elasticsearch- and es- from any plugin name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T15:51:25Z</created><updated>2015-07-09T15:56:45Z</updated><resolved>2015-07-09T15:56:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-09T15:55:45Z" id="120047016">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `quorum_active` flag to cluster health and waitForQuorumActive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12159</link><project id="" key="" /><description>Add the ability on top of status to know if a quorum of shards are active per index (so if there is at least one index without a quorum, it will be false).
 Also, allow to wait for it in the cluster health API.
 This change also centralizes all of these logic into a single enum class and makes sure the places where we check for it in our code base uses the same logic, with simple unit tests for it.
</description><key id="94079628">12159</key><summary>Add `quorum_active` flag to cluster health and waitForQuorumActive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2015-07-09T15:30:03Z</created><updated>2015-07-19T10:51:27Z</updated><resolved>2015-07-19T10:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-09T15:47:13Z" id="120043597">nice, this should solve #4755 I think?
</comment><comment author="bleskes" created="2015-07-09T15:48:20Z" id="120044228">The change looks good and I like the new enum + isMet. However, since the write consistency is configurable per index for initial recovery and per write request, I think it makes sense to allow people to specify they consistency they want to wait for and also pick up on the index setting for the flag on the cluster health... 
</comment><comment author="kimchy" created="2015-07-09T15:51:06Z" id="120045186">@bleskes We don't allow to configure write consistency per index on "writes", we do allow to configure it per index for recovery, but those are completely 2 different things. For this reason, for the sake of simplicity, a simple "do we have quorum" (regardless of settings and such) makes sense.
</comment><comment author="s1monw" created="2015-07-09T18:13:35Z" id="120091677">LGTM except of that I think we need to add some documentation for this feature. I left some minor comments
</comment><comment author="s1monw" created="2015-07-09T18:14:06Z" id="120092038">&gt;  For this reason, for the sake of simplicity, a simple "do we have quorum" (regardless of settings and such) makes sense.

+1 keep things simple and do not confuse people :)
</comment><comment author="bleskes" created="2015-07-12T07:24:24Z" id="120698516">&gt;  We don't allow to configure write consistency per index on "writes"

What I meant tis that we allow to configure it per request and also to configure the default using settings (`action.write_consistency`) .

&gt; +1 keep things simple and do not confuse people :)

Agreed but if things are not consistent between APIs I think we end up causing confusion rather than making it better.

I'm not sure about the background for this change (feels like it's part of a bigger plan), but I want to repeat my suggestion from #4755 (@javanna you have the memory of an elephant - thanks for reminding me) :

&gt; To me, yellow means “cluster is fully functional albeit not conforming to the required replication factor”. Red means that some operations can not be performed. With this definition in mind, I think we should signal an shard (and thus index &amp; cluster) as red if one can not index into it due to the `write_consistency` settings (which also be `all` , and not `quorum`) 

If we do that, we can keep the current wait for logic as is and people can wait for yellow? 

Note: the suggestion should probably be adjusted to take `index.recovery.initial_shards` into account as well. For as long as we have it.. (different discussion)
</comment><comment author="clintongormley" created="2015-07-12T10:46:24Z" id="120707098">&gt; If we do that, we can keep the current wait for logic as is and people can wait for yellow?

I don't think we should change the existing meaning of yellow - it will just cause a lot of confusion as the existing meanings are well known and this change would be silent.

With the proposed change, users can wait for quorum or wait for green (if consistency is all).  They can also do this on a per-index basis.  I think that is clear, simple, and backwards compatible.
</comment><comment author="kimchy" created="2015-07-19T10:51:15Z" id="122647161">ok, after a few discussions about it, I will break this pull request. First, the common way to parse what qualities to an active out of total is good, so I will keep this around.

Also, I want to use this opportunity to get away from the consistency level enum.

Last, we can add in a followup pull request anything that has to do with cluster health.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove elasticsearch- from name of official plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12158</link><project id="" key="" /><description>This change fixes the plugin manager to trim `elasticsearch-` and `es-` prefixes from plugin names
for our official plugins. This restores the old behavior prior to #11805.

Closes #12143
</description><key id="94076095">12158</key><summary>remove elasticsearch- from name of official plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T15:16:57Z</created><updated>2015-07-09T15:26:21Z</updated><resolved>2015-07-09T15:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-09T15:19:55Z" id="120030664">Looks good to me.
</comment><comment author="dadoonet" created="2015-07-09T15:21:03Z" id="120031267">Thank you @jaymode!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file></files><comments><comment>strip elasticsearch- and es- from any plugin name</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file></files><comments><comment>Merge pull request #12158 from jaymode/plugin_name_prefix</comment></comments></commit></commits></item><item><title>Fix pluginmanager permissions for bin/ scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12157</link><project id="" key="" /><description>Today it will remove all permissions and only set execute bit:

```
---x--x--x
```

Instead we should preserve existing permissions, and just add
read and execute to whatever is there.

Closes #12142
</description><key id="94067435">12157</key><summary>Fix pluginmanager permissions for bin/ scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T14:42:06Z</created><updated>2015-07-10T16:10:19Z</updated><resolved>2015-07-09T14:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-09T14:45:07Z" id="120008670">+1
</comment><comment author="jaymode" created="2015-07-09T14:45:40Z" id="120008783">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file></files><comments><comment>Merge pull request #12157 from rmuir/plugin_permissions</comment></comments></commit></commits></item><item><title>Query refactoring: SpanNearQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12156</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217 

This PR is agains the query refactoring branch.
</description><key id="94058229">12156</key><summary>Query refactoring: SpanNearQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-07-09T14:05:50Z</created><updated>2016-03-11T11:51:13Z</updated><resolved>2015-07-09T15:48:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-09T14:15:24Z" id="119997075">looks good left a few minor comments
</comment><comment author="cbuescher" created="2015-07-09T14:25:29Z" id="120000443">@javanna left a few comments regarding your questions.
</comment><comment author="cbuescher" created="2015-07-09T15:01:24Z" id="120017452">@javanna Added the comments and made default settings for flags public. 
</comment><comment author="javanna" created="2015-07-09T15:02:45Z" id="120017843">LGTM besides the minor comment I left
</comment><comment author="javanna" created="2015-08-28T10:13:57Z" id="135727308">This change is breaking for the java api as it removes setter for mandatory slop parameter which needs to be set in the constructor instead.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file></files><comments><comment>Merge pull request #12156 from cbuescher/feature/query-refactoring-spannear</comment></comments></commit></commits></item><item><title>allow settings to be passed to client for external test cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12155</link><project id="" key="" /><description>This change allows custom settings to be passed to the client for the external test cluster,
which is necessary when additional settings need to be passed to the client in order to
properly communicate with the external test cluster.
</description><key id="94050814">12155</key><summary>allow settings to be passed to client for external test cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T13:41:09Z</created><updated>2015-07-09T13:57:19Z</updated><resolved>2015-07-09T13:57:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-09T13:56:22Z" id="119982249">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/ExternalTestCluster.java</file></files><comments><comment>Merge pull request #12155 from jaymode/external_test_cluster_settings</comment></comments></commit></commits></item><item><title>Changed SHA calculation in license checker to assume binary files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12154</link><project id="" key="" /><description>Related to #12117

Closes #12118
</description><key id="94045556">12154</key><summary>Changed SHA calculation in license checker to assume binary files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>build</label></labels><created>2015-07-09T13:18:53Z</created><updated>2015-07-10T16:07:12Z</updated><resolved>2015-07-10T12:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-09T13:19:30Z" id="119961395">+1!
</comment><comment author="rmuir" created="2015-07-09T14:12:03Z" id="119995282">I checked on a windows VM (with all plugins, but not core/ due to tar issues, see below), it works and fixes the checksum issue! thanks!

I think for now, we should just push this one, and keep the checker temporarily disabled on windows because it requires not just perl.exe, but a more complicated setup with unzip.exe and a tar.exe that supports .tar.gz (this one is tricky to find on windows). And I think we should be doing `mvn verify` builds on windows in jenkins etc, even though we do not today.

Maybe in a followup issue the script could be changed to use http://perldoc.perl.org/Archive/Extract.html or something similar if its generally expected to be bundled with perl? 
</comment><comment author="clintongormley" created="2015-07-10T16:07:12Z" id="120445971">&gt; Maybe in a followup issue the script could be changed to use http://perldoc.perl.org/Archive/Extract.html or something similar if its generally expected to be bundled with perl?

Archive::Extract is no longer in perl core.  I'll have a look for alternatives
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Will range aggregation support 'lte/lt/gt/gte'?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12153</link><project id="" key="" /><description>The range query support  'lte/lt/gt/gte', will the range aggregation support too? 
For price of goods, I want to use range [0,50], [50,100], ...
</description><key id="94036522">12153</key><summary>Will range aggregation support 'lte/lt/gt/gte'?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thihy</reporter><labels /><created>2015-07-09T12:33:35Z</created><updated>2015-07-09T13:10:34Z</updated><resolved>2015-07-09T13:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-09T13:10:32Z" id="119956249">Duplicate of https://github.com/elastic/elasticsearch/issues/5249
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds new script API to ValuesSourceMetricsAggregationBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12152</link><project id="" key="" /><description>A method for the new Script API were missing in the ValuesSourceMetricsAggregationBuilder. This change adds the missing method and deprecates the old Script API methods
</description><key id="94035942">12152</key><summary>Adds new script API to ValuesSourceMetricsAggregationBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T12:30:37Z</created><updated>2015-07-10T12:10:29Z</updated><resolved>2015-07-10T12:10:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-10T11:43:46Z" id="120392964">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make 2.0.0.beta1-SNAPSHOT the current version.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12151</link><project id="" key="" /><description>Today everything is tied to having the next version as the latest.
In order to work towards 2.0.0.beta1 we need to fix all the usage of
2.0.0-SNAPSHOT to reflect the version we will release soon.
Usually we do this on the release branch but to simplify things I wanna
keep this on master for now and move to 2.1.0-SNAPSHOT on master once
we created a 2.0 branch.

Closes #12148
</description><key id="94016301">12151</key><summary>Make 2.0.0.beta1-SNAPSHOT the current version.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T10:56:29Z</created><updated>2015-07-10T16:50:14Z</updated><resolved>2015-07-09T19:26:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-09T12:24:34Z" id="119942627">+1

i don't think code review works for changes like these. We have to rely on tests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't require fields in alias filters to exist in the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12150</link><project id="" key="" /><description>Before alias filters were parsed at alias creation time. Therefor it was critical that fields used in alias filters  exist in the mapping, otherwise the the alias filter wouldn't work correctly.

Now that alias filters are parsed at request time, the restriction that a field needs to exist at alias creation time is no longer valid. This PR removes that restriction.
</description><key id="94007887">12150</key><summary>Don't require fields in alias filters to exist in the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aliases</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T10:08:08Z</created><updated>2015-07-10T15:53:03Z</updated><resolved>2015-07-10T14:50:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-10T14:11:51Z" id="120418573">LGTM. Can you add a description to the PR?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file></files><comments><comment>Merge pull request #12150 from martijnvg/aliases/remove_strict_filter_parsing</comment></comments></commit></commits></item><item><title>Add global search timeout setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12149</link><project id="" key="" /><description>Add dynamically updatable global search timeout cluster level setting.
</description><key id="94004562">12149</key><summary>Add global search timeout setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Settings</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T09:50:03Z</created><updated>2015-07-17T13:23:16Z</updated><resolved>2015-07-17T13:23:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/search/MockSearchService.java</file></files><comments><comment>Add global search timeout setting</comment></comments></commit></commits></item><item><title>Change version to return lowercase beta1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12148</link><project id="" key="" /><description>The Version class (and the pom file?) need to be changed to use lowercase beta1.

Plus add a test
</description><key id="94003551">12148</key><summary>Change version to return lowercase beta1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T09:45:00Z</created><updated>2015-07-09T19:26:56Z</updated><resolved>2015-07-09T19:26:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/VersionTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesTests.java</file><file>core/src/test/java/org/elasticsearch/codecs/CodecTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file></files><comments><comment>Make 2.0.0.beta1-SNAPSHOT the current version.</comment></comments></commit></commits></item><item><title>Remove double call to elect primaries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12147</link><project id="" key="" /><description>There is no need to call the elect logic twice, we used to need it, but no longer since we handle dangling replicas for unassigned primaries properly
</description><key id="94000978">12147</key><summary>Remove double call to elect primaries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T09:30:10Z</created><updated>2015-07-10T15:42:46Z</updated><resolved>2015-07-09T09:44:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-09T09:31:13Z" id="119889103">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file></files><comments><comment>Merge pull request #12147 from kimchy/remove_double_elect</comment></comments></commit></commits></item><item><title>Use IndexWriter.hasPendingChanges() to detect if a flush is needed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12146</link><project id="" key="" /><description>This is required to ensure we flush the indexwriter / commit all changes
even if no docs are indexed. This is important if a merge happend after
the last flush and no further docs have been added. Otherwise merges
will not be committed if no docs where added/updated/deleted.

This is equivalent to what master does today.

Closes #12134
</description><key id="93994541">12146</key><summary>Use IndexWriter.hasPendingChanges() to detect if a flush is needed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Index APIs</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-07-09T08:54:21Z</created><updated>2015-07-10T15:52:36Z</updated><resolved>2015-07-09T09:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-07-09T09:05:48Z" id="119881610">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>hashCode function isn't consistent with Objects.hash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12145</link><project id="" key="" /><description>In the query refactoring branch, the way we compute AbstractQueryBuilder#hashCode is the following:

hasCode = 31 \* Objects.hash(query_name, boost) + doHashCode(child_class)

= 31 \* Objects.hash(A, B) + doHashCode(C)

= 31 \* f(A, B) + f(C) where f = Objects.hash

= 31 \* (31 \* (31 + A) + B) + 31 \* C

!= (31 \* (31 \* (31 + A) + B)) + C = f(A, B, C) = Objects.hash(A, B, C)
</description><key id="93994005">12145</key><summary>hashCode function isn't consistent with Objects.hash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-09T08:51:35Z</created><updated>2015-08-14T12:02:11Z</updated><resolved>2015-08-14T12:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-07-14T08:41:19Z" id="121168038">Can you elaborate on the implications of this?
</comment><comment author="alexksikes" created="2015-07-14T11:38:44Z" id="121211106">I don't think this is a bug, it is just a remark. Do we want to stay consistent with Objects.hash?
</comment><comment author="javanna" created="2015-08-14T12:02:10Z" id="131086824">Closed by https://github.com/elastic/elasticsearch/commit/1cfe580cb7c9da5657efcaaf2513ff07bcf93a0d
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file></files><comments><comment>Modified AbstractQueryBuilder default hashcode impl</comment></comments></commit></commits></item><item><title>Wrong Aggregation of uniq counts / sum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12144</link><project id="" key="" /><description>I analyse netflow data. Data is send to nprobe, which in turn writes it natively to elasticsearch. 
Elastic search version: 1.5.2
Kibana version: 4.0.2

I do a "uniq count" of "IPV4_SRC_ADDR" for a single destination address. 
X-Axis: 
Aggregation: Date Histogram
Interval: Minute
This give me around 10 to 20 uniq IP's in average. Now, when I switch to an "hourly" Intervall, I get an average of 100 !

This becomes FAR WORSE when I do a "sum in_bytes" aggregationover 2 days. While the system measures around 200 kb/s, kibana switches to an multi hour aggregation and reports 140m bytes. I assume that "m" stands for "million" ?  I can only get "realistic" information if I split the timeframe in much smaller pieces and push the interval to minutes / seconds. 

Why is there such a grave, misleading difference in aggregation when doing a "sum"? And why is the only hint for this problem the very tiny "info" icon above the interval?
I understand, that doing a "sum" on a timeframe of 2 hours will sum all available data to a single point in the graph. Is there any way to warn about the fact, that the aggregation has a "blur" factor of X? Where X is the Factor of the Interval?

![screen shot 2015-07-09 at 09 34 07](https://cloud.githubusercontent.com/assets/1741662/8590312/9ed3c28a-261f-11e5-80a2-e3590a40ce01.png)
![screen shot 2015-07-09 at 09 34 25](https://cloud.githubusercontent.com/assets/1741662/8590313/9ed451dc-261f-11e5-987e-958a0c59b08a.png)
</description><key id="93981404">12144</key><summary>Wrong Aggregation of uniq counts / sum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">new-gen23</reporter><labels /><created>2015-07-09T07:47:38Z</created><updated>2015-08-24T10:01:27Z</updated><resolved>2015-08-24T09:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-09T14:25:55Z" id="120000542">Could you paste your mapping (curl -XGET 'http://localhost:9200/INDEX_NAME/_mapping/TYPE') in a gist and link it here. It might help us determine what might be going on.
</comment><comment author="new-gen23" created="2015-08-24T09:59:42Z" id="134124147">Sorry, had no time to follow the problem. Data was logrotated and is gone. Can not replicate the issue. Please close this ticket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recent changes to Pluginmanager broke naming convention</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12143</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/11805 accidentally broke the naming convention not stripping of the 'elasticsearch' / 'es' parts of the beginning of the plugin names.

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/PluginManager.java#L754-L756 runs the pluginmanager before stripping of the names which happens after that.
</description><key id="93977473">12143</key><summary>Recent changes to Pluginmanager broke naming convention</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">electrical</reporter><labels><label>:Plugins</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T07:22:45Z</created><updated>2015-07-09T15:51:26Z</updated><resolved>2015-07-09T15:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file></files><comments><comment>strip elasticsearch- and es- from any plugin name</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file></files><comments><comment>remove elasticsearch- from name of official plugins</comment></comments></commit></commits></item><item><title>PluginManager screws up file permissions for stuff in bin/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12142</link><project id="" key="" /><description>It gives them _only_ execute permission. But instead it should add execute bit, not remove everything else.
</description><key id="93973471">12142</key><summary>PluginManager screws up file permissions for stuff in bin/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T06:59:11Z</created><updated>2015-07-09T14:50:48Z</updated><resolved>2015-07-09T14:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-09T14:50:33Z" id="120010285">This is a 2.0-only problem. 1.x uses a different mechanism from java.io.File to add executable perms.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file></files><comments><comment>Fix pluginmanager permissions for bin/ scripts</comment></comments></commit></commits></item><item><title>Add back documentation on disable _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12141</link><project id="" key="" /><description>Currently, our 1.x doc (https://www.elastic.co/guide/en/elasticsearch/reference/1.x/mapping-source-field.html#mapping-source-field) includes information on:
- Ability to disable _source
- includes/excludes functionality for _source being deprecated in 1.6.0

In https://github.com/elastic/elasticsearch/pull/10915, we removed the ability to disable _source, and then it was subsequently added back in https://github.com/elastic/elasticsearch/pull/11171.  But it appears that when we added this back, we haven't gone back and updated the documentation to re-add the information on disabling _source and the includes/excludes usage.  So the latest doc in master is missing this information:  

https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-source-field.html#include-exclude (doc in master)

https://github.com/elastic/elasticsearch/commits/d32a80f37b1e09d37b1556ba5cce74d0b76bb8c4/docs/reference/mapping/fields/source-field.asciidoc (history of doc changes)

This is a request to add back the documentation on disabling source and the caveats mentioned here (https://github.com/elastic/elasticsearch/issues/11116#issuecomment-101320891) now that the feature has returned.  And since the feature is back, I think it also means that includes/excludes is no longer "deprecated"?
</description><key id="93969029">12141</key><summary>Add back documentation on disable _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-07-09T06:34:46Z</created><updated>2015-07-10T22:55:11Z</updated><resolved>2015-07-10T13:53:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-07-10T22:55:11Z" id="120546454">thx! @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Updated the source field docs to remove deprecation of includes/excludes</comment></comments></commit></commits></item><item><title>how to make the query that looks  like "A|B"?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12140</link><project id="" key="" /><description>Hello ,I met a difficult problem when i import elasticsearch to my project , it exists  logical operation like "A|B" in my  requirements,so ,how to make the query that looks  like "A|B"  in ealsticsearch?
</description><key id="93940512">12140</key><summary>how to make the query that looks  like "A|B"?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mlc0202</reporter><labels /><created>2015-07-09T02:57:30Z</created><updated>2015-07-14T09:13:38Z</updated><resolved>2015-07-09T06:16:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-09T06:16:32Z" id="119834309">Please ask questions on discuss.elastic.co. We can help there.

Have a look at the query DSL (bool) or search using a query string with "A B".
</comment><comment author="mlc0202" created="2015-07-14T09:13:37Z" id="121174764">Unfortunately ,I have tried it,but it didn't resolve my problem,i describe it  more serious   in #12226
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>BulkRequestBuilder.add() has typo in JavaDoc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12139</link><project id="" key="" /><description>Closes #12138
</description><key id="93936858">12139</key><summary>BulkRequestBuilder.add() has typo in JavaDoc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">israelekpo</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-09T02:36:30Z</created><updated>2015-07-09T09:49:48Z</updated><resolved>2015-07-09T06:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-09T06:13:06Z" id="119832693">Thanks!
</comment><comment author="israelekpo" created="2015-07-09T09:49:48Z" id="119892860">You are welcome. I am glad I could help.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file></files><comments><comment>Merge pull request #12139 from israelekpo/patch-1</comment></comments></commit></commits></item><item><title>Java API Documentation - org.elasticsearch.action.bulk.BulkRequestBuilder.add() has typo in JavaDoc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12138</link><project id="" key="" /><description>Two of the add() methods in org.elasticsearch.action.bulk.BulkRequestBuilder has DeleteRequest instead of UpdateRequest or UpdateRequestBuilder in the JavaDoc.

The JavaDoc should be modified to site the correct class names.
</description><key id="93935990">12138</key><summary>Java API Documentation - org.elasticsearch.action.bulk.BulkRequestBuilder.add() has typo in JavaDoc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">israelekpo</reporter><labels /><created>2015-07-09T02:30:29Z</created><updated>2015-07-09T06:12:55Z</updated><resolved>2015-07-09T06:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file></files><comments><comment>BulkRequestBuilder.add() has typo in JavaDoc.</comment></comments></commit></commits></item><item><title>Handle upserts failing when document has already been created by another process </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12137</link><project id="" key="" /><description>Closes #11506
</description><key id="93925150">12137</key><summary>Handle upserts failing when document has already been created by another process </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">vedil</reporter><labels><label>:CRUD</label><label>bug</label></labels><created>2015-07-09T01:06:18Z</created><updated>2015-08-28T11:22:35Z</updated><resolved>2015-08-28T11:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-07-10T02:06:41Z" id="120200687">The unit test that you submitted still fails (randomly) even after your pull request. For example:

```
mvn clean test \
-Pdev \
-Dtests.seed=806B4E52F9B20C5B \
-Dtests.class=org.elasticsearch.action.IndicesRequestTests \
-Dtests.slow=true \
-Dtests.method="testIndexNUpdateUpsert" \
-Des.logger.level=ERROR \
-Dtests.assertion.disabled=false \
-Dtests.security.manager=true \
-Dtests.heap.size=512m \
-Dtests.locale=no_NO_NY \
-Dtests.timezone=America/Miquelon
```

will fail before and after your pull request.
</comment><comment author="vedil" created="2015-07-12T14:23:54Z" id="120725192">so my change does not fix the issue, i agree that a consistent test case would be of great use, i will try to write one, probably using thread pool to create multiple update/upsert operations
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Mention plugin version compatibility in upgrade guide</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12136</link><project id="" key="" /><description>Add note regarding plugin version compatibility - for example, the AWS plugin can crash a node at startup when versions mismatch.
</description><key id="93913994">12136</key><summary>[DOCS] Mention plugin version compatibility in upgrade guide</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tylerjl</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T23:38:52Z</created><updated>2015-07-12T11:26:22Z</updated><resolved>2015-07-12T11:26:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-09T06:20:13Z" id="119835334">Actually all plugins will need to be updated (but _site plugins?).
</comment><comment author="tylerjl" created="2015-07-09T16:58:39Z" id="120069660">Thanks @dadoonet - I changed the wording slightly to make that clearer.
</comment><comment author="dadoonet" created="2015-07-09T17:00:08Z" id="120070124">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12136 from tylerjl/docs/upgrade_guide_plugins</comment></comments></commit></commits></item><item><title>Improve error handling of ClassCastException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12135</link><project id="" key="" /><description>Example in 1.6:

```
POST cast/type/1
{
  "ip_str":"0.0.0.0"
}
PUT cast2
{
  "mappings": {
    "type":{
      "properties": {
        "ip_str":{
          "type":"ip"
        }
      }
    }
  }
}
POST cast2/type/1
{
  "ip_str":"0.0.0.0"
}

GET cast/_mapping
GET cast2/_mapping

GET cast,cast2/_search?search_type=count
{
  "aggs": {
    "NAME": {
      "terms": {
        "field": "ip_str",
        "size": 10
      }
    }
  }
}
```

Returns:

```
{
   "error": "ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.StringTerms$Bucket]; ",
   "status": 503
}
```

Above is a simplified example.  Would be nice to improve the error handling above, eg. tell the user that cast index's type mapping has ip_str defined as a string while cast2 index' type mapping has ip_str defined as an ip type and they will have to reindex to address the type mismatch, etc..
</description><key id="93903896">12135</key><summary>Improve error handling of ClassCastException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-07-08T22:24:19Z</created><updated>2015-08-17T07:36:56Z</updated><resolved>2015-08-17T07:36:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ashishnerkar1" created="2015-07-15T06:00:06Z" id="121499406">Hi All, I am new to ElasticSearch &amp; want to start contributing to this project. I am interested in working on this issue. Can someone point me to appropriate resources &amp; steps to start working on this issue.
</comment><comment author="clintongormley" created="2015-07-15T11:11:00Z" id="121584454">@ashishnerkar1 read https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md
</comment><comment author="HarishAtGitHub" created="2015-07-30T21:17:36Z" id="126491258">sorry for the unnecessary noise(created above) due to my commit in other repo (when trying to reproduce the issue)...
</comment><comment author="xuzha" created="2015-08-05T23:02:58Z" id="128177134">Look like we resolved this already.  This is the result:

```
{
   "error": {
      "root_cause": [],
      "type": "reduce_search_phase_exception",
      "reason": "[reduce] ",
      "phase": "query",
      "grouped": true,
      "failed_shards": [],
      "caused_by": {
         "type": "class_cast_exception",
         "reason": "org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.StringTerms$Bucket"
      }
   },
   "status": 503
}
```
</comment><comment author="clintongormley" created="2015-08-06T09:32:58Z" id="128308665">@xuzha would still be nice to mention the actual field name
</comment><comment author="HarishAtGitHub" created="2015-08-11T17:51:31Z" id="129990604">@clintongormley , I am working on this first elastic search issue of mine . I was not able to exactly do what you asked for but I made the exception little more understandable .
Is the following ok ?

``` javascript
{  
   "error":{  
      "root_cause":[  

      ],
      "type":"reduce_search_phase_exception",
      "reason":"[reduce] ",
      "phase":"query",
      "grouped":true,
      "failed_shards":[  

      ],
      "caused_by":{  
            "type":"bucket_type_mis_match_exception",
            "reason":"Bucket types are Incompatible",
             "caused_by":{  
                      "type":"incomparable_exception",
                       "reason":"The Buckets cannot be compared as one is of type LongTerms and the other is of type StringTerms",
                                  "caused_by":{  
                                              "type":"class_cast_exception",
                                                "reason": "org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.StringTerms$Bucket"
            }
         }
      }
   },
   "status":503
}
```

I introduced two exceptions IncomparableException.java and BucketTypeMisMatchException.java,
so as to better understand exceptions instead of just throwing class Cast exception which can be anything.  I threw IncomparableException in StringTerms.java where the comparison takes place and failed.
and caught the exception at InternalTerms at line : ordered.insertWithOverflow(b); which caused the comparision.

I wanted to achieve the goal "mention the actual field name" goal  but I don't know where to get actual field name from as the exception happens in InternalTerms level at which level I don't know how to find the field name.May be I am missing something. 

I will debug further to achieve "mention the actual field name" goal ..

Before that can you tell me if the new error message makes it a little bit more readable and understanding ? . I made the exceptions in such a way that at least people who have read the internal reading from https://www.elastic.co/guide/en/elasticsearch/reference/1.6/search-aggregations-bucket-terms-aggregation.html can understand.

Please correct me if I am wrong .
I am working to make it perfect ..

[I found that even https://github.com/elastic/elasticsearch/issues/12675 is connected to this as per the basic idea of the issue .]
</comment><comment author="clintongormley" created="2015-08-11T18:18:23Z" id="130001766">@HarishAtGitHub that sounds like progress!  One thing to mention is that we have reduced the number of exceptions in master, so we wouldn't  be keen on adding new ones.  I'm guessing you're developing against 1.7?  Rather target master.  If you want to open a PR to get some feedback about what you've done so far, feel free.  May save some development time.
</comment><comment author="HarishAtGitHub" created="2015-08-11T18:31:52Z" id="130005619">I am working on my [fork](https://github.com/HarishAtGitHub/elasticsearch) which is just 62 commits behind actual master . 
and my build says elasticsearch-2.0.0-beta1-SNAPSHOT . so I think it is almost new ...
yes I saw many exceptions like [here](https://gist.github.com/HarishAtGitHub/ae7fc8a69563964e2b97)(ya agreed that's a huge number 154) , but I found nothing suited my requirement , so went on with a new one .
may be I am wrong, I will revisit and try to live with existing one's if possible .

I will clean up my small work and will open a PR by tomorrow ..
</comment><comment author="HarishAtGitHub" created="2015-08-12T08:13:51Z" id="130212530">@clintongormley as per your suggestion . I have made a pull request https://github.com/elastic/elasticsearch/pull/12821 with a detailed commit log, explaining why I did it like this .

this is just PATCH-1 . this will be extended so that other terms can also handle the exceptions on comparing the same way ...

Please let me know if the approach is ok, so that I can continue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Synced flush not taking very well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12134</link><project id="" key="" /><description>I _think_ we're talking about a bug here: https://discuss.elastic.co/t/synced-flush-causes-node-to-restart/24220

It could be that we're both just having trouble but it seems like for some shards synced flush just claims that there are pending operations all the time. Its like reference counting is off or something. I've turned up the logging to 11 and I'll let the system run overnight and then try another synced flush again and hopefully will have something more concrete.

In the mean time it'd be nice to be able to get more insight into the IndexWriter for a shard specifically which condition is causing it to say it has pending operations.
</description><key id="93891086">12134</key><summary>Synced flush not taking very well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2015-07-08T21:09:52Z</created><updated>2015-07-30T12:24:57Z</updated><resolved>2015-07-30T12:24:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-09T08:31:10Z" id="119873559">I replied to the discuss list... I think it's a bug on 1.x though...  master should be ok here
</comment><comment author="mikemccand" created="2015-07-09T09:05:22Z" id="119881555">LGTM
</comment><comment author="nik9000" created="2015-07-29T00:55:30Z" id="125793374">Can we close this now that that pull request is merged?
</comment><comment author="clintongormley" created="2015-07-30T12:24:57Z" id="126306361">Hmm github seems to not be closing issues properly..  Closed by #12146
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] Failure retrieving fields from bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12133</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_centos/5592/consoleText

Related to https://github.com/elastic/elasticsearch/pull/12114

```
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=BCBC09EB75B9088F -Dtests.class=org.elasticsearch.test.rest.Rest5Tests -Dtests.slow=true -Dtests.method="test {yaml=bulk/40_fields/Fields}" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=be -Dtests.timezone=Australia/Darwin -Dtests.rest.blacklist=cat.recovery/10_basic/*
FAILURE 0.31s J0 | Rest5Tests.test {yaml=bulk/40_fields/Fields} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [bulk] returned [400 Bad Request] [{"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"Malformed action/metadata line [1], expected a simple value for field [fields] but found [START_ARRAY]"}],"type":"illegal_argument_exception","reason":"Malformed action/metadata line [1], expected a simple value for field [fields] but found [START_ARRAY]"},"status":400}]
   &gt;    at __randomizedtesting.SeedInfo.seed([BCBC09EB75B9088F:34E83631DB456577]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:99)
   &gt;    at org.elasticsearch.test.rest.ElasticsearchRestTestCase.test(ElasticsearchRestTestCase.java:373)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="93851905">12133</key><summary>[CI] Failure retrieving fields from bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2015-07-08T17:59:16Z</created><updated>2015-07-08T18:01:23Z</updated><resolved>2015-07-08T18:01:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-08T18:01:00Z" id="119679240">Fixed by 799e801
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TEST] Add REST awaitsFix for bulk fields test</comment></comments></commit></commits></item><item><title>PluginManager: Support basic auth against proxies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12132</link><project id="" key="" /><description>In order to support basic auth against proxies, this
commit introduces support for this and also adds a
`--proxy` argument for the plugin manager.

it also changes the default authenticator (which is used by HttpUrlConnection) and adds the auth info, in case it is used. As the PluginManager is only loaded on `bin/plugin` I do not think this is a problem, but feel free to correct me here.

Closes #11001 

**Important**: This PR adds a new dependency for littleproxy, which in turn has a dependency on netty 4. This is not a problem from any classpath point of view as `org.jboss` is used in netty3 and `io.netty` in netty 4. However as I did not want to reimplement HTTP proxy functionality for now I opted on this. We could potentially replace this with Jetty or roll our own hopefully correct proxy impl. Would be happy if there are any opinions about this. Another problem is, that these tests only work with the security manager disabled.

TODO: Documentation, put the dependency into the right pom (only makes sense if there is consensus that this is the right testing approach). Add `@network` annotations
</description><key id="93838119">12132</key><summary>PluginManager: Support basic auth against proxies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label></labels><created>2015-07-08T16:49:17Z</created><updated>2016-03-10T11:16:43Z</updated><resolved>2016-03-10T11:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-03T13:39:21Z" id="127236963">ok, I removed the dependency on netty4 and littleproxy but just having a small dumb nett-based logging HTTP server, that checks if certain headers are configured appropriately... feels much better now
</comment><comment author="jpountz" created="2015-08-10T13:10:51Z" id="129438116">LGTM
</comment><comment author="spinscale" created="2015-08-10T14:02:35Z" id="129465228">given the discussion around [#12445](https://github.com/elastic/elasticsearch/pull/12445) and supporting basic auth for SSL only, do you think we should do the same here?
</comment><comment author="clintongormley" created="2015-08-10T14:10:45Z" id="129468618">@spinscale yes
</comment><comment author="spinscale" created="2015-08-11T15:05:55Z" id="129921384">tinkered around with this, but currently lacking a smart idea how to test SSL+Basic auth with proxies (as it is not forwarding based as with plain HTTP, otherwise MITM would be possible, which is not)... good ideas welcome.. will move to other 2.0 issues for now
</comment><comment author="dadoonet" created="2015-08-18T09:45:22Z" id="132146122">Just linking here to the documentation which needs to be updated within this PR: https://github.com/elastic/elasticsearch/blob/master/docs/plugins/plugin-script.asciidoc#proxy-settings
</comment><comment author="clintongormley" created="2016-03-10T11:16:43Z" id="194796045">This PR is out of date. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed typos in examples on common-terms-query.asciidoc. JSON was inva…</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12131</link><project id="" key="" /><description>…lid before
</description><key id="93826104">12131</key><summary>Fixed typos in examples on common-terms-query.asciidoc. JSON was inva…</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jakommo</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T15:59:19Z</created><updated>2015-07-09T07:02:07Z</updated><resolved>2015-07-09T07:01:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-09T07:02:01Z" id="119847401">thanks @jakommo !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12131 from jakommo/master</comment></comments></commit></commits></item><item><title>Remove mapper references from Engines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12130</link><project id="" key="" /><description>This was previously necessary with MapperAnalyzer, but there are no more uses.
</description><key id="93823257">12130</key><summary>Remove mapper references from Engines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T15:46:40Z</created><updated>2015-07-09T15:07:59Z</updated><resolved>2015-07-08T15:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T15:56:59Z" id="119634361">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file></files><comments><comment>Merge pull request #12130 from rjernst/fix/doc-mapper-in-engine</comment></comments></commit></commits></item><item><title>Expose Lucene's new TopTermsBlendedFreqScoringRewrite.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12129</link><project id="" key="" /><description>This rewrite method is interesting because it computes scores as if all terms
had the same frequencies, which avoids disappointments with ranking when a fuzzy
query ranks typos first given that they are less frequent than the correct term.
</description><key id="93798568">12129</key><summary>Expose Lucene's new TopTermsBlendedFreqScoringRewrite.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T14:07:35Z</created><updated>2015-07-09T14:17:40Z</updated><resolved>2015-07-08T14:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T14:12:59Z" id="119592993">Note that I also cleaned up how rewrite methods are parsed given that `constant_score_auto` and `constant_score_filter` have been deprecated in favour of `constant_score`. And used ParseField instead of `if (underscore_case_string.equals(rewriteMethod) || camelCaseString.equals(rewriteMethod))`.
</comment><comment author="javanna" created="2015-07-08T14:49:35Z" id="119609737">LGTM
</comment><comment author="clintongormley" created="2015-07-09T14:16:15Z" id="119997323">awesome!
</comment><comment author="jpountz" created="2015-07-09T14:17:40Z" id="119997797">@clintongormley the thanks need to go to @markharwood who did the hard work!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Merge pull request #12129 from jpountz/enhancement/expose_blended_rewrite</comment></comments></commit></commits></item><item><title>[plugins] move integration tests to REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12128</link><project id="" key="" /><description>follow up of #12091
</description><key id="93784587">12128</key><summary>[plugins] move integration tests to REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>:Plugin Analysis Smartcn</label><label>:Plugin Analysis Stempel</label><label>:Plugin Lang JS</label><label>:Plugin Lang Python</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T13:05:13Z</created><updated>2015-07-08T14:23:22Z</updated><resolved>2015-07-08T13:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-08T13:16:28Z" id="119575180">looks good, thank you
</comment><comment author="dadoonet" created="2015-07-08T13:33:41Z" id="119580308">@tlrx Note that I did not change the `delete-by-query` plugin. Do you think you could remove all Java Integration Tests in favor of REST tests? May be it's not doable? 
</comment><comment author="tlrx" created="2015-07-08T13:41:42Z" id="119582655">@dadoonet I don't get why we should move all `delete-by-query` integration tests to REST tests, they don't have the same purpose.  `delete-by-query` already has few REST tests that checks the plugin works during integration tests. 

Also some integration tests like running concurrent delete-by-queries will be hard to do in a REST test. So I'm +1 for keeping tests like they are :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clean up handling of missing values when merging shard results on the coordinating node.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12127</link><project id="" key="" /><description>Today shards are responsible for producing one sort value per document, which
is later used on the coordinating node to resolve the global top documents.
However, this is problematic on string fields with
`missing: _first, order: desc` or `missing: _last, order: asc` given that there
is no such thing as a string that compares greater than any other string. Today
we use a string containing a single code point which is the maximum allowed code
point but this is a hack: instead we should inform the coordinating node that
the document had no value and let it figure out how it should be sorted
depending on whether missing values should be sorted first or last.

Close #9155
</description><key id="93781800">12127</key><summary>Clean up handling of missing values when merging shard results on the coordinating node.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T12:50:16Z</created><updated>2015-07-10T15:40:46Z</updated><resolved>2015-07-09T06:55:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T16:00:55Z" id="119635474">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefFieldComparatorSource.java</file><file>core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file></files><comments><comment>Merge pull request #12127 from jpountz/fix/sort_merge</comment></comments></commit></commits></item><item><title>Cleanup ShardRoutingState uses and hide implementation details of ClusterInfo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12126</link><project id="" key="" /><description /><key id="93778719">12126</key><summary>Cleanup ShardRoutingState uses and hide implementation details of ClusterInfo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T12:38:53Z</created><updated>2015-07-09T14:07:40Z</updated><resolved>2015-07-08T12:54:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-08T12:39:30Z" id="119562114">@kimchy can you take a look
</comment><comment author="kimchy" created="2015-07-08T12:44:25Z" id="119565600">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consolidate ShardRouting construction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12125</link><project id="" key="" /><description>Simplify and consolidate ShardRouting construction. Make sure that there is really only one place it gets created, when a shard is first created in unassigned state, and from there on, it is either copy constructed or built internally as a target for relocation.
This change helps make sure within our codebase data carries over by the ShardRouting is not lost as the shard goes through transitions, and can help simplify the addition of more data on it (like uuid).
For testing, a centralized TestShardRouting allows to create testable versions of ShardRouting, that are not needed to be as strict as the non test codebase. This can be cleanup more later on, but it is a good start.
</description><key id="93767874">12125</key><summary>Consolidate ShardRouting construction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T11:35:13Z</created><updated>2015-07-09T14:01:03Z</updated><resolved>2015-07-08T12:16:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-07-08T11:36:04Z" id="119546774">@s1monw can you have a look?
</comment><comment author="s1monw" created="2015-07-08T11:52:33Z" id="119549038">LGTM - left a tiny comment... feel free to push after fixing
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/ShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/TestShardRouting.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestBase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/PriorityComparatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushUnitTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file></files><comments><comment>Consolidate ShardRouting construction</comment><comment>Simplify and consolidate ShardRouting construction. Make sure that there is really only one place it gets created, when a shard is first created in unassigned state, and from there on, it is either copy constructed or built internally as a target for relocation.</comment><comment>This change helps make sure within our codebase data carries over by the ShardRouting is not lost as the shard goes through transitions, and can help simplify the addition of more data on it (like uuid).</comment><comment>For testing, a centralized TestShardRouting allows to create testable versions of ShardRouting, that are not needed to be as strict as the non test codebase. This can be cleanup more later on, but it is a good start.</comment><comment>closes #12125</comment></comments></commit></commits></item><item><title>Fix PrefixQueryBuilder to support an Object value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12124</link><project id="" key="" /><description>The parser takes an Object value, so should the builder.

Relates to #12032
</description><key id="93765303">12124</key><summary>Fix PrefixQueryBuilder to support an Object value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Java API</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T11:17:32Z</created><updated>2015-07-13T13:15:42Z</updated><resolved>2015-07-13T13:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-07-13T13:15:41Z" id="120923863">Closing same reasoning as #12076 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RangeQuery on date field cannot be nested inside SpanMultiTermQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12123</link><project id="" key="" /><description>This PR explains the issue in #12122 by adding an integration test to SearchQueryTests. The additional query setup triggers the following internal parsing exception.

```
nested: SearchParseException[failed to parse search source [{"query":{"span_or":{"clauses":[{"span_multi":{"match":{"range":{"date":{"from":"2010-01-01","to":"2011-01-01","include_lower":true,"include_upper":true}}}}}]}}}]]; nested: QueryParsingException[spanMultiTerm [match] must be of type multi term query]; 
```

This PR only adds the test code for #12122 since I have no idea for a fix yet. Mainly to illustrate the problem and discuss solutions.
</description><key id="93758305">12123</key><summary>RangeQuery on date field cannot be nested inside SpanMultiTermQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>bug</label><label>won't fix</label></labels><created>2015-07-08T10:36:02Z</created><updated>2016-03-11T11:51:12Z</updated><resolved>2015-08-11T09:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-08T11:35:13Z" id="119546663">The only reason `LateParsingQuery` exists is for the percolator... Because queries are parsed at index time and we want to resolve `now` at percolate time. We could hack to make it work here (just invoking rewrite in query is instance of `LateParsingQuery` in `SpanMultiTermQueryParser#parse`) but that doesn't feel right to me. 

Rather what I'm leaning towards to, is to just forbid the usage of `now` in the percolator. This way we don't need to LateParsingQuery work around any more. (unless there is a better solution here)
</comment><comment author="cbuescher" created="2015-07-08T15:25:39Z" id="119624489">Thanks, I have the feeling that the `now` option in percolator is quiet useful and people wouldn't like removing it. What are the other downsides of catching this special case in the parsing step and then rewriting it?
</comment><comment author="martijnvg" created="2015-07-08T18:37:01Z" id="119692124">&gt; Thanks, I have the feeling that the now option in percolator is quiet useful and people wouldn't like removing it.

I have the same feeling :(

&gt; What are the other downsides of catching this special case in the parsing step and then rewriting it?

It adds complexity and the LateQueryParsing workaround is leaking to other places in the code base, but if we 're okay with catching this special case, then I think we can go for it. We should just put a big comment on top of that if statement.

This if statement should make the test pass:

``` java
if (subQuery instanceof DateFieldMapper.DateFieldType.LateParsingQuery) {
   subQuery = ((DateFieldMapper.DateFieldType.LateParsingQuery) subQuery).rewrite(null);
}
```

(Also LateParsingQuery needs to be made public)
</comment><comment author="cbuescher" created="2015-07-09T14:15:21Z" id="119997052">Okay, such SpanMultiTermQuery (with nestes date RangeQuery) would not work with `now` in percolator then, or rather have `now` fixed at the moment of rewrite?
</comment><comment author="cbuescher" created="2015-07-10T14:01:32Z" id="120416155">Tried to make test pass with the suggested workaround, but now test complains with `java.lang.IllegalStateException: field "date" was indexed without position data; cannot run SpanQuery [...] at org.apache.lucene.search.spans.SpanWeight.scorer(SpanWeight.java:98)` which makes perfect sense because I cannot see why a date field would have positions. Maybe it makes sense to not allow this kind of range query inside a Span Query and we should reject it? One would have to look at the mapping of the field though. I guess Range queries on numeric fields in this context also don't make much sense?
</comment><comment author="javanna" created="2015-07-10T14:17:16Z" id="120419738">I agree @cbuescher probably disallowing it is the best way forward at least for now
</comment><comment author="cbuescher" created="2015-08-11T09:09:49Z" id="129783368">On the query refactoring branch we now decided to reject SpanMultiTermQueries that wrap inner queries that don't produce MultiTerm lucene queries like the date field RangeQuery, so I'm closing this PR.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RangeQuery on date field doesn't work inside SpanMultiTermQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12122</link><project id="" key="" /><description>SpanMultiTermQuery should wrap around any MultiTermQuery, including all variations on RangeQuery. However, when used on a field that has a date mapping, RangeQueryParser will return a LateParsingQuery via DateFieldMapper#rangeQuery(). Unfortunately LateParsingQuery doesn't extend MultiTermQuery, causing the later type check in SpanMultiTermQueryParser to fail.
</description><key id="93756827">12122</key><summary>RangeQuery on date field doesn't work inside SpanMultiTermQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>won't fix</label></labels><created>2015-07-08T10:28:13Z</created><updated>2015-08-11T09:16:14Z</updated><resolved>2015-08-11T09:15:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-07-08T10:29:15Z" id="119534231">I have added an integration test to SearchQueryTests that triggers this behaviour which I will add in a separate PR.
</comment><comment author="javanna" created="2015-07-08T10:58:05Z" id="119540894">good catch @cbuescher given that SpanMultiTermQueryBuilder used to be a simple marker interface, I wonder what happens if LateParsingQuery implements it (or extends from it)? Would that fix it or cause more problems?
</comment><comment author="cbuescher" created="2015-07-08T11:19:30Z" id="119544683">LateParsingQuery is a Lucene query, so it's not part of the ES query builder hierarchy. It would have to extend org.apache.lucene.search.MultiTermQuery. Tried that but then a couple of things need to be changed, not sure about all the steps needed there and if that might be the right thing. 
</comment><comment author="cbuescher" created="2015-08-11T09:15:59Z" id="129785148">After revisiting this, I think that the current behaviour of the SpanMultiTermQuery parser of throwing a QueryParsingException when the wrapped query is a range query on a date field makes sense. As noted in #12123 even if we rewrite LateParsingQuery, the target date field does not contain positional data, so this kind of query shouldn't work. Closing this with as 'won't fix'
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>sort by script_file using java client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12121</link><project id="" key="" /><description>I can use script_file query.

``` json
{
     "sort" : {
     "_script" : {
         "script_file" : "stationdistance",
             "lang" : "javascript",
             "type" : "number",
             "params" : {
                 "lat" : 39.915631,
                 "lon" : 116.581696
             },
             "order" : "asc"
         }
     }
 }
```

I got correct result,In this way.

When using java client, It have errors. :

``` java
 ScriptSortBuilder stationSort = SortBuilders.scriptSort("stationdistance", "number").setParams(depParams)
                    .order(SortOrder.ASC);
            SearchResponse searchResponse = client.prepareSearch(index)
                    .setTypes(type)
                    .setQuery(queryBuilder)
                    .addSort(stationSort)
                    .setSize(size)
                    .execute().actionGet();

```

it has a exception : 

``` java
ScriptException[scripts of type [inline], operation [search] and lang [groovy] are disabled]; 
```

In java, how I use script_file? I can not find the api supported script_file

Thank you for your help.
Best regards,
</description><key id="93727661">12121</key><summary>sort by script_file using java client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mouzt</reporter><labels /><created>2015-07-08T08:07:21Z</created><updated>2015-07-08T09:07:20Z</updated><resolved>2015-07-08T09:07:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-08T09:07:19Z" id="119507910">Please use discuss.elastic.co for questions. We can help you there.

BTW have a look at the upcoming documentation for 2.0.0 here: https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/java-specialized-queries.html#java-query-dsl-script-query
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>failed to sync translog(1.5.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12120</link><project id="" key="" /><description>[2015-07-08 16:00:53,447][WARN ][index.engine             ] [ES48225] [apksigs2015][32] failed to sync translog
[2015-07-08 16:00:53,451][WARN ][indices.cluster          ] [ES48225] [[apksigs2015][32]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [apksigs2015][32] failed to recover shard
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:290)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.translog.TranslogCorruptedException: translog corruption while reading from stream
        at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:72)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:260)
        ... 4 more
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No type mapped for [0]
        at org.elasticsearch.index.translog.Translog$Operation$Type.fromId(Translog.java:237)
        at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:66)
        ... 5 more
</description><key id="93727228">12120</key><summary>failed to sync translog(1.5.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">287400117</reporter><labels /><created>2015-07-08T08:05:33Z</created><updated>2015-08-07T09:10:22Z</updated><resolved>2015-07-08T08:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-08T08:08:37Z" id="119486496">this is fixed in the upcoming 2.0 this is unfortunately expected to see these kind of corruptions on 1.5
</comment><comment author="287400117" created="2015-07-08T08:10:41Z" id="119486970">Now, how do I fix it
</comment><comment author="kuduk" created="2015-08-07T06:33:40Z" id="128611572">I resolved a similar problem caused by out of disk space. But in my log there are also something like that:

[2015-08-07 08:16:36,817][WARN ][cluster.action.shard     ] [Lady Octopus] [logstash-2015.08.04][3] received shard failed for [logstash-2015.08.04][3], node[mM2oHJ1ERzSknGsLlSbanQ], [P], s[INITIALIZING], indexUUID [_KQOGo3tQj6iRxYBkZk7TQ], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[logstash-2015.08.04][3] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: ElasticsearchIllegalArgumentException[No version type match [95]]; ]]

so I could delete corrupted indicies in /var/lib/elasticsearch/elasticsearch/nodes/0/indices
</comment><comment author="287400117" created="2015-08-07T06:41:19Z" id="128613205">I also solved the problem, By removing translog, But some data may be lost
</comment><comment author="kuduk" created="2015-08-07T09:10:22Z" id="128648784">In my case i was lucky i deleted only indicies, my data stay on logstash :) 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filters-Aggregation does not work with children-aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12119</link><project id="" key="" /><description>Hello,

I don't know whether this is a bug, a feature or I did not understand something, but consider the following mapping/data:

```
DELETE /test

PUT /test
{
    "mappings": {
        "document": {
            "properties": {
                "a": {
                    "type": "string"
                }
            }
        },
        "subdocument": {
            "_parent": {
               "type": "document"
            }, 
            "properties": {
                "str": {
                    "type": "string"
                },
                "int": {
                    "type": "integer"
                }
            }
        }
    }
}
POST /test/document/1
{
    "a": "x"   
}
POST /test/subdocument/1?parent=1
{
    "str": "a",
    "int": 1 
}

```

If I try to aggregate the data with a combination of a children-aggregation and a filters-aggregation I get results, which seem wrong to me:

```
POST /test/document/_search
{
   "query": {
      "match_all": {}
   },
   "aggregations": {
      "subdocuments": {
         "children": {
            "type": "subdocument"
         },
         "aggregations": {
            "f": {
               "filters": {
                  "filters": {
                     "0": {
                        "term": {
                           "int": 1
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```

gives back

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "document",
            "_id": "1",
            "_score": 1,
            "_source": {
               "a": "x"
            }
         }
      ]
   },
   "aggregations": {
      "subdocuments": {
         "doc_count": 1,
         "f": {
            "buckets": {
               "0": {
                  "doc_count": 0
               }
            }
         }
      }
   }
}
```

If I filter on a string-field I get some results as expected:

```
POST /test/document/_search
{
   "query": {
      "match_all": {}
   },
   "aggregations": {
      "subdocuments": {
         "children": {
            "type": "subdocument"
         },
         "aggregations": {
            "f": {
               "filters": {
                  "filters": {
                     "0": {
                        "term": {
                           "str": "a"
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```

gives:

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "document",
            "_id": "1",
            "_score": 1,
            "_source": {
               "a": "x"
            }
         }
      ]
   },
   "aggregations": {
      "subdocuments": {
         "doc_count": 1,
         "f": {
            "buckets": {
               "0": {
                  "doc_count": 1
               }
            }
         }
      }
   }
}
```

Anyhow, the filters-aggregations with a filter on the int-field is working without the children-aggregation:

```
POST /test/subdocument/_search
{
   "query": {
      "match_all": {}
   },
   "aggregations": {
      "f": {
         "filters": {
            "filters": {
               "0": {
                  "term": {
                     "int": 1
                  }
               }
            }
         }
      }
   }
}

{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "subdocument",
            "_id": "1",
            "_score": 1,
            "_source": {
               "str": "a",
               "int": 1
            }
         }
      ]
   },
   "aggregations": {
      "f": {
         "buckets": {
            "0": {
               "doc_count": 1
            }
         }
      }
   }
}
```

I'm using ES 1.6. 
</description><key id="93723951">12119</key><summary>Filters-Aggregation does not work with children-aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlmnw</reporter><labels /><created>2015-07-08T07:55:23Z</created><updated>2015-07-08T08:32:49Z</updated><resolved>2015-07-08T08:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlmnw" created="2015-07-08T08:32:28Z" id="119493593">Solved it:

```
POST /test/document/_search
{
   "query": {
      "match_all": {}
   },
   "aggregations": {
      "subdocuments": {
         "children": {
            "type": "subdocument"
         },
         "aggregations": {
            "f": {
               "filters": {
                  "filters": {
                     "0": {
                        "term": {
                           "int": 1
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```

should be:

```
POST /test/document/_search
{
   "query": {
      "match_all": {}
   },
   "aggregations": {
      "subdocuments": {
         "children": {
            "type": "subdocument"
         },
         "aggregations": {
            "f": {
               "filters": {
                  "filters": {
                     "0": {
                        "term": {
                           "subdocument.int": 1
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix license verification on windows?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12118</link><project id="" key="" /><description>I had to disable license verification on windows so that `mvn verify` would work in #12117

I installed strawberry perl and unzip.exe in my path, but the script will consistently compute the wrong sha1 checksum for lucene-analysis-kuromoji jar (i was testing with plugins/analysis-kuromoji)... i've verified (with windows fciv) that the checksum is correct, so there is something not right there.

Not sure its critical to fix this, its just omitted on windows right now.
</description><key id="93715341">12118</key><summary>fix license verification on windows?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>test</label></labels><created>2015-07-08T07:06:53Z</created><updated>2015-08-04T12:23:56Z</updated><resolved>2015-08-04T12:23:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-08T07:38:16Z" id="119471408">for now I think it's fine to just run this on unix though. @clintongormley what do you think?
</comment><comment author="rmuir" created="2015-08-04T12:23:56Z" id="127582060">we turned this on for windows in #12528
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get integration tests working on windows.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12117</link><project id="" key="" /><description>We also run our license checker in `mvn verify`, but there
are problems with checksum calculation on windows there, so I've
disabled the license checker on windows to prevent those false fails.
</description><key id="93711514">12117</key><summary>Get integration tests working on windows.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T06:43:54Z</created><updated>2015-07-08T06:58:14Z</updated><resolved>2015-07-08T06:58:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T06:47:47Z" id="119458920">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12117 from rmuir/windows_integ_tests</comment></comments></commit></commits></item><item><title>Search Scroll witch keep alive prevent shards from closing (during Index closing)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12116</link><project id="" key="" /><description>Say my scroll keep alive time is set to 1 minute.
1. I'm executing a search with that scroll settings
2. Immediately after receiving my search result I'm closing and reopening the index
3. For exactly one minute I get the following error message and my cluster is red
4. after 1 minute my cluster is green.

```
[2015-06-22 14:48:13,846][INFO ][cluster.metadata         ] [Grim Hunter] closing indices [[de_v4]]
[2015-06-22 14:48:22,263][INFO ][cluster.metadata         ] [Grim Hunter] opening indices [[de_v4]]
[2015-06-22 14:48:27,308][WARN ][indices.cluster          ] [Grim Hunter] [[de_v4][2]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [de_v4][2] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [de_v4][2], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-06-22 14:48:27,309][WARN ][cluster.action.shard     ] [Grim Hunter] [de_v4][2] received shard failed for [de_v4][2], node[iTOuyuGSRLab6ZZxwnhb3g], [P], s[INITIALIZING], indexUUID [jsOkLa-_Q8GSPkKAXXfsoQ], reason [shard failure [failed to create shard][IndexShardCreationException[[de_v4][2] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [de_v4][2], timed out after 5000ms]; ]]
[2015-06-22 14:48:32,309][WARN ][indices.cluster          ] [Grim Hunter] [[de_v4][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [de_v4][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [de_v4][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more

[... trimmed ...]

[2015-06-22 14:49:17,414][WARN ][cluster.action.shard     ] [Grim Hunter] [de_v4][0] received shard failed for [de_v4][0], node[iTOuyuGSRLab6ZZxwnhb3g], [P], s[INITIALIZING], indexUUID [jsOkLa-_Q8GSPkKAXXfsoQ], reason [shard failure [failed to create shard][IndexShardCreationException[[de_v4][0] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [de_v4][0], timed out after 5000ms]; ]]
[2015-06-22 14:49:17,414][WARN ][cluster.action.shard     ] [Grim Hunter] [de_v4][3] received shard failed for [de_v4][3], node[iTOuyuGSRLab6ZZxwnhb3g], [P], s[INITIALIZING], indexUUID [jsOkLa-_Q8GSPkKAXXfsoQ], reason [master [Grim Hunter][iTOuyuGSRLab6ZZxwnhb3g][u-excus][inet[/192.168.1.181:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]
```

_Update:_
Today I adjusted the logging settings and briefly before the cluster is getting green:

```
[2015-06-23 12:21:00,100][DEBUG][search                   ] [Prime Mover] freeing search context [1], time [1435054860015], lastAccessTime [1435054776777], keepAlive [60000]
[2015-06-23 12:21:00,101][DEBUG][search                   ] [Prime Mover] freeing search context [2], time [1435054860015], lastAccessTime [1435054776777], keepAlive [60000]
```

btw: [clearing](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html#_clear_scroll_api) (manually) all scroll ids during closing / opening process solves the problem (as temporary workaround); cluster state will be green immediately after reopening the index.

```
curl -XDELETE localhost:9200/_search/scroll/_all
```

Tested with Elasticsearch v1.6.0
</description><key id="93710003">12116</key><summary>Search Scroll witch keep alive prevent shards from closing (during Index closing)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">cleemansen</reporter><labels><label>:Scroll</label><label>bug</label></labels><created>2015-07-08T06:37:29Z</created><updated>2015-07-10T12:37:42Z</updated><resolved>2015-07-10T12:37:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cleemansen" created="2015-07-08T06:37:57Z" id="119455628">Similar issue see #8940.
</comment><comment author="clintongormley" created="2015-07-10T09:22:05Z" id="120315537">We should close all search contexts when closing an index in order to abort the scroll.
</comment><comment author="martijnvg" created="2015-07-10T09:33:20Z" id="120319529">The `SearchService` class maintains the open search contexts (for all searches and shards on a node). When we close an index, we just close the shard, but we check `SearchService` and clear search contexts of shards being closed. If do this, then this should fix the problem.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Only clear open search ctx if the index is delete or closed via API</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/test/java/org/elasticsearch/search/scroll/SearchScrollTests.java</file></files><comments><comment>Free all pending search contexts if index is closed or removed</comment></comments></commit></commits></item><item><title>Refactors ScriptQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12115</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="93708318">12115</key><summary>Refactors ScriptQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-08T06:31:00Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-24T10:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-11T06:52:17Z" id="120588099">left a small comment, looks good besides that
</comment><comment author="alexksikes" created="2015-07-13T16:38:25Z" id="120988963">@javanna I updated the PR and rebased it. Thanks for the review.
</comment><comment author="alexksikes" created="2015-07-15T09:35:23Z" id="121549932">@javanna Thanks for the review. I updated the PR, please see my response to your comment.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java</file></files><comments><comment>Refactors ScriptQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Add support for retrieving fields in bulk updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12114</link><project id="" key="" /><description>This commit adds support to retrieve fields when using the bulk update API. This functionality was previously available for the update API
but not for the bulk update API.

Closes #11527
</description><key id="93696595">12114</key><summary>Add support for retrieving fields in bulk updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Bulk</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T05:03:20Z</created><updated>2015-07-08T16:47:33Z</updated><resolved>2015-07-08T16:25:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-07-08T14:18:13Z" id="119594288">Note that this pull request includes a REST test that fails in current master (reflecting that master does not currently support retrieving fields in bulk updates using any of the three forms) but passes with this pull request.
</comment><comment author="jpountz" created="2015-07-08T14:33:35Z" id="119601000">I left one question about how we should parse this `fields` parameter.
</comment><comment author="jasontedor" created="2015-07-08T15:49:49Z" id="119631881">@jpountz I agree as this would be consistent with how we handle `fields` in the request body in other parts of the API. I pushed a change that reflects this.

I also pushed a change that updates the bulk API documentation.
</comment><comment author="jpountz" created="2015-07-08T15:50:40Z" id="119632052">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file></files><comments><comment>Merge pull request #12114 from jasontedor/feature/11527</comment></comments></commit></commits></item><item><title>Can We Process English With Elasticsearch ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12113</link><project id="" key="" /><description>Hi,
Can we process English with Elasticsearch via Analyzer of something like that?
</description><key id="93696047">12113</key><summary>Can We Process English With Elasticsearch ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LinhTruongVan</reporter><labels /><created>2015-07-08T04:57:52Z</created><updated>2015-07-08T06:36:33Z</updated><resolved>2015-07-08T06:28:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-08T06:28:27Z" id="119452062">Please join us on discuss.elastic.co. We can help there.

BTW use the english analyzer. https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer
</comment><comment author="LinhTruongVan" created="2015-07-08T06:36:33Z" id="119455262">Thanks you ^^
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix tophints noise.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12112</link><project id="" key="" /><description>previously this was done wrong, junit4 ant tasks were brought into
the test classpath. This created jar hell for tests, and also encouraged
people to use _internal_ stuff like its bundled guava in tests.

also the task was set to be lenient and ignore errors. And we were
passing in a messload of unnecessary classpaths to run this. It
only needs the module classpath: the explicit ant dependencies we declare.
</description><key id="93694518">12112</key><summary>Fix tophints noise.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T04:47:32Z</created><updated>2015-07-08T04:56:09Z</updated><resolved>2015-07-08T04:56:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T04:50:29Z" id="119431670">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12112 from rmuir/fixtophints</comment></comments></commit></commits></item><item><title>print PID when starting ES in integ tests too.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12111</link><project id="" key="" /><description>This could be useful for debugging if something crazy happens.
</description><key id="93687888">12111</key><summary>print PID when starting ES in integ tests too.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T04:05:50Z</created><updated>2015-07-08T04:24:38Z</updated><resolved>2015-07-08T04:24:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T04:11:14Z" id="119420772">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12111 from rmuir/printPID</comment></comments></commit></commits></item><item><title>Refactors WildcardQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12110</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="93686546">12110</key><summary>Refactors WildcardQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-08T03:56:48Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-24T12:55:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-11T06:46:34Z" id="120587932">left a few comments, looks good though
</comment><comment author="alexksikes" created="2015-07-13T16:14:25Z" id="120983698">@javanna Thanks for the review. I rebased and addressed all comments.
</comment><comment author="MaineC" created="2015-07-14T08:05:26Z" id="121159864">Left one question, looks good otherwise.
</comment><comment author="alexksikes" created="2015-07-15T09:40:35Z" id="121553486">@MaineC Thanks for the quick review. Should I just go ahead and push this?
</comment><comment author="javanna" created="2015-07-24T12:38:47Z" id="124507021">LGTM besides the two minor comments I left
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java</file></files><comments><comment>Refactors WildcardQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Change JarHell to operate on Path instead of URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12109</link><project id="" key="" /><description>This converts the tracking of jars and classes in JarHell to use
Path objects, instead of URL. This makes for nicer printing
of the underlying path when an error does occur.
</description><key id="93680759">12109</key><summary>Change JarHell to operate on Path instead of URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T03:16:14Z</created><updated>2015-07-10T04:53:55Z</updated><resolved>2015-07-08T03:19:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-08T03:17:33Z" id="119409387">looks good
</comment><comment author="rmuir" created="2015-07-10T04:53:55Z" id="120225297">This change breaks builds like http://build-us-00.elastic.co/job/es_core_master_window-2008/1763 because it inappropriately uses Path.endsWith(".jar"). 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file></files><comments><comment>Merge pull request #12109 from rjernst/fix/jar-hell-path</comment></comments></commit></commits></item><item><title>Add validation of snapshot FileInfo during parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12108</link><project id="" key="" /><description>Making sure that the file info that we read from the snapshot is still sane.
</description><key id="93661810">12108</key><summary>Add validation of snapshot FileInfo during parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T00:41:35Z</created><updated>2015-07-09T15:49:34Z</updated><resolved>2015-07-08T19:52:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-08T16:33:32Z" id="119651783">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Harden integration tests setup/teardown more</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12107</link><project id="" key="" /><description>there is more to do here, but this is already a lot more robust.
- don't clean workspace in teardown, it might be useful for debugging if stuff fails.
- kill ES/clean workspace in setup, so things always work even in the case of ^C
- use pidfile to kill
- fail if kill errors
- refactor a bit more logic here

Part of #12063
</description><key id="93658496">12107</key><summary>Harden integration tests setup/teardown more</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-08T00:17:01Z</created><updated>2015-07-08T01:05:09Z</updated><resolved>2015-07-08T01:05:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-07-08T01:04:06Z" id="119389459">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12107 from rmuir/harden_integration_tests</comment></comments></commit></commits></item><item><title>Tests: Add unit tests for JarHell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12106</link><project id="" key="" /><description /><key id="93656061">12106</key><summary>Tests: Add unit tests for JarHell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T23:55:03Z</created><updated>2015-07-07T23:58:33Z</updated><resolved>2015-07-07T23:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-07T23:57:02Z" id="119376957">+1 This looks fantastic. these tests are pure evil, but i guess we will see from jenkins if JDKs allow it :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/bootstrap/JarHellTests.java</file></files><comments><comment>Merge pull request #12106 from rjernst/tests/jar-hell</comment></comments></commit></commits></item><item><title>Add more detailed error reporting to Snapshot/Restore parsing code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12105</link><project id="" key="" /><description>"Maybe use similar technique as we do in QueryParsingException and also report the location"
</description><key id="93652106">12105</key><summary>Add more detailed error reporting to Snapshot/Restore parsing code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label></labels><created>2015-07-07T23:21:59Z</created><updated>2015-07-07T23:21:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Integ setup should unzip org.elasticsearch:elasticsearch:zip</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12104</link><project id="" key="" /><description>This property is set by maven, and unlike the current hack, during
a multimodule build will be set to the correct thing.

Otherwise today sometimes we run integ tests with outdated ES
artifacts, which makes for incredibly confusing failures.

Closes #12101
</description><key id="93650374">12104</key><summary>Integ setup should unzip org.elasticsearch:elasticsearch:zip</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-07T23:09:50Z</created><updated>2015-07-07T23:15:41Z</updated><resolved>2015-07-07T23:15:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-07T23:10:43Z" id="119370914">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12104 from rmuir/harden_integration_tests</comment></comments></commit></commits></item><item><title>Custom mapping types can no longer be processed by ObjectMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12103</link><project id="" key="" /><description>I compile against master snapshots and use a custom mapping type, called `ref`.

This mapping works well

```
{
  "doc" : {
    "properties": {
      "authorName": {
        "type": "string"
      },
      "authorID": {
        "type": "ref",
        "ref_index": "authors",
        "ref_type": "authors",
        "ref_fields": [
          "author"
        ],
        "to": [
          "dc.creator"
        ]
      },
      "dc": {
        "properties": {
          "creator": {
            "type": "string"
          }
        }
      }
    }
  }
}
```

but with this mapping

```
{
  "doc" : {
    "properties": {
      "author": {
        "properties": {
          "authorName": {
            "type": "string"
          },
          "authorID": {
            "type": "ref",
            "ref_index": "authors",
            "ref_type": "authors",
            "ref_fields": [
              "author"
            ],
            "to": [
              "dc.creator"
            ]
          }
        }
      },
      "dc": {
        "properties": {
          "creator": {
            "type": "string"
          }
        }
      }
    }
  }
}
```

I get an exception since this commit https://github.com/elastic/elasticsearch/commit/2cc0382cf0160f8ef179b62cb4418853382a6901 

```
org.elasticsearch.index.mapper.MapperParsingException: mapping [doc]
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:365)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: Field name [author.authorID] cannot contain '.'
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:283)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:228)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:203)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:315)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:228)
    at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:137)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:208)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:189)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:371)
    at org.elasticsearch.index.mapper.MapperService.assertSerialization(MapperService.java:325)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:316)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:255)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:362)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

But I do not have a field `author.authorID` so the error message seems to be wrong.

How can I fix this? Am I supposed to only use custom mapping types on root object level? Is this a bug? 
</description><key id="93636841">12103</key><summary>Custom mapping types can no longer be processed by ObjectMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2015-07-07T21:47:33Z</created><updated>2015-07-08T15:35:57Z</updated><resolved>2015-07-08T15:35:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T04:39:25Z" id="119428806">Thanks for reporting @jprante! This seems like a bug. I'm investigating.
</comment><comment author="jprante" created="2015-07-08T06:57:18Z" id="119460562">I found a solution, in the custom mapper implementation, this change makes the error disappear:

From

```
    @Override
    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
        builder.startObject(name());
        builder.field("type", CONTENT_TYPE);
```

to 

```
    @Override
    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
        builder.startObject(simpleName());
        builder.field("type", CONTENT_TYPE);
        ...
```

Maybe other (custom) mapper implementations need to serialize to `simpleName()`, too?
</comment><comment author="rjernst" created="2015-07-08T07:11:36Z" id="119465424">Ah yes, that would cause the problem! name() now returns the full path, while simpleName() is the immediate field name and intended for serialization as you are now using it. 
</comment><comment author="jpountz" created="2015-07-08T15:35:56Z" id="119628322">Closing as the problem looks solved to me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow use of bouncycastle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12102</link><project id="" key="" /><description>Today it will fail with securityexception, for example if called by pdfbox.
</description><key id="93636527">12102</key><summary>Allow use of bouncycastle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T21:45:26Z</created><updated>2015-07-08T17:05:51Z</updated><resolved>2015-07-07T22:15:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-07T22:14:27Z" id="119359334">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file></files><comments><comment>Merge pull request #12102 from rmuir/bouncy</comment></comments></commit></commits></item><item><title>is there a bug in plugin info serialization?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12101</link><project id="" key="" /><description>I keep seeing this failing integration tests. run 'mvn verify -Dskip.unit.tests' from the top level a few times to hit it.

Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 830
    at org.jboss.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:261)
    at org.elasticsearch.action.admin.cluster.node.info.PluginInfo.readFrom(PluginInfo.java:131)
    at org.elasticsearch.action.admin.cluster.node.info.PluginInfo.readPluginInfo(PluginInfo.java:125)
    at org.elasticsearch.action.admin.cluster.node.info.PluginsInfo.readFrom(PluginsInfo.java:66)
    at org.elasticsearch.action.admin.cluster.node.info.PluginsInfo.readPluginsInfo(PluginsInfo.java:58)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:219)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readNodeInfo(NodeInfo.java:180)
    at org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse.readFrom(NodesInfoResponse.java:51)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:176)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:137)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</description><key id="93631384">12101</key><summary>is there a bug in plugin info serialization?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-07T21:17:54Z</created><updated>2015-07-07T23:15:39Z</updated><resolved>2015-07-07T23:15:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-07T21:26:49Z" id="119347248">I'm digging in. This could be a bug in integ test build logic too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Integ setup should unzip org.elasticsearch:elasticsearch:zip</comment></comments></commit></commits></item><item><title>Update default value to meters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12100</link><project id="" key="" /><description /><key id="93631203">12100</key><summary>Update default value to meters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jldbasa</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-07-07T21:17:09Z</created><updated>2015-07-09T15:20:00Z</updated><resolved>2015-07-09T15:19:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-08T17:02:36Z" id="119663279">Hi @jldbasa 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jldbasa" created="2015-07-08T17:08:46Z" id="119666101">done
</comment><comment author="clintongormley" created="2015-07-09T15:20:00Z" id="120030728">thanks @jldbasa 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>analysis-phonetic integration tests fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12099</link><project id="" key="" /><description>e.g. http://build-us-00.elastic.co/job/es_core_master_regression/2658/console

```
java.lang.AssertionError: expected [2xx] status code but api [indices.analyze] returned [503 Service Unavailable] 
[{"error":{"root_cause":[{"type":"no_shard_available_action_exception",
"reason":"No shard available for org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@2ed65822]"}],
```
</description><key id="93628361">12099</key><summary>analysis-phonetic integration tests fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-07T21:02:20Z</created><updated>2015-07-07T21:11:52Z</updated><resolved>2015-07-07T21:11:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-07T21:03:50Z" id="119340245">Maybe because as opposed to the other analysis rest tests, it needs to make an index, but needs to wait until the index is yellow at least to run analyze api??
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>wait for yellow in integration tests. closes #12099</comment></comments></commit></commits></item><item><title>Precise and consistent MapperParsingException messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12098</link><project id="" key="" /><description>The "error" field value that is returned for a MappingParserException is inconsistent and imprecise, which makes it difficult to programatically address.  This may be related to #7983.

For example:

``` sh
# dynamically create a mapping
curl -XPOST 'http://localhost:9200/error_test/my_docs' -d '{ "complex_obj": { "data": { "value": { "data": { "name": "alice" } } } } }'
# violate it
curl -XPOST 'http://localhost:9200/error_test/my_docs' -d '{ "complex_obj": { "data": { "value": { "data": "alice" } } } }'
{"error":"MapperParsingException[object mapping for [my_docs] tried to parse field [complex_obj] as object, but got EOF, has a concrete value been provided to it?]","status":400}
# violate it in the same way but with an additional root field
curl -XPOST 'http://localhost:9200/error_test/my_docs' -d '{ "complex_obj": { "data": { "value": { "data": "alice" } } }, "data": "a simple string" }'
{"error":"MapperParsingException[object mapping for [complex_obj] tried to parse field [data] as object, but got EOF, has a concrete value been provided to it?]","status":400}
```

The two error messages returned are inconsistent.  The first square bracket'ed name is the document type in the first instance and in the other is a field name.

The messages also do not indicate the exact field causing the exception.  It is difficult to determine which field does the field name refer to if there are multiple fields with the same name.  It would be more helpful to provide the full dotted path of the field the caused the exception, such as `complex_ob.data.value.data`.
</description><key id="93613950">12098</key><summary>Precise and consistent MapperParsingException messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">f-s-p</reporter><labels /><created>2015-07-07T19:52:19Z</created><updated>2015-07-08T17:13:38Z</updated><resolved>2015-07-08T17:13:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T04:31:47Z" id="119427241">This has to do with how the mappings are structured internally (the type is a fake "root object" mapper). However, in 2.0, all field name references have been changed to use the full path (the dot notation you mention).

As for programatically addressing, I don't know that this can ever be possible. Dealing with mapping errors isn't something that is intended to be done programatically. The mappings try to give as much context information as they can, but the format of the exception message is not in any way stable, and may change at any time.
</comment><comment author="clintongormley" created="2015-07-08T17:13:38Z" id="119667660">In master, both of the above requests return the same exception:

```
"object mapping for [complex_obj.data.value.data] tried to parse field [data] as object, but found a concrete value"
```

closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Is there an analyzer for ancient greek? or anyway I extend the existing analyzer for it?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12097</link><project id="" key="" /><description>I require an Ancient greek analyzer as the existing Greek analyzer is not compatible with the ancient greek.

Is there any analyzer or should I consider extending the analyzer with new language???
</description><key id="93609635">12097</key><summary>Is there an analyzer for ancient greek? or anyway I extend the existing analyzer for it?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apanimesh061</reporter><labels /><created>2015-07-07T19:31:13Z</created><updated>2015-07-08T16:57:59Z</updated><resolved>2015-07-08T16:57:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-08T16:57:58Z" id="119660414">Hi @apanimesh061 

We don't have an analyzer for ancient greek - you may have to come up with something yourself.  Also, try the ICU plugin https://github.com/elastic/elasticsearch-analysis-icu
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>simple query  NullPointerException only on 1.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12096</link><project id="" key="" /><description>This works on 1.5:

```
GET _search
{
  "query": {
    "query_string": {
      "query": "*",
      "fields": [
        "_id"
      ]
    }
  }
}
```

returns:

```
{
    took: 1
    timed_out: false
    _shards: {
        total: 5
        successful: 5
        failed: 0
    }
    hits: {
        total: 0
        max_score: null
        hits: [ ]
    }
}
```

but in elasticsearch 1.6 returns:

{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[f97FCaZyRKiUQ5ApvqkldA][facebook][0]: SearchParseException[[facebook][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"_","fields":["_id"]}}}]]]; nested: NullPointerException; }{[f97FCaZyRKiUQ5ApvqkldA][facebook][1]: SearchParseException[[facebook][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"_","fields":["_id"]}}}]]]; nested: NullPointerException; }{[f97FCaZyRKiUQ5ApvqkldA][facebook][2]: SearchParseException[[facebook][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"_","fields":["_id"]}}}]]]; nested: NullPointerException; }{[f97FCaZyRKiUQ5ApvqkldA][facebook][3]: SearchParseException[[facebook][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"_","fields":["_id"]}}}]]]; nested: NullPointerException; }{[f97FCaZyRKiUQ5ApvqkldA][facebook][4]: SearchParseException[[facebook][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"*","fields":["_id"]}}}]]]; nested: NullPointerException; }]",
    "status": 400
}
</description><key id="93602148">12096</key><summary>simple query  NullPointerException only on 1.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Zincr0</reporter><labels><label>feedback_needed</label></labels><created>2015-07-07T18:50:35Z</created><updated>2015-07-09T15:46:46Z</updated><resolved>2015-07-09T15:46:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Zincr0" created="2015-07-07T19:02:32Z" id="119304481">workaround: 
{"query": {"query_string": {"query": "*"}}}
</comment><comment author="clintongormley" created="2015-07-08T16:56:02Z" id="119658867">Hi @Zincr0 

I'm unable to replicate this on 1.6.0.  What is the mapping of your `facebook` index?  Do you have the full stack trace from the logs?

Your workaround simply searches the `_all` field instead of the `_id` field.
</comment><comment author="Zincr0" created="2015-07-08T18:33:04Z" id="119691153">facebook was an empty index. 
I'm using elasticsearch 1.5.2 now, so i can't replicate the bug anymore. 
I guess you can close this.
</comment><comment author="clintongormley" created="2015-07-09T15:46:46Z" id="120043347">this is fixed in master

closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[phonetic] move integration tests to REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12095</link><project id="" key="" /><description>We can keep only unit tests in plugins instead of starting each time a local node and running tests against it.

Also follow up of #12091
</description><key id="93594753">12095</key><summary>[phonetic] move integration tests to REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Phonetic</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T18:17:22Z</created><updated>2015-07-08T08:07:11Z</updated><resolved>2015-07-08T08:05:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-08T08:02:56Z" id="119483163">+1
</comment><comment author="s1monw" created="2015-07-08T08:03:36Z" id="119483834">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[phonetic] move integration tests to REST tests</comment></comments></commit></commits></item><item><title>Mappings: Restrict unexpected characters from field names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12094</link><project id="" key="" /><description>You can manage to slip unexpected characters into field names, like `\n`, `\0` (`\u0000`), `\t`, `\'`, `\"`, etc. I think that we should prevent this, but the change would create a backwards incompatible restriction.

Big caveat/counter argument: as long as they're escaped, these unexpected characters _are_ valid JSON.
</description><key id="93594299">12094</key><summary>Mappings: Restrict unexpected characters from field names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>breaking</label><label>enhancement</label></labels><created>2015-07-07T18:14:51Z</created><updated>2015-07-08T15:41:29Z</updated><resolved>2015-07-08T15:41:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T15:41:27Z" id="119629916">Duplicate of https://github.com/elastic/elasticsearch/issues/9059
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Give a better exception when a jar contains same classfile twice.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12093</link><project id="" key="" /><description>And ignore the known issue with xmlbeans for now... though it may
cause us issues ultimately: https://issues.apache.org/jira/browse/XMLBEANS-499
</description><key id="93585196">12093</key><summary>Give a better exception when a jar contains same classfile twice.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>PITA</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T17:33:02Z</created><updated>2015-07-08T16:59:50Z</updated><resolved>2015-07-07T20:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-07-07T19:58:43Z" id="119317965">LGTM ... I had no idea a .zip file was allowed to have exactly the same name more than once!
</comment><comment author="rmuir" created="2015-07-07T20:00:37Z" id="119318534">I opened https://issues.apache.org/jira/browse/TIKA-1675 just to try to make the issue more well known at least. I am not sure if tika can remove the dependency but the bug has sat unfixed for a long time.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file></files><comments><comment>Merge pull request #12093 from rmuir/no_fucking_way</comment></comments></commit></commits></item><item><title>Simplify CacheKey used for scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12092</link><project id="" key="" /><description>Replaced the CacheKey class with a static method that returns a String.
</description><key id="93584728">12092</key><summary>Simplify CacheKey used for scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T17:30:07Z</created><updated>2016-02-01T21:04:01Z</updated><resolved>2015-07-07T21:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-07T20:14:52Z" id="119321763">LGTM. Do we have any tests for the cache?
</comment><comment author="jdconrad" created="2015-07-07T20:43:01Z" id="119330826">@rjernst Thanks for the review.  Added a test specifically for this, but previously created tests also offer significant coverage.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file></files><comments><comment>Simplify CacheKey used for scripts</comment></comments></commit></commits></item><item><title>Plugin rest tests for analysis-phonetic, cloud-*, and lang-*</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12091</link><project id="" key="" /><description>Closes #12062
</description><key id="93583115">12091</key><summary>Plugin rest tests for analysis-phonetic, cloud-*, and lang-*</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>test</label></labels><created>2015-07-07T17:21:28Z</created><updated>2015-07-08T11:58:37Z</updated><resolved>2015-07-07T17:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-07T17:25:51Z" id="119275490">looks great! Thank you!
</comment><comment author="dadoonet" created="2015-07-07T18:16:44Z" id="119290061">I have some issues while running Rest tests in phonetic module. 
I did not test other modules yet.

```
mvn clean install -DskipTests ; cd plugins/analysis-phonetic; mvn install -Dtests.rest=true
```

gives 

```
Tests run: 4, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 8.213 sec &lt;&lt;&lt; FAILURE! - in org.elasticsearch.index.analysis.AnalysisPhoneticRestIT
test {yaml=analysis_phonetic/10_metaphone/Metaphone}(org.elasticsearch.index.analysis.AnalysisPhoneticRestIT)  Time elapsed: 4.397 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError: expected [2xx] status code but api [indices.analyze] returned [503 Service Unavailable] [{"error":{"root_cause":[{"type":"no_shard_available_action_exception","reason":"No shard available for [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@26dd825d]"}],"type":"no_shard_available_action_exception","reason":"No shard available for [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@26dd825d]"},"status":503}]
        at __randomizedtesting.SeedInfo.seed([FBEBE5457042DE03:73BFDA9FDEBEB3FB]:0)
        at org.junit.Assert.fail(Assert.java:88)
        at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:99)
        at org.elasticsearch.test.rest.ElasticsearchRestTestCase.test(ElasticsearchRestTestCase.java:373)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1627)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:836)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:872)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:886)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
        at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
        at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
        at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
        at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
        at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:845)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:747)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:781)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:792)
        at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
        at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
        at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
        at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
        at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
        at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
        at java.lang.Thread.run(Thread.java:745)

test {yaml=analysis_phonetic/40_search/Index phonetic content}(org.elasticsearch.index.analysis.AnalysisPhoneticRestIT)  Time elapsed: 1.138 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError: field [hits.total] doesn't match the expected value
Expected: &lt;1&gt;
     but: was &lt;0&gt;
```

Do you guys have the same kind of issue? `no_shard_available_action_exception`
</comment><comment author="dadoonet" created="2015-07-07T21:11:22Z" id="119342293">For the record, opened issue for this https://github.com/elastic/elasticsearch/issues/12099
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptSearchTests.java</file></files><comments><comment>[python] move integration tests to REST tests</comment></comments></commit><commit><files><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptSearchTests.java</file></files><comments><comment>[Javascript] move integration tests to REST tests</comment></comments></commit><commit><files><file>plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/SimplePolishIntegrationTests.java</file></files><comments><comment>[Stempel] move integration tests to REST tests</comment></comments></commit><commit><files><file>plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/SimpleSmartChineseIntegrationTests.java</file></files><comments><comment>[Smartcn] move integration tests to REST tests</comment></comments></commit><commit><files><file>plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/ICUIntegrationTests.java</file></files><comments><comment>[ICU] move integration tests to REST tests</comment></comments></commit><commit><files><file>plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/SimplePhoneticIntegrationTests.java</file></files><comments><comment>[phonetic] move integration tests to REST tests</comment></comments></commit><commit><files><file>plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/AnalysisPhoneticRestIT.java</file><file>plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/CloudAWSRestIT.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/CloudAzureRestIT.java</file><file>plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java</file><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/LangJavaScriptRestIT.java</file><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/LangPythonScriptRestIT.java</file></files><comments><comment>Merge pull request #12091 from clintongormley/plugin_rest_tests</comment></comments></commit></commits></item><item><title>Default fuzzy transpositions to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12090</link><project id="" key="" /><description>This commit defaults fuzzy_transpositions on fuzzy queries to true. This means that by default, tranpositions will now count as a single
edit.

Closes #9278
</description><key id="93582504">12090</key><summary>Default fuzzy transpositions to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T17:18:02Z</created><updated>2015-07-08T00:27:13Z</updated><resolved>2015-07-08T00:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-07T22:36:21Z" id="119364176">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove AbstractFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12089</link><project id="" key="" /><description>AbstractFieldMapper is the only direct base class of FieldMapper.
This change moves all AbstractFieldMapper functionality into
FieldMapper, since there is no need for 2 levels of abstraction.
</description><key id="93563636">12089</key><summary>Remove AbstractFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T15:54:15Z</created><updated>2015-08-13T14:05:26Z</updated><resolved>2015-07-08T04:36:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-07T21:17:21Z" id="119344172">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MetadataFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsTests.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file></files><comments><comment>Merge pull request #12089 from rjernst/refactor/field-mapper-collapse</comment></comments></commit></commits></item><item><title>Update scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12088</link><project id="" key="" /><description>removed underscore for "_doc" to "doc"
</description><key id="93562101">12088</key><summary>Update scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lu3do</reporter><labels /><created>2015-07-07T15:47:25Z</created><updated>2015-07-09T15:28:39Z</updated><resolved>2015-07-09T15:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-08T16:46:07Z" id="119655173">Hi @johnfrederik 

Thanks for the PR. Please could i ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="lu3do" created="2015-07-08T17:53:16Z" id="119677069">signed it. 
</comment><comment author="clintongormley" created="2015-07-09T15:28:34Z" id="120034497">thanks @johnfrederik - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update scripting.asciidoc</comment></comments></commit></commits></item><item><title>Failure during the fetch phase of scan should invoke the failed fetch…</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12087</link><project id="" key="" /><description>… phase handler.

This commit fixes an issue where during a failure in the fetch phase of a scan the wrong failure handler was invoked.

Closes #12086
</description><key id="93559855">12087</key><summary>Failure during the fetch phase of scan should invoke the failed fetch…</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T15:36:55Z</created><updated>2015-07-08T16:43:41Z</updated><resolved>2015-07-07T15:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-07T15:38:06Z" id="119241745">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failure during the fetch phase of scan invokes the wrong failure handler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12086</link><project id="" key="" /><description>A failure during the fetch phase of a scan mistakenly invokes the `onFailedQueryPhase` handler, but should invoke the `onFailedFetchPhase` handler. As such, in cases of failure during the fetch phase both the query and fetch statistics will be wrong.
</description><key id="93558242">12086</key><summary>Failure during the fetch phase of scan invokes the wrong failure handler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>bug</label></labels><created>2015-07-07T15:29:31Z</created><updated>2015-07-07T15:40:16Z</updated><resolved>2015-07-07T15:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Failure during the fetch phase of scan should invoke the failed fetch phase handler.</comment></comments></commit></commits></item><item><title>Calculate artifact checksums in maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12085</link><project id="" key="" /><description>Today we calculate the checksums in a python scritp to do the release.
We can also do this way simpler in maven using an ant task.
</description><key id="93549877">12085</key><summary>Calculate artifact checksums in maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T14:55:02Z</created><updated>2015-07-10T11:31:04Z</updated><resolved>2015-07-10T11:31:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-07-07T15:53:42Z" id="119246881">should we stick with ant here or check if there is a mvn plugin that takes care of all artifacts and checksums those (which should include the rpm at some point).. I know there is a maven checksums plugin, but dont have personal experience with it TBH. Feels like we are sneaking ant in here and then have two things to maintain.
</comment><comment author="dadoonet" created="2015-07-07T15:56:00Z" id="119247513">Well:

https://maven.apache.org/plugins/maven-install-plugin/examples/installing-checksums.html

```
mvn install -DcreateChecksum=true
```

Do we need a plugin?
</comment><comment author="dadoonet" created="2015-07-07T16:22:58Z" id="119259543">If we change in `/pom.xml`:

``` xml
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt;
                    &lt;version&gt;2.5.2&lt;/version&gt;
                    &lt;configuration&gt;
                        &lt;createChecksum&gt;true&lt;/createChecksum&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
```

I think we are done.

If you do it and then run `mvn install`, your `.m2` dir will have:

```
-rw-r--r--  1 dpilato  staff     57019  7 jul 16:55 elasticsearch-2.0.0-SNAPSHOT.pom
-rw-r--r--  1 dpilato  staff    615374  7 jul 18:19 elasticsearch-2.0.0-SNAPSHOT-tests.jar
-rw-r--r--  1 dpilato  staff   9009779  7 jul 18:19 elasticsearch-2.0.0-SNAPSHOT.jar
-rw-r--r--  1 dpilato  staff   5164015  7 jul 18:19 elasticsearch-2.0.0-SNAPSHOT-sources.jar
-rw-r--r--  1 dpilato  staff   9118928  7 jul 18:19 elasticsearch-2.0.0-SNAPSHOT-test-sources.jar
-rw-r--r--  1 dpilato  staff  13247908  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-shaded.jar
-rw-r--r--  1 dpilato  staff  28638178  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.tar.gz
-rw-r--r--  1 dpilato  staff  28643294  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.zip
-rw-r--r--  1 dpilato  staff  28522768  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.deb
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.jar.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.jar.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-tests.jar.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-tests.jar.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-test-sources.jar.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-test-sources.jar.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-sources.jar.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-sources.jar.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-shaded.jar.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT-shaded.jar.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 maven-metadata-local.xml.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 maven-metadata-local.xml.md5
-rw-r--r--  1 dpilato  staff      2033  7 jul 18:20 maven-metadata-local.xml
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.zip.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.zip.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.tar.gz.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.tar.gz.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.pom.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.pom.md5
-rw-r--r--  1 dpilato  staff        40  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.deb.sha1
-rw-r--r--  1 dpilato  staff        32  7 jul 18:20 elasticsearch-2.0.0-SNAPSHOT.deb.md5
```

Is it what we we are looking for? 
</comment><comment author="dadoonet" created="2015-07-07T16:25:14Z" id="119260133">And same for all plugins:

```
ll ~/.m2/repository/org/elasticsearch/plugin/elasticsearch-analysis-kuromoji/2.0.0-SNAPSHOT/
total 9168
-rw-r--r--  1 dpilato  staff     1373  7 jul 16:55 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.pom
-rw-r--r--  1 dpilato  staff    20877  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.jar
-rw-r--r--  1 dpilato  staff    14128  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT-sources.jar
-rw-r--r--  1 dpilato  staff       40  7 jul 18:20 maven-metadata-local.xml.sha1
-rw-r--r--  1 dpilato  staff       32  7 jul 18:20 maven-metadata-local.xml.md5
-rw-r--r--  1 dpilato  staff     1110  7 jul 18:20 maven-metadata-local.xml
-rw-r--r--  1 dpilato  staff       40  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.zip.sha1
-rw-r--r--  1 dpilato  staff       32  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.zip.md5
-rw-r--r--  1 dpilato  staff  4596459  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.zip
-rw-r--r--  1 dpilato  staff       40  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.pom.sha1
-rw-r--r--  1 dpilato  staff       32  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.pom.md5
-rw-r--r--  1 dpilato  staff       40  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.jar.sha1
-rw-r--r--  1 dpilato  staff       32  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.jar.md5
-rw-r--r--  1 dpilato  staff       40  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT-sources.jar.sha1
-rw-r--r--  1 dpilato  staff       32  7 jul 18:20 elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT-sources.jar.md5
```
</comment><comment author="s1monw" created="2015-07-09T18:43:56Z" id="120101748">@dadoonet I mean that solves the problem, I guess we just need to adopt the release script to upload those checksums instead then?
</comment><comment author="dadoonet" created="2015-07-09T19:03:34Z" id="120109338">@s1monw I think so.
</comment><comment author="s1monw" created="2015-07-09T19:39:22Z" id="120122319">@dadoonet I updated the PR
</comment><comment author="dadoonet" created="2015-07-09T19:50:10Z" id="120125115">LGTM. Will you update the release script in another PR?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>make integ testing a bit more picky</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12084</link><project id="" key="" /><description>Especially the fact that if bin/plugin returns an error, we should fail.
</description><key id="93542953">12084</key><summary>make integ testing a bit more picky</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T14:22:40Z</created><updated>2015-07-07T16:20:33Z</updated><resolved>2015-07-07T16:20:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-07T15:59:38Z" id="119248434">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12084 from rmuir/more_robust</comment></comments></commit></commits></item><item><title>Fix RegexpQueryBuilder#maxDeterminizedStates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12083</link><project id="" key="" /><description>Value was improperly set to `true`.

Relates to #11896
</description><key id="93512153">12083</key><summary>Fix RegexpQueryBuilder#maxDeterminizedStates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.6.2</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T11:47:01Z</created><updated>2015-07-27T10:51:29Z</updated><resolved>2015-07-07T11:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-07T11:47:47Z" id="119179634">LGTM
</comment><comment author="alexksikes" created="2015-07-07T12:16:05Z" id="119186756">We misread the variable name in our haste. Reverted by https://github.com/elastic/elasticsearch/commit/4f9855261a60a56ee9e3851d519ab448e5d112a0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a terms aggregator based on ordinals that does not need global ordinals.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12082</link><project id="" key="" /><description>Apart from the `map` execution mode which works on values and only really exists
for scripted terms aggregation, all our execution modes for string fields
leverage global ordinals.

There are two main issues that users who run terms aggregations run into:
either memory usage, and global ordinals help here by having a single count map
instead of one per segment, or rebuilding global ordinals upon refresh, for
which there isn't really any option right now. So I would like to add a new
execution mode which leverages ordinals (on the contrary to `map`) but not
global ordinals.

For now, this new execution mode is called `per_segment` and is never used by
default. I intend to only have it as an escape hatch for some time so that users
who have issues with global ordinals building can experiment with it. Then we
can think about using it by default in some cases.
</description><key id="93505751">12082</key><summary>Add a terms aggregator based on ordinals that does not need global ordinals.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>feature</label></labels><created>2015-07-07T11:10:19Z</created><updated>2015-08-13T14:25:24Z</updated><resolved>2015-07-15T22:35:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-07T11:39:49Z" id="119178611">@jpountz and I discussed this and it seems the main use case for this mode is for using sparse queries where not too many ordinals will be hit and populated. I wonder (though not sure) whether it makes sense to use sparse ordinal arrays and if so, how would that compare with the existing map based execution mode.
</comment><comment author="jpountz" created="2015-07-15T22:35:02Z" id="121768247">Closing for now, will reopen if there is further interest in it, potentially as a plugin.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure logging configuration is loaded in plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12081</link><project id="" key="" /><description>This prevents log4j warnings printed out, when installing a plugin
due to the JarHell class using an ESLogger.

Closes #12064
</description><key id="93504643">12081</key><summary>Ensure logging configuration is loaded in plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T11:02:09Z</created><updated>2015-08-13T13:47:57Z</updated><resolved>2015-07-07T13:00:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-07T11:37:33Z" id="119178314">LGTM 
</comment><comment author="dadoonet" created="2015-07-07T11:45:39Z" id="119179342">I think we also have this issue when running REST tests in plugins BTW. Or when running tests from IntelliJ in plugins.
</comment><comment author="spinscale" created="2015-07-07T12:25:05Z" id="119188878">@dadoonet can you show me how to reproduce that? RestTests are running fine for me, with logging...also in intellij
</comment><comment author="rmuir" created="2015-07-07T12:27:04Z" id="119189255">Looks good. I think @dadoonet refers to the new rest tests mechanism that runs in mvn verify. You can run it with `mvn verify -Dskip.unit.tests` to see. This mechanism unzips the elasticsearch.zip and installs the plugin with bin/plugin, before running rest tests against that external cluster, so thats why you see it there.
</comment><comment author="rmuir" created="2015-07-07T12:31:28Z" id="119190253">and also thanks for tracking this one down :)
</comment><comment author="spinscale" created="2015-07-07T12:55:41Z" id="119195263">ah, so in order to not have the messages occur in `mvn verify`, I had to run `mvn install` before - which is obvious after reading the ant file. If we want to have run this as part of the release, we just shouldnt forget to run that, as we dont do know. will get it in, in a sec
</comment><comment author="rmuir" created="2015-07-07T13:01:51Z" id="119197598">we can remove the `mvn install` requirement so that it uses the "reactor" classpath in the future. it just means using a different property set by the maven build itself from that ant logic.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Import information from an Oracle database</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12080</link><project id="" key="" /><description>Hi!

I read that the rivers plugins have been deprecated. How we import information from an Oracle database?
</description><key id="93501794">12080</key><summary>Import information from an Oracle database</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">antoniogarcez</reporter><labels /><created>2015-07-07T10:46:03Z</created><updated>2015-07-07T13:44:45Z</updated><resolved>2015-07-07T13:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-07T13:44:16Z" id="119205767">There is a jdbc importer standalone application created by @jprante [1] and the Logstash project is working on a jdbc input too.

Please note that we try to keep the Github issues list for project issues such as bugs and feature requests. We have discussion forums available for asking questions like these and getting help using the software: https://discuss.elastic.co

[1] https://github.com/jprante/elasticsearch-jdbc
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>field "xxx" was indexed without position data; cannot run PhraseQuery (term=storing)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12079</link><project id="" key="" /><description>Hi,

I do the next query via Kibana:

```
_type:xxx-dev AND message:'Storing the yyy zzz' AND data.values:*test@email.com*
```

And receive the next response:
![image](https://cloud.githubusercontent.com/assets/839290/8544385/1e3c8efc-24a5-11e5-9766-e7347b6b07a7.png)

However, the message is analyzed, in the template:
![image](https://cloud.githubusercontent.com/assets/839290/8544392/30611274-24a5-11e5-8f81-8ee6ed2190c5.png)

(some time ago it was not_analyzed)
![image](https://cloud.githubusercontent.com/assets/839290/8544440/96bcc7f2-24a5-11e5-81ee-b43102b36b43.png)

If I put the message AND to the end of the query - it works well:

```
_type:xxx-dev AND data.values:*test@email.com* AND message:'Storing the yyy zzz'
```

How I can overcome this?

Regards,
</description><key id="93501373">12079</key><summary>field "xxx" was indexed without position data; cannot run PhraseQuery (term=storing)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PavelPolyakov</reporter><labels /><created>2015-07-07T10:43:01Z</created><updated>2015-07-08T18:03:32Z</updated><resolved>2015-07-08T18:03:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="PavelPolyakov" created="2015-07-08T14:55:21Z" id="119612240">still interested in the answer
</comment><comment author="jpountz" created="2015-07-08T15:51:13Z" id="119632172">Can you provide us with the mappings of this message field on all your types?

The query would look something like `GET /${index_name}/_mapping/field/message`. https://www.elastic.co/guide/en/elasticsearch/reference/1.6/indices-get-field-mapping.html
</comment><comment author="PavelPolyakov" created="2015-07-08T16:14:52Z" id="119643947">@jpountz 

Here it is:

```
{
    "logstash-2015.07.08": {
        "mappings": {
            "zzz": {
                "message": {
                    "full_name": "message",
                    "mapping": {
                        "message": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                }
            },
            "yyy": {
                "message": {
                    "full_name": "message",
                    "mapping": {
                        "message": {
                            "type": "string",
                            "store": true
                        }
                    }
                }
            },
            "xxx": {
                "message": {
                    "full_name": "message",
                    "mapping": {
                        "message": {
                            "type": "string",
                            "store": true
                        }
                    }
                }
            }
        }
    }
}
```

Do you think it happens because of the zzz type?
I would update it, to see what happens tomorrow.
</comment><comment author="jpountz" created="2015-07-08T18:03:32Z" id="119679868">@PavelPolyakov Indeed, it very likely happened because of zzz. Unfortunately this is a known issue on 1.x that all fields of the same index that have the same name need to have consistent mappings. In Elasticsearch 2.x this will be enforced and creating a type with different mappings will raise an error, which will be less trappy.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix example curl command in docs for count query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12078</link><project id="" key="" /><description>The method is GET but should be POST when using POST data with an ES count request.
</description><key id="93500605">12078</key><summary>Fix example curl command in docs for count query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tschubotz</reporter><labels /><created>2015-07-07T10:38:05Z</created><updated>2015-07-07T15:21:07Z</updated><resolved>2015-07-07T15:21:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-07T11:49:39Z" id="119179879">No GET is fine.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>New node created after reboot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12077</link><project id="" key="" /><description>Hi,

I have installed ELK in the same machine and there is no ES cluster configuration.

After rebooting the server a new node appeared. So now, I have Node0 and Node1. Node0 has all the previous logs but now, I can´t get access to it via Kibana. Node1 has the data since the server was restarted.

What can I do?

Regards
</description><key id="93498946">12077</key><summary>New node created after reboot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">behindthefirewalls</reporter><labels /><created>2015-07-07T10:28:38Z</created><updated>2015-07-07T12:44:09Z</updated><resolved>2015-07-07T12:44:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-07T12:44:09Z" id="119192808">Please use discuss.elastic.co. We can help you there.

Closing. Not an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix RegexpQueryBuilder to support an Object value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12076</link><project id="" key="" /><description>The parser takes an Object value, so should the builder.

Relates to #11896
</description><key id="93492309">12076</key><summary>Fix RegexpQueryBuilder to support an Object value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels /><created>2015-07-07T10:01:54Z</created><updated>2015-07-13T07:48:12Z</updated><resolved>2015-07-13T07:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-07T10:08:02Z" id="119152892">the change LGTM but let's wait a sec to see whether this is the way forward or not. See [here](https://github.com/elastic/elasticsearch/pull/11896#issuecomment-119151877)
</comment><comment author="javanna" created="2015-07-08T12:37:22Z" id="119560917">I discussed this PR and #12124 with @jpountz . Seems like it makes little sense (probably none) to support objects here, as only fields that are actually strings work against the regex/prefix query. It initially seemed like if the only java api was missing support for objects it should have been updated, but we are now wondering whether it would make sense to do the other way around and go back to Strings in parsers and mappers around regexp and prefix. @clintongormley do you have an opinion on this? @kimchy do you have some more history on why we moved to objects in this change back in 2012 https://github.com/elastic/elasticsearch/commit/22077d1c5ffbfddb84972810ee1679e33ad90f6c :) ?

@alexksikes sorry for conflicting opinions here let's see if we can sort this out now :)
</comment><comment author="javanna" created="2015-07-09T14:19:05Z" id="119998226">After some discussion I think my initial idea of moving the java api builder over to supporting a string was wrong. We should rather do the other way around and move the parser etc. back to String, given that supporting objects makes very little sense. Same for #12124. Sorry @alexksikes :)
</comment><comment author="alexksikes" created="2015-07-09T22:15:18Z" id="120163352">I do have to agree to this as well. Accepting an Object means anything could go in, but not everything has a valid string representation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[kuromoji] move integration tests to REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12075</link><project id="" key="" /><description>We can keep only unit tests in plugins instead of starting each time a local node and running tests against it.
</description><key id="93492165">12075</key><summary>[kuromoji] move integration tests to REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Kuromoji</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T10:00:54Z</created><updated>2015-07-07T15:42:18Z</updated><resolved>2015-07-07T15:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-07T10:01:08Z" id="119151327">@s1monw @rmuir WDYT?
</comment><comment author="rmuir" created="2015-07-07T12:13:57Z" id="119185899">+1

The log4j.properties should not be needed in src/test, we already have this file in test classpath. Current Logger issues are caused by non-test problems (e.g. bin/plugin)
</comment><comment author="s1monw" created="2015-07-07T12:23:04Z" id="119188579">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make MultiTermQueryBuilder an interface again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12074</link><project id="" key="" /><description>This PR is against the query-refactoring branch.
</description><key id="93483080">12074</key><summary>Make MultiTermQueryBuilder an interface again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-07T09:26:31Z</created><updated>2015-07-07T19:08:46Z</updated><resolved>2015-07-07T19:08:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-07T09:37:02Z" id="119137607">LGTM besides the comments I left
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file></files><comments><comment>Make MultiTermQueryBuilder an interface again</comment></comments></commit></commits></item><item><title>Query refactoring: SpanFirstQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12073</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217
</description><key id="93479996">12073</key><summary>Query refactoring: SpanFirstQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-07T09:08:44Z</created><updated>2016-03-11T11:51:15Z</updated><resolved>2015-07-07T09:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-07T09:30:15Z" id="119136541">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12073 from cbuescher/feature/query-refactoring-spanfirst</comment></comments></commit></commits></item><item><title>Query Refactoring: Make EmptyQueryBuilder implement QueryBuilder directly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12072</link><project id="" key="" /><description>By extending AbstractQueryBuilder, EmptyQueryBuilder had setters for boost and
queryname which defeats its original purpose of beeing a stand-in
singleton for empty queries. By directly implementing QueryBuilder (and
temporarily also extending ToXContentToBytes) this is prevented

This PR is agains the query refactoring branch.
</description><key id="93479809">12072</key><summary>Query Refactoring: Make EmptyQueryBuilder implement QueryBuilder directly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-07T09:07:27Z</created><updated>2016-03-11T11:51:15Z</updated><resolved>2015-07-07T09:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-07T09:27:58Z" id="119135997">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java</file></files><comments><comment>Merge pull request #12072 from cbuescher/feature/query-refactoring-fixemptyqb</comment></comments></commit></commits></item><item><title>Faster recovery from simulated disurptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12071</link><project id="" key="" /><description>In testing infra, one can simulate node GCs, network issues and other problems by adding a disruption to the test cluster. Those disruption are automatically removed after the test is done. At the moment each disruption indicates how long it will take the cluster to heal once the disruption is removed and the test cluster waits for this amount of time. However, more often than not this is an upper bound, causing a much longer wait than needed.  Instead we should push the responsibility of healing to the disruption it self, where we can be smarter about what we wait for.
</description><key id="93456137">12071</key><summary>Faster recovery from simulated disurptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>test</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T07:04:30Z</created><updated>2015-07-07T12:19:28Z</updated><resolved>2015-07-07T12:19:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-07T07:04:52Z" id="119095707">@martijnvg can you take a look?
</comment><comment author="martijnvg" created="2015-07-07T07:47:44Z" id="119107992">@bleskes Left a question, but this change is looking good.
</comment><comment author="bleskes" created="2015-07-07T11:49:55Z" id="119179918">@martijnvg I updated the PR. can you take a second look?
</comment><comment author="martijnvg" created="2015-07-07T12:14:40Z" id="119186098">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/BlockClusterStateProcessing.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/LongGCDisruption.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/NetworkPartition.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/NoOpDisruptionScheme.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/ServiceDisruptionScheme.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/SingleNodeDisruption.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/SlowClusterStateProcessing.java</file></files><comments><comment>Tests: Faster recovery from simulated disurptions</comment></comments></commit></commits></item><item><title>add integration tests for analysis plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12070</link><project id="" key="" /><description>Adds very simple tests for `analysis-icu`, `analysis-kuromoji`, `analysis-smartcn`, and `analysis-stempel` so we know these plugins are working.

makes progress on #12062 
</description><key id="93441826">12070</key><summary>add integration tests for analysis plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T05:30:07Z</created><updated>2015-07-07T05:41:13Z</updated><resolved>2015-07-07T05:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-07T05:30:53Z" id="119076356">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/analysis-icu/src/main/java/org/elasticsearch/indices/analysis/IcuIndicesAnalysis.java</file><file>plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/AnalysisICURestIT.java</file><file>plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/AnalysisKuromojiRestIT.java</file><file>plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/AnalysisSmartChineseRestIT.java</file><file>plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/AnalysisPolishRestIT.java</file></files><comments><comment>Merge pull request #12070 from rmuir/analysis_rest_tests</comment></comments></commit></commits></item><item><title>Count scans in search stats and add metrics for scrolls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12069</link><project id="" key="" /><description>Each scroll on a scan causes a query to be executed. This commit adds support for these indirect queries to count against the search stats.
Additionally, this commit adds three new search stats: scroll_count, scroll_time_in_millis, and scroll_current. scroll_count tracks the
number of completed scrolls. scroll_time_in_millis tracks the total time that scrolls were held open. scroll_current tracks the number of
scrolls currently open.

Closes #9109
</description><key id="93439217">12069</key><summary>Count scans in search stats and add metrics for scrolls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T05:02:47Z</created><updated>2015-07-07T15:41:33Z</updated><resolved>2015-07-07T14:24:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-07T06:48:52Z" id="119090864">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enforce field names do not contain dot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12068</link><project id="" key="" /><description>Field names containing dots can cause problems. For example, @jpountz
made this recreation which causes no error, but can result in a
serialization exception if the type already exists:
https://gist.github.com/jpountz/8c66817e00a322b81f85

But this is not just a potential conflict. It also has larger problems,
since only the leaf mapper is created. The intermediate "foo" object
field would not exist if only "foo.bar" was in the mappings.

This change forbids the use of dots in field names. It also
fixes an issue with passing through the update_all_types setting,
which was always set to true whenever a type already existed (!).

I do not think we should worry about backwards compatibility here. This
should be a hard break (and added to the migration plugin).
</description><key id="93414385">12068</key><summary>Enforce field names do not contain dot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-07T01:28:47Z</created><updated>2015-08-13T17:36:05Z</updated><resolved>2015-07-07T15:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-07T06:50:36Z" id="119091068">The change looks good to me, but I'd like to get @clintongormley 's take on this.
</comment><comment author="clintongormley" created="2015-07-07T15:32:52Z" id="119240556">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file></files><comments><comment>Merge pull request #12068 from rjernst/fix/mapper-names-conflict</comment></comments></commit></commits></item><item><title>Properly fix the default regex flag to ALL for RegexpQueryParser and Builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12067</link><project id="" key="" /><description>The documentation mentions that the default flag is `ALL` while the code sets it to `-1`.

Relates to #11896
</description><key id="93403167">12067</key><summary>Properly fix the default regex flag to ALL for RegexpQueryParser and Builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Java API</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T23:49:47Z</created><updated>2015-07-07T12:29:53Z</updated><resolved>2015-07-07T12:29:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-07T07:47:55Z" id="119108021">can you add some description of where this came from? It is a discrepancy between code and docs right?
</comment><comment author="javanna" created="2015-07-07T12:13:13Z" id="119185787">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file></files><comments><comment>Properly fix the default regex flag to ALL for RegexpQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Fix potentially unpositioned enum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12066</link><project id="" key="" /><description>We should make sure the enum is correctly positioned before calling on PostingsEnum#freq(). 
</description><key id="93386547">12066</key><summary>Fix potentially unpositioned enum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>bug</label><label>v1.4.5</label><label>v1.5.3</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T21:59:21Z</created><updated>2015-07-14T13:45:38Z</updated><resolved>2015-07-06T22:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2015-07-06T22:00:10Z" id="119008919">Fixed with https://github.com/elastic/elasticsearch/commit/afe9c52f07100e731e633530af340d0d1b5265ee
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[build] Update to Apache Maven PMD Plugin 3.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12065</link><project id="" key="" /><description>http://maven.apache.org/plugins/maven-pmd-plugin/

You should specify the version in your project's plugin configuration:

``` xml
&lt;plugin&gt;
 &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
 &lt;artifactId&gt;maven-pmd-plugin&lt;/artifactId&gt;
 &lt;version&gt;3.5&lt;/version&gt;
&lt;/plugin&gt;
```
## Release Notes - Apache Maven PMD Plugin - Version 3.5

Bug
- [MPMD-208] Warning about deprecated Rule name when using rulesets/maven.xml
- [MPMD-205] Javascript violations won't fail the build

Improvement
- [MPMD-211] Upgrade plexus-resources from 1.0-alpha-7 to 1.1
- [MPMD-209] Upgrade to PMD 5.3.2
- [MPMD-206] Make the sourceDirectories configurable

New Feature
- [MPMD-207] Support Javascript and JSP for CPD
</description><key id="93368722">12065</key><summary>[build] Update to Apache Maven PMD Plugin 3.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T20:21:52Z</created><updated>2015-07-06T21:21:25Z</updated><resolved>2015-07-06T21:20:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-06T21:09:53Z" id="118999255">LGTM.

I'll let others say otherwise if they disagree, but in my opinion, pull requests are not required for such changes. cc @s1monw 
</comment><comment author="dadoonet" created="2015-07-06T21:18:59Z" id="119001086">Yeah. An issue might be enough for such changes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>plugin installation gives lots of logger warnings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12064</link><project id="" key="" /><description>Try running `mvn verify` for a plugin and you will see that `bin/plugin` causes a lot of noise:

```
[echo] Installing plugin elasticsearch-delete-by-query...
[exec] -&gt; Installing elasticsearch-delete-by-query/2.0.0-SNAPSHOT...log4j:WARN No appenders could be found for logger (org.elasticsearch.bootstrap).
[exec] 
[exec] log4j:WARN Please initialize the log4j system properly.Trying file:/home/rmuir/workspace/elasticsearch/plugins/delete-by-query/target/releases/elasticsearch-de......
[exec] 
[exec] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Downloading .DONE
[exec] 
[exec] Installed elasticsearch-delete-by-query/2.0.0-SNAPSHOT into /home/rmuir/workspace/elasticsearch/plugins/delete-by-query/target/integ-tests/elasticsearch-2.0.0-SNAPSHOT/plugins/2.0.0-SNAPSHOT
```
</description><key id="93365297">12064</key><summary>plugin installation gives lots of logger warnings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T20:06:13Z</created><updated>2015-07-08T15:24:42Z</updated><resolved>2015-07-07T13:00:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-06T20:13:45Z" id="118981810">Sounds like log4j.properties is not in the classloader.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Plugins: Ensure logging configuration is loaded in plugin manager</comment></comments></commit></commits></item><item><title>try to improve integration tests setup/teardown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12063</link><project id="" key="" /><description>Integration test logic starts up ES with all defaults (and for plugins installs a plugin). the teardown phase basically does 'killall elasticsearch'. Currently this stuff is just done the same way the python release script did things.

Ideally these could be more fine-grained, maybe read from a PID file instead, maybe use a non-default port number, etc.
</description><key id="93364895">12063</key><summary>try to improve integration tests setup/teardown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-06T20:03:14Z</created><updated>2015-08-26T13:00:45Z</updated><resolved>2015-08-26T13:00:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-08-10T12:03:38Z" id="129423252">I think we can we close, since `stop-external-cluster` uses the pid file for killing the process, see https://github.com/elastic/elasticsearch/blob/master/dev-tools/src/main/resources/ant/integration-tests.xml#L281

we still might work on the TODO though
</comment><comment author="clintongormley" created="2015-08-12T14:18:24Z" id="130319197">@brwe please could you look at this TODO
</comment><comment author="brwe" created="2015-08-26T12:40:50Z" id="134990725">todo has been taken care of in #12961. can we close this?
</comment><comment author="clintongormley" created="2015-08-26T13:00:45Z" id="135000245">Looks like it, thanks @brwe 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>check with jps that the pid file contains a pid that actually is an elasticsearch process</comment></comments></commit></commits></item><item><title>add simple rest tests for plugins where possible.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12062</link><project id="" key="" /><description>Currently only the delete by query plugin has any rest tests that verify the plugin works during integration tests. Other plugins just get installed and ES starts up but thats it as far as verification that they are "working". It would be better if e.g. the analyzer ones had very simple tests that ensured the plugin worked.
</description><key id="93364099">12062</key><summary>add simple rest tests for plugins where possible.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2015-07-06T19:58:54Z</created><updated>2015-07-08T11:58:56Z</updated><resolved>2015-07-08T11:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-08T11:58:55Z" id="119550051">Closed by https://github.com/elastic/elasticsearch/pull/12091
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add integration test harness for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12061</link><project id="" key="" /><description>Similar to es-core integration tests, in `verify` we unzip ES, install the plugin, run any rest tests it might have against the external cluster, then shut down.

Even if a plugin doesn't have any rest tests, at least we install and start ES with it...
</description><key id="93355866">12061</key><summary>Add integration test harness for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-06T19:17:55Z</created><updated>2015-07-06T20:03:57Z</updated><resolved>2015-07-06T20:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-06T19:24:31Z" id="118969808">LGTM - did you just sneak in our new build system... good old ANT
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/test/rest/DeleteByQueryRestIT.java</file></files><comments><comment>Merge pull request #12061 from rmuir/plugin-integration-tests</comment></comments></commit></commits></item><item><title>Fix: Use correct OpType on Failure in BulkItemResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12060</link><project id="" key="" /><description>When a bulk request fails on a Delete or Update request, the BulkItemResponse
reports incorrect "index" operation in the response. This PR fixes this
for the case of closed indices as reported in #9821 but also for
other failures and adds tests for the two cases covered.

Closes #9821
</description><key id="93342922">12060</key><summary>Fix: Use correct OpType on Failure in BulkItemResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Bulk</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T18:25:53Z</created><updated>2016-03-11T11:51:17Z</updated><resolved>2015-07-07T07:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-06T19:20:41Z" id="118969104">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/test/java/org/elasticsearch/document/BulkTests.java</file></files><comments><comment>Merge pull request #12060 from cbuescher/fix/9821</comment></comments></commit></commits></item><item><title>Support date math in index names for all index based apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12059</link><project id="" key="" /><description>Very often it is useful to query indices within a particular timeframe. For example in the case of weekly indices all the indices of the last month. This can be achieved via aliases, however this does require that something is updating the aliases each time new indices get introduced. Also when the search request (or part of the search request) is stored elsewhere already updating the search request and the alias itself isn't straight forward, because now the search request needs to be maintained in two places.

Instead it would be better if date math is supported in the index names option for all index based apis (search, index, percolate etc.). By adding support for the following format in index names: 

```
&lt;static_name{date_math_expr{optional_date_format}}&gt;
```

The following index names can be specified:
- `&lt;logstash-{now/d}&gt;` translates into `logstash-2024.03.22`
- `&lt;logstash-{now/M}&gt;` translates into `logstash-2024.03.01`
- `&lt;logstash-{now/M{YYYY.MM}}&gt;` translates into `logstash-2024.03`
- `&lt;logstash-{now/M-1M{YYYY.MM}}&gt;` translates into `logstash-2024.02`

The data math expressions should be resolved before wildcard expressions are resolved during the concrete index name resolution.

Before this feature can be added the code that currently exists for index name wildcard expansions, index to concrete index resolutions should be refactored first to support this nicely without hacks. See #12058.
</description><key id="93320244">12059</key><summary>Support date math in index names for all index based apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T16:24:24Z</created><updated>2015-07-29T15:42:47Z</updated><resolved>2015-07-29T15:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-13T17:03:26Z" id="120994108">Very cute.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/AutoCreateIndex.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/DateMathExpressionResolverTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java</file><file>core/src/test/java/org/elasticsearch/indices/DateMathIndexExpressionsIntegrationTests.java</file></files><comments><comment>Added date math support in index names</comment></comments></commit></commits></item><item><title>Refactor MetaData to split off the concrete index name logic to a dedicated service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12058</link><project id="" key="" /><description>Changes in a nutshell:
- All expression logic is now encapsulated by `ExpressionResolver` interface.
- `MetaData#convertFromWildcards()` gets replaced by `WildcardExpressionResolver`.
- All of the indices expansion methods are being moved from `MetaData` class to the new `IndexNameExpressionResolver` class.
- All single index expansion optimisations are removed.

The logic for resolving a concrete index name from an expression has been moved from the MetaData class to IndexNameExpressionResolver class. The logic has been cleaned up and simplified were was possible without breaking bwc. Mainly the logic that deals if there is just one index expression is no more, mainly to make to code simpler. Also the notion of aliasOrIndex has been changed to index expression.

The IndexNameExpressionResolver translates index name expressions into concrete indices. The list of index name expressions are first delegated to the known IndexExpressionFilter. An IndexExpressionFilter is responsible for translating if possible an expression into a concrete indices or aliases otherwise the expression is left untouched. Concretely this means converting wildcard expressions into concrete indices or aliases, but in the future other implementations could convert expressions based on different rules.

Beyond this there is also some follow up work to do:
- Replace SnapshotUtils#filterIndices by using the IndexNameExpressionResolver
- During the lifetime of a request index expressions or resolved to concrete indices multiple times. (finding the search routing, whether aliases have filters, checking for cluster blocks and just resolving the index name expressions to concrete indices) This can be reduced to a single lookup. 
- Revise the in memory data structures used by MetaData. This is interesting in the case of many indices and aliases. For example I think that `MetaData#aliasAndIndexToIndexMap` can be removed in order to save heap memory at the cost of some extra look ups at request time. Also a completely different approach can be taken to store index and aliases with their respective IndexMetaData and AliasMetaData into a FST.
</description><key id="93309759">12058</key><summary>Refactor MetaData to split off the concrete index name logic to a dedicated service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T15:37:12Z</created><updated>2015-07-10T13:34:26Z</updated><resolved>2015-07-10T13:34:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-09T13:26:10Z" id="119964061">I did a first review round, the change looks good, left a few comments. 

It is not the easiest PR to review given that we are moving whole methods and slightly changing them too meanwhile. I think that the most important aspects are that `convertFromWildcards` gets replaced by `WildcardExpressionFilter` and we potentially support for more filters that will come in the future. All of the indices expansion methods are being moved from MetaData to the new IndexNameExpressionResolver dedicated class, and meanwhile we removed all of the optimizations around single index usecase etc. anything else that might have missed?
</comment><comment author="martijnvg" created="2015-07-09T13:43:11Z" id="119971417">thanks @javanna! and those changes are correct! (nothing missed)
</comment><comment author="martijnvg" created="2015-07-09T22:31:57Z" id="120165807">@javanna I updated the PR.
</comment><comment author="javanna" created="2015-07-10T08:30:18Z" id="120297317">left a couple more comments
</comment><comment author="martijnvg" created="2015-07-10T11:58:33Z" id="120394926">@javanna I updated the PR.
</comment><comment author="javanna" created="2015-07-10T12:08:52Z" id="120396653">left a bunch of minor comments, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolverTests.java</file></files><comments><comment>Deduplicate concrete indices after indices resolution</comment></comments></commit></commits></item><item><title>Query refactoring: SpanContainingQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12057</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

PR is against query refactoring branch.

Relates to #10217 
</description><key id="93307561">12057</key><summary>Query refactoring: SpanContainingQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-07-06T15:30:55Z</created><updated>2016-03-11T11:51:21Z</updated><resolved>2015-07-06T17:13:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-06T15:55:17Z" id="118907747">left a few minor comments
</comment><comment author="cbuescher" created="2015-07-06T16:45:33Z" id="118921800">Changed the test query setup to use switch.
</comment><comment author="javanna" created="2015-07-06T17:01:47Z" id="118925185">LGTM
</comment><comment author="javanna" created="2015-08-28T10:12:48Z" id="135727148">This change is breaking for the java api as it removes setters for mandatory big/little inner span queries. Both arguments now have to be supplied at construction time instead and have to be non-null.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java</file></files><comments><comment>Merge pull request #12057 from cbuescher/feature/query-refactoring-spancontaining</comment></comments></commit></commits></item><item><title>Nested geo_* type with doc values and include_in_parent fails with multiple values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12056</link><project id="" key="" /><description>If you create a `nested` `geo_*` type and you attempt to use both `include_in_parent` and `doc_values`, then you cannot index multiple values. If you remove either setting, then it works.

```
PUT /test
{
  "mappings": {
    "type": {
      "properties": {
        "location": {
          "type": "nested",
          "include_in_parent": true, 
          "properties": {
            "point": {
              "type": "geo_point",
              "doc_values": true
            }
          }
        }
      }
    }
  }
}

# Works
PUT /test/type/1
{
  "location" : {
    "point" : [0, 0]
  }
}

# Fails
PUT /test/type/2
{
  "location" : [
    {
      "point" : [0, 0]
    },
    {
      "point" : [1, 1]
    }
  ]
}
```

The first attempt works as expected, but the second attempt fails with an odd exception.

``` json
{
   "error": "IllegalArgumentException[DocValuesField \"location.point\" appears more than once in this document (only one value is allowed per field)]",
   "status": 500
}
```

This is related to https://github.com/elastic/elasticsearch/issues/10653#issuecomment-94097576, which notes that `include_in_parent` is ignored for `geo_*` types, but it does not suggest that it will fail upon trying to make use of it. This behavior should probably be explicitly blocked. Note: changing `geo_point` to some type that does work with `include_in_parent`, such as `long`, does work.
</description><key id="93302924">12056</key><summary>Nested geo_* type with doc values and include_in_parent fails with multiple values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>v1.7.1</label></labels><created>2015-07-06T15:13:38Z</created><updated>2015-09-19T13:26:07Z</updated><resolved>2015-09-19T13:26:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-08T15:14:02Z" id="119620336">For `geo_point`:
BinaryDocValues only supports a single value. Multiple values must be encoded within the binary value returned. This works when changing to eg `long` because then the field is using SortedNumericDocValues, which supports multiple values. However, `geo_point` has code (which at a glance looks correct to me) which handles multiple values when encoding. So this seems like a bug. Maybe @nknize has ideas here on what could be going wrong.

For `geo_shape`:
As mentioned in the referenced comment, the way the custom token producer for shapes works only allows a single pass, but `include_in_parent` works by simply copying the reference to the token producer, and the second iteration (for the parent) silently fails. I agree we should simply disallow this with an error message until it can be fixed. I'll put up a PR. 
</comment><comment author="pickypg" created="2015-07-08T15:24:50Z" id="119624286">Just to add for @nknize, it does work with multiple values _without_ `include_in_parent`.
</comment><comment author="nknize" created="2015-09-08T17:44:23Z" id="138645694">This worked for me in 2.x. @pickypg can you confirm? @clintongormley Would you like me to diff with 1.7.x and backport a patch?
</comment><comment author="pickypg" created="2015-09-08T17:57:57Z" id="138648907">@nknize Confirmed that this appears to work in 2.0.0-beta1.
</comment><comment author="clintongormley" created="2015-09-19T13:26:07Z" id="141667971">@nknize Thanks, but I think we're good with 2.0.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failure to recover shards after the disk was full</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12055</link><project id="" key="" /><description>On one of our servers running Elasticsearch, some other process wrote to many logfiles such that the disk was out of space. After deleting these logfiles and rebooting the system, Elasticsearch did not recover.

We are running on a single server, using Elasticsearch 1.5.2

I believe we manages to recover by deleting some of the *.recovering files in the elasticsearch data directories, however it would be great if Elasticsearch could recover as much as possible by itself.

```
[2015-07-03 14:09:37,196][WARN ][cluster.action.shard     ] [mxserver] [abds-historic-snapshots-2015-07-03][1] received shard failed for [abds-historic-snapshots-2015-07-03][1], node[9HgooclMS6W9m-1lqKxV8Q], [P], s[INITIALIZING], indexUUID [6unWDyfbQ_yF9XTYAlMz4g], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[abds-historic-snapshots-2015-07-03][1] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: ElasticsearchIllegalArgumentException[No version type match [116]]; ]]
[2015-07-03 14:09:37,205][WARN ][index.engine             ] [mxserver] [abds-instance][0] failed to sync translog
[2015-07-03 14:09:37,206][WARN ][indices.cluster          ] [mxserver] [[abds-instance][0]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [abds-instance][0] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:290)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.index.translog.TranslogCorruptedException: translog corruption while reading from stream
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:72)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:260)
    ... 4 more
Caused by: org.elasticsearch.ElasticsearchException: failed to read [abdstrack][AdsbTrack-7668367]
    at org.elasticsearch.index.translog.Translog$Index.readFrom(Translog.java:522)
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:68)
    ... 5 more
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [48]
    at org.elasticsearch.index.VersionType.fromValue(VersionType.java:307)
    at org.elasticsearch.index.translog.Translog$Index.readFrom(Translog.java:519)
    ... 6 more
[2015-07-03 14:09:37,206][WARN ][cluster.action.shard     ] [mxserver] [abds-instance][0] received shard failed for [abds-instance][0], node[9HgooclMS6W9m-1lqKxV8Q], [P], s[INITIALIZING], indexUUID [H8FyNbqATmWQ6p8RYSGncw], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[abds-instance][0] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: ElasticsearchException[failed to read [abdstrack][AdsbTrack-7668367]]; nested: ElasticsearchIllegalArgumentException[No version type match [48]]; ]]
[2015-07-03 14:09:37,216][WARN ][index.engine             ] [mxserver] [abds-historic-snapshots-2015-07-03][1] failed to sync translog
[2015-07-03 14:09:37,217][WARN ][indices.cluster          ] [mxserver] [[abds-historic-snapshots-2015-07-03][1]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [abds-historic-snapshots-2015-07-03][1] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:290)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.index.translog.TranslogCorruptedException: translog corruption while reading from stream
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:72)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:260)
    ... 4 more
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [116]
    at org.elasticsearch.index.VersionType.fromValue(VersionType.java:307)
    at org.elasticsearch.index.translog.Translog$Create.readFrom(Translog.java:376)
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:68)
    ... 5 more
```

Note: This issue seems very similar to #10606 which I have reported before.
</description><key id="93301869">12055</key><summary>Failure to recover shards after the disk was full</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">WellingR</reporter><labels /><created>2015-07-06T15:08:18Z</created><updated>2017-01-10T18:05:16Z</updated><resolved>2015-07-08T09:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="287400117" created="2015-07-08T08:31:44Z" id="119493210">The problem you solved?
</comment><comment author="s1monw" created="2015-07-08T09:38:50Z" id="119523564">this will be fixed in Elasticsearch 2.0. It will unlikely make it into 1.x series since it depends on a large amount of changes that are only in 2.0
</comment><comment author="autrejacoupa" created="2015-07-08T18:33:27Z" id="119691242">When is Elasticsearch 2.0 scheduled for release?
</comment><comment author="balaji006" created="2015-09-01T14:47:13Z" id="136747898">Delete .recovery file inside the translog folder

Eg:/es/elasticsearch-1.7.1/data/[elasticsearch_clustername]/nodes/0/indices/[indexname]/2/translog/
</comment><comment author="tony-bye" created="2015-11-12T22:51:06Z" id="156262260">I also had this kind of error after a partition was full, and deleting the .recovery files as balaji006 suggested worked fine.  I had a lot of affected index/shard directories, but after deleting each .recovery file elasticsearch worked fine again.
====Update
Oops, spoke too soon.  Now all queries give me "All shards failed for phase: [query]"
</comment><comment author="sa-shukla" created="2015-11-23T18:22:47Z" id="159018557">I am running ElasticSearch 2.0, but am still receiving IndexShard Recovery failures:

[2015-11-23 18:03:32,670][WARN ][cluster.action.shard     ] [The Russian] [logstash-2015.10.24][4] received shard failed for [logstash-2015.10.24][4], node[omb9PXHUTXqpKeesvkCbPw], [P], v[742647], s[INITIALIZING], a[id=XUctUOPUQLiHXyK2J9gdlg], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-11-23T18:03:32.486Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IllegalStateException[latest found translog has a lower generation that the excepcted uncommitted 1421133423283 &gt; -1]; ]], indexUUID [jf5m3aXaQLyH9gMhwMBuDQ], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IllegalStateException[latest found translog has a lower generation that the excepcted uncommitted 1421133423283 &gt; -1]; ]
[logstash-2015.10.24][[logstash-2015.10.24][4]] IndexShardRecoveryException[failed recovery]; nested: IllegalStateException[latest found translog has a lower generation that the excepcted uncommitted 1421133423283 &gt; -1];
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:183)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: latest found translog has a lower generation that the excepcted uncommitted 1421133423283 &gt; -1
        at org.elasticsearch.index.translog.Translog.upgradeLegacyTranslog(Translog.java:253)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:185)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:131)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1349)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1344)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:889)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:866)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:249)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:60)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:133)
        ... 3 more

There has been no disk full issue since my upgrade to 2.0, so possibility of recovery file getting corrupted is very low.

Any fixes / workaround would be very much appreciated.

Regards,
Sagar
</comment><comment author="kpcool" created="2015-12-09T11:09:39Z" id="163190639">Today, the disk got full and ElasticSearch is not able to go back again. Isn't there a built-in system that prevents such failures. I agree that we should be monitoring the hard space and not let this happen in first place, but some times things happen.

My setup is a single node at present.

I don't see a clear way to recover the node. A post at https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html seemed to help, but still few indices got corrupted and I have no way to recovering them.

At the end, I ended up deleted the indices, but that's not the way it should be. Such things must be taken care of ultimatley
</comment><comment author="CorbMax" created="2016-01-07T10:54:22Z" id="169628693">Same issue here. Applied tips at https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html but not solved.

This is really really disappointing! 
</comment><comment author="bleskes" created="2016-01-08T10:01:24Z" id="169951962">@CorbMax (and @kpcool ) which ES versions are you on?
</comment><comment author="CorbMax" created="2016-01-08T10:08:24Z" id="169953037">I'm on 2.0, but going to upgrade to 2.1
Unfortunately I was obliged to delete indexes to unlock the system...
</comment><comment author="starkers" created="2016-02-02T23:52:18Z" id="178893201">I think I've experienced the same just after updating from 2.1.0 to 2.2.0 (from official stable PPA)

Its only a few devel indexes but the recovery seems to just (after stopping elastic, growing the disk, starting elastic) filled up the disk very quickly with translog "stuff"

I'm just going to delete the stuff but this shouldn't be difficult to replicate.
</comment><comment author="bleskes" created="2016-02-03T11:04:22Z" id="179167212">@starkers clean please capture the files and logs before deleting, and share them somewhere? these things are typically not easy to reproduce :(
</comment><comment author="systeminsightsbuild" created="2016-02-06T06:04:47Z" id="180699393">Should this not be reopned? I just a disk full and now and getting 

```
[2016-02-06 06:01:59,643][WARN ][cluster.action.shard     ] [ops-elk-1] [logstash-2016.02.05][2] received shard failed for ...
```

This is for 2.1.1
</comment><comment author="bleskes" created="2016-02-06T09:54:56Z" id="180727053">@systeminsightsbuild sadly there can be many reasons for this can of failure. this specific issue is about translog corruption due to a failure to fully write an operation. This is fixed in 2.0. There might be other issues as well. It's hard to tell from the log line you sent as it misses the part that tells why the shard failed. If you can post that (and feel free open a new issue), we can see what's going on. 
</comment><comment author="likaiguo" created="2016-03-14T19:22:10Z" id="196484649">it works!!!

balaji006 commented on 1 Sep 2015
Delete .recovery file inside the translog folder

Eg:/es/elasticsearch-1.7.1/data/[elasticsearch_clustername]/nodes/0/indices/[indexname]/2/translog/

Thanks @balaji006 
</comment><comment author="ambodi" created="2016-04-06T13:20:12Z" id="206367708">@simonw This is still there for 2.2.3. 

@balaji006 workaround fixed the issue, but I think that needs to be addressed. 
</comment><comment author="bleskes" created="2016-04-07T09:03:42Z" id="206770732">@ambodi can open a new issue with the details of what you saw? this can come in many flavors. I'm also curious how you had a `.recovering` translog file, which is not used in 2.x
</comment><comment author="ambodi" created="2016-04-14T10:03:27Z" id="209860816">@bleskes here is what I see:

`2016-04-14T10:02:45.691973552Z Caused by: org.elasticsearch.index.translog.TranslogCorruptedException: translog corruption while reading from stream
2016-04-14T10:02:45.691977952Z  at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:72)
2016-04-14T10:02:45.691982252Z  at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:260)
2016-04-14T10:02:45.691992452Z  ... 4 more`

@bleskes we upgraded from 1.5 to 2.2.3
</comment><comment author="bleskes" created="2016-04-14T11:48:27Z" id="209896447">@ambodi thx. That exception stack trace refers to a class that has been removed in the 2.x series. The code that generated this exception is therefore from your 1.5 version. This makes me think something went wrong with your upgrade and that that node is still on 1.5.

PS. I taking you mean upgrade to 2.2.3 (as you wrote before) and not 2.8
</comment><comment author="tamsky" created="2016-06-12T22:04:11Z" id="225463266">For reference, an original thread with a complete set of instructions for this error is at:
- https://groups.google.com/d/msg/elasticsearch/HtgNeUJ5uao/AdMssa0WnJMJ

And to correct mistakes found above:
- Everywhere above that mentions files with suffix "`.recovery`" is mistaken.   The correct suffix is `.recovering`

For us, after stopping ES, moving these `.recovering` files to another filesystem, and then starting ES, our cluster was able to recover.  (ES version 1.6.2)
</comment><comment author="jamshid" created="2017-01-09T20:57:07Z" id="271405177">@tamsky the link doesn't work, maybe the elasticsearch group was deleted/moved?
FWIW I found this issue because I had a problem with ES 2.3.3 running out of disk space and then not recovering properly. But I guess it's not related to this issue since the .recovering file is no longer used? Sorry don't have logs from the ES 2.3.3 problem.</comment><comment author="tamsky" created="2017-01-10T18:05:16Z" id="271650684">Thanks for pointing out the group is gone.

I'm disappointed the ES team invalidated (and made unsearchable by old URL) all those groups links after their [bulk import](https://discuss.elastic.co/t/google-groups-mailing-list-import-notes/23944) and [announcement](https://discuss.elastic.co/t/elasticsearch-google-groups-imported/23797).   I've learned my lesson : at a minimum, quote the thread subject.

A bit of spelunking later, I found a citation containing both thread URL and subject
[ ES failed to recover after crash ]
- http://repository.tudelft.nl/islandora/object/uuid:1db911a0-18e0-49b5-ac9d-b6700f9b60ab/datastream/OBJ/download

Here's the migrated thread:
https://discuss.elastic.co/t/es-failed-to-recover-after-crash/8195

I **guess** the message I had linked to was this
    https://discuss.elastic.co/t/es-failed-to-recover-after-crash/8195/5
but my [comment giving corrections](https://github.com/elastic/elasticsearch/issues/12055#issuecomment-225463266) seems out of place or already corrected.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove network stats &amp; info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12054</link><project id="" key="" /><description>This pull request removes Network stats and Network info.

The following information are removed from `network` section of the  Node Stats API:
- `network.tcp`
- `network.tcp.active_opens`
- `network.tcp.passive_opens`  
- `network.tcp.curr_estab`
- `network.tcp.in_segs`
- `network.tcp.out_segs`
- `network.tcp.retrans_segs`
- `network.tcp.estab_resets`
- `network.tcp.attempt_fails`
- `network.tcp.in_errs`
- `network.tcp.out_rsts`

The following information are removed from `network` section of the  Node Info API:
- `network.refresh_interval_in_millis`
- `network.primary_interface`
- `network.primary_interface.address`
- `network.primary_interface.name`
- `network.primary_interface.mac_address`
</description><key id="93299046">12054</key><summary>Remove network stats &amp; info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T14:56:33Z</created><updated>2015-07-08T13:18:08Z</updated><resolved>2015-07-07T19:34:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-06T14:57:08Z" id="118880544">@kimchy Can you have a look please? thanks
</comment><comment author="kimchy" created="2015-07-07T14:43:53Z" id="119225385">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update fs stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12053</link><project id="" key="" /><description>This pull request:
- removes the sigar-specific FS stats: 
  - `fs.*.dev`
  - `fs.*.disk_reads`
  - `fs.*.disk_writes`
  - `fs.*.disk_io_op`
  - `fs.*.disk_read_size_in_bytes`
  - `fs.*.disk_write_size_in_bytes`
  - `fs.*.disk_io_size_in_bytes`
  - `fs.*.disk_queue`
  - `fs.*.disk_service_time`
- add default implementation based on JMX beans for other FS stats
- add documentation for FS stats
</description><key id="93279174">12053</key><summary>Update fs stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T13:30:40Z</created><updated>2015-07-08T13:17:28Z</updated><resolved>2015-07-07T20:27:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-07-07T15:26:43Z" id="119238709">can we rename `FsStats.Info` to `FsInfo.Path`? other than that, LGTM
</comment><comment author="tlrx" created="2015-07-07T20:51:19Z" id="119335111">@kimchy thanks for the review, I merged it with the renaming to `FsInfo.Path`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Delete unsupported `strategy` parameter in filtered-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12052</link><project id="" key="" /><description>Looking at how FilteredQueryParser already ignores the strategy parameter, also the documentation for this parameter should be deleted.
</description><key id="93274759">12052</key><summary>Docs: Delete unsupported `strategy` parameter in filtered-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label></labels><created>2015-07-06T13:08:12Z</created><updated>2016-03-11T11:51:21Z</updated><resolved>2015-07-06T14:58:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-06T14:19:15Z" id="118870069">LGTM. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12052 from cbuescher/docs-strategy-filtered</comment></comments></commit></commits></item><item><title>Update filtered-query.asciidoc, delete unsupported `strategy` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12051</link><project id="" key="" /><description>Looking at how FilteredQueryParser already ignores the `strategy` parameter, also the documentation for this parameter should be deleted.
</description><key id="93273659">12051</key><summary>Update filtered-query.asciidoc, delete unsupported `strategy` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels /><created>2015-07-06T13:02:03Z</created><updated>2015-07-06T13:11:02Z</updated><resolved>2015-07-06T13:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>A doc has many geo_point fileds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12050</link><project id="" key="" /><description>would like to be able to sort based on the closest matching location in the document
mapper is:(stations is one array filed)

"stations":{
                "properties":{
                    "station_name":{"type":"string","index":"analyzed"},
                    "station_position":{"type":"geo_point","lat_lon": true},
                    "type":{"type":"integer"}
                }
            }

stations: 
[
{
    "station_name": "bla bla",
    "station_pos": {
        "lat": 39.985303,
       "lon": 116.313024
    },
    "type": 1
},
{
    "station_name": "bbbbb",
    "station_pos": {
        "lat": 34.97503,
       "lon": 117.313024
    },
    "type": 1
}
]

I got an Exception :
failed to find mapper for [station_pos] for geo distance based sort

how I sort them by distance?
</description><key id="93272669">12050</key><summary>A doc has many geo_point fileds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mouzt</reporter><labels /><created>2015-07-06T12:56:54Z</created><updated>2015-07-06T13:24:43Z</updated><resolved>2015-07-06T13:24:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-06T13:24:42Z" id="118853152">Please use discuss.elastic.co for those questions.

Not an issue here.

BTW note that it seems you are mixing `station_position` and `station_pos`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update OS stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12049</link><project id="" key="" /><description>This pull request:
- removes the sigar-specific OS stats: 
  - `os.uptime_in_millis`
  - `os.cpu`
  - `os.cpu.sys`
  - `os.cpu.user`
  - `os.cpu.idle`
  - `os.cpu.usage`
  - `os.cpu.stolen`
  - `os.mem.actual_free_in_bytes`
  - `os.mem.actual_used_in_bytes`
  - `os.cpu.vendor`
  - `os.cpu.model`
  - `os.cpu.mhz`
  - `os.cpu.total_cores`
  - `os.cpu.total_sockets`
  - `os.cpu.cores_per_socket`
  - `os.cpu.cache_size`
- add default implementation based on JMX beans for other OS stats
- move some process fields from Nodes Info to Nodes Stats API (os.mem.total, os.swap.total)
- add documentation for OS stats &amp; OS info

Besides `os.name` we could also provide `os.arch` and `os.version` using `org.apache.lucene.util.Constants`.
</description><key id="93267167">12049</key><summary>Update OS stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T12:26:15Z</created><updated>2016-01-11T21:06:36Z</updated><resolved>2015-07-08T15:56:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-07T09:11:27Z" id="119133106">Note: similar to what has been done in #12043, we can also use `OsProbe` as a singleton here and call it in `Bootstrap` class before security manager is installed.
</comment><comment author="kimchy" created="2015-07-07T14:54:17Z" id="119227939">left a few comments, I would also change to only have "getters" in our Info/Stats classes (we are moving towards that). Also, great idea on having arch and such in os info
</comment><comment author="tlrx" created="2015-07-08T14:49:31Z" id="119609718">@kimchy I updated the code with your comments, including `os.arch`/`os.version` and comments from #12043 (catch throwable, benchmark class, updated doc). Can you have a look please?

Benchmark results (100 000 iterations for each method):

```
testing 'getTotalPhysicalMemorySize' method...
total [1290] ms, avg [0.0129] ms

testing 'getFreePhysicalMemorySize' method...
total [1269] ms, avg [0.01269] ms

testing 'getTotalSwapSpaceSize' method...
total [20] ms, avg [2.0E-4] ms

testing 'getFreeSwapSpaceSize' method...
total [20] ms, avg [2.0E-4] ms

testing 'getSystemLoadAverage' method...
total [256] ms, avg [0.00256] ms
```
</comment><comment author="kimchy" created="2015-07-08T14:56:01Z" id="119612504">LGTM.

I think @rcmuir mentioned it on the other thread, but it would be nice (maybe in another change) to have the tests verify we get non -1 values for platform we know we support it, and -1 for platforms we know is not supported. This will help know where we need to potentially go the extra mile and see how we can support it for certain platform.
</comment><comment author="tlrx" created="2015-07-08T15:57:22Z" id="119634564">@kimchy thanks!

&gt; I think @rcmuir mentioned it on the other thread, but it would be nice (maybe in another change) to have the tests verify we get non -1 values for platform we know we support it, and -1 for platforms we know is not supported.

I agree, this is a very pertinent suggestion. I'll do it in another pull request.
</comment><comment author="NickCraver" created="2016-01-11T21:06:36Z" id="170689838">The `load_average` portion of this change introduces a fair bit of pain in monitoring. While many things moved from the 1.x API, that's not a pain point - that's a "if X is null, use Y" solution. This is the only hard breaking type change. The `load_average` going from an array to a single value is much more painful in most static languages; since the same type can't be used, one must duplicate quite a bit of code or get crazy with generics to address something this nested in the API. It'd be much easier if this was still an array with a single value.

The change also presents problems for the future, if we can actually get all 3 load values down the road then the API has to be changed _back_ and cause yet another break. Was this considered with changes like this? 

It's a bit late to actually fix it now, but I'd like for these breaks to be considered a bit harder with future changes. With the monitoring APIs specifically most consumers need to support multiple major versions at the same time.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java</file><file>core/src/main/java/org/elasticsearch/monitor/os/OsStats.java</file><file>core/src/test/java/org/elasticsearch/monitor/os/OsProbeTests.java</file></files><comments><comment>Merge pull request #16061 from jasontedor/normalize-unavailable-load-average</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/env/Environment.java</file><file>core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java</file><file>core/src/main/java/org/elasticsearch/monitor/os/OsStats.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/test/java/org/elasticsearch/monitor/os/OsProbeTests.java</file></files><comments><comment>Merge pull request #15907 from jasontedor/load-average</comment></comments></commit></commits></item><item><title>where is default config mapping file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12048</link><project id="" key="" /><description>I just put a json file in config/mappings/_default directory, but I find it doesn't make sense, I just want to know where is the default mapping file, because I just using log stash to put data into ES.
</description><key id="93266397">12048</key><summary>where is default config mapping file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hy05190134</reporter><labels /><created>2015-07-06T12:21:11Z</created><updated>2015-07-06T20:48:16Z</updated><resolved>2015-07-06T20:48:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-06T20:48:14Z" id="118993886">@hy05190134 we are using github for bug reports and feature requests. Please use our support forums at https://discuss.elastic.co/ for questions like this. By the way, I think it might be easier to achieve what you are doing using an index template instead of overriding default mapping.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor ValuesSource to separate Parsing from Factory logic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12047</link><project id="" key="" /><description>ValuesSourceConfig is now evaluated in the ValuesSourceAggregatorFactory instead of ValueSourceParser. This means that the ValueSourceParser purely deals with parsing the XContent and the logic requiring access to the mappings etc. on the shard is left to the ValuesSourceAggregatorFactory. This means that, in the future, the parsing logic can be moved to the coordinating node.
</description><key id="93264870">12047</key><summary>Refactor ValuesSource to separate Parsing from Factory logic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2015-07-06T12:15:47Z</created><updated>2015-07-10T18:47:57Z</updated><resolved>2015-07-10T13:04:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-06T21:46:47Z" id="119006386">I'm not too happy with resolving potentially a lot of things in createAggregator given that it can be called for every bucket.

It makes me wonder that we might want to keep something simple like the current builders for serialization, and add a method on these builders to create the factories, which would in turn create the aggregators? It would also allow us to move some parameters of the AggregatorFactory.create method that are always the same such as the AggregationContext to this new method?
</comment><comment author="colings86" created="2015-07-07T12:20:12Z" id="119188188">@jpountz As discussed, I have moved the logic for calculating the ValuesSourceConfig to a new AggregatorFactory.init() method that is called once before the top-level aggregators are created. Could you take another look?
</comment><comment author="jpountz" created="2015-07-07T12:24:43Z" id="119188829">Thanks, I left some new questions.
</comment><comment author="colings86" created="2015-07-07T13:23:11Z" id="119201418">Good idea with the context. Was quite easy to implement, although the createInternal() still has the context passed in at the moment to limit the size of the change for now. I actually think it's good to have the context passed in here as its obvious the aggregation context is available in the subclass rather than having to know its available form the parent. It also means we can keep the aggregation context read only in the parent.
</comment><comment author="jpountz" created="2015-07-07T13:28:09Z" id="119202407">LGTM
</comment><comment author="colings86" created="2015-07-10T13:02:37Z" id="120405091">This change is going to be applied to a `feature/aggs-refactoring` off master instead of making the change off the feature/query-refactoring branch
</comment><comment author="colings86" created="2015-07-10T13:04:15Z" id="120405339">Merged into `feature/aggs-refactoring` branch in https://github.com/elastic/elasticsearch/commit/0453036d32164a7d91d4658a97048612e1923361
</comment><comment author="clintongormley" created="2015-07-10T17:10:40Z" id="120462356">@colings86 I think we should not label these PRs as 2.0 while they are against a different branch.
</comment><comment author="colings86" created="2015-07-10T18:47:57Z" id="120492241">@clintongormley sure, think that was me on autopilot ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Script bin/plugin doesn't use different plugin path set in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12046</link><project id="" key="" /><description>Hi,
my elasticsearch setup has set different path for plugin directory. 
When installing new plugin, binary "bin/plugin" does not check the different path I have set in config file and uses the default path. 
My first impression was that script will read config/elasticsearch.yml file to get the path but it is ignored.

Of course, it is possible to hack around this "bin/plugin" script, but my intention was to not to touch scripts coming with installation. I would prefer to pass argument to the script or simply accept config values in elasticsearch.yml.

Is this behavior as it suppose to be? Or I do something wrong. :)

Elasticsearch version. 1.6.

Thanks,
Rado
</description><key id="93255811">12046</key><summary>Script bin/plugin doesn't use different plugin path set in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">radoondas</reporter><labels><label>:Packaging</label><label>docs</label></labels><created>2015-07-06T11:26:46Z</created><updated>2015-08-15T16:04:43Z</updated><resolved>2015-08-15T16:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-08T10:20:48Z" id="119533119">@radoondas is this using the Zip or tar.gz packages? or RPM/deb?
</comment><comment author="radoondas" created="2015-07-08T20:15:07Z" id="119718243">I am using latest version of ES 1.6.0 and it is downloaded as zip. 
OS: SUSE Enterprise linux, 64bit
</comment><comment author="clintongormley" created="2015-07-10T09:28:17Z" id="120317352">Have you moved the location of the `config` directory?

If so, try doing:

```
CONF_DIR=/path/to/config/dir bin/plugin install pluginname
```
</comment><comment author="radoondas" created="2015-07-28T09:49:54Z" id="125526575">Sorry for late answer. 

But, as you guessed correctly, I have changed the config directory too and did not mention it. 
As you suggested, this solution works and it is sufficient for all cases where also config directory is in different location.

Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>[TEST] NettyTransportMultiPortTests now checks selected random ports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12045</link><project id="" key="" /><description>NettyTransportMultiPortTests now checks selected random ports are free before using them

This fix will be ported to 1.7 and 2.0
</description><key id="93248549">12045</key><summary>[TEST] NettyTransportMultiPortTests now checks selected random ports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>test</label><label>v1.6.0</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T10:52:32Z</created><updated>2015-07-06T11:05:55Z</updated><resolved>2015-07-06T10:56:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-06T10:54:31Z" id="118811258">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create $PID_DIR if not exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12044</link><project id="" key="" /><description>When you istall elasticsearch with the deb package (apt-get install elasticsearch), you have an error when starting elasticsearch :
sudo service elasticsearch start
[....] Starting Elasticsearch Server:touch: cannot touch `/var/run/elasticsearch                                                                    /elasticsearch.pid': No such file or directory
failed!

If the $PID_DIR doesn't exist, elasticsearch can't start.
</description><key id="93237571">12044</key><summary>Create $PID_DIR if not exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reversTeam</reporter><labels /><created>2015-07-06T09:58:14Z</created><updated>2015-07-06T12:44:14Z</updated><resolved>2015-07-06T12:44:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="reversTeam" created="2015-07-06T10:07:52Z" id="118793100">I have signed the document "INDIVIDUAL CONTRIBUTOR LICENSE AGREEMENT"
</comment><comment author="tlrx" created="2015-07-06T10:17:00Z" id="118798570">@reversTeam thanks for your contribution.

I think this issue has been reported in #11594 and fixed in #11674. Can you please double-check that  #11674 solves your problem? thanks!
</comment><comment author="reversTeam" created="2015-07-06T12:44:14Z" id="118842704">Thank's it's ok for me
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update process stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12043</link><project id="" key="" /><description>This pull request:
- removes the sigar-specific process stats: `process.mem.resident`, `process.mem.share`
- add default implementation based on JMX beans for all other process stats
- make ProcessProbe a singleton so that it can be loaded before security manager
- move some process fields from Nodes Info to Nodes Stats API (max file descriptors, mlockall)
- add documentation for process stats &amp; process info
</description><key id="93231670">12043</key><summary>Update process stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-06T09:25:06Z</created><updated>2015-07-08T13:44:58Z</updated><resolved>2015-07-08T13:13:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-06T09:27:34Z" id="118785052">I'm updating the Nodes Stats and Nodes Info API piece by piece... Here is the `process` part. 

@kimchy @rmuir @clintongormley Can you have a look please? thanks.
</comment><comment author="kimchy" created="2015-07-07T14:58:44Z" id="119229360">left a couple of notes, more thoughts:
- I think we should keep the mlockall flag in ProcessInfo.
- Only have getters in Info/Stats
- I suggest we skip getting the user/sys cpu time for the process using threads, I am not terribly sure about it, we can alway add it afterwards?
</comment><comment author="tlrx" created="2015-07-08T12:29:06Z" id="119558576">@kimchy @rmuir I updated the code with your comments. Can you please look again? thanks.

&gt; I suggest we skip getting the user/sys cpu time for the process using threads, I am not terribly sure about it, we can alway add it afterwards?

I removed process cpu user/sys time. I also added a `ProcessProbeBenchmark` class which produces the following results for 100 000 iterations:

```
testing 'getOpenFileDescriptorCount' method...
total [1347] ms, avg [0.01347] ms

testing 'getMaxFileDescriptorCount' method...
total [12] ms, avg [1.2E-4] ms

testing 'getTotalVirtualMemorySize' method...
total [890] ms, avg [0.0089] ms

testing 'getProcessCpuPercent' method...
total [3955] ms, avg [0.03955] ms

testing 'getProcessCpuTotalTime' method...
total [44] ms, avg [4.4E-4] ms

calculating process CPU user time with 'getAllThreadIds + getThreadUserTime' methods...
execution time [total: 2880 ms, avg: 0.0288 ms] for 100000 iterations with average result of 2.5158615E9

calculating process CPU user time with 'getAllThreadIds + getThreadUserTime(long[])' methods...
execution time [total: 2763 ms, avg: 0.02763 ms] for 100000 iterations with average result of 3.4250589E9
```

Calculating user cpu time for the process using threads takes ~0.03 ms. The last benchmark test uses `com.sun.management.ThreadMXBean` to retrieve thread user times for all threads at once.
</comment><comment author="kimchy" created="2015-07-08T12:35:35Z" id="119559754">left a minor comment, LGTM otherwise
</comment><comment author="tlrx" created="2015-07-08T13:15:12Z" id="119574953">@kimchy thanks for the review! I merged with Throwable catch and updated documentation.

@rmuir you were interested in the time the probe takes on Linux: I added the `ProcessProbeBenchmark` class, feel free to comment here if needed.
</comment><comment author="rmuir" created="2015-07-08T13:37:52Z" id="119581759">Thanks for investigating the performance! I think the benchmark is useful because in the past, there has been issues with stats apis being on the slow side. 
</comment><comment author="kimchy" created="2015-07-08T13:44:58Z" id="119583320">++, it helps us decide if we want to add it down the road
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactors TermsQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12042</link><project id="" key="" /><description>Refactors TermsQueryBuilder and Parser for #10217. Also removes deprecated
TermsLookupQuery class and unused or deprecated options `execution`,
`min_should_match`, `lookup_cache`.

This PR is against the query-refactoring branch.
</description><key id="93211769">12042</key><summary>Refactors TermsQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-06T07:34:44Z</created><updated>2015-08-26T12:02:27Z</updated><resolved>2015-08-26T12:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-08-07T16:07:31Z" id="128748504">I did a first review, can you please rebase and address comments? Also, I would like to know from @s1monw and @jpountz what they think about having a Client within IndexQueryParseService, like we are adding here, so that we can retrieve it through QueryShardContext and execute the needed get calls (still on the data node). Wonder if there's any other/better way, I couldn't come up with any.
</comment><comment author="alexksikes" created="2015-08-12T13:33:53Z" id="130303783">@javanna Thanks for the review. Just a couple of notes here. 

1) `execution` and `lookup_cache` although available in the builder, are never used in the parser. 

2) `min_should_match` is used in the parser but marked as deprecated, and can never be set in the builder.

3) TermsLookupQueryBuilder is a pure delegate to TermsQueryBuilder, and Queries.termsLookupQuery directly creates a new TermsQueryBuilder.

I think we should only document the change in migrate doc for 2) and 3) 
</comment><comment author="alexksikes" created="2015-08-12T15:04:39Z" id="130335614">@javanna rebased and addressed comments. We should still figure out how best to mock a handleTermsLookup or a Client in our test. This will also be useful for queries such as MLT. Thanks for the review.
</comment><comment author="javanna" created="2015-08-13T16:49:00Z" id="130756522">I did another round of review, left some comments. As for your notes:
- `execution` is deprecated, in master we read it and ignore it, but we don't throw error when we find it. With this change we do. I think we have to revert this part to be bw compatible
- `cache` seems to be a bug in the builder, cause it supports it but the parser doesn't, so I would expect es to break whenever you set it through the java api or you have cache in your terms lookup query. Can you verify this is the case? If so we should fix this in master and remove the setter from the java api. I think this may have happened when we merged queries and filters, @jpountz might know more about it.
- `min_should_match` is deprecated but still supported in master through parser. This got removed when merging filters and queries too. I am not sure what we should do. If we support it in the parser we have to support it in the builder too in our branch, so either we make a breaking change in our branch or we restore it in the builder. We might want to address this in master too for clarity. @jpountz do you have an opinion on what we should do here?

&gt; TermsLookupQueryBuilder is a pure delegate to TermsQueryBuilder, and Queries.termsLookupQuery directly creates a new TermsQueryBuilder.

yes we had to do this in the early stages of the query-refactoring branch, cause a single builder pointed to multiple builders, which is not possible anymore. We went for leaving the old builder around so we don't break bw comp.
</comment><comment author="jpountz" created="2015-08-14T08:32:51Z" id="131024096">+1 to remove lookupCache from the builder in master. I'm probably the one who forgot to change it, sorry for that. :)

&gt; min_should_match is deprecated but still supported in master through parser. This got removed when merging filters and queries too. I am not sure what we should do. If we support it in the parser we have to support it in the builder too in our branch, so either we make a breaking change in our branch or we restore it in the builder. We might want to address this in master too for clarity. @jpountz do you have an opinion on what we should do here?

The reasoning was that Java users can easily address compilation errors so it was fine to remove in the builder. However, clients for scripting languages don't know until they try so we had to keep bw compat support in the REST layer. If the query-refactoring branch needs to have the same builders and parsers, I think it's best to add it back and deprecated it.
</comment><comment author="javanna" created="2015-08-14T09:52:51Z" id="131053352">See #12870 I pushed those fixed to master and merged them back to the query-refactoring branch, you just need to rebase your PR, here are the summarized changes:
- disableCoord and minimumShouldMatch are back in the builder although deprecated, we still have to parse them and serialize them etc.
- lookupCache has been removed from the builder
- `execution` should still be read in the parser otherwise an exception gets thrown when found, which shouldn't be the case
</comment><comment author="alexksikes" created="2015-08-14T15:51:44Z" id="131157731">@javanna I rebased it and addressed the comments. I haven't addressed the mocking of fetch terms yet though.
</comment><comment author="alexksikes" created="2015-08-15T09:41:36Z" id="131322204">@javanna I have addressed all the comments. We can wait for the other PRs to get in master before rebasing. Thanks for the review.
</comment><comment author="alexksikes" created="2015-08-18T14:02:35Z" id="132219101">@javanna rebased and all comments addressed. I think this one should also be very close. Thanks for the review.
</comment><comment author="javanna" created="2015-08-18T14:20:00Z" id="132227955">I left a few comments, maybe @cbuescher wants to have a look too.
</comment><comment author="cbuescher" created="2015-08-19T15:49:35Z" id="132645269">I also left a couple of comments, please excuse that I initially didn't get the whole constructor logic in the builders right and for that reason left a few remarks too much which I corrected on the second pass. 
</comment><comment author="alexksikes" created="2015-08-20T14:59:50Z" id="133041229">@javanna @cbuescher I'm glad we reached a consensus. All comments should have been addressed. Thanks for the review.
</comment><comment author="javanna" created="2015-08-21T09:46:52Z" id="133356235">thanks for addressing comments, I did another round and left some comments, mostly minor, this is not far at all.
</comment><comment author="javanna" created="2015-08-25T09:57:59Z" id="134542832">This is much closer now I did another round of review and left some comments
</comment><comment author="alexksikes" created="2015-08-25T16:13:37Z" id="134655667">@javanna I addressed all the comments. Hopefully it should be good to go. Thank you for the review.
</comment><comment author="javanna" created="2015-08-26T07:37:14Z" id="134878823">I did a last review round, left some minor comments. LGTM otherwise
</comment><comment author="javanna" created="2015-08-26T10:44:00Z" id="134942195">left one tiny comment, LGTM though
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/search/termslookup/TermsLookupFetchService.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTest.java</file></files><comments><comment>Refactors TermsQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Large aggregation query -&gt; OOM -&gt; Index corruption -&gt; data loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12041</link><project id="" key="" /><description>We have experienced data loss in ES (1.5.2) after running very heavy aggregation queries. We get OOMs (which is not ideal, but tolerable, given our context) but also index corruption and data loss (which is bad).

Any ideas as to what has happened here - and how this could be prevented? Please see the log excerpt:

```
TIMESTAMP="2015-06-29 11:55:49,689" LEVEL="WARN" THREAD="[elasticsearch[server05][search][T#20]]" HOST="server05" LOGGER="index.search.slowlog.query" MESSAGE="[server05] [ua_log_domain_event_2015_06][4] took[27.1m], took_millis[1626309], types[], stats[], search_type[COUNT], total_shards[12], source[(removed)], extra_source[], "
TIMESTAMP="2015-06-29 11:55:49,735" LEVEL="WARN" THREAD="[elasticsearch[server05][scheduler][T#1]]" HOST="server05" LOGGER="monitor.jvm" MESSAGE="[server05] [gc][old][266076][102] duration [18.7m], collections [69]/[19.9m], total [18.7m]/[25.8m], memory [7.9gb]-&gt;[7.8gb]/[7.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [65.2mb]-&gt;[16.5mb]/[66.5mb]}{[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]}"
TIMESTAMP="2015-06-29 11:55:49,752" LEVEL="WARN" THREAD="[elasticsearch[server05][http_server_boss][T#1]{New I/O server boss #51}]" HOST="server05" LOGGER="netty.channel.socket.nio.AbstractNioSelector" MESSAGE="Failed to accept a connection."
java.lang.OutOfMemoryError: Java heap space
TIMESTAMP="2015-06-29 11:55:49,752" LEVEL="WARN" THREAD="[elasticsearch[server05][management][T#4]]" HOST="server05" LOGGER="transport.netty" MESSAGE="[server05] Failed to send error message back to client for action [indices:monitor/stats[s]]"
java.lang.OutOfMemoryError: Java heap space
TIMESTAMP="2015-06-29 11:55:49,773" LEVEL="WARN" THREAD="[elasticsearch[server05][[http_server_worker.default]][T#1]{New I/O worker #18}]" HOST="server05" LOGGER="netty.channel.socket.nio.AbstractNioSelector" MESSAGE="Unexpected exception in the selector loop."
java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
TIMESTAMP="2015-06-29 11:55:49,774" LEVEL="WARN" THREAD="[elasticsearch[server05][[http_server_worker.default]][T#5]{New I/O worker #22}]" HOST="server05" LOGGER="transport.netty" MESSAGE="[server05] exception caught on transport layer [[id: 0x99ba54a2, /10.247.21.4:33540 =&gt; /10.247.20.5:9300]], closing connection"
java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.io.stream.CachedStreamInput.instance(CachedStreamInput.java:48) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.io.stream.CachedStreamInput.cachedHandles(CachedStreamInput.java:62) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:109) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
TIMESTAMP="2015-06-29 11:55:49,774" LEVEL="WARN" THREAD="[elasticsearch[server05][management][T#4]]" HOST="server05" LOGGER="transport.netty" MESSAGE="[server05] Actual Exception"
java.lang.OutOfMemoryError: Java heap space
    at java.io.UnixFileSystem.resolve(UnixFileSystem.java:108) ~[na:1.8.0_31]
    at java.io.File.&lt;init&gt;(File.java:367) ~[na:1.8.0_31]
    at org.apache.lucene.store.FSDirectory$1.accept(FSDirectory.java:221) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at java.io.File.list(File.java:1161) ~[na:1.8.0_31]
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:237) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.elasticsearch.index.store.fs.DefaultFsDirectoryService$1.listAll(DefaultFsDirectoryService.java:57) ~[elasticsearch-1.5.2.jar:na]
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1464) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1456) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1443) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store.stats(Store.java:285) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:600) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:139) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:208) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:56) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:339) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:325) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36) ~[elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
TIMESTAMP="2015-06-29 11:55:49,783" LEVEL="WARN" THREAD="[elasticsearch[server05][generic][T#95]]" HOST="server05" LOGGER="index.store" MESSAGE="[server05] [metrics-2015-06][0] failed to build store metadata. checking segment info integrity (with commit [no])"
java.nio.file.NoSuchFileException: /data/project/nodes/0/indices/metrics-2015-06/0/index/_2wk2.si
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[na:1.8.0_31]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[na:1.8.0_31]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[na:1.8.0_31]
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177) ~[na:1.8.0_31]
    at java.nio.channels.FileChannel.open(FileChannel.java:287) ~[na:1.8.0_31]
    at java.nio.channels.FileChannel.open(FileChannel.java:335) ~[na:1.8.0_31]
    at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:81) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FileSwitchDirectory.openInput(FileSwitchDirectory.java:172) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:683) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:892) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:791) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:768) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store.getMetadata(Store.java:222) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.store.Store.getMetadataOrEmpty(Store.java:185) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:160) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:141) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:62) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36) [elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
TIMESTAMP="2015-06-29 11:55:49,774" LEVEL="WARN" THREAD="[elasticsearch[server05][[http_server_worker.default]][T#9]{New I/O worker #26}]" HOST="server05" LOGGER="netty.channel.socket.nio.AbstractNioSelector" MESSAGE="Unexpected exception in the selector loop."
java.lang.OutOfMemoryError: Java heap space
TIMESTAMP="2015-06-29 11:55:49,808" LEVEL="WARN" THREAD="[elasticsearch[server05][[http_server_worker.default]][T#5]{New I/O worker #22}]" HOST="server05" LOGGER="transport.netty" MESSAGE="[server05] exception caught on transport layer [[id: 0x99ba54a2, /10.247.21.4:33540 :&gt; /10.247.20.5:9300]], closing connection"
java.io.StreamCorruptedException: invalid internal transport message format, got (1d,63,6c,75)
    at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
TIMESTAMP="2015-06-29 11:55:49,818" LEVEL="INFO" THREAD="[elasticsearch[server05][generic][T#95]]" HOST="server05" LOGGER="index.store" MESSAGE="[server05] [metrics-2015-06][0] Failed to open / find files while reading metadata snapshot"
```
</description><key id="93203635">12041</key><summary>Large aggregation query -&gt; OOM -&gt; Index corruption -&gt; data loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kristoffer-dyrkorn</reporter><labels><label>:Core</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-07-06T06:41:29Z</created><updated>2015-09-04T11:07:18Z</updated><resolved>2015-08-25T13:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-06T09:06:11Z" id="118780100">This message is worrisome - it seems you have 7GB of memory, which took 18.7 minutes to GC? What kind of hardware are you running on? did you disable swap?

```
[server05] [gc][old][266076][102] duration [18.7m], collections [69]/[19.9m], total [18.7m]/[25.8m], memory [7.9gb]-&gt;[7.8gb]/[7.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [65.2mb]-&gt;[16.5mb]/[66.5mb]}{[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]}"
```

Once you've reached an OOM you can better restart the node as many unexpected things can happen. It's especially bad of the oom happened in the network stack (like yours did). Can you specify some more details about the corruption reported? 
</comment><comment author="kristoffer-dyrkorn" created="2015-07-06T10:05:26Z" id="118792737">Each data node (like the one above) in our cluster has 16 GB RAM and 8 GB heap.  The aggregations are very heavy, so long GCs are as such not the main problem here (we need to find ways to limit the queries on our part).

My main question here is: Should an OOM occur on one node, would shard corruption - and then data loss - then be one of the possible (expected) outcomes?

The net effect of the crash has been a data loss of 20% of the original data for a specific set of indices. By looking at shard information we see that some shards have less data (take less space on disk) than they should. We also see missing documents in the affected indices - very evenly spread out across our primary key (an event number id - our data are time series).

We suspect that due to OOM some shards have been corrupted and then discarded. We have 1 replica for each shard (that is, 1 primary and 1 replica) in the cluster but this has not prevented data loss. (Yes, we could have had more replicas but that is not the main issue here.)

It seems that long GC pauses have made data nodes unable to connect to the master (we see traces of that in the logs on the data nodes) so an additional factor here might be that the master has not had a chance to become aware of the shard corruption.

Mlockall is set to true by us (in elasticsearch.yml) but reading out the actual value from /_nodes/process shows the actual value is false. We are alredy aware of that - we believe the reason is that we have to use virtual machines on VMWare.

We have not disabled swap explicity, so we run on OS defaults. We are using RHEL (7.0, I think).
</comment><comment author="bleskes" created="2015-07-06T18:26:09Z" id="118947389">&gt; The net effect of the crash has been a data loss of 20% of the original data for a specific set of indices. By looking at shard information we see that some shards have less data (take less space on disk) than they should.

How much less? Shards may very in size based on their merge schedules.

&gt; We also see missing documents in the affected indices - very evenly spread out across our primary key (an event number id - our data are time series).

Do you delete documents ? if so, you should take them into account as the stats report the total amount of documents in the index. It has a separate entry for deleted docs (which will be purge away by the background merge process).

Are there any other signs of corruption? A log entry would be great.

&gt; It seems that long GC pauses have made data nodes unable to connect to the master (we see traces of that in the logs on the data nodes) so an additional factor here might be that the master has not had a chance to become aware of the shard corruption.

A non responsive will be removed from the cluster and it's shard will be assigned somewhere else. Once the node rejoins (after the GC finishes) it will be reassigned shards, which will be resynced to the primaries.
</comment><comment author="kristoffer-dyrkorn" created="2015-07-07T10:06:12Z" id="119152280">&gt;  How much less? Shards may very in size based on their merge schedules.

The indices are time-partitioned (one index for each month of data). Each index has 6 shards. The distribution of data for the May index - after our problems - was as follows:

Shard 0, 2, 3, 4, 5: 4.1-4.2 Gb
Shard 1: 650-660 Mb (primary &amp; replica)

For the February index, the distribution was:

Shard 1: 115 bytes / 79 bytes (primary &amp; replica)
Shard 0, 2, 3, 4, 5: 0.95 - 1.1 Gb

For other indices (not affected by data loss) we see very even shard sizes, within 5% of each other. So we suspect merge schedules are not a factor here.

&gt; Do you delete documents ? 

No. This is mostly an append-only system. At times we update existing documents (which we realise may be implemented internally in Lucene as delete + add), but also here we do not think the differences are due to deletes (i.e. updates), as the indices that not were affected by data loss look very similar to each other and they are all quite different to the ones affected.

&gt; Are there any other signs of corruption? A log entry would be great.

This is for us the the main sign of corruption:

```
java.nio.file.NoSuchFileException: /data/project/nodes/0/indices/metrics-2015-06/0/index/_2wk2.si
```

I will look for other log messages and update this issue. 

We see problems on other data nodes as well (OOMs, missing cluster synchronisation) during the failure period. Here is an example of events from a different node.

First occurence of an OOM:

```
TIMESTAMP="2015-06-29 12:08:42,117" LEVEL="WARN" THREAD="[elasticsearch[server06][refresh][T#3]]" HOST="server06" LOGGER="index.shard" MESSAGE="[server06] [event_2015_06][4] Failed to perform scheduled engine refresh"
org.elasticsearch.index.engine.RefreshFailedEngineException: [event_2015_06][4] Refresh failed
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:576) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:565) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1095) ~[elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
java.lang.OutOfMemoryError: Java heap space
TIMESTAMP="2015-06-29 12:08:42,117" LEVEL="WARN" THREAD="[elasticsearch[server06][flush][T#2]]" HOST="server06" LOGGER="index.engine" MESSAGE="[server06] [event_2015_06][4] failed engine [lucene commit failed]"
org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
    at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:700) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:714) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3100) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.elasticsearch.index.engine.InternalEngine.commitIndexWriter(InternalEngine.java:1226) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:637) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:593) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.shard.IndexShard.flush(IndexShard.java:675) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:203) [elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
Caused by: java.lang.OutOfMemoryError: Java heap space
```

The node is then unable to let the master know of the shard failure:

```
TIMESTAMP="2015-06-29 12:10:32,819" LEVEL="WARN" THREAD="[elasticsearch[server06][clusterService#updateTask][T#1]]" HOST="server06" LOGGER="indices.cluster" MESSAGE="[server06] [[event_2015_06][4]] marking and sending shard failed due to [master [null] marked shard as started, but shard has not been created, mark shard as failed]"
```

The node is then unable to create shards:

```
TIMESTAMP="2015-06-29 12:15:44,793" LEVEL="WARN" THREAD="[elasticsearch[server06][clusterService#updateTask][T#1]]" HOST="server06" LOGGER="indices.cluster" MESSAGE="[server06] [[event_2015_06][0]] marking and sending shard failed due to [failed to create shard]"
org.elasticsearch.index.shard.IndexShardCreationException: [event_2015_06][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:699) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:600) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:183) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:467) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188) [elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158) [elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [event_2015_06][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:415) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:343) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310) ~[elasticsearch-1.5.2.jar:na]
    ... 9 common frames omitted
```

&gt; Once the node rejoins (after the GC finishes) it will be reassigned shards, which will be resynced
&gt; to the primaries.

We experienced problems with reassignments so we had to allocate shards manually to get ES out of its "red state". It seems we hit the case described here:

https://github.com/elastic/elasticsearch/issues/11309

At least, this comment fits very well with our hypothesis (and previous observations):

&gt; I suspect that the same behavior will occur if both primary and replica shards have index
&gt; corruptions.

Still, my main question is: Should an OOM occur on one node, would shard corruption - and then data loss - then be one of the possible (expected) outcomes? 

It would be very valuable for us to have your opinion on whether data loss (on a single node) could be expected should an OOM occur.
</comment><comment author="kristoffer-dyrkorn" created="2015-07-07T13:15:23Z" id="119199964">Searching ES logs for information about the February index (where some shards were emptied) gave the following result:

```
TIMESTAMP="2015-06-29 13:18:56,511" LEVEL="WARN" THREAD="[elasticsearch[server03][generic][T#5]]" HOST="server03" LOGGER="indices.cluster" MESSAGE="[server03] [[event_2015_02][1]] marking and sending shard failed due to [failed recovery]"
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [event_2015_02][1] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:157) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112) ~[elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [event_2015_02][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:131) ~[elasticsearch-1.5.2.jar:na]
    ... 4 common frames omitted
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(default(mmapfs(/data/project/nodes/0/indices/event_2015_02/1/index),niofs(/data/project/nodes/0/indices/event_2015_02/1/index)), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:881) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:769) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:458) ~[lucene-core-4.10.4.jar:4.10.4 1662817 - mike - 2015-02-27 16:38:43]
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:89) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122) ~[elasticsearch-1.5.2.jar:na]
    ... 4 common frames omitted
```

This is the first occurrence of log entry from a data node that has handled those shards (within the period we had failures). So it seems at this point in time, the directory containing this shard (shard 1) has been unexpectedly emptied.

A bit before this, we see that the master node cannot contact this data node. These two log entries (one above, one below) are the only ones that show up in our log analysis tool and that give hints on what happened to the shards.

```
TIMESTAMP="2015-06-29 12:28:09,173" LEVEL="WARN" THREAD="[elasticsearch[server01][clusterService#updateTask][T#1]]" HOST="server01" LOGGER="gateway.local" MESSAGE="[server01] [event_2015_02][1]: failed to list shard stores on node [o7gM5FhPRSKUVqQpxPHq5A]"
org.elasticsearch.action.FailedNodeException: Failed node [o7gM5FhPRSKUVqQpxPHq5A]
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178) ~[elasticsearch-1.5.2.jar:na]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529) ~[elasticsearch-1.5.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
Caused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [server04][inet[/10.247.21.4:9300]][internal:cluster/nodes/indices/shard/store[n]] request_id [3857481] timed out after [30000ms]
    ... 4 common frames omitted
```
</comment><comment author="bleskes" created="2015-07-23T11:02:54Z" id="124058524">&gt; Still, my main question is: Should an OOM occur on one node, would shard corruption - and then data loss - then be one of the possible (expected) outcomes?

OOM should not cause data corruption and certainly not shards to be removed. 

Is the following something that you can reproduce/happens often? I would love to have some trace logs enabled and debug further... 

```
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [event_2015_02][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
```
</comment><comment author="kristoffer-dyrkorn" created="2015-08-07T12:19:44Z" id="128686286">We are working on reproducing the issue now, will post further details here. 
</comment><comment author="clintongormley" created="2015-08-07T12:58:40Z" id="128693515">@kristoffer-dyrkorn this may be related to the issue fixed in https://github.com/elastic/elasticsearch/pull/12487 so I would advise upgrading.
</comment><comment author="kristoffer-dyrkorn" created="2015-08-25T13:34:48Z" id="134586896">We have not been able to recreate the issue and cannot spend further resources on trying. So I am afraid we cannot reach a final conclusion on what went wrong. 

We will however upgrade now.
</comment><comment author="clintongormley" created="2015-08-25T13:50:59Z" id="134592456">thanks @kristoffer-dyrkorn - closing for now. 
</comment><comment author="nilsga" created="2015-09-04T11:07:18Z" id="137705956">This issue happende again today (still on 1.5.2), with the same sequence of errors as described earlier. It does indeed look very similar to https://github.com/elastic/elasticsearch/pull/12487. We lost four shards (2 x primary and replica) in this crash, and both primary and replica were located on the nodes that went OOM. After nodes had been restarted, the files and directories for shard 4 and 5 were no longer on disk on the affected data nodes. Both nodes had long GC pauses, so I guess there were good chances for a race condition to happen. Luckily, it only affected one of the indices, which are backed up nightly, so we were able to restore all the data.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Prepare plugin and integration docs for 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12040</link><project id="" key="" /><description>- Centralised plugin docs in docs/plugins/
- Moved integrations into same docs
- Moved community clients into the clients section of the docs
- Removed docs/community

Closes #11734
Closes #11724
Closes #11636
Closes #11635
Closes #11632
Closes #11630
Closes #12046 
Closes #12438 
Closes #12579 
</description><key id="93122047">12040</key><summary>Docs: Prepare plugin and integration docs for 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>review</label><label>v2.0.0-beta1</label></labels><created>2015-07-05T15:51:56Z</created><updated>2015-08-21T09:38:35Z</updated><resolved>2015-08-15T16:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-05T16:38:07Z" id="118637128">I left some comments. I really like how the documentation is now rendered using same look and feel than the core documentation. With sections... Really great improvement. And searchable I guess! \o/

While reading the documentation, I was also thinking of something which is out of the scope of this PR.

I think that we should split "cloud" plugins in "discovery" plugins and "repository" plugins. They are serving different purposes. It appears really clearly when you read the documentation...
And there is not really common code between the 2 features. But this is something we can talk about in the future :)
</comment><comment author="clintongormley" created="2015-07-05T17:41:30Z" id="118640194">thanks for the review @dadoonet - i've fixed the errors.  
</comment><comment author="clintongormley" created="2015-07-05T17:41:49Z" id="118640211">waiting for a review from @skearns64 
</comment><comment author="dadoonet" created="2015-07-05T20:35:09Z" id="118666341">@clintongormley I commented again. Apart from that it looks good to me.
</comment><comment author="skearns64" created="2015-07-05T22:32:35Z" id="118672393">This is an awesome re-org; it's going to make things a lot easier to find. 
I left a few minor comments, suggestions, and questions, but other than that, LGTM. 
</comment><comment author="nik9000" created="2015-07-27T17:38:35Z" id="125282970">Reviewed. For the most part it looks fine. Just left a few small changes which can totally go in other pull requests.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12040 from clintongormley/plugin_docs</comment></comments></commit></commits></item><item><title>Calculation of ES CPU Usage(in %) goes beyond 100%</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12039</link><project id="" key="" /><description>Hi,

We have 3 Master, 2 Client and 19 Data nodes in our ES cluster. 
Each of the data nodes have 8 VCPUs . We are using the following Rest API URL to get the values of nodes.process.cpu.percent(A) and nodes.os.cpu.total_cores(B),
http://localhost:9200/_cluster/stats?human&amp;pretty
We are trying to calculate ES CPU Usage in percentage

 using the formula, ((A / (100 *B)) \* 100 . This computation is giving us values which go over 100%. 

We observed that the value of (B) as 8 since there are 8 VCPUs in each of the nodes and due to this  the value of (A) should be within 800. But we observed that the value of (A) goes beyond 800 which is causing the percentage to shoot up over 100.

![esstats](https://cloud.githubusercontent.com/assets/11938154/8512277/9a903a00-2359-11e5-8a24-0fa17278c2f9.jpg)

Please can you let us know if our calculations are correct and if so then is the value of (A) from the REST API having some issue.

Thanks 
Shruthi  
</description><key id="93121148">12039</key><summary>Calculation of ES CPU Usage(in %) goes beyond 100%</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shruthisantosh</reporter><labels /><created>2015-07-05T15:37:15Z</created><updated>2015-07-05T17:29:18Z</updated><resolved>2015-07-05T17:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T17:29:17Z" id="118639403">Hi @shruthisantosh 

There are bugs in that statistic coming from sigar.  I'm going to close this issue as we have removed Sigar completely in master https://github.com/elastic/elasticsearch/pull/12010
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Early validation of index requests for bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12038</link><project id="" key="" /><description>In the Java API for adding index requests to a bulk request, the index requests are not validated immediately. This can lead to unexpected NullPointerExceptions when the source is examined.

This pull request adds early validation. It helps to detect bogus index requests before they are sent to the cluster in bulk.

Also, some bulk request test cases are included to check successful validation.
</description><key id="93100422">12038</key><summary>Early validation of index requests for bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>:Bulk</label><label>bug</label><label>stalled</label></labels><created>2015-07-05T10:36:53Z</created><updated>2016-05-14T08:03:17Z</updated><resolved>2016-05-14T08:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-06T09:25:01Z" id="118784660">Thx @jprante . It feels more inline with the rest of the code to do these things in BulkRequest#validateRequest . Maybe something there this missing  (ala a null pointer check)? 
</comment><comment author="jprante" created="2015-07-06T10:47:15Z" id="118806533">@bleskes it seems that BulkRequest#validate() is never used in the current code.
</comment><comment author="bleskes" created="2015-07-07T07:22:37Z" id="119101510">It should be called by TransportAction#execute(Request, org.elasticsearch.action.ActionListener&lt;Response&gt;)  . I wonder how you tested? 
</comment><comment author="jprante" created="2015-07-07T07:32:25Z" id="119104287">The tests are very simple. NPEs can happen before TransportAction#execute.

ES 1.6 examples how to trigger NPE:

```
BulkRequestBuilder builder = new BulkRequestBuilder(client)
                .add(Requests.indexRequest());
```

```
BulkRequestBuilder builder = new BulkRequestBuilder(client)
                .add((IndexRequest)null);
```

```
IndexRequestBuilder r = new IndexRequestBuilder(client);
        BulkRequestBuilder builder = new BulkRequestBuilder(client)
                .add(r.request());
```

```
client.prepareBulk()
                .add(Requests.indexRequest().index(null).type(null).id(null))
```

That is why early validation is needed to reject invalid IndexRequests from being added to a BulkRequest.
</comment><comment author="danielmitterdorfer" created="2016-03-11T09:13:59Z" id="195279691">Hi @jprante,

sorry that the feedback took a while. I've checked your examples against the latest master and the only case that is causing trouble (i.e. NPEs) now is: `BulkRequestBuilder builder = new BulkRequestBuilder(client, BulkAction.INSTANCE).add((IndexRequest)null);`

I agree that it is a programmer error if a `null` value is passed here. We should fail fast in this case and add a precondition check in [`BulkRequest#internalAdd()`](https://github.com/elastic/elasticsearch/blob/192872cadb28f5c886b2a5f5d84807837c2afd7a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java#L128) and all other spots where we add a request to the internal collection. As we can rely on Java 7, I'd use [`Objects#requireNonNull()](http://docs.oracle.com/javase/7/docs/api/java/util/Objects.html#requireNonNull%28T,%20java.lang.String%29) from the JDK for the precondition check. 

Do want to give it a shot?
</comment><comment author="dakrone" created="2016-04-06T20:59:15Z" id="206565379">Hi @jprante is there still interest in working on this PR?
</comment><comment author="danielmitterdorfer" created="2016-05-14T08:03:14Z" id="219207239">I have now created a new PR #18347 that implements the necessary not-null precondition checks and I am closing this PR in favor of the new one.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file></files><comments><comment>Add not-null precondition check in BulkRequest</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file></files><comments><comment>Add not-null precondition check in BulkRequest</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file></files><comments><comment>Add not-null precondition check in BulkRequest</comment></comments></commit></commits></item><item><title>Refactors WrapperQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12037</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="93049870">12037</key><summary>Refactors WrapperQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-07-04T21:26:22Z</created><updated>2015-08-28T10:20:38Z</updated><resolved>2015-08-17T17:54:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-06T08:51:44Z" id="118777906">left some comments
</comment><comment author="alexksikes" created="2015-07-13T15:20:10Z" id="120965002">@javanna Thanks for the review. I updated the PR.
</comment><comment author="MaineC" created="2015-07-14T08:21:57Z" id="121164678">Added some comments.
</comment><comment author="alexksikes" created="2015-07-15T09:47:31Z" id="121558644">@MaineC Thanks I have addressed your comment.
</comment><comment author="alexksikes" created="2015-07-24T14:59:15Z" id="124551168">@javanna I addressed the comments and rebased. You can take a look. Thank you.
</comment><comment author="javanna" created="2015-07-24T15:08:02Z" id="124553103">I did another round, left some comments, also we need some discussion on when we effectively parse the binary query here...
</comment><comment author="alexksikes" created="2015-07-30T13:06:11Z" id="126316445">@javanna I rebased it to incorporate the latest changes. Something I had to do is move supportsBoostAndQueryName() method to AbstractQueryBuilder. Please see comment in code. Thank you.
</comment><comment author="javanna" created="2015-08-04T09:27:59Z" id="127540392">I had a look, left a comment, thinking more about it this query will give us headache, especially in relation to #12527. This is the only query that needs a `QueryParseContext` in `toQuery`, along with the `QueryShardContext`. We will need to discuss this with @cbuescher and see what we can do about it.
</comment><comment author="javanna" created="2015-08-07T15:32:35Z" id="128737613">@alexksikes could you please rebase and address comments?
</comment><comment author="alexksikes" created="2015-08-11T15:45:59Z" id="129937835">@javanna I rebased and addressed the comments. Thanks for the review.
</comment><comment author="javanna" created="2015-08-13T15:17:11Z" id="130726664">did another round, left a few comments
</comment><comment author="alexksikes" created="2015-08-15T12:29:38Z" id="131366575">@javanna comments addressed. Thanks for the review.
</comment><comment author="javanna" created="2015-08-17T10:04:48Z" id="131764389">I did a final round, one old comment on parsing wasn't addressed yet, plus left a minor comment on testing.
</comment><comment author="alexksikes" created="2015-08-17T17:19:23Z" id="131894357">@javanna addressed the comments and rebased. Thanks for the review.
</comment><comment author="javanna" created="2015-08-17T17:31:11Z" id="131897816">LGTM besides the tiny comment I left
</comment><comment author="javanna" created="2015-08-28T10:20:38Z" id="135728241">This change is breaking for the java API as it removes the `WrapperQueryBuilder(byte[] source, int offset, int length)` constructor, use `WrapperQueryBuilder(byte[] source)` instead.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTest.java</file></files><comments><comment>Refactors WrapperQueryBuilder and Parser</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTest.java</file></files><comments><comment>Refactors WrapperQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>[build] cloud-aws doesn't register s3 repos anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12036</link><project id="" key="" /><description>Reported in https://github.com/elastic/elasticsearch/issues/11647#issuecomment-118523861

&gt; btw, I think you broke some plugins on Master, cloud-aws doesn't register s3 repos anymore.

```
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.s3.S3Repository
  while locating org.elasticsearch.repositories.Repository

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:140)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:368)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:55)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:110)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
    at com.amazonaws.AmazonWebServiceClient.&lt;init&gt;(AmazonWebServiceClient.java:129)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:432)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:414)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.getClient(InternalAwsS3Service.java:153)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.client(InternalAwsS3Service.java:82)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:125)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: org.apache.http.protocol.HttpContext
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 37 more
```

Closes #12034.
</description><key id="93035748">12036</key><summary>[build] cloud-aws doesn't register s3 repos anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-04T18:14:50Z</created><updated>2015-08-13T16:31:45Z</updated><resolved>2015-07-08T14:22:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-04T18:15:11Z" id="118539773">@rmuir Can you review it please?
</comment><comment author="rmuir" created="2015-07-08T13:41:30Z" id="119582558">I'm a bit confused why the delete by query pom needs changes (why only that one?). I guess i don't understand the root issue here: is it that we don't have a proper test-framework module and depend on a test-jar (and maven transitive dependencies do not work there)
</comment><comment author="dadoonet" created="2015-07-08T13:48:45Z" id="119584570">Exact @rmuir: as we don't have (yet) a module for test framework transitive dependency don't work so anyone who wants to use `httpclient` in tests has to explicitly define it in the `pom.xml`.

That's what happened with PR #11516: https://github.com/elastic/elasticsearch/pull/11516/files#diff-ef94b341bb064a2092d371fa4cb7504fR58
but it was unfortunately added in `plugins/pom.xml` with `test` scope.

As we don't explicitly define this dependency in cloud-aws (it's a transitive one), its default `compile` scope was overloaded by the one in `plugins/pom.xml`.

So my change is only to move this lib in each project which actually needs it. That being said, with the latest commit I pushed, this PR might now fails REST tests.

Yeah: sadly we don't have a REST Test module which brings its needed `httpclient` dependency.
</comment><comment author="dadoonet" created="2015-07-08T13:52:11Z" id="119586310">Yeah. Rebasing makes that failing now:

```
Caused by: java.lang.ClassNotFoundException: org.apache.http.conn.HttpClientConnectionManager
```

I need to push a new commit
</comment><comment author="rmuir" created="2015-07-08T13:53:15Z" id="119586525">&gt; So my change is only to move this lib in each project which actually needs it. That being said, with the latest commit I pushed, this PR might now fails REST tests

can we avoid this? Seems like instead the ones using it should declare it as compile dependency.
</comment><comment author="rmuir" created="2015-07-08T13:54:29Z" id="119586795">Also i really think we need a test here. Regardless of what happens here this is too fragile. the aws plugin is not working but no tests fail.
</comment><comment author="dadoonet" created="2015-07-08T13:58:13Z" id="119588080">It's an anti-pattern IMO (declaring a dependency that is never used directly in your code) but yes I can do it and add a TODO in the pom.xml to remove it when we will have a REST Test module.

&gt; Also i really think we need a test here. Regardless of what happens here this is too fragile. the aws plugin is not working but no tests fail.

Some tests exists but have been disabled: https://github.com/elastic/elasticsearch-cloud-aws/issues/211
They only run with `-Dtests.thirdparty=true -Dtests.config=/path/to/config`

I agree that we seriously need to invest time on this.
</comment><comment author="rmuir" created="2015-07-08T14:03:09Z" id="119589975">&gt; Some tests exists but have been disabled: elastic/elasticsearch-cloud-aws#211
&gt; They only run with -Dtests.thirdparty=true -Dtests.config=/path/to/config

I think short-term we need jenkins builds going that run these third party tests. I think they are not happening in jenkins for any of these cloud modules, which is bad, because its the majority of their testing today. I know they are sometimes flaky too but we have to do something.
</comment><comment author="rmuir" created="2015-07-08T14:07:43Z" id="119591775">REST test framework needs to share some of the blame here. Why in the world does it use httpclient? Why not just use e.g. HttpURLConnection? 

I think its dangerous for it to bring in this commonly used dependency only for test purposes. Because its in the test classpath, but may not be in the actual compile classpath and can hide bugs. There might be other bugs around this too.
</comment><comment author="dadoonet" created="2015-07-08T14:09:52Z" id="119592275">&gt; REST test framework needs to share some of the blame here. Why in the world does it use httpclient? Why not just use e.g. HttpURLConnection?

:smile: indeed. I did not even think about it!

I can try to look if I can replace it... 
</comment><comment author="dadoonet" created="2015-07-08T14:18:10Z" id="119594278">@rmuir I pushed a new change
</comment><comment author="rmuir" created="2015-07-08T14:19:59Z" id="119594682">+1 thank you @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactors TypeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12035</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="93030402">12035</key><summary>Refactors TypeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-04T17:01:59Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-24T10:13:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-06T08:26:06Z" id="118773201">left a few comments
</comment><comment author="alexksikes" created="2015-07-13T13:12:52Z" id="120923375">@javanna Thanks for the review. I updated the PR accordingly.
</comment><comment author="javanna" created="2015-07-14T06:14:44Z" id="121140960">did another round, left some comments
</comment><comment author="alexksikes" created="2015-07-15T10:28:45Z" id="121569080">@javanna I updated the PRs and responded to the comments.
</comment><comment author="alexksikes" created="2015-07-24T10:01:17Z" id="124460916">@javanna it's rebased, you can take a look. Thank you.
</comment><comment author="javanna" created="2015-07-24T10:04:10Z" id="124461438">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java</file></files><comments><comment>Refactors TypeQuery</comment></comments></commit></commits></item><item><title>[build] cloud-aws doesn't register s3 repos anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12034</link><project id="" key="" /><description>Reported in https://github.com/elastic/elasticsearch/issues/11647#issuecomment-118523861

&gt; btw, I think you broke some plugins on Master, cloud-aws doesn't register s3 repos anymore.

```
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.s3.S3Repository
  while locating org.elasticsearch.repositories.Repository

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:140)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:368)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:55)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:110)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
    at com.amazonaws.AmazonWebServiceClient.&lt;init&gt;(AmazonWebServiceClient.java:129)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:432)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:414)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.getClient(InternalAwsS3Service.java:153)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.client(InternalAwsS3Service.java:82)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:125)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: org.apache.http.protocol.HttpContext
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 37 more
```
</description><key id="93029583">12034</key><summary>[build] cloud-aws doesn't register s3 repos anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>blocker</label><label>bug</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-04T16:51:12Z</created><updated>2015-07-08T14:22:22Z</updated><resolved>2015-07-08T14:22:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-04T17:06:39Z" id="118534918">It sounds like that this dependency in plugin `pom.xml` is the culprit here:

``` xml
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;
            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
```

httpclient and its dependencies (httpcore) is marked as a test dependency.
But httpclient is also a transitive dependency for aws sdk:

```
[INFO] +- com.amazonaws:aws-java-sdk-ec2:jar:1.10.0:compile
[INFO] |  \- com.amazonaws:aws-java-sdk-core:jar:1.10.0:compile
[INFO] |     +- commons-logging:commons-logging:jar:1.1.3:compile
[INFO] |     \- org.apache.httpcomponents:httpclient:jar:4.3.6:compile
[INFO] |        +- org.apache.httpcomponents:httpcore:jar:4.3.3:compile
[INFO] |        \- commons-codec:commons-codec:jar:1.6:compile
```

When httpclient is marked with a `test` scope, it's ignored then as it has precedence on transitive dependencies. `mvn dependency:tree` gives:

```
[INFO] --- maven-dependency-plugin:2.10:tree (default-cli) @ elasticsearch-cloud-aws ---
[INFO] org.elasticsearch.plugin:elasticsearch-cloud-aws:jar:2.0.0-SNAPSHOT
[INFO] +- com.amazonaws:aws-java-sdk-ec2:jar:1.10.0:compile
[INFO] |  \- com.amazonaws:aws-java-sdk-core:jar:1.10.0:compile
[INFO] +- com.amazonaws:aws-java-sdk-s3:jar:1.10.0:compile
[INFO] |  \- com.amazonaws:aws-java-sdk-kms:jar:1.10.0:compile
[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test
[INFO] +- org.apache.lucene:lucene-test-framework:jar:5.2.1:test
[INFO] |  +- org.apache.lucene:lucene-codecs:jar:5.2.1:test
[INFO] |  +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.14:test
[INFO] |  +- junit:junit:jar:4.11:test
[INFO] |  \- org.apache.ant:ant:jar:1.8.2:test
[INFO] +- org.elasticsearch:elasticsearch:test-jar:tests:2.0.0-SNAPSHOT:test
[INFO] |  \- com.twitter:jsr166e:jar:1.1.0:provided
[INFO] +- org.elasticsearch:elasticsearch:jar:2.0.0-SNAPSHOT:provided
[INFO] +- org.apache.lucene:lucene-core:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-queries:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-memory:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-highlighter:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-queryparser:jar:5.2.1:provided
[INFO] |  \- org.apache.lucene:lucene-sandbox:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-suggest:jar:5.2.1:provided
[INFO] |  \- org.apache.lucene:lucene-misc:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-join:jar:5.2.1:provided
[INFO] |  \- org.apache.lucene:lucene-grouping:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-spatial:jar:5.2.1:provided
[INFO] +- org.apache.lucene:lucene-expressions:jar:5.2.1:provided
[INFO] |  +- org.antlr:antlr-runtime:jar:3.5:provided
[INFO] |  +- org.ow2.asm:asm:jar:4.1:provided
[INFO] |  \- org.ow2.asm:asm-commons:jar:4.1:provided
[INFO] +- com.spatial4j:spatial4j:jar:0.4.1:provided
[INFO] +- com.vividsolutions:jts:jar:1.13:provided
[INFO] +- com.github.spullara.mustache.java:compiler:jar:0.8.13:provided
[INFO] +- com.google.guava:guava:jar:18.0:provided
[INFO] +- com.carrotsearch:hppc:jar:0.7.1:provided
[INFO] +- joda-time:joda-time:jar:2.8:provided
[INFO] +- org.joda:joda-convert:jar:1.2:provided
[INFO] +- com.fasterxml.jackson.core:jackson-core:jar:2.5.3:provided
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:provided
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:provided
[INFO] |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.5.3:compile
[INFO] |  |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.5.0:compile
[INFO] |  \- org.yaml:snakeyaml:jar:1.12:provided
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:provided
[INFO] +- io.netty:netty:jar:3.10.0.Final:provided
[INFO] +- com.ning:compress-lzf:jar:1.0.2:provided
[INFO] +- com.tdunning:t-digest:jar:3.0:provided
[INFO] +- org.apache.commons:commons-lang3:jar:3.3.2:provided
[INFO] +- commons-cli:commons-cli:jar:1.2:provided
[INFO] +- org.codehaus.groovy:groovy-all:jar:indy:2.4.0:provided
[INFO] +- log4j:log4j:jar:1.2.17:provided
[INFO] +- log4j:apache-log4j-extras:jar:1.2.17:provided
[INFO] +- org.slf4j:slf4j-api:jar:1.6.2:provided
[INFO] +- net.java.dev.jna:jna:jar:4.1.0:provided
[INFO] \- org.apache.httpcomponents:httpclient:jar:4.3.6:test
[INFO]    +- org.apache.httpcomponents:httpcore:jar:4.3.3:test
[INFO]    +- commons-logging:commons-logging:jar:1.1.3:compile
[INFO]    \- commons-codec:commons-codec:jar:1.6:test
```

We should change the scope of `httpclient` explicitly for every plugin which requires it. Will come with a PR for it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[build] cloud-aws doesn't register s3 repos anymore</comment></comments></commit></commits></item><item><title>Snapshots repo not found after upgrade to 1.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12033</link><project id="" key="" /><description>I upgraded from 1.4.4 to 1.6.0 but now my snapshot repositories are can no longer be found via RESTful API.  I can see the repository attributes with this command:

GET _snapshot/twitter_backup3/

{
   "twitter_backup3": {
      "type": "fs",
      "settings": {
         "compress": "true",
         "location": "/media/snaps3"
      }
   }
}

But when I try to look at one of the snapshots I get the following error:

GET _snapshot/twitter_backup3/2015-07-01

{
   "error": "RemoteTransportException[[r5-9-37][inet[/10.202.245.1:9300]][cluster:admin/snapshot/get]]; nested: RepositoryMissingException[[twitter_backup3] missing]; ",
   "status": 404
}

The snapshot files are still visible on disk.  Also I'm not having this problem on 1.5.2.
</description><key id="93024892">12033</key><summary>Snapshots repo not found after upgrade to 1.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vichargrave</reporter><labels /><created>2015-07-04T16:10:49Z</created><updated>2015-07-05T19:26:52Z</updated><resolved>2015-07-05T17:26:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-04T16:45:02Z" id="118533259">I think it could be due because in 1.6, you need to add a new setting in `elasticsearch.yml`: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository

Try to add: `path.repo: ["/media"]` and restart your node.
</comment><comment author="vichargrave" created="2015-07-04T19:33:14Z" id="118544934">I tried this out on Elasticsearch on my MacOS.  It seems that I have to restart ES for these settings to take affect.  Is that correct?
</comment><comment author="dadoonet" created="2015-07-04T20:05:26Z" id="118547370">Yes. I wrote "restart your node".
</comment><comment author="clintongormley" created="2015-07-05T17:26:24Z" id="118639295">@vichargrave I assume @dadoonet 's advice fixed the issue, so I'll close 
</comment><comment author="vichargrave" created="2015-07-05T19:26:52Z" id="118654095">@dadoonet Right, sorry about that.  Thanks to the help.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactors PrefixQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12032</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="93023250">12032</key><summary>Refactors PrefixQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-04T15:45:04Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-24T10:06:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-06T08:03:09Z" id="118765273">left a couple of small comments
</comment><comment author="alexksikes" created="2015-07-07T20:56:23Z" id="119337034">@javanna @cbuescher Thanks for the review. I updated the PR accordingly.
</comment><comment author="javanna" created="2015-07-08T08:14:00Z" id="119487728">looks good to me, @cbuescher can you have a final look plese?
</comment><comment author="cbuescher" created="2015-07-08T08:48:35Z" id="119501999">Did another round of reviews, left just two question/comments regarding setup of test query, otherwise looks good.
</comment><comment author="alexksikes" created="2015-07-09T09:05:13Z" id="119881531">@javanna @cbuescher Thanks for the review. I rebased the PR and updated it.
</comment><comment author="javanna" created="2015-07-09T16:19:33Z" id="120055849">left a couple of minor comments, I would wait for #12124 (or corresponding adapted PR) before getting this in though.
</comment><comment author="cbuescher" created="2015-07-09T16:34:54Z" id="120061890">Also did another round and left a couple of comments.
</comment><comment author="alexksikes" created="2015-07-13T12:48:56Z" id="120918147">@javanna Thanks for the review. I rebased and updated the PR accordingly.
</comment><comment author="alexksikes" created="2015-07-15T10:47:30Z" id="121573786">@javanna Thanks for the review. We need to address https://github.com/elastic/elasticsearch/pull/11865#discussion_r34343176 before it can be pushed though, and waiting on https://github.com/elastic/elasticsearch/pull/12204 as well.
</comment><comment author="alexksikes" created="2015-07-24T09:49:11Z" id="124458754">@javanna it's rebased, you can take a look. Thank you.
</comment><comment author="javanna" created="2015-07-24T09:52:59Z" id="124459428">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java</file></files><comments><comment>Refactors PrefixQuery</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file></files><comments><comment>PrefixQueryParser takes a String as value like its Builder</comment></comments></commit></commits></item><item><title>Refactoring of Indices Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12031</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="92998819">12031</key><summary>Refactoring of Indices Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-04T10:15:05Z</created><updated>2015-09-14T21:30:55Z</updated><resolved>2015-08-18T13:36:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-07-06T11:25:09Z" id="118824144">Left a few comments, however I'm not sure how we should make the clusterService accessible in the builder. I think we need it there, but not sure how to make it accessible. For now using QueryParseContext and inject it there might be okay, on the long run we might need a similar context object but not for parsing but for the `toQuery()` step.
</comment><comment author="alexksikes" created="2015-07-07T20:58:16Z" id="119337939">I agree this is something we need to discuss. This is more temporary until we figure this out.
</comment><comment author="javanna" created="2015-08-07T15:02:38Z" id="128724610">hey @alexksikes I finally got back to this, did a review round and played around with this as well. I think it is not far, could you please rebase it and update it? I would love to get it in soon, refactoring this query will mean some more cleanups that can be done afterwards in QueryParseContext I think. Thanks!
</comment><comment author="javanna" created="2015-08-07T15:06:52Z" id="128725411">btw you should stumble upon a test in SearchQueryIT that will fail (`testIndicesQuerySkipParsing`). I think that it cannot run successfully till we have rafactored nested queries, hence we need to mark @AwaitsFix and make sure we don't forget to get back to it.
</comment><comment author="alexksikes" created="2015-08-11T13:41:13Z" id="129877034">@javanna I addressed all the comments and rebased. Thanks for the review.
</comment><comment author="javanna" created="2015-08-11T14:30:18Z" id="129906086">I did another round, looks good. Left a few more comments, maybe @cbuescher wants to have a look too?
</comment><comment author="alexksikes" created="2015-08-12T10:15:07Z" id="130248972">@javanna I addressed the comments. Not exactly sure how best to setup the cluster service for our test. Thanks for the review.
</comment><comment author="cbuescher" created="2015-08-12T10:40:43Z" id="130253258">I think this is good, left only two small comments.
</comment><comment author="javanna" created="2015-08-13T10:29:36Z" id="130605527">I did another round and left a few comments. I think we are also missing some documentation for the new introduced query, unless we decide to not expose it to users, which we are doing at the moment though. @clintongormley do you have an opinion about this? Do you think a match_none query would be useful to users or shall we keep it internal only?
</comment><comment author="alexksikes" created="2015-08-15T11:53:58Z" id="131359465">@javanna I addressed the comments. I am not exactly sure how best to inject the indices to the test cluster service, if you have any ideas?
</comment><comment author="clintongormley" created="2015-08-17T10:56:41Z" id="131781126">&gt; Do you think a match_none query would be useful to users or shall we keep it internal only?

I think it is for internal use only
</comment><comment author="alexksikes" created="2015-08-17T11:42:04Z" id="131791757">Given that the builder API now takes a QueryBuilder for the noMatch query, I would think we can't make it internal. So it would have to be public, and used for the sole purpose of being able to specify a query that matches none?
</comment><comment author="javanna" created="2015-08-17T12:58:55Z" id="131809480">&gt; Given that the builder API now takes a QueryBuilder for the noMatch query, I would think we can't make it internal. So it would have to be public, and used for the sole purpose of being able to specify a query that matches none?

Here is how we can make that query internal: https://github.com/elastic/elasticsearch/pull/12031#discussion_r37177175 . People can still use if from the java api if they want to as the class is public, but it will not be exposed as other regular queries as it has no parser etc. 
</comment><comment author="javanna" created="2015-08-17T14:37:52Z" id="131843611">I did another round of review and opened #12937 to expand testing here.
</comment><comment author="alexksikes" created="2015-08-17T16:41:44Z" id="131885321">OK then once we get #12937 in, I'll address all the comments and rebase. Thanks.
</comment><comment author="javanna" created="2015-08-18T07:32:15Z" id="132102778">#12937 is in, you can go ahead and rebase.
</comment><comment author="alexksikes" created="2015-08-18T12:53:37Z" id="132197316">@javanna rebased and all comments addressed. I think it should be ready. Thanks for the review.
</comment><comment author="javanna" created="2015-08-18T13:13:49Z" id="132201212">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java</file><file>core/src/test/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Refactoring of Indices Query</comment></comments></commit></commits></item><item><title>Refactoring of MissingQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12030</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="92919726">12030</key><summary>Refactoring of MissingQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-03T18:28:50Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-24T09:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-06T07:54:49Z" id="118762430">left two tiny comments, looks good though
</comment><comment author="alexksikes" created="2015-07-07T20:20:40Z" id="119324293">@javanna Thanks for the review. I updated the PR accordingly.
</comment><comment author="javanna" created="2015-07-08T08:03:31Z" id="119483763">left a couple of minor comments, @cbuescher do you mind having a look too?
</comment><comment author="cbuescher" created="2015-07-08T08:31:10Z" id="119492922">Also did a round of reviews, left some remarks.
</comment><comment author="alexksikes" created="2015-07-13T08:08:13Z" id="120844163">@javanna @cbuescher thanks for the review. I updated the PR accordingly.
</comment><comment author="javanna" created="2015-07-13T08:25:15Z" id="120846724">did another round, left a few small comments
</comment><comment author="alexksikes" created="2015-07-13T08:55:56Z" id="120855778">@javanna I updated the PR. Thanks.
</comment><comment author="MaineC" created="2015-07-14T08:38:45Z" id="121167279">Left a few comments.
</comment><comment author="alexksikes" created="2015-07-15T12:29:42Z" id="121601843">@MaineC Thanks for the review. I updated the PR.
</comment><comment author="alexksikes" created="2015-07-23T15:24:22Z" id="124138892">@javanna It's rebased you can take a look. Thank you.
</comment><comment author="javanna" created="2015-07-23T15:27:56Z" id="124139990">left one minor comment
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java</file></files><comments><comment>Refactoring of MissingQuery</comment></comments></commit></commits></item><item><title>[maven] change groupId / artifactId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12029</link><project id="" key="" /><description>When we generate our project, we can get something like:

```
├── dev-tools
├── elasticsearch
├── elasticsearch-parent
├── elasticsearch-plugin
├── plugin
│   ├── elasticsearch-analysis-icu
│   ├── elasticsearch-analysis-kuromoji
│   ├── elasticsearch-analysis-phonetic
│   ├── elasticsearch-analysis-smartcn
│   ├── elasticsearch-analysis-stempel
│   ├── elasticsearch-cloud-aws
│   ├── elasticsearch-cloud-azure
│   ├── elasticsearch-cloud-gce
│   ├── elasticsearch-delete-by-query
│   ├── elasticsearch-lang-javascript
│   └── elasticsearch-lang-python
├── rest-api-spec
└── securemock
```

I propose here to use a common naming for artifacts: start always with `elasticsearch-`.
Also, move `elasticsearch-plugin` to `org.elasticsearch.plugin` groupId.

So we could have:

```
├── elasticsearch
├── elasticsearch-dev-tools
├── elasticsearch-parent
├── elasticsearch-rest-api-spec
├── elasticsearch-securemock
├── plugin
│   ├── elasticsearch-analysis-icu
│   ├── elasticsearch-analysis-kuromoji
│   ├── elasticsearch-analysis-phonetic
│   ├── elasticsearch-analysis-smartcn
│   ├── elasticsearch-analysis-stempel
│   ├── elasticsearch-cloud-aws
│   ├── elasticsearch-cloud-azure
│   ├── elasticsearch-cloud-gce
│   ├── elasticsearch-delete-by-query
│   ├── elasticsearch-lang-javascript
│   ├── elasticsearch-lang-python
│   └── elasticsearch-plugin
```
</description><key id="92917324">12029</key><summary>[maven] change groupId / artifactId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T18:03:36Z</created><updated>2015-07-06T15:24:46Z</updated><resolved>2015-07-06T15:24:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-03T19:08:18Z" id="118409671">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update bulk.json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12028</link><project id="" key="" /><description /><key id="92914137">12028</key><summary>Update bulk.json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sarimzafar</reporter><labels /><created>2015-07-03T17:39:56Z</created><updated>2015-07-05T17:18:26Z</updated><resolved>2015-07-05T17:18:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T17:18:25Z" id="118639033">Did you open this in error?  Your change just indents some JSON incorrectly, no?

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Special case the `_index` field in queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12027</link><project id="" key="" /><description>This change allows queries to test the name of the index by referring to the _index field in query clauses. Adding this capability means we can deprecate the specialist indices query.

IndexFieldMapper is changed to make the term(s) query factory methods instead produce match_all or match_none queries based on tests of the index name.

Closes #3316
</description><key id="92909521">12027</key><summary>Special case the `_index` field in queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T16:55:58Z</created><updated>2015-07-08T10:18:40Z</updated><resolved>2015-07-08T09:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-05T20:05:00Z" id="118662063">LGTM!
</comment><comment author="markharwood" created="2015-07-06T10:35:26Z" id="118804476">Pushed to master https://github.com/elastic/elasticsearch/commit/f95981b97760ae8f2b248f143f360173b7572918
</comment><comment author="jpountz" created="2015-07-07T22:38:26Z" id="119364459">@markharwood Should it be closed since it was pushed?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file></files><comments><comment>Mappings: Remove ability to configure _index</comment></comments></commit></commits></item><item><title>add integration test harness to maven build</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12026</link><project id="" key="" /><description>Our unit testing setup is designed for unit tests, and its not really appropriate for integration tests (its not totally realistic).

Instead I think we should add real integration test setup, with the maven plugins designed to do that, that runs after packaging, unzips elasticsearch, starts up external cluster, runs rest tests against it, and then shuts down the cluster. People already know the rest tests framework and we already have a bunch of tests.

I need this kind of thing to fix stuff like jar hell issues for good, and to have more confidence in stuff like security manager and other tricky things.

Today we kind of have it, but its wierd and run from a python release script. Instead I think integ tests should be a regular part of our build.

I made a very simple initial impl and it finds bugs already ;)

As a followup I would like to extend this to plugins (I will start with delete-by-query, it already has rest tests defined). So please don't worry too much about where the logic currently is, we can refactor as needed after that.
</description><key id="92905076">12026</key><summary>add integration test harness to maven build</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-07-03T16:25:52Z</created><updated>2015-07-06T14:16:56Z</updated><resolved>2015-07-06T14:16:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-03T16:29:11Z" id="118387331">More information, we use maven failsafe plugin, so it runs in `mvn verify`, which is after all packaging tasks. 

Note that if you run `mvn verify`, don't be surprised to see it run the unit tests first (just like `mvn package` does). This is just how maven works.

I added `-Dskip.unit.tests` and `-Dskip.integ.tests` properties for more fine grained control of `-DskipTests` (which still works and just turns them both off).
</comment><comment author="dadoonet" created="2015-07-03T16:35:23Z" id="118388771">That's fantastic @rmuir! I love it.
</comment><comment author="rjernst" created="2015-07-04T19:01:42Z" id="118542747">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/rest/RestIT.java</file></files><comments><comment>Merge pull request #12026 from rmuir/integ_tests</comment></comments></commit></commits></item><item><title>JAVA_HOME not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12025</link><project id="" key="" /><description>I just installed elasticsearch and I already added JAVA_HOME variable to /etc/environment but elasticsearch keep asking me to add JAVA_HOME. my java installation directory is /home/mockie/softwares/jdk1.8.0_45. I'm pretty sure this is the correct path since SOLR can find it and run successfully.

I tried to edit /etc/default/elasticsearch by adding JAVA_HOME=/home/mockie/softwares/jdk1.8.0_45 variable at the top of the file but it didn't work too with error `touch: cannot touch ‘/var/run/elasticsearch/elasticsearch.pid’: No such file or directory`.
</description><key id="92899193">12025</key><summary>JAVA_HOME not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mockiemockiz</reporter><labels /><created>2015-07-03T15:45:23Z</created><updated>2015-07-03T19:38:32Z</updated><resolved>2015-07-03T19:38:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Search for results based on aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12024</link><project id="" key="" /><description>The use of aggregation is popular for the use case of **In this date range, how many visits we have in total?**

However, what should I do when I want something reverse: **On which days does I have the visits range from x to y?**

I can think of we still have to use Aggregation to compute total visits of each day, and from that result, we will do a Range Aggregation to filter the result. But I can not come up with a query. 

Thanks a lot for your help!
</description><key id="92891358">12024</key><summary>Search for results based on aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ptgamr</reporter><labels /><created>2015-07-03T14:58:53Z</created><updated>2015-07-04T00:28:09Z</updated><resolved>2015-07-04T00:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-07-04T00:23:15Z" id="118439815">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting and design help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="ptgamr" created="2015-07-04T00:27:59Z" id="118441181">thanks @markwalkom , here is the relevance link to the thread: https://discuss.elastic.co/t/search-for-results-based-on-aggregations/24888/1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Generify Index and Shard exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12023</link><project id="" key="" /><description>Today we have a intermediate hierarchy for shard and index exceptions
which makes it hard to introduce generic exceptions like ResourceNotFoundException
intoduced in this commit. This commit breaks up the hierarchy by adding index and shard
as a special internal header that gets rendered for every exception that fills that header.
This commit removes dedicated exceptions like `IndexMissingException` or
`IndexShardMissingException` in favour of `ResourceNotFoundException`
</description><key id="92889168">12023</key><summary>Generify Index and Shard exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T14:45:44Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-14T14:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-07-03T15:33:45Z" id="118375730">this is a great start! left some comments
</comment><comment author="s1monw" created="2015-07-06T20:28:17Z" id="118988166">I added a new commit @uboness 
</comment><comment author="bleskes" created="2015-07-07T10:18:27Z" id="119159909">I went through this and left some comments here and there. My main concern is that we are inconsistent in using IndexNotFoundException and it's base class  ResourceNotFoundException (with it's index set). I think we should make a choice and be consistent. I tend to using a dedicated IndexNotFoundException. I think it will make the code easier to read. 
</comment><comment author="s1monw" created="2015-07-14T08:33:48Z" id="121166416">@bleskes @uboness I pushed new commits
</comment><comment author="bleskes" created="2015-07-14T09:22:47Z" id="121177840">Thx @s1monw . Left some minor comments which I don't think merit another round. LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Refactoring: Set default minimumShouldMatch for BoolQueryBuilder in filter context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12022</link><project id="" key="" /><description>In BoolQueryParser we currently set minimumShouldMatch to at least 1 (see #11915 for initial discussion), if there is no other value supplied in the JSON and the BoolQuery sits inside a filter. This should only happen when the first clause gets added to the should-clauses. 

Because we will only know if a QueryBuilder will generate a non-null query when calling its `toQuery` method later, this should happen in the BoolQueryBuilder. The problem with this is that currently now we are using the isFilter() state of the QueryParseContex, but we don't set this when traversing the query-tree when generating the lucene queries in `toQuery()` yet.

First we will need to dividing the context into a query parse context and a query context I think and then start setting  correct filter flag when entery subclauses in filter context like we now to in `parseInnerFilter()` in QueryParseContext.

This issue only affects the query refactoring feature branch.
</description><key id="92877805">12022</key><summary>Query Refactoring: Set default minimumShouldMatch for BoolQueryBuilder in filter context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-07-03T13:35:25Z</created><updated>2015-08-07T16:20:05Z</updated><resolved>2015-08-07T16:20:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-07-03T13:45:43Z" id="118356257">Maybe it's also possible to handle the default setting for `minimumShouldMatch` similarly for filter context and query context. The conditional clause this was introduced as a result of merging BoolQueryParser and BoolFilterParser (a0af88e), maybe it is possible to remove this special case which would make it possible to set it on `toQuery` without having the filter context information available. 
</comment><comment author="javanna" created="2015-08-04T07:55:08Z" id="127511094">just so we don't forget, `isFilter` is used also in other queries while parsing (e.g. fuzzy query), this logic needs to move to the `toQuery` method, which means that we can probably get rid of this piece of state in the context.
</comment><comment author="cbuescher" created="2015-08-07T16:20:05Z" id="128752490">Closing this, since it is addressed in #12731 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GC Log - Duplicate GCTimeStamps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12021</link><project id="" key="" /><description>Hi,

We have any issue with duplicate GCTimeStamps appearing in GC logs, e.g. -

```
1.027: Total time for which application threads were stopped: 0.0000230 seconds
1.686: [GC1.687: [DefNew
Desired survivor size 4358144 bytes, new threshold 1 (max 6)
- age   1:    8716288 bytes,    8716288 total
: 68160K-&gt;8512K(76672K), 0.0284560 secs] 68160K-&gt;13947K(253632K), 0.0287330 secs] [Times: user=0.02 sys=0.00, real=0.03 secs]
1.715: Total time for which application threads were stopped: 0.0288730 seconds
2.731: Total time for which application threads were stopped: 0.0001220 seconds
3.045: Total time for which application threads were stopped: 0.0000670 seconds
3.736: [GC3.736: [DefNew
Desired survivor size 4358144 bytes, new threshold 1 (max 6)
- age   1:    8716288 bytes,    8716288 total
: 76672K-&gt;8512K(76672K), 0.0445610 secs] 82107K-&gt;20595K(253632K), 0.0446170 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]
```

Notice the repeated timestamp after `[GC`.

We're seeing this behaviour on Ubuntu 14.04.2 LTS using ES 1.5.2 and 1.6.0 with both the OpenJDK JRE 7 and Oracle JRE 7.  I'm not seeing it on OS X 10.10 however.

I've tested other Java apps with the same setup and they don't have duplicate timestamps so this appears to be Elasticsearch specific.

GCDateStamps also get duplicated if we enable them via ES_JAVA_OPTS e.g. -

```
2015-07-03T13:29:43.859+0000: 594133.063: [GC2015-07-03T13:29:43.859+0000: 594133.063: [ParNew: 281570K-&gt;11211K(306688K), 0.0516960 secs] 2889367K-&gt;2619012K(15673088K), 0.0517990 secs] [Times: user=0.20 sys=0.00, real=0.05 secs]
2015-07-03T13:30:24.341+0000: 594173.544: [GC2015-07-03T13:30:24.341+0000: 594173.544: [ParNew: 283851K-&gt;10915K(306688K), 0.0490020 secs] 2891652K-&gt;2618718K(15673088K), 0.0491160 secs] [Times: user=0.19 sys=0.01, real=0.05 secs]
2015-07-03T13:31:16.119+0000: 594225.322: [GC2015-07-03T13:31:16.119+0000: 594225.322: [ParNew: 283555K-&gt;10013K(306688K), 0.0528850 secs] 2891358K-&gt;2618616K(15673088K), 0.0529900 secs] [Times: user=0.21 sys=0.00, real=0.05 secs]
2015-07-03T13:31:56.255+0000: 594265.458: [GC2015-07-03T13:31:56.255+0000: 594265.458: [ParNew: 282653K-&gt;11148K(306688K), 0.0504550 secs] 2891256K-&gt;2619756K(15673088K), 0.0505590 secs] [Times: user=0.20 sys=0.00, real=0.05 secs]
2015-07-03T13:32:46.685+0000: 594315.888: [GC2015-07-03T13:32:46.685+0000: 594315.888: [ParNew: 283788K-&gt;12110K(306688K), 0.0531040 secs] 2892396K-&gt;2620723K(15673088K), 0.0532130 secs] [Times: user=0.21 sys=0.00, real=0.06 secs]
```

This is causing problems when trying to parse the logs in our logging system.

Any suggestions would be appreciated.

Thanks,

Matt.
</description><key id="92873208">12021</key><summary>GC Log - Duplicate GCTimeStamps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattmonkey83</reporter><labels><label>:Logging</label></labels><created>2015-07-03T13:14:11Z</created><updated>2015-07-06T14:58:14Z</updated><resolved>2015-07-06T14:58:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattmonkey83" created="2015-07-06T14:58:13Z" id="118881064">Sorry - I've since noticed this behaviour on the same setup for a different Java app.

Only seems to occur when the '-XX:+UseConcMarkSweepGC' option is used.

And it actually does appear to be present on OS X too when using Oracle Java 1.7.0_79-b15.  Not sure how I missed it previously.

I've since tried using Oracle JRE 8 on Ubuntu 14.04.2 LTS and the issue does **not** occur.  So am assuming this is a Java 7 issue.  Will try to raise an issue via OpenJDK project.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix FuzzyQuery to properly handle Object, number, dates or String. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12020</link><project id="" key="" /><description>This makes FuzzyQueryBuilder and Parser take an Object as a value using the
same logic as termQuery, so that numbers, dates or Strings would be properly
handled.

Relates #11865
</description><key id="92868887">12020</key><summary>Fix FuzzyQuery to properly handle Object, number, dates or String. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T12:48:36Z</created><updated>2015-07-08T09:00:38Z</updated><resolved>2015-07-08T09:00:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-03T12:53:10Z" id="118339640">I think we also wanted to change the parser to parse the value as an object instead of a string?
</comment><comment author="alexksikes" created="2015-07-03T12:54:18Z" id="118339781">Yep I wasn't sure of that, I'll fix this.
</comment><comment author="jpountz" created="2015-07-07T11:31:53Z" id="119177252">I left some comments.
</comment><comment author="jpountz" created="2015-07-07T14:07:35Z" id="119213807">LGTM
</comment><comment author="alexksikes" created="2015-07-08T07:48:50Z" id="119473725">@jpountz @javanna Thanks for the review. I updated the PR accordingly.
</comment><comment author="javanna" created="2015-07-08T08:08:49Z" id="119486537">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file></files><comments><comment>Fix FuzzyQuery to properly handle Object, number, dates or String.</comment></comments></commit></commits></item><item><title>ZenDiscovery: #11960 failed to remove eager reroute from node join</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12019</link><project id="" key="" /><description>This commit fixes it and adds an assert that an initial cluster state from master never has shards assigned to this node
</description><key id="92851914">12019</key><summary>ZenDiscovery: #11960 failed to remove eager reroute from node join</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>bug</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T11:05:17Z</created><updated>2015-07-03T20:13:33Z</updated><resolved>2015-07-03T20:08:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-07-03T11:36:45Z" id="118323227">LGTM
</comment><comment author="bleskes" created="2015-07-03T20:13:24Z" id="118419678">pushed to 2.0 &amp; 1.x . Had to remove the assertion as there is a case where the joining node will have shards assigned to it upon first publish - i.e., a master failure followed by another master publishing it's last known state.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file></files><comments><comment>ZenDiscovery: #11960 failed to remove eager reroute from node join</comment></comments></commit></commits></item><item><title>ElasticSearch 1.6 fails to start with an empty/missing fstab</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12018</link><project id="" key="" /><description>Hi,

I'm running ES in a FreeBSD jail, so there is no fstab and no mount point visible. And since I upgraded ES to version 1.6, it no longer start because it fails to obtain a lock because "Mount point not found in fstab" (file permissions are ok).

```
[2015-07-03 12:22:10,088][INFO ][node                     ] [Awesome Android] version[1.6.0], pid[42071], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-03 12:22:10,088][INFO ][node                     ] [Awesome Android] initializing ...
[2015-07-03 12:22:10,092][INFO ][plugins                  ] [Awesome Android] loaded [], sites []
[2015-07-03 12:22:10,133][ERROR][bootstrap                ] Exception
org.elasticsearch.ElasticsearchIllegalStateException: Failed to obtain node lock, is the following location writable?: [/var/db/elasticsearch/mon]
        at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:158)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:162)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:77)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:245)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.io.IOException: failed to obtain lock on /var/db/elasticsearch/mon/nodes/49
        at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:145)
        ... 5 more
Caused by: java.io.IOException: Mount point not found in fstab
        at sun.nio.fs.BsdFileStore.findMountEntry(BsdFileStore.java:86)
        at sun.nio.fs.UnixFileStore.&lt;init&gt;(UnixFileStore.java:65)
        at sun.nio.fs.BsdFileStore.&lt;init&gt;(BsdFileStore.java:40)
        at sun.nio.fs.BsdFileSystemProvider.getFileStore(BsdFileSystemProvider.java:53)
        at sun.nio.fs.BsdFileSystemProvider.getFileStore(BsdFileSystemProvider.java:37)
        at sun.nio.fs.UnixFileSystemProvider.getFileStore(UnixFileSystemProvider.java:368)
        at java.nio.file.Files.getFileStore(Files.java:1413)
        at org.elasticsearch.env.NodeEnvironment.getFileStore(NodeEnvironment.java:256)
        at org.elasticsearch.env.NodeEnvironment.access$000(NodeEnvironment.java:62)
        at org.elasticsearch.env.NodeEnvironment$NodePath.&lt;init&gt;(NodeEnvironment.java:75)
        at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:134)
        ... 5 more
```

Why does it even need to access /etc/fstab just to obtain a lock?
</description><key id="92845822">12018</key><summary>ElasticSearch 1.6 fails to start with an empty/missing fstab</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peikk0</reporter><labels><label>:Core</label></labels><created>2015-07-03T10:31:48Z</created><updated>2017-03-09T19:21:49Z</updated><resolved>2015-07-10T09:29:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T17:01:56Z" id="118638358">It looks up the type of filesystem I think to determine whether it is an SSD or not, as that changes the default number of merge threads.

@mikemccand any ideas here?
</comment><comment author="mikemccand" created="2015-07-05T22:58:02Z" id="118674772">For 1.x we use this for for diagnostics (log filesystem type, mount point, free space for each path.data on node init) and from JmxFsProbe (pulling "fs" node stats).  This was done in #10502 and #10527 ...

In 2.0 we also log the spins detection (SSD or not).  Currently ES does not default merge schedule defaults according to spins; rather, we always use aggressive settings (more than one merge thread).  I'm not sure we should change to Lucene's defaults... that could be a sudden change on ES users upgrading.

Before, JmxFsProbe would only call Files.getFileStore API when you asked for fs stats (and sigar wasn't used), so you wouldn't hit this unless you pulled fs stats w/o sigar, but now we cache the FileStore on init instead.
</comment><comment author="clintongormley" created="2015-07-07T14:12:15Z" id="119216568">@mikemccand so what should we do in this case (freebsd jail) where there is not fstab?
</comment><comment author="mikemccand" created="2015-07-07T15:07:16Z" id="119233187">@peikk0 Can you configure your jail to include an fstab?  I don't have much experience with jails in FreeBSD but on some quick googling it seems like this is possible, e.g. https://forums.freebsd.org/threads/jail-conf.34741/
</comment><comment author="peikk0" created="2015-07-07T15:24:23Z" id="119238124">Those fstab are used by the host and are not visible inside the jail. Jails are not allowed to mount anything by default anyway and don't have access to devices, or else it could compromise the host and other jails.

Anyways, even if I create a fake fstab in the jail, it still won't start:

```
yavin ~ # mount
corellia/usr/jails/mon on / (zfs, local, noatime, nfsv4acls)
yavin ~ # cat /etc/fstab
corellia/usr/jails/mon / zfs noauto 0 0
yavin ~ # service elasticsearch console
[2015-07-07 17:19:02,750][INFO ][node                     ] [Mantra] version[1.6.0], pid[3165], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-07 17:19:02,750][INFO ][node                     ] [Mantra] initializing ...
[2015-07-07 17:19:02,754][INFO ][plugins                  ] [Mantra] loaded [], sites []
{1.6.0}: Initialization Failed ...
- ElasticsearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/var/db/elasticsearch/mon]]
        IOException[failed to obtain lock on /var/db/elasticsearch/mon/nodes/49]
                IOException[Mount point not found in fstab]
```

IMO, a proper fix would be to catch the exception and use a safe default in this case.
</comment><comment author="rmuir" created="2015-07-07T15:26:21Z" id="119238614">I don't think thats necessarily safe. Its abnormal that you cannot retrieve information from the filestore. I don't think we should hide the error condition.
</comment><comment author="rmuir" created="2015-07-07T15:29:26Z" id="119239422">also keep in mind, I think the filestore is used to know the amount of disk space.
</comment><comment author="peikk0" created="2015-07-07T15:37:11Z" id="119241520">/etc/fstab is not a reliable source anyway, it defines what should be mounted, but not what actually is. How about simply using the output of `mount`? Or something equivalent (no /proc/mounts on FreeBSD either, or /proc/ at all).
</comment><comment author="rmuir" created="2015-07-07T15:39:01Z" id="119241960">Those are issues to take up with the bsd port of the openjdk IMO. we are just using the only way in java to do it, and thats to call Files.getFileStore
</comment><comment author="clintongormley" created="2015-07-10T09:29:53Z" id="120318379">It sounds like this configuration can't be supported - we need access to this info, and we rely on Java to provide it.  

Closing
</comment><comment author="dneades" created="2015-10-09T08:03:51Z" id="146791007">I just ran into this problem – it’s exceedingly unhelpful behaviour for those of us wishing to jail elasticsearch.
</comment><comment author="rmuir" created="2015-10-09T10:33:19Z" id="146829653">You need to open issues with oracle about it. There is nothing we can do.
</comment><comment author="peikk0" created="2015-10-09T10:40:08Z" id="146830807">I've read it works fine with OpenJDK 8, I haven't tried it yet though.
</comment><comment author="dneades" created="2015-10-09T10:48:02Z" id="146831826">@peikk0, thank you for the pointer. Unfortunately, that doesn’t seem to be the case, at least in my configuration (I have a data directory nullfs mounted into the jail’s directory hierarchy). I have OpenJDK8 installed (and uninstalled OpenJDK7 just to make sure that JDK 8 is being used), but I still encounter the problem.
</comment><comment author="dneades" created="2015-10-09T11:43:21Z" id="146840836">Having investigated this a little further, it seems that setting the jail’s `enforce_statfs` property to `1` allows Elasticsearch to obtain the information it needs (at least when running with OpenJDK 8). From the jail (8) man page:

```
enforce_statfs
             This determines what information processes in a jail are able to
             get about mount points.  It affects the behaviour of the follow‐
             ing syscalls: statfs(2), fstatfs(2), getfsstat(2), and
             fhstatfs(2) (as well as similar compatibility syscalls).  When
             set to 0, all mount points are available without any restric‐
             tions.  When set to 1, only mount points below the jail's chroot
             directory are visible.  In addition to that, the path to the
             jail's chroot directory is removed from the front of their path‐
             names.  When set to 2 (default), above syscalls can operate only
             on a mount-point where the jail's chroot directory is located.
```

Perhaps this information will be useful to anyone else who encounters this issue.
</comment><comment author="peikk0" created="2015-10-09T11:54:11Z" id="146842279">Good catch! I just tried it and it works with OpenJDK 7 too!
</comment><comment author="rmuir" created="2015-10-09T11:58:12Z" id="146843657">Nice solution! I think we should return this information in the error if we hit exception trying to pull the filestores on freebsd. I will take care of it.
</comment><comment author="dneades" created="2015-10-09T13:56:17Z" id="146878487">@peikk0, thank you for the confirmation, that’s good to know.

@rmuir, an informational message would no doubt be very helpful, good call!
</comment><comment author="marcoc610" created="2015-10-30T08:07:11Z" id="152455392">I have experienced the same problem on Linux (so no jails, no chroot) when the path.data is in a BTRFS subvolume that is not mounted.
Actually I think that it is more a java issue, but I wonder if there is any workaround for btrfs subvolumes too... And, yes, there is one: mounting the subvolumes explicitly solved the issue for me.
</comment><comment author="hydrapolic" created="2016-04-19T07:44:11Z" id="211779181">This is still an issue on Linux.
</comment><comment author="remram44" created="2017-03-09T17:30:33Z" id="285421492">I am running into this on Linux. I don't think it is right for elasticsearch to go through that much magic to determine if locking should work. At the very least, I'd expect it to at least *try* to lock rather than exiting with an error "Failed to obtain node lock" although the conditions are right.</comment><comment author="dakrone" created="2017-03-09T18:28:44Z" id="285437570">@remram44 are you running this on linux with no virtualization? without an `/etc/fstab` file? Maybe you can give us more information about your setup.</comment><comment author="remram44" created="2017-03-09T18:54:01Z" id="285444674">I'm running from a chroot, so there is no entry in `/proc/mounts` for `/`. Adding some fstab with fake info gives out `ElasticsearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/var/lib/elasticsearch/elasticsearch]]`</comment><comment author="remram44" created="2017-03-09T18:55:10Z" id="285444988">Moving the directory I chroot, creating an empty dir where it was and mounting the new location to the old with `mount -o bind`, I can get elasticsearch to start. Tell me again why you need elasticsearch's root to be its own filesystem? Surely if the rest of the world can lock files without it, elasticsearch can manage? Why add so much magic when it will only break things?</comment><comment author="remram44" created="2017-03-09T19:21:49Z" id="285452452">[strace log](https://gist.github.com/remram44/1102e9b37f07c058e88e27cf36ff5c7e)</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/env/ESFileStore.java</file><file>core/src/main/java/org/elasticsearch/env/Environment.java</file></files><comments><comment>Give a better exception when running from freebsd jail without enforce_statfs=1</comment></comments></commit></commits></item><item><title>Deprecate `indices` query in favour of making `_index` queryable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12017</link><project id="" key="" /><description>The `indices` query allows the user to run different queries on different indices, eg to create a union of docs from different indices, where the conditions to be applied to each index are different.

A simpler way to write this query would be to make `_index` usable in the `term`, `terms`, `match`, `query_string`, and `simple_query_string`queries (see #3316).

However, this change also depends on the changes in #12016 to allow unmapped fields to be handled gracefully across all queries.
</description><key id="92844694">12017</key><summary>Deprecate `indices` query in favour of making `_index` queryable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>deprecation</label></labels><created>2015-07-03T10:24:07Z</created><updated>2016-08-26T13:14:22Z</updated><resolved>2016-04-14T16:09:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-05T20:10:15Z" id="118663092">+1
</comment><comment author="colings86" created="2015-07-06T07:29:50Z" id="118756119">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file></files><comments><comment>Deprecate Indices query</comment></comments></commit></commits></item><item><title>Queries and unmapped fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12016</link><project id="" key="" /><description>As discussed in #11806, different queries handle unmapped fields differently.  The `nested`,  `has_child`, `has_parent`, `geo_*`, and `function_score` query throw exceptions, while all of the others match no docs instead.

We need to make the behaviour uniform across queries.  

The solution we came up with in FixItFriday is:
- if a field doesn't exist in an index, then replace the query with a `FieldNotFound` query, which matches no documents
- if `explain` is turned on, then the `field_not_found` query will be part of the output
- if the `strict_mappings` is passed, then the request will throw a field not found exception instead.

This preserves the ease of use of, eg:

```
GET _search?name=john
```

without making search requests even more verbose, but makes debugging bad field names easy.
</description><key id="92844373">12016</key><summary>Queries and unmapped fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>enhancement</label></labels><created>2015-07-03T10:22:54Z</created><updated>2017-06-01T09:40:51Z</updated><resolved>2016-04-14T14:33:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-03T13:12:44Z" id="118346133">Maybe we should then also have a FieldNotFound agg to have similar behaviour for aggs too?
</comment><comment author="nik9000" created="2015-07-03T14:21:57Z" id="118362741">&gt; Maybe we should then also have a FieldNotFound agg to have similar behaviour for aggs too?

From a migration standpoint its always better to return no results than to blow up. I often deploy code that relies on the "if the field isn't there then nothing happens" behavior and create the field over time. Its just more sane that way. So I'm +1 on that.

Last I checked the term and phrase suggester return exceptions if the field doesn't exist and it'd be nicer if they didn't. Not critical. Just nicer.
</comment><comment author="rjernst" created="2015-07-04T18:51:18Z" id="118542324">Having a "field not found" query seems very odd (just the name). What about returning a MatchNoDocsQuery, but extending that to have a message about _why_ match no docs was chosen (essentially giving a reason, since match no docs alone doesn't make any sense when building a real query)?
</comment><comment author="clintongormley" created="2015-07-05T17:24:29Z" id="118639225">@rjernst makes sense
</comment><comment author="colings86" created="2016-04-14T14:33:25Z" id="209972550">We now have https://github.com/elastic/elasticsearch/pull/17748 and https://github.com/elastic/elasticsearch/pull/17751 which add an `ignore_unmapped` option to nested, parent/child and geo queries so the user can optionally ignore unmapped field or have an error reported. There is also a `missing` option that can be used on fieldValueFactor in the FunctionScoreQuery. Will those we can deal with unmapped fields on all queries so I'm going to close this issue. If people disagree we can reopen and discuss
</comment><comment author="tragiclifestories" created="2017-06-01T09:40:51Z" id="305443409">&gt; Will those we can deal with unmapped fields on all queries so I'm going to close this issue.

How do any of these help with a bool query? Use case is a variant on the 'multiple indices' one: I've recently started indexing a field that is used to exclude results with bool/must_not. This works with 2.4, not with 5.x. I don't see how this could be rewritten as any of the queries that support `ignore_unmapped`</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Numbers.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoExecType.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file></files><comments><comment>Refactors GeoBoundingBoxQueryBuilder/-Parser</comment></comments></commit></commits></item><item><title>Don't special-case on ElasticsearchWrapperException in toXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12015</link><project id="" key="" /><description>the specialization can cause stack overflows if an exception is a
ElasticsearchWrapperException as well as a ElasticsearchException.
This commit just relies on the unwrap logic now to find the cause and only
renders if we the rendering exception is the cause otherwise forwards
to the generic exception rendering.

Closes #11994
</description><key id="92843208">12015</key><summary>Don't special-case on ElasticsearchWrapperException in toXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T10:17:46Z</created><updated>2015-07-05T17:02:22Z</updated><resolved>2015-07-03T11:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-03T10:20:52Z" id="118307060">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completely move doc values and fielddata settings to field types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12014</link><project id="" key="" /><description>While MappedFieldType contains settings for doc values and fielddata,
AbstractFieldMapper had special logic in its constructor that
required setting these on the field type from there. This change
removes those settings from the AbstractFieldMapper constructor.
As a result, defaultDocValues(), and defaultFieldDataType() are
no longer needed, and defaultFieldType() is now an internal detail
of AbstractFieldMapper.
</description><key id="92842341">12014</key><summary>Completely move doc values and fielddata settings to field types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T10:12:16Z</created><updated>2015-07-08T11:56:38Z</updated><resolved>2015-07-06T17:21:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-06T17:03:32Z" id="118925900">Thanks @jpountz, I pushed a new commit.
</comment><comment author="jpountz" created="2015-07-06T17:03:59Z" id="118926070">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MetadataFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMetadataMapper.java</file></files><comments><comment>Merge pull request #12014 from rjernst/remove/mapper-doc-values</comment></comments></commit></commits></item><item><title>Fast Vector Highlighter highlights only first 1024 terms for prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12013</link><project id="" key="" /><description>**Fast Vector Highlighter** in combination with **prefix** query **highlights** only **first 1024** suitable **terms**.

**All next terms are ignored for highlighting !!!** :disappointed:

This fvh behavior differs from **plain highlighter** (which highlighted all 1346 terms in my testing index) and **prefix query** behavior and I didn't found any documented setting which could increase this limit.
</description><key id="92828939">12013</key><summary>Fast Vector Highlighter highlights only first 1024 terms for prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zdeseb</reporter><labels /><created>2015-07-03T09:09:18Z</created><updated>2015-07-05T18:14:32Z</updated><resolved>2015-07-05T17:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-03T14:06:40Z" id="118359985">&gt; Fast Vector Highlighter in combination with prefix query highlights only first 1024 suitable terms.

Its hard coded:
http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-highlighter/4.10.4/org/apache/lucene/search/vectorhighlight/FieldQuery.java#62

&gt; This fvh behavior differs from plain highlighter (which highlighted all 1346 terms in my testing index) and prefix query behavior and I didn't found any documented setting which could increase this limit.

The [experimental highlighter](https://github.com/wikimedia/search-highlighter) has a fix for this: it highlights the automaton for the prefix query.

For the most part its better to do term lookups on edgeNGrams than it is to do prefix queries. Rather, its faster at query time.
</comment><comment author="zdeseb" created="2015-07-03T15:59:52Z" id="118380824">Thanks for answer.

So I will try to use Experimental Highlighter instead of FVH (but it's name sounds little bit dangerously :smile:). I hope it will not be out of the frying pan into the fire :smile: 

But at least it will be better to add info about this incompatibility to [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#fast-vector-highlighter) (if it is not possible to ensure consistent behavior)
</comment><comment author="nik9000" created="2015-07-03T16:04:36Z" id="118382510">&gt; But at least it will be better to add info about this incompatibility to documentation (if it is not possible to ensure consistent behavior)

You can totally update that document. Click on the edit button and replace the `1.6` in the url with `master` and github will walk you through making a pull request. I hope. You'd have to sign the CLA to get it merged but it'd be a great thing to do!

Ping @clintongormley - that link isn't working properly. See above.

&gt; So I will try to use Experimental Highlighter instead of FVH (but it's name sounds little bit dangerously :smile:). I hope it will not be out of the frying pan into the fire :smile:

Yeah - the name is unfortunate. It was experimental nine months ago. Right now its doing all the highlighting on site search on wikipedia and is working fine. It had some interesting bugs at times but it gets lots of exercise so we found them and squashed them. Its newer than the other hihglighters and not as widely used so there are certainly some rough edges that we have yet to find though.
</comment><comment author="clintongormley" created="2015-07-05T17:14:22Z" id="118638895">&gt; Ping @clintongormley - that link isn't working properly. See above.

@nik9000 which link?
</comment><comment author="nik9000" created="2015-07-05T17:49:36Z" id="118640780">The edit links in the docs open up a page that says your fork is out of
date... Or maybe that is just me.... If it is then its more a github bug
than a docs bug. I figured clicking the link would do the right thing
regardless of how up to date my fork is.
On Jul 5, 2015 1:14 PM, "Clinton Gormley" notifications@github.com wrote:

&gt; Ping @clintongormley https://github.com/clintongormley - that link
&gt; isn't working properly. See above.
&gt; 
&gt; @nik9000 https://github.com/nik9000 which link?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/12013#issuecomment-118638895
&gt; .
</comment><comment author="clintongormley" created="2015-07-05T18:14:32Z" id="118645881">ok - that's a github thing then
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>after DELETE old snapshot,can't resotre snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12012</link><project id="" key="" /><description>we do a backup for the elasticsearch everyday(there will create a snapshot everyday),where I delete the old snapshots,only leave the latest 7 snapshots on the hdfs.then I execute the restore the snapshot, I found that we can't restore the snapshot
[Failed to start shard, message [IndexShardGatewayRecoveryException[[userindex][6] failed recovery]; nested: IndexShardRestoreFailedException[[userindex][6] restore failed]; nested: IndexShardRestoreFailedException[[userindex][6] failed to restore snapshot [snapshot_20150624]]; nested: IndexShardRestoreFailedException[[userindex][6] Failed to fetch index version after copying it over]; nested: CorruptIndexException[codec mismatch: actual codec=Lucene46SegmentInfo vs expected codec=segments (resource: BufferedChecksumIndexInput(MMapIndexInput(path="/es/elasticsearch-1.2.2/data/elasticsearch-kt/nodes/0/indices/userindex/6/index/segments_4bol")))]; ]]
</description><key id="92828364">12012</key><summary>after DELETE old snapshot,can't resotre snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JasonBian</reporter><labels /><created>2015-07-03T09:05:42Z</created><updated>2015-07-05T16:56:21Z</updated><resolved>2015-07-05T16:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T16:56:20Z" id="118638188">HI @JasonBian 

It looks like you have a corrupt file in your snapshot.  I suggest:
- reindexing your `userindex` to a new index (just in case)
- try to optimize the `userindex` (to rewrite any bad segments)
- make a fresh snapshot
- try to restore it... hopefully this will work

A number of corruption problems have been fixed in recent versions. You should upgrade as soon as possible.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cluster stuck in loop "failed to create shard"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12011</link><project id="" key="" /><description>Hi,

Our process involve a bulk loading of one index (index_b) while a second is used in production (index_a), at the end of the bulk loading we swap the alias (index_alias) to index_b and close the first one, this process happen three times a day. Here's a more detailled list of actions we perform on the index
- (index used in production =&gt; index_a)
- open index_b
- disable refresh_interval
- bulk load in index_b
- send a refresh request
- optimize
- wait for each node to have merges: 0 for index_b
- swap alias
- close index_a

During the bulk loading of index_b (or index_a since the process happen 3 times a day) two of the three nodes will be stuck in relocating loop of one or multiple shards, not always the same on a different node with the following log :

```
[2015-07-02 08:57:40,322][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard
   at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
   at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
   at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
   at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms
   at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
   at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
   at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
   ... 9 more
[2015-07-02 08:57:40,339][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 73587
[2015-07-02 08:57:45,339][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][0] failed to create shard
   at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
   at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
   at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
   at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][0], timed out after 5000ms
   at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
   at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
   at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
   ... 9 more
[2015-07-02 08:57:50,340][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard
   at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
   at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
   at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
   at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
   at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms
   at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
   at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
   at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
   ... 9 more
[2015-07-02 08:57:50,341][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2015-07-02 08:57:50,345][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 73588
```

The nodes will (until restart) exchange cluster state version indefinitely 

```
received cluster state version 76797
received cluster state version 76798
received cluster state version 76799
received cluster state version 76800
received cluster state version 76801
received cluster state version 76802 
```

At this stage the cluster is green, but when the second time our process run, it'll trigger a yellow and sometimes a red cluster

Things we've tried:
- upgrading (1.5.2, 1.5.3, 1.6.0)
- rolling restart of each e.s process
- complete rolling replacement of each of the 3 nodes (including data on disk)
- adding more nodes (from 3 to 5)

The only way to fix this is to restart one of the nodes involved in the relocation process

```
[2015-07-02 08:57:59,708][INFO ][node                     ] [elasticsearch-nodes2.localdomain] stopping ...
[2015-07-02 08:57:59,725][DEBUG][discovery.zen.fd         ] [elasticsearch-nodes2.localdomain] [master] stopping fault detection against master [[elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}], reason [zen disco stop]
[2015-07-02 08:57:59,792][INFO ][node                     ] [elasticsearch-nodes2.localdomain] stopped
[2015-07-02 08:57:59,792][INFO ][node                     ] [elasticsearch-nodes2.localdomain] closing ...
[2015-07-02 08:57:59,799][DEBUG][com.amazonaws.http.IdleConnectionReaper] Reaper thread:
java.lang.InterruptedException: sleep interrupted
   at java.lang.Thread.sleep(Native Method)
   at com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:112)
[2015-07-02 08:57:59,800][DEBUG][com.amazonaws.http.IdleConnectionReaper] Shutting down reaper thread.
[2015-07-02 08:58:09,806][WARN ][cluster.action.index     ] [elasticsearch-nodes2.localdomain] [shopper_chargement_prod_a] failed to lock all shards for index - timed out after 30 seconds
[2015-07-02 08:58:09,814][INFO ][node                     ] [elasticsearch-nodes2.localdomain] closed



[2015-07-02 08:58:41,442][INFO ][node                     ] [elasticsearch-nodes2.localdomain] version[1.6.0], pid[45115], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-02 08:58:41,443][INFO ][node                     ] [elasticsearch-nodes2.localdomain] initializing ...
[2015-07-02 08:58:41,459][INFO ][plugins                  ] [elasticsearch-nodes2.localdomain] loaded [cloud-aws], sites [HQ, whatson, kopf]
[2015-07-02 08:58:41,499][INFO ][env                      ] [elasticsearch-nodes2.localdomain] using [1] data paths, mounts [[/srv/data (/dev/mapper/lvm--raid--0-lvm0)]], net usable_space [471.4gb], net total_space [499.6gb], types [xfs]
[2015-07-02 08:58:44,350][INFO ][node                     ] [elasticsearch-nodes2.localdomain] initialized
[2015-07-02 08:58:44,350][INFO ][node                     ] [elasticsearch-nodes2.localdomain] starting ...
[2015-07-02 08:58:44,527][INFO ][transport                ] [elasticsearch-nodes2.localdomain] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.210.14.19:9300]}
[2015-07-02 08:58:44,546][INFO ][discovery                ] [elasticsearch-nodes2.localdomain] es-cluster/p6IyMeFHRCey0kRffbHgDw
[2015-07-02 08:58:48,598][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] detected_master [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}, added {[elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},[elasticsearch-nodes1.localdomain][DsZ08lNfSF6EwvAMSoehng][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [[elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}])
[2015-07-02 08:58:48,606][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2015-07-02 08:58:48,606][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] updating [indices.recovery.translog_size] from [512kb] to [2mb]
[2015-07-02 08:58:48,649][INFO ][http                     ] [elasticsearch-nodes2.localdomain] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.210.14.19:9200]}
[2015-07-02 08:58:48,649][INFO ][node                     ] [elasticsearch-nodes2.localdomain] started
[2015-07-02 08:59:07,416][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 73591
[2015-07-02 08:59:07,417][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]
[2015-07-02 08:59:07,424][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 73592
[2015-07-02 08:59:07,842][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] started recovery from [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}, id [1]
[2015-07-02 08:59:07,844][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][0] [1]
[2015-07-02 08:59:07,845][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] starting recovery from [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-02 08:59:07,858][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] started recovery from [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}, id [2]
[2015-07-02 08:59:07,859][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][1] [2]
[2015-07-02 08:59:07,859][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] starting recovery from [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-02 08:59:07,866][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
   at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
   at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
   at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
   at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
   at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
   at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   at java.lang.Thread.run(Thread.java:745)
[2015-07-02 08:59:07,866][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
   at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
   at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
   at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
   at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
   at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
   at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   at java.lang.Thread.run(Thread.java:745)
[2015-07-02 08:59:07,868][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [1] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])
[2015-07-02 08:59:07,868][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [2] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])
[2015-07-02 08:59:08,369][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][1] [2]
[2015-07-02 08:59:08,369][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][0] [1]
[2015-07-02 08:59:08,370][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] starting recovery from [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-02 08:59:08,370][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] starting recovery from [elasticsearch-nodes3.localdomain][o_9lrwRfTn6bbIztX9OCvA][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
```

Setup:
AWS 3 nodes, eu-west1 a,b,c zones, nginx proxy on each of the three nodes, one ELB with SSL offloading and round robin on the three nginx
data stored on EBS volumes

ES config:

```
cluster.name: es-cluster
node.name: elasticsearch-nodes2.localdomain
node.max_local_storage_nodes: 1

index.mapper.dynamic: true
action.auto_create_index: true
action.disable_delete_all_indices: true

path.conf: /usr/local/etc/elasticsearch
path.data: /srv/data/elasticsearch/data
path.logs: /srv/data/elasticsearch/logs

bootstrap.mlockall: true

http.port: 9200

gateway.expected_nodes: 1
discovery.type: ec2

discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.multicast.enabled: false

cloud.node.auto_attributes: true
cloud.aws.region: eu-west-1
discovery.ec2.tag: custom-es-cluster-tag

action.disable_delete_all_indices: true
discovery.zen.fd.ping_timeout: 15s
gateway.recover_after_nodes: 2
gateway.recover_after_time: 5m
http.compression: true
http.cors.allow-origin: '*'
http.cors.enabled: true
indices.store.throttle.max_bytes_per_sec: 100mb
threadpool.bulk.queue_size: 3000
transport.tcp.compress: true
```

ES Index settings:

```
{
 "index_b":{
   "settings":{
     "index":{
       "creation_date":"1435765694828",
       "uuid":"JjqbLn6CS1q0nwaw5HhIpA",
       "analysis":{
         "filter":{
           "my_word_delimiter":{
             "type":"word_delimiter",
             "split_on_numerics":"true"
           },
           "french_stemmer":{
             "type":"stemmer",
             "name":"light_french"
           },
           "limit_token":{
             "type":"limit",
             "max_token_count":"7"
           },
           "french_stopwords":{
             "type":"stop",
             "stopwords":"_french_"
           }
         },
         "analyzer":{
           "word_delimiter_stopwfr":{
             "filter":[
               "my_word_delimiter",
               "asciifolding",
               "lowercase",
               "french_stopwords"
             ],
             "tokenizer":"digitletter"
           },
           "word_delimiter":{
             "filter":[
               "my_word_delimiter",
               "lowercase",
               "asciifolding"
             ],
             "tokenizer":"digitletter"
           },
           "word_delimiter_stopwfr_stemfr":{
             "filter":[
               "my_word_delimiter",
               "asciifolding",
               "lowercase",
               "french_stopwords",
               "french_stemmer"
             ],
             "tokenizer":"digitletter"
           },
           "word_delimiter_limit":{
             "filter":[
               "my_word_delimiter",
               "lowercase",
               "limit_token",
               "asciifolding"
             ],
             "tokenizer":"digitletter"
           }
         },
         "tokenizer":{
           "digitletter":{
             "pattern":"[^\\p{Ll}\\p{Lu}0-9]+",
             "type":"pattern"
           }
         }
       },
       "number_of_replicas":"1",
       "number_of_shards":"6",
       "refresh_interval":"-1",
       "version":{
         "created":"1060099"
       }
     }
   }
 }
}
```
</description><key id="92823271">12011</key><summary>cluster stuck in loop "failed to create shard"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">womwombat</reporter><labels><label>:Core</label><label>bug</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-07-03T08:35:19Z</created><updated>2017-02-17T16:26:12Z</updated><resolved>2016-01-18T10:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T16:52:28Z" id="118638036">Hi @womwombat 

My first guess would be heavy I/O caused by the optimize process.  You say you're using EBS volumes, without provisioned IOPS?  EBS is pretty slow unless you have provisioned IOPS, and you've set the merge throttling to a too high value for EBS.

So I think the optimize uses all the I/O which stops Elasticsearch from obtaining the file system lock quickly enough.  Try reducing the throttling.

as a side note, you have `gateway.recover_after_nodes` set to 2, but `expected_nodes` set to 1.  This doesn't make sense.  It waits for the expected nodes and only falls back to recover_after_nodes if it is still waiting when the recover_after_time period has expired.  You should set expected nodes to 3 in your case (or however many nodes you plan on having)
</comment><comment author="womwombat" created="2015-07-09T15:26:04Z" id="120033587">Hi,

As a side note, this issue arise suddently, we've been running this cluster for weeks (~ 8 weeks at least), but following your wise advise we have set the indices.store.throttle.max_bytes_per_sec: 5mb.

We have fixed what was a typo in the expected_nodes / gateway_recover_after_nodes too.

However the issue still goes on :/

```
[2015-07-08 08:30:49,271][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][2]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][2] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][2], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:30:49,317][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89042
[2015-07-08 08:30:54,359][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:30:59,360][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][5]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][5] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][5], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:30:59,372][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89043
[2015-07-08 08:31:04,413][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][4]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][4] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][4], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:31:09,414][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][3]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][3] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][3], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:31:14,415][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:31:14,424][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89044
[2015-07-08 08:31:14,466][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2015-07-08 08:31:14,474][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89045
[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:31:38,228][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:32:05,411][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89046
[2015-07-08 08:32:05,412][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]
[2015-07-08 08:32:05,418][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89047
[2015-07-08 08:32:10,460][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:15,461][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][5]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][5] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][5], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:20,462][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][2]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][2] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][2], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:20,475][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89048
[2015-07-08 08:32:25,516][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][4]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][4] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][4], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:30,517][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:35,517][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][3]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][3] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][3], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:35,532][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89049
[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:32:38,118][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:32:40,573][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][1]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][1] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][1], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:45,574][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][5]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [index_a][5] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [index_a][5], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-07-08 08:32:45,584][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89050
[2015-07-08 08:32:45,626][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2015-07-08 08:32:45,662][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89051
[2015-07-08 08:33:39,685][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:33:39,686][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:33:39,686][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:33:39,686][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:34:41,144][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:34:41,145][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:34:41,145][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:34:41,145][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:35:37,612][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:36:41,024][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:36:41,025][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:36:41,025][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:36:41,025][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:37:14,212][INFO ][node                     ] [elasticsearch-nodes2.localdomain] stopping ...
[2015-07-08 08:37:14,254][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
    at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
    at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:307)
    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:331)
    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-07-08 08:37:14,256][DEBUG][discovery.zen.fd         ] [elasticsearch-nodes2.localdomain] [master] stopping fault detection against master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_
storage_nodes=1}], reason [zen disco stop]
[2015-07-08 08:37:14,296][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$2.handleException(TransportShardReplicationOperationAction.java:481)
    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:178)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:37:14,370][INFO ][node                     ] [elasticsearch-nodes2.localdomain] stopped
[2015-07-08 08:37:14,370][INFO ][node                     ] [elasticsearch-nodes2.localdomain] closing ...
[2015-07-08 08:37:14,378][DEBUG][com.amazonaws.http.IdleConnectionReaper] Reaper thread:
java.lang.InterruptedException: sleep interrupted
    at java.lang.Thread.sleep(Native Method)
    at com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:112)
[2015-07-08 08:37:14,378][DEBUG][com.amazonaws.http.IdleConnectionReaper] Shutting down reaper thread.
```

And when we stop/start the node:

```
[2015-07-08 08:37:57,146][INFO ][node                     ] [elasticsearch-nodes2.localdomain] version[1.6.0], pid[33261], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-08 08:37:57,146][INFO ][node                     ] [elasticsearch-nodes2.localdomain] initializing ...
[2015-07-08 08:37:57,163][INFO ][plugins                  ] [elasticsearch-nodes2.localdomain] loaded [cloud-aws], sites [HQ, whatson, kopf]
[2015-07-08 08:37:57,203][INFO ][env                      ] [elasticsearch-nodes2.localdomain] using [1] data paths, mounts [[/srv/data (/dev/mapper/lvm--raid--0-lvm0)]], net usable_space [475.9gb], net total_space [499.6gb], types [xfs]
[2015-07-08 08:38:00,074][INFO ][node                     ] [elasticsearch-nodes2.localdomain] initialized
[2015-07-08 08:38:00,074][INFO ][node                     ] [elasticsearch-nodes2.localdomain] starting ...
[2015-07-08 08:38:00,254][INFO ][transport                ] [elasticsearch-nodes2.localdomain] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.210.14.19:9300]}
[2015-07-08 08:38:00,273][INFO ][discovery                ] [elasticsearch-nodes2.localdomain] cluster-es/t7ZN91B7Se2qT6NsRRau5g
[2015-07-08 08:38:04,343][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] detected_master [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}, added {[elastics
earch-nodes3.localdomain][Bg5OX3aoTK-pWNImAkf-vw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_z
one=eu-west-1c, max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}])
[2015-07-08 08:38:04,350][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2015-07-08 08:38:04,350][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] updating [indices.recovery.translog_size] from [512kb] to [2mb]
[2015-07-08 08:38:04,351][INFO ][indices.store            ] [elasticsearch-nodes2.localdomain] updating indices.store.throttle.max_bytes_per_sec from [5mb] to [100mb], note, type is [MERGE]
[2015-07-08 08:38:04,394][INFO ][http                     ] [elasticsearch-nodes2.localdomain] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.210.14.19:9200]}
[2015-07-08 08:38:04,394][INFO ][node                     ] [elasticsearch-nodes2.localdomain] started
[2015-07-08 08:39:14,425][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:39:14,425][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:39:14,426][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:39:14,426][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:39:35,655][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89054
[2015-07-08 08:39:35,655][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] removed {[elasticsearch-nodes3.localdomain][Bg5OX3aoTK-pWNImAkf-vw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},}, reason: zen-disco-rece
ive(from master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}])
[2015-07-08 08:39:35,745][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89055
[2015-07-08 08:39:36,153][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][4] starting recovery from local ...
[2015-07-08 08:39:36,155][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] using existing shard data, translog id [1436278480889]
[2015-07-08 08:39:36,158][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] try recover from translog file translog-1436278480889 locations: [/srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/4/translog]
[2015-07-08 08:39:36,159][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] Translog file found in /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/4/translog - renaming
[2015-07-08 08:39:36,159][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] Renamed translog from translog-1436278480889 to translog-1436278480889.recovering
[2015-07-08 08:39:36,167][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][5] starting recovery from local ...
[2015-07-08 08:39:36,170][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] using existing shard data, translog id [1436278480893]
[2015-07-08 08:39:36,171][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] try recover from translog file translog-1436278480893 locations: [/srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/5/translog]
[2015-07-08 08:39:36,171][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] Translog file found in /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/5/translog - renaming
[2015-07-08 08:39:36,171][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] Renamed translog from translog-1436278480893 to translog-1436278480893.recovering
[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] recovering translog file: /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/5/translog/translog-1436278480893.recovering length: 17
[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] recovering translog file: /srv/data/elasticsearch/data/cluster-es/nodes/0/indices/index_b/4/translog/translog-1436278480889.recovering length: 17
[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][4] ignoring translog EOF exception, the last operation was not properly written
java.io.EOFException
    at org.elasticsearch.common.io.stream.InputStreamStreamInput.readByte(InputStreamStreamInput.java:43)
    at org.elasticsearch.common.io.stream.StreamInput.readInt(StreamInput.java:116)
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:59)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:267)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:39:36,527][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_b][5] ignoring translog EOF exception, the last operation was not properly written
java.io.EOFException
    at org.elasticsearch.common.io.stream.InputStreamStreamInput.readByte(InputStreamStreamInput.java:43)
    at org.elasticsearch.common.io.stream.StreamInput.readInt(StreamInput.java:116)
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:59)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:267)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:39:36,532][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery completed from local, took [379ms]
    index    : files           [13] with total_size [3gb], took[6ms]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [13] with total_size [3gb]
    start    : took [368ms], check_index [0s]
    translog : number_of_operations [0], took [4ms]
[2015-07-08 08:39:36,532][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery completed from local, took [364ms]
    index    : files           [13] with total_size [3.1gb], took[3ms]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [13] with total_size [3.1gb]
    start    : took [356ms], check_index [0s]
    translog : number_of_operations [0], took [4ms]
[2015-07-08 08:39:36,536][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89056
[2015-07-08 08:39:59,521][DEBUG][com.amazonaws.internal.SdkSSLSocket] shutting down output of ec2.eu-west-1.amazonaws.com/178.236.7.129:443
[2015-07-08 08:39:59,521][DEBUG][com.amazonaws.internal.SdkSSLSocket] closing ec2.eu-west-1.amazonaws.com/178.236.7.129:443
[2015-07-08 08:40:24,346][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89057
[2015-07-08 08:40:24,346][INFO ][cluster.service          ] [elasticsearch-nodes2.localdomain] added {[elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1},}, reason: zen-disco-receiv
e(from master [[elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}])
[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:35,973][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:41,857][DEBUG][action.bulk              ] [elasticsearch-nodes2.localdomain] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-07-08 08:40:49,191][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89058
[2015-07-08 08:40:49,192][INFO ][cluster.routing.allocation.decider] [elasticsearch-nodes2.localdomain] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]
[2015-07-08 08:40:49,205][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89059
[2015-07-08 08:40:49,305][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][1] starting recovery from local ...
[2015-07-08 08:40:49,306][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][1] no translog id set (indexShouldExist [false])
[2015-07-08 08:40:49,318][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery completed from local, took [13ms]
    index    : files           [0] with total_size [0b], took[1ms]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [0] with total_size [0b]
    start    : took [11ms], check_index [0s]
    translog : number_of_operations [0], took [0s]
[2015-07-08 08:40:49,322][DEBUG][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][5] starting recovery from local ...
[2015-07-08 08:40:49,324][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][5] cleaning existing shard, shouldn't exists
[2015-07-08 08:40:49,618][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89060
[2015-07-08 08:40:49,640][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [1]
[2015-07-08 08:40:49,641][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][3] [1]
[2015-07-08 08:40:49,645][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:49,657][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:40:49,657][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [1] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])
[2015-07-08 08:40:49,844][DEBUG][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][5] failed to list file details
java.io.FileNotFoundException: segments_d
    at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:255)
    at org.apache.lucene.store.FileSwitchDirectory.fileLength(FileSwitchDirectory.java:147)
    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)
    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)
    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:171)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:40:49,844][TRACE][index.gateway.local      ] [elasticsearch-nodes2.localdomain] [index_a][5] no translog id set (indexShouldExist [false])
[2015-07-08 08:40:49,848][TRACE][index.gateway            ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery completed from local, took [526ms]
    index    : files           [0] with total_size [0b], took[522ms]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [0] with total_size [0b]
    start    : took [3ms], check_index [0s]
    translog : number_of_operations [0], took [0s]
[2015-07-08 08:40:49,855][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89061
[2015-07-08 08:40:49,872][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [2]
[2015-07-08 08:40:49,872][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][2] [2]
[2015-07-08 08:40:49,876][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:49,885][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:40:49,885][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [2] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])
[2015-07-08 08:40:50,157][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][3] [1]
[2015-07-08 08:40:50,162][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:50,173][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89062
[2015-07-08 08:40:50,318][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}, mark_as_relocated false
[2015-07-08 08:40:50,326][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] Recovery with sync ID 12837074 numDocs: 12837074 vs. true
[2015-07-08 08:40:50,326][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] skipping [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1} - identical sync id [AU5pGODmRtbfpQ748m1P] found on both source and target
[2015-07-08 08:40:50,327][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [3.7ms]
[2015-07-08 08:40:50,328][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: start
[2015-07-08 08:40:50,340][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89063
[2015-07-08 08:40:50,386][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][2] [2]
[2015-07-08 08:40:50,389][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:50,795][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][2] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][2] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [index_a][2] from primary shard with sync id but number of docs differ: 12627628 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
[2015-07-08 08:40:50,796][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][2] failing recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [2]. Send shard failure: [true]
[2015-07-08 08:40:50,799][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][2]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [index_a][2]: Recovery failed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1} into [elasticse
arch-nodes2.localdomain][t7ZN91B7Se2qT6NsRRau5g][elasticsearch-nodes2.localdomain][inet[/10.210.14.19:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][2] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][2] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [index_a][2] from primary shard with sync id but number of docs differ: 12627628 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
[2015-07-08 08:40:51,257][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: start took [929.1ms]
[2015-07-08 08:40:51,257][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: updating current mapping to master
[2015-07-08 08:40:51,261][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] no translog operations (id: [1436278480889]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai
lability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] sending final batch of [0][0b] (total: [0], id: [1436278480889]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in
et[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [239.7micros]
[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] no translog operations (id: [1436278480889]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai
lability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:51,262][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] sending final batch of [0][0b] (total: [0], id: [1436278480889]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in
et[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:51,264][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][4] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [1.5ms]
[2015-07-08 08:40:55,355][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] marking recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1} as done, id [1]
[2015-07-08 08:40:55,355][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][3] recovery completed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_l
ocal_storage_nodes=1}, took[5.7s]
   phase1: recovered_files [0] with total_size of [0b], took [1ms], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [252ms]
         : recovered [0] transaction log operations, took [0s]
   phase3: recovered [0] transaction log operations, took [2ms]
[2015-07-08 08:40:55,356][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89064
[2015-07-08 08:40:55,385][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89065
[2015-07-08 08:40:55,400][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [3]
[2015-07-08 08:40:55,401][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][2] [3]
[2015-07-08 08:40:55,403][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:55,404][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}, mark_as_relocated false
[2015-07-08 08:40:55,406][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] Recovery with sync ID 12940164 numDocs: 12940164 vs. true
[2015-07-08 08:40:55,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] skipping [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1} - identical sync id [AU5pGM3poRUqHcJOE4ov] found on both source and target
[2015-07-08 08:40:55,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [2ms]
[2015-07-08 08:40:55,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: start
[2015-07-08 08:40:55,407][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:40:55,407][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [3] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])
[2015-07-08 08:40:55,424][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89066
[2015-07-08 08:40:55,438][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [4]
[2015-07-08 08:40:55,438][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][3] [4]
[2015-07-08 08:40:55,440][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:55,443][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-07-08 08:40:55,443][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] will retrying recovery with id [4] in [500ms] (reason [source node does not have the shard listed in its state as allocated on the node])
[2015-07-08 08:40:55,613][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: start took [207.2ms]
[2015-07-08 08:40:55,614][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: updating current mapping to master
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] no translog operations (id: [1436278480893]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai
lability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] sending final batch of [0][0b] (total: [0], id: [1436278480893]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in
et[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [164.1micros]
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] no translog operations (id: [1436278480893]) to send to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_avai
lability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:55,617][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] sending final batch of [0][0b] (total: [0], id: [1436278480893]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][in
et[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:40:55,618][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][5] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [936micros]
[2015-07-08 08:40:55,907][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_b][2] [3]
[2015-07-08 08:40:55,910][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:55,944][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][3] [4]
[2015-07-08 08:40:55,946][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:40:56,315][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][3] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][3] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [index_a][3] from primary shard with sync id but number of docs differ: 12599260 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
[2015-07-08 08:40:56,316][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] failing recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [4]. Send shard failure: [true]
[2015-07-08 08:40:56,316][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][3]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [index_a][3]: Recovery failed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1} into [elasticse
arch-nodes2.localdomain][t7ZN91B7Se2qT6NsRRau5g][elasticsearch-nodes2.localdomain][inet[/10.210.14.19:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][3] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][3] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [index_a][3] from primary shard with sync id but number of docs differ: 12599260 (elasticsearch-nodes1.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
[2015-07-08 08:41:00,460][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] marking recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1} as done, id [3]
[2015-07-08 08:41:00,461][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_b][2] recovery completed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_l
ocal_storage_nodes=1}, took[5s]
   phase1: recovered_files [0] with total_size of [0b], took [1ms], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [159ms]
         : recovered [0] transaction log operations, took [0s]
   phase3: recovered [0] transaction log operations, took [2ms]
[2015-07-08 08:41:00,461][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89067
[2015-07-08 08:41:00,487][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}, mark_as_relocated false
[2015-07-08 08:41:00,489][INFO ][indices.recovery         ] [elasticsearch-nodes2.localdomain] Recovery with sync ID 0 numDocs: 12635618 vs. true
[2015-07-08 08:41:00,501][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89068
[2015-07-08 08:41:00,520][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] started recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca
l_storage_nodes=1}, id [5]
[2015-07-08 08:41:00,520][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][4] [5]
[2015-07-08 08:41:00,522][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] starting recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loc
al_storage_nodes=1}
[2015-07-08 08:41:01,589][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] Got exception on recovery
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][4] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][4] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [index_a][4] from primary shard with sync id but number of docs differ: 12533048 (elasticsearch-nodes3.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
[2015-07-08 08:41:01,590][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][4] failing recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca
l_storage_nodes=1}, id [5]. Send shard failure: [true]
[2015-07-08 08:41:01,590][WARN ][indices.cluster          ] [elasticsearch-nodes2.localdomain] [[index_a][4]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [index_a][4]: Recovery failed from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1} into [elasticsea
rch-nodes2.localdomain][t7ZN91B7Se2qT6NsRRau5g][elasticsearch-nodes2.localdomain][inet[/10.210.14.19:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [index_a][4] Phase[1] Execution failed
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:898)
    at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [index_a][4] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:431)
    at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:893)
    ... 10 more
Caused by: java.lang.IllegalStateException: try to recover [index_a][4] from primary shard with sync id but number of docs differ: 12533048 (elasticsearch-nodes3.localdomain, primary) vs 0(elasticsearch-nodes2.localdomain)
    at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:177)
    ... 11 more
[2015-07-08 08:41:05,539][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89069
[2015-07-08 08:41:05,569][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89070
[2015-07-08 08:41:05,587][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89071
[2015-07-08 08:41:05,601][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] started recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca
l_storage_nodes=1}, id [6]
[2015-07-08 08:41:05,601][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][0] [6]
[2015-07-08 08:41:05,601][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] starting recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loc
al_storage_nodes=1}
[2015-07-08 08:41:05,606][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] starting recovery to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}, mark_as_relocated false
[2015-07-08 08:41:05,607][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: recovering [segments_1], does not exists in remote
[2015-07-08 08:41:05,607][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: recovering_files [1] with total_size [79b], reusing_files [0] with total_size [0b]
[2015-07-08 08:41:05,634][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] starting recovery to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}, mark_as_relocated false
[2015-07-08 08:41:05,636][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_28.cfe], does not exists in remote
[2015-07-08 08:41:05,636][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_28.si], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_28.cfs], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_29.si], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_29.cfs], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_29.cfe], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_2b.si], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_2b.cfe], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_2b.cfs], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_2a.cfs], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_2a.cfe], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [_2a.si], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering [segments_f], does not exists in remote
[2015-07-08 08:41:05,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: recovering_files [13] with total_size [5.3mb], reusing_files [0] with total_size [0b]
[2015-07-08 08:41:05,638][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89072
[2015-07-08 08:41:05,653][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] started recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1}, id [7]
[2015-07-08 08:41:05,653][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] collecting local files for [index_a][3] [7]
[2015-07-08 08:41:05,655][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] starting recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_lo
cal_storage_nodes=1}
[2015-07-08 08:41:05,809][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89073
[2015-07-08 08:41:06,045][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase1] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: took [410.1ms]
[2015-07-08 08:41:06,045][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: start
[2015-07-08 08:41:06,054][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: start took [8.3ms]
[2015-07-08 08:41:06,054][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: updating current mapping to master
[2015-07-08 08:41:06,057][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:41:06,077][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase1] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [471.4ms]
[2015-07-08 08:41:06,078][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: start
[2015-07-08 08:41:06,082][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: start took [3.5ms]
[2015-07-08 08:41:06,082][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: updating current mapping to master
[2015-07-08 08:41:06,085][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:41:06,094][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241kb] (total: [18462], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain
][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,131][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.3kb] (total: [55022], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:06,349][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.4kb] (total: [19222], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,473][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.8kb] (total: [56302], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:06,506][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.3kb] (total: [19792], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,634][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.5kb] (total: [20205], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,670][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.5kb] (total: [56735], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:06,742][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.9kb] (total: [20205], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,789][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][245.1kb] (total: [56735], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:06,833][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [20205], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,877][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [56832], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:06,924][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.5kb] (total: [20640], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:06,963][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.2kb] (total: [57160], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,019][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][240.7kb] (total: [21338], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,043][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.3kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,107][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][243kb] (total: [21499], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain
][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,111][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.5kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,172][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.2kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,192][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [21499], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,234][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.3kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,278][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.2kb] (total: [21499], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,294][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.6kb] (total: [57992], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,356][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.5kb] (total: [58611], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,365][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [22220], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,485][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.7kb] (total: [22528], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,487][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.8kb] (total: [59122], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,555][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,571][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.2kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,620][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.7kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,657][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.4kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,685][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.6kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,741][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain
][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,749][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.3kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,808][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.6kb] (total: [59661], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,825][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.9kb] (total: [23207], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,909][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.1kb] (total: [23567], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:07,953][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [60497], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:07,990][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending final batch of [439][200.2kb] (total: [24338], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.loca
ldomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,039][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,051][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase2] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: took [1.9s]
[2015-07-08 08:41:08,051][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase3] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:41:08,062][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][352.6kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,097][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][242.4kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,103][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.3kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,129][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][301.7kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,157][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.6kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,164][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,183][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain
][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,209][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending batch of [1000][241.5kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdoma
in][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,225][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,287][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.5kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,295][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] sending final batch of [436][105.5kb] (total: [24875], id: [1436250922082]) translog operations to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.loca
ldomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_local_storage_nodes=1}
[2015-07-08 08:41:08,349][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,413][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [61334], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,476][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [61991], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,500][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][5] recovery [phase3] to [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loca
l_storage_nodes=1}: took [448.8ms]
[2015-07-08 08:41:08,562][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.1kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,707][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.4kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,775][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.9kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,841][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.1kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,905][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:08,969][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.8kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,030][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.3kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,167][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.4kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,243][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.2kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,256][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89074
[2015-07-08 08:41:09,307][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89075
[2015-07-08 08:41:09,310][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,386][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][239.9kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,445][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.8kb] (total: [62686], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,720][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][240.4kb] (total: [62854], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,810][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [63120], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,895][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.7kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:09,979][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,063][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.8kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,145][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.3kb] (total: [63977], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,268][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241kb] (total: [64410], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,499][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.8kb] (total: [65269], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,630][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [65277], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,773][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89076
[2015-07-08 08:41:10,799][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.6kb] (total: [65305], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,893][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.4kb] (total: [65341], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:10,975][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [65363], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,080][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,189][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,294][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending final batch of [995][370.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.loca
ldomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,393][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase2] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [5.3s]
[2015-07-08 08:41:11,393][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: sending transaction log operations
[2015-07-08 08:41:11,399][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][299.2kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] marking recovery from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_loc
al_storage_nodes=1} as done, id [7]
[2015-07-08 08:41:11,406][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][3] recovery completed from [elasticsearch-nodes1.localdomain][fX9BOXiSR5asrQOg9F8L8g][elasticsearch-nodes1.localdomain][inet[/10.210.14.138:9300]]{aws_availability_zone=eu-west-1c, max_l
ocal_storage_nodes=1}, took[5.7s]
   phase1: recovered_files [12] with total_size of [6.9mb], took [992ms], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [13ms]
         : recovered [38341] transaction log operations, took [3.4s]
   phase3: recovered [7286] transaction log operations, took [881ms]
[2015-07-08 08:41:11,409][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89077
[2015-07-08 08:41:11,440][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.4kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,479][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][304.5kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,521][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,561][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.9kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,597][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,637][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][241.8kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,674][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][354.1kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,714][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][242kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain
][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,743][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending batch of [1000][535.3kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdoma
in][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:11,773][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] sending final batch of [395][991.8kb] (total: [65390], id: [1436337649307]) translog operations to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.loca
ldomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local_storage_nodes=1}
[2015-07-08 08:41:12,092][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][1] recovery [phase3] to [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_local
_storage_nodes=1}: took [699.4ms]
[2015-07-08 08:41:12,684][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89078
[2015-07-08 08:41:12,859][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] marking recovery from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_loca
l_storage_nodes=1} as done, id [6]
[2015-07-08 08:41:12,859][TRACE][indices.recovery         ] [elasticsearch-nodes2.localdomain] [index_a][0] recovery completed from [elasticsearch-nodes3.localdomain][_WBKY5VgSPScb2rcEM8Ejw][elasticsearch-nodes3.localdomain][inet[/10.210.14.50:9300]]{aws_availability_zone=eu-west-1a, max_lo
cal_storage_nodes=1}, took[7.2s]
   phase1: recovered_files [1] with total_size of [108b], took [13ms], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [4ms]
         : recovered [54870] transaction log operations, took [5.8s]
   phase3: recovered [10795] transaction log operations, took [668ms]
[2015-07-08 08:41:12,862][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89079
[2015-07-08 08:41:12,934][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89080
[2015-07-08 08:41:19,194][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89081
[2015-07-08 08:46:20,640][DEBUG][discovery.zen.publish    ] [elasticsearch-nodes2.localdomain] received cluster state version 89082
```
</comment><comment author="clintongormley" created="2015-07-17T10:41:02Z" id="122241136">Hi @womwombat 

We had a long talk about this internally, and need some more info to track down what is happening here.  Please could you give us hot threads output while you see the "failed to lock" message happening, as follows:

```
curl -XGET "http://localhost:9202/_nodes/hot_threads?threads=10000&amp;ignore_idle_threads=false"
```
- Could we have the logs from the moment you start the optimize, through closing the index, and reopening the other index.
- Do you use scroll requests? If so, what scroll timeout do you set?
- Could you try removing optimize from your process and see if that helps?
- Another wild suggestion - do your servers use Haswell CPUs? Wondering if you are suffering from the futex bug https://github.com/elastic/elasticsearch/issues/11526
</comment><comment author="womwombat" created="2015-08-07T08:49:54Z" id="128643925">Hi,
- The log bellow is the only log I have, maybe you want TRACE from something else ?
- we don't use scroll requests
- we can't right now, but we'll do this next week
- our servers are not using haswell CPU 
- We have migrated all the nodes form AWS to on premises brand-new servers, with 64G ram, 256G SSD, and the pb still occurs in 1.7.0
- here is a full log of one of the event yesterday

```
[2015-08-06 19:36:37,742][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][4]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][4] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][4], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-08-06 19:36:47,848][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-08-06 19:36:57,931][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][4]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][4] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][4], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more

----

There're 3000 lines of this error, I've skipped them for readability

----


[2015-08-06 20:00:15,485][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-08-06 20:00:25,722][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-08-06 20:00:35,742][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-08-06 20:00:38,747][DEBUG][action.bulk              ] [node-01] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-08-06 20:00:44,367][INFO ][node                     ] [node-01] stopping ...
[2015-08-06 20:00:44,396][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
    at org.elasticsearch.cluster.service.InternalClusterService.doStop(InternalClusterService.java:174)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
    at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:310)
    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:334)
    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:82)
[2015-08-06 20:00:44,400][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.doRun(TransportSearchQueryThenFetchAction.java:152)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-08-06 20:00:44,425][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.doRun(TransportSearchQueryThenFetchAction.java:152)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-08-06 20:00:45,808][WARN ][indices.cluster          ] [node-01] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
[2015-08-06 20:00:45,828][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:138)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)
    at org.elasticsearch.action.bulk.TransportBulkAction$2.onFailure(TransportBulkAction.java:355)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAsFailed(TransportShardReplicationOperationAction.java:536)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onClusterServiceClose(TransportShardReplicationOperationAction.java:509)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onClose(ClusterStateObserver.java:217)
    at org.elasticsearch.cluster.service.InternalClusterService.add(InternalClusterService.java:236)
    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:146)
    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:96)
    at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:88)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.retry(TransportShardReplicationOperationAction.java:501)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.retryBecauseUnavailable(TransportShardReplicationOperationAction.java:655)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.doRun(TransportShardReplicationOperationAction.java:362)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$3.onNewClusterState(TransportShardReplicationOperationAction.java:504)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.postAdded(ClusterStateObserver.java:201)
    at org.elasticsearch.cluster.service.InternalClusterService$1.run(InternalClusterService.java:248)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-08-06 20:00:46,409][INFO ][node                     ] [node-01] stopped
[2015-08-06 20:00:46,409][INFO ][node                     ] [node-01] closing ...
[2015-08-06 20:00:56,428][WARN ][cluster.action.index     ] [node-01] [my_custom_index_b] failed to lock all shards for index - timed out after 30 seconds
[2015-08-06 20:00:56,440][INFO ][node                     ] [node-01] closed
[2015-08-06 20:00:58,249][INFO ][node                     ] [node-01] version[1.7.0], pid[27455], build[929b973/2015-07-16T14:31:07Z]
[2015-08-06 20:00:58,249][INFO ][node                     ] [node-01] initializing ...
[2015-08-06 20:00:58,362][INFO ][plugins                  ] [node-01] loaded [], sites [HQ, kopf, whatson]
[2015-08-06 20:00:58,411][INFO ][env                      ] [node-01] using [1] data paths, mounts [[/srv (/dev/sdb)]], net usable_space [219.4gb], net total_space [237.7gb], types [xfs]
[2015-08-06 20:01:01,277][INFO ][node                     ] [node-01] initialized
[2015-08-06 20:01:01,278][INFO ][node                     ] [node-01] starting ...
[2015-08-06 20:01:01,485][INFO ][transport                ] [node-01] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.211:9300]}
[2015-08-06 20:01:01,502][INFO ][discovery                ] [node-01] my_cluster/65f-xeTqT3msqPDbYlbtew
[2015-08-06 20:01:11,165][INFO ][cluster.service          ] [node-01] detected_master [node-02][-_LoaCn7TaqMjAkRDaHTQA][node-02][inet[/192.168.1.212:9300]], added {[node-02][-_LoaCn7TaqMjAkRDaHTQA][node-02][
inet[/192.168.1.212:9300]],[node-03][zdxOnF2qQrG1rJ4czVMXUA][node-03][inet[/192.168.1.213:9300]],}, reason: zen-disco-receive(from master [[node-02][-_LoaCn7TaqMjAkRDaHTQA][node-02][inet[/192.168.1.212:9300]]])
[2015-08-06 20:01:11,235][INFO ][http                     ] [node-01] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.211:9200]}
[2015-08-06 20:01:11,235][INFO ][node                     ] [node-01] started
```
</comment><comment author="clintongormley" created="2015-08-07T11:28:54Z" id="128679291">Hi @womwombat 

Thanks for the logs.  Restarting was the right thing to do here.  Could you look back in the logs on that server to before the problem started, to find out what triggered this issue?
</comment><comment author="womwombat" created="2015-08-07T11:54:54Z" id="128682464">Hi,

Thanks for your help :)

The log above was on node-01, here is log on node-02 (looks like there's what's your looking for)

```
[2015-08-06 19:36:02,594][INFO ][cluster.metadata         ] [node-02] [my_custom_index_b] deleting index
[2015-08-06 19:36:32,663][INFO ][cluster.metadata         ] [node-02] [my_custom_index_b] creating index, cause [api], templates [], shards [6]/[1], mappings []
[2015-08-06 19:36:37,745][WARN ][cluster.action.shard     ] [node-02] [my_custom_index_b][4] received shard failed for [my_custom_index_b][4], node[n8Ry5JsZT4WiXqr16ux4dg], [P], s[INITIALIZING], unassigned_info[[reason=INDEX_CREATED], at[2015-08-06T17:36
:32.664Z]], indexUUID [mcr4fgrtRwaHgITxhd3qrg], reason [shard failure [failed to create shard][IndexShardCreationException[[my_custom_index_b][4] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [my_custom_index_b][4], timed out after 50
00ms]; ]]
[2015-08-06 19:36:42,803][WARN ][indices.cluster          ] [node-02] [[my_custom_index_b][0]] marking and sending shard failed due to [failed to create shard]
org.elasticsearch.index.shard.IndexShardCreationException: [my_custom_index_b][0] failed to create shard
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [my_custom_index_b][0], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:582)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:510)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)
    ... 9 more
```

And on node-03:

```
[2015-08-06 19:38:27,714][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:39:29,515][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:40:29,701][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:41:30,813][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:41:30,928][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:42:31,519][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:42:31,600][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:43:32,715][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:44:32,801][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:44:32,884][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:45:33,165][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:45:33,165][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:46:33,894][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:46:33,968][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:47:34,267][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:47:34,293][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:48:34,505][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:48:34,577][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:49:35,501][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:52:36,197][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:53:36,611][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:54:36,775][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:58:41,294][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 19:59:38,439][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 20:00:42,834][DEBUG][action.bulk              ] [node-03] observer: timeout notification from cluster serv
ice. timeout setting [1m], time since start [1m]
[2015-08-06 20:00:45,853][INFO ][cluster.service          ] [node-03] removed {[node-01][n8Ry5JsZT4WiXqr16ux
4dg][node-01][inet[/192.168.1.211:9300]],}, reason: zen-disco-receive(from master [[node-02][-_LoaCn7TaqMjA
kRDaHTQA][node-02][inet[/192.168.1.212:9300]]])
[2015-08-06 20:01:11,144][INFO ][cluster.service          ] [node-03] added {[node-01][65f-xeTqT3msqPDbYlbte
w][node-01][inet[/192.168.1.211:9300]],}, reason: zen-disco-receive(from master [[node-02][-_LoaCn7TaqMjAkR
DaHTQA][node-02][inet[/192.168.1.212:9300]]])
```
</comment><comment author="clintongormley" created="2015-08-07T12:55:36Z" id="128692856">So essentially the shards for the new index can't be allocated because the shard from the old index is still hanging around.  Something is holding on to it, but not sure what.  eg a scroll request would be one option but it could be many things.  That hot threads output (while the node is in this state) might help here, plus as much information as you can give us about any exceptions that you see (not necessarily related to allocation) plus queries that you run etc etc.
</comment><comment author="womwombat" created="2015-08-08T11:14:37Z" id="128966255">Hi,

The hot thread output is rather large (598ko) so I paste it here http://justpaste.it/es-hot-thread

Regards,
</comment><comment author="zombiepig01" created="2015-08-17T21:45:33Z" id="131971810">I am seeing this same issue on my (small) cluster.  I bulk-index about 200M documents with index.refresh_interval:-1 and index.number_of_replicas:0.  Shortly after setting index.number_of_replicas:1, I started seeing the failed shard errors.  Seems to be repeatable.  Let me know if I can send any info that might help.
</comment><comment author="rakesh91" created="2015-09-02T17:16:20Z" id="137176129">Even i am seeing the same issue, after upgrade from 1.4 to 1.6 version and made replication 0 from 1. So I upgraded to 1.7, then this issue stopped. But, now cluster keeps going down, by giving "shard failure" error and eventually node getting disconnected. 

Had never seen this issue while it was in 1.4 version.
</comment><comment author="clintongormley" created="2015-09-05T12:05:00Z" id="137948324">@rakesh91 @zombiepig01  what exceptions are you seeing in your logs?

are you deleting an index then creating a new index with the same name?
</comment><comment author="rakesh91" created="2015-09-06T17:16:17Z" id="138101221">@clintongormley when cluster go down, in master 

[2015-09-04 15:40:43,172][WARN ][cluster.action.shard     ] [esmaster2] [logstash-2015.09.04][9] received shard failed for [logstash-2015.09.04][9], node[2eOY2_zxRwiNtbZRmVxL5g], [P], s[INITIALIZING], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-09-04T10:10:34.254Z]], indexUUID [pGvcrpH3SzSlX46yYiqQcw], reason [shard failure [failed to create shard][IndexShardCreationException[[logstash-2015.09.04][9] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [logstash-2015.09.04][9], timed out after 5000ms]; ]]

No I am not deleting the index and re-create with same name, but i delete 28 days old index.

Upon cluster restart, it will be fine, but few times it will fix on its own
</comment><comment author="matthewgard1" created="2015-09-16T18:49:38Z" id="140840508">upgraded from 1.4.4 -&gt; 1.7.2
/_bulk (indexing)

looping errors : 

shard-failed ([INDEX-2015-34][1], node[9xgitQcHRR2kPZ7JYpKBtg], [P], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-09-16T18:26:29.047Z], details[shard failure [failed to create shard][IndexShardCreationException[[INDEX-2015-34][1] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [INDEX-2015-34][1], timed out after 5000ms]; ]]]), reason [shard failure [failed to create shard][IndexShardCreationException[[INDEX-2015-34][1] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [INDEX-2015-34][1], timed out after 5000ms]; ]] 
</comment><comment author="clintongormley" created="2015-10-08T11:48:21Z" id="146514305">If you're still seeing this "LockObtainFailedException", please get a hot threads dump with the following command:

```
curl -XGET -s localhost:9200/_nodes/hot_threads?threads=10000
```

In particular, we're looking for output like this:

```
 0.0% (0s out of 500ms) cpu usage by thread 'elasticsearch[host-23][[index-abc-123][0]: Lucene Merge Thread #111975]'
 10/10 snapshots sharing following 14 elements
 java.lang.Object.wait(Native Method)
 java.lang.Thread.join(Thread.java:1245)
 java.lang.Thread.join(Thread.java:1319)
 org.apache.lucene.index.ConcurrentMergeScheduler.sync(ConcurrentMergeScheduler.java:281)
 org.apache.lucene.index.ConcurrentMergeScheduler.close(ConcurrentMergeScheduler.java:262)
 org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.close(ConcurrentMergeSchedulerProvider.java:141)
 org.elasticsearch.index.merge.EnableMergeScheduler.close(EnableMergeScheduler.java:56)
 org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2114)
 org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2089)
 org.apache.lucene.index.IndexWriter.tragicEvent(IndexWriter.java:4686)
 org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3839)
 org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:409)
 org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
 org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:486)
```

This is related to this Lucene bug (https://issues.apache.org/jira/browse/LUCENE-6670) where an OOM is thrown on a merge thread.
</comment><comment author="krisb78" created="2015-12-12T13:10:03Z" id="164148668">I seem to be suffering from the same problem. 

Interestingly enough, I managed to make it go away on 1.7.4 (https://github.com/elastic/elasticsearch/issues/12926) by removing all nested fields (https://www.elastic.co/guide/en/elasticsearch/reference/2.1/nested.html) from my mappings and reindexing all the data.

I upgraded to 2.0.0 a couple of days ago and then to 2.1.0 yesterday. Today I noticed that the problem returned - could the upgrade have caused it?

I can see shards stuck in INITIALIZING again:

```
[LIVE] krisba@cubitsearch-client-1:~$ curl localhost:9200/_cat/shards | grep INIT
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0joint_user_summary_v1_all               2 r INITIALIZING                   10.0.1.238 cubitsearch-4
joint_user_summary_v1_all               2 r INITIALIZING                   10.0.1.239 cubitsearch-5
joint_user_summary_v1_all               2 r INITIALIZING                   10.0.1.236 cubitsearch-2
joint_user_summary_v1_all               1 r INITIALIZING                   10.0.1.237 cubitsearch-3
joint_user_summary_v1_all               4 r INITIALIZING                   10.0.1.238 cubitsearch-4
joint_user_summary_v1_all               4 r INITIALIZING                   10.0.1.239 cubitsearch-5
joint_user_summary_v1_all               0 r INITIALIZING                   10.0.1.237 cubitsearch-3
joint_user_summary_v1_all               0 r INITIALIZING                   10.0.1.236 cubitsearch-2
```

Here's a log sample from cubitsearch-4:

```
Dec 12 12:53:40 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
Dec 12 12:53:40 cubitsearch-4 elasticsearch: #011... 7 more
Dec 12 12:53:41 cubitsearch-4 elasticsearch: [2015-12-12 12:53:41,541][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][1], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8697], s[INITIALIZING], a[id=KIkQeU0rTCSzZtFPl0YtlQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:42:23.164Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms]; ]]]
Dec 12 12:53:41 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011... 7 more
Dec 12 12:53:41 cubitsearch-4 elasticsearch: [2015-12-12 12:53:41,549][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][0], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8605], s[INITIALIZING], a[id=IQeeXvCCTweMQpf2O-ncWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:43:05.921Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][0], timed out after 5000ms]; ]]]
Dec 12 12:53:41 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
Dec 12 12:53:41 cubitsearch-4 elasticsearch: #011... 7 more
Dec 12 12:53:42 cubitsearch-4 elasticsearch: [2015-12-12 12:53:42,608][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][1], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8697], s[INITIALIZING], a[id=KIkQeU0rTCSzZtFPl0YtlQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:42:23.164Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms]; ]]]
Dec 12 12:53:42 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011... 7 more
Dec 12 12:53:42 cubitsearch-4 elasticsearch: [2015-12-12 12:53:42,610][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][0], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8605], s[INITIALIZING], a[id=IQeeXvCCTweMQpf2O-ncWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:43:05.921Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][0], timed out after 5000ms]; ]]]
Dec 12 12:53:42 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
Dec 12 12:53:42 cubitsearch-4 elasticsearch: #011... 7 more
Dec 12 12:53:44 cubitsearch-4 elasticsearch: [2015-12-12 12:53:44,514][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][1], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8697], s[INITIALIZING], a[id=KIkQeU0rTCSzZtFPl0YtlQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:42:23.164Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms]; ]]]
Dec 12 12:53:44 cubitsearch-4 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: Caused by: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
Dec 12 12:53:44 cubitsearch-4 elasticsearch: #011... 7 more
Dec 12 12:53:44 cubitsearch-4 elasticsearch: [2015-12-12 12:53:44,515][DEBUG][action.admin.indices.stats] [cubitsearch-4] [indices:monitor/stats] failed to execute operation for shard [[joint_user_summary_v1_all][0], node[3mL6ye16Ru2o05yVzjoUuQ], [R], v[8605], s[INITIALIZING], a[id=IQeeXvCCTweMQpf2O-ncWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-12T12:43:05.921Z], details[failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][0], timed out after 5000ms]; ]]]
```

And here's the output of `curl -XGET -s localhost:9200/_nodes/hot_threads?threads=10000`:

[hot_threads.txt](https://github.com/elastic/elasticsearch/files/60134/hot_threads.txt)
</comment><comment author="krisb78" created="2015-12-12T18:05:50Z" id="164172767">Ho-hum. It fixed itself...
</comment><comment author="clintongormley" created="2015-12-14T13:32:05Z" id="164437888">@krisb78 these log messages are just saying that you are trying to search on these shards before they have recovered.
</comment><comment author="zombiepig01" created="2015-12-14T23:42:08Z" id="164595850">@clintongormley -- I recently started seeing those messages again:  here is the thread dump:

https://gist.github.com/zombiepig01/41bad73814d27fe3a018
</comment><comment author="jindov" created="2015-12-16T03:03:29Z" id="164971801">I have a large index, about 150GB, this index is stucked whern restart es (running 1 node), this is log:

```
[logstash-2015.12.15][[logstash-2015.12.15][4]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2015.12.15][[logstash-2015.12.15][4]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-16 09:57:24,397][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][3], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=Q8evo1x3Rgy0Ytpd7zpLGQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]
[logstash-2015.12.15][[logstash-2015.12.15][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2015.12.15][[logstash-2015.12.15][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-16 09:57:24,398][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][1], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=Av2uA3StSOW_8S72m7EqOw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]
[logstash-2015.12.15][[logstash-2015.12.15][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2015.12.15][[logstash-2015.12.15][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-16 09:57:24,398][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][2], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=3s_-6jJ7Q8WxfAQsFZBJMw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]
[logstash-2015.12.15][[logstash-2015.12.15][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2015.12.15][[logstash-2015.12.15][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-16 09:57:24,399][DEBUG][action.admin.indices.stats] [log03] [indices:monitor/stats] failed to execute operation for shard [[logstash-2015.12.15][0], node[gEc7JgdmSVyu-EzzdNn7bA], [P], v[7], s[INITIALIZING], a[id=H4oJgM5dR0GS73NiK7Glsg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-16T02:54:37.577Z]]]
[logstash-2015.12.15][[logstash-2015.12.15][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2015.12.15][[logstash-2015.12.15][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
```

is it a bug?
</comment><comment author="clintongormley" created="2015-12-16T11:36:04Z" id="165076456">@jindov see https://github.com/elastic/elasticsearch/issues/12011#issuecomment-164437888
</comment><comment author="clintongormley" created="2016-01-18T10:43:43Z" id="172494582">The original issue here `LockObtainFailedException: Can't lock shard` seems to have been resolved in recent releases.  Closing for now.  Please reopen if you're seeing the same thing on 2.1 or higher.
</comment><comment author="kovrus" created="2016-03-25T14:18:44Z" id="201303811">@clintongormley do you have any clue what was the cause of the issue and what could have fixed it in &gt; 2.1 ?
</comment><comment author="bleskes" created="2016-03-25T14:57:49Z" id="201321821">@kovrus we had some deadlocks fixed - those were preventing the shard lock from being freed. Do see this with a version &gt;2.1?
</comment><comment author="krisb78" created="2016-06-02T12:15:43Z" id="223273994">I'm hitting this old chestnut again. I upgraded from 2.3.0 to 2.3.3 and all indices are fine (green) except the largest one, which is still yellow.

Shards are spending considerable time in the VERIFY_INDEX stage, but then they fail to initialise:

On the node that tries to initialise a shard, I'm seeing:

```
Jun  2 12:07:00 cubitsearch-5 elasticsearch: [2016-06-02 12:07:00,089][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][2]] marking and sending shard failed due to [failed to create shard]
Jun  2 12:07:00 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][2]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms];
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
Jun  2 12:07:00 cubitsearch-5 elasticsearch: #011... 10 more
Jun  2 12:07:05 cubitsearch-5 elasticsearch: [2016-06-02 12:07:05,223][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][4]] marking and sending shard failed due to [failed to create shard]
Jun  2 12:07:05 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][4]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][4], timed out after 5000ms];
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][4], timed out after 5000ms
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
Jun  2 12:07:05 cubitsearch-5 elasticsearch: #011... 10 more
Jun  2 12:07:10 cubitsearch-5 elasticsearch: [2016-06-02 12:07:10,318][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][1]] marking and sending shard failed due to [failed to create shard]
Jun  2 12:07:10 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms];
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
Jun  2 12:07:10 cubitsearch-5 elasticsearch: #011... 10 more
Jun  2 12:07:15 cubitsearch-5 elasticsearch: [2016-06-02 12:07:15,421][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][1]] marking and sending shard failed due to [failed to create shard]
Jun  2 12:07:15 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][1]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms];
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][1], timed out after 5000ms
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
Jun  2 12:07:15 cubitsearch-5 elasticsearch: #011... 10 more
Jun  2 12:07:20 cubitsearch-5 elasticsearch: [2016-06-02 12:07:20,830][WARN ][indices.cluster          ] [cubitsearch-5] [[joint_user_summary_v1_all][2]] marking and sending shard failed due to [failed to create shard]
Jun  2 12:07:20 cubitsearch-5 elasticsearch: [joint_user_summary_v1_all][[joint_user_summary_v1_all][2]] ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms];
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:601)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at java.lang.Thread.run(Thread.java:745)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: Caused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [joint_user_summary_v1_all][2], timed out after 5000ms
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:609)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:537)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011at org.elasticsearch.index.IndexService.createShard(IndexService.java:306)
Jun  2 12:07:20 cubitsearch-5 elasticsearch: #011... 10 more
```

The node that "sends" the shard to cubitsearch-5 says:

```
Jun  2 11:28:28 cubitsearch-2 elasticsearch: [2016-06-02 11:28:28,961][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [2214039ms] ago, timed out [1314039ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [479687]
Jun  2 11:34:15 cubitsearch-2 elasticsearch: [2016-06-02 11:34:15,131][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1849861ms] ago, timed out [949861ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [482623]
Jun  2 11:47:14 cubitsearch-2 elasticsearch: [2016-06-02 11:47:14,682][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1537976ms] ago, timed out [637976ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [487423]
Jun  2 12:06:51 cubitsearch-2 elasticsearch: [2016-06-02 12:06:51,344][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1807416ms] ago, timed out [907416ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [507910]
Jun  2 12:09:32 cubitsearch-2 elasticsearch: [2016-06-02 12:09:32,897][WARN ][transport                ] [cubitsearch-2] Received response for a request that has timed out, sent [1975624ms] ago, timed out [1075624ms] ago, action [internal:index/shard/recovery/prepare_translog], node [{cubitsearch-5}{Dxpr53kUS9qN41nLnLUOXA}{10.0.1.239}{10.0.1.239:9300}{max_local_storage_nodes=1, master=false}], id [507493]
```

I tried restarting the nodes a couple of times already and managed to get some more shards to initialise, but some of them are still missing. 
</comment><comment author="clintongormley" created="2016-06-02T13:11:21Z" id="223286581">@krisb78 what operations are happening on your cluster at the same time? esp, any scrolls?
</comment><comment author="clintongormley" created="2016-06-02T13:11:37Z" id="223286638">delete index and recreate?
</comment><comment author="krisb78" created="2016-06-02T13:27:32Z" id="223290869">Already check the scrolls, nothing like that is happening.

What seems to have helped a bit was deleting this index manually  (i.e., removing the index directory) before restarting a node.  I managed to get shards to initialise on 3 nodes by doing that, but the remaining 2 won't budge.

I'm holding off from deleting &amp; recreating the index for now - I have done it to fix this issue before, but I wouldn't want to have to do that on every upgrade...
</comment><comment author="s1monw" created="2016-06-02T13:30:26Z" id="223291666">@krisb78 is it only one node that is causing problems? if so you can try restarting it. Can I the output of `localhost:9200/_cat/recovery`
</comment><comment author="krisb78" created="2016-06-02T14:08:19Z" id="223302704">It was all nodes, really...

Here's what _cat/recovery is saying about this problematic index:

```
joint_user_summary_v1_all                 0 1861391 replica done         10.0.1.239 10.0.1.237 n/a n/a 147 100.0% 17955289909 100.0% 147 17955289909 0      100.0% 0
joint_user_summary_v1_all                 0 2303300 replica done         10.0.1.239 10.0.1.235 n/a n/a 147 100.0% 17955289909 100.0% 147 17955289909 0      100.0% 0
joint_user_summary_v1_all                 1 1931733 replica done         10.0.1.239 10.0.1.236 n/a n/a 162 100.0% 17654494329 100.0% 162 17654494329 0      100.0% 0
joint_user_summary_v1_all                 1 1459125 replica done         10.0.1.239 10.0.1.237 n/a n/a 162 100.0% 17654494329 100.0% 162 17654494329 0      100.0% 0
joint_user_summary_v1_all                 1 856778  replica done         10.0.1.239 10.0.1.235 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
joint_user_summary_v1_all                 2 463469  replica done         10.0.1.239 10.0.1.236 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
joint_user_summary_v1_all                 2 472860  replica done         10.0.1.239 10.0.1.237 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
joint_user_summary_v1_all                 2 1059848 replica done         10.0.1.239 10.0.1.235 n/a n/a 124 100.0% 13853113660 100.0% 124 13853113660 0      100.0% 0
joint_user_summary_v1_all                 3 1717997 replica done         10.0.1.239 10.0.1.236 n/a n/a 128 100.0% 14859107452 100.0% 128 14859107452 0      100.0% 0
joint_user_summary_v1_all                 3 900335  replica done         10.0.1.239 10.0.1.237 n/a n/a 128 100.0% 14859107452 100.0% 128 14859107452 0      100.0% 0
joint_user_summary_v1_all                 3 183371  replica verify_index 10.0.1.236 10.0.1.239 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
joint_user_summary_v1_all                 3 1272129 replica done         10.0.1.239 10.0.1.235 n/a n/a 128 100.0% 14859107452 100.0% 128 14859107452 0      100.0% 0
joint_user_summary_v1_all                 4 1611829 replica done         10.0.1.239 10.0.1.236 n/a n/a 125 100.0% 16055840146 100.0% 125 16055840146 0      100.0% 0
joint_user_summary_v1_all                 4 1250961 replica done         10.0.1.239 10.0.1.237 n/a n/a 125 100.0% 16055840146 100.0% 125 16055840146 0      100.0% 0
joint_user_summary_v1_all                 4 708638  replica verify_index 10.0.1.236 10.0.1.238 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
joint_user_summary_v1_all                 4 708630  replica verify_index 10.0.1.236 10.0.1.239 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
joint_user_summary_v1_all                 4 753186  replica done         10.0.1.239 10.0.1.235 n/a n/a 0   0.0%   0           0.0%   0   0           0      100.0% 0
```

My theory is that for some reason `prepare_translog` takes too long:

```
Jun  2 13:37:09 cubitsearch-5 elasticsearch: Caused by: ReceiveTimeoutTransportException[[cubitsearch-5][10.0.1.239:9300][internal:index/shard/recovery/prepare_translog] request_id [709272] timed out after [900001ms]]
```

Once this timeout happens, the `Can't lock shard` errors follow until the node is restarted.

Looking at the code, this timeout is dictated by `indices.recovery.internal_action_timeout` or `indices.recovery.internal_action_long_timeout`.

The documentation doesn't say that these settings are customisable, but I tried to change them anyway now to see if it changes anything.
</comment><comment author="s1monw" created="2016-06-02T14:18:04Z" id="223305689">@bleskes does this ring any bell? I think we cancel both sides of recovery on such a failure?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove sigar completely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12010</link><project id="" key="" /><description>This commit removes completely Sigar from the codebase.
</description><key id="92822804">12010</key><summary>Remove sigar completely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-03T08:33:03Z</created><updated>2015-07-08T13:18:15Z</updated><resolved>2015-07-03T13:59:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-03T08:35:30Z" id="118276482">@rmuir can you have a look please? thanks
</comment><comment author="s1monw" created="2015-07-03T09:31:23Z" id="118293508">LGTM
</comment><comment author="rmuir" created="2015-07-03T13:42:51Z" id="118355898">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES Memory Breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12009</link><project id="" key="" /><description>Hello all,

We are running Graylog server v1.01 and Graylog-web-interface v1.01on ES v1.4.4 on SLES 11 SP3 platform, today we logon graylog-web-interface and found HTTP 400 error, 

the ES logs as below:

[2015-07-03 10:03:03,415][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 646251060 [616.3mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:03,443][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 646253680 [616.3mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:03,449][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 646252426 [616.3mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:03,455][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 646251888 [616.3mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:03,785][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 651460534 [621.2mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:05,902][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 649663091 [619.5mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:06,489][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 646252146 [616.3mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking
[2015-07-03 10:03:06,533][WARN ][indices.breaker          ] [FIT CN ES Node] [FIELDDATA] New used memory 646251688 [616.3mb] from field [timestamp] would be larger than configured breaker: 633785548 [604.4mb], breaking

we think that's the ES memory breaker setting as below link described:
https://www.elastic.co/guide/en/elasticsearch/reference/1.4/index-modules-fielddata.html

we have some questions:
1. which parameters should be responsible for our ES error,
   i mean which parameter we should be change and change to which value? 
2. as ES documents mentioned, all parameter already have default value so why we still have encountered this error ? 

Thanks for your great support.
</description><key id="92796599">12009</key><summary>ES Memory Breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaman478</reporter><labels /><created>2015-07-03T05:36:13Z</created><updated>2015-07-03T05:52:21Z</updated><resolved>2015-07-03T05:52:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-03T05:52:20Z" id="118245033">Memory circuit breaker is indeed involved here as it's protecting you from OOM exceptions.

Not an issue. Please join us on discuss.elastic.co where we and the community can help you.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduced ESMessageFormat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12008</link><project id="" key="" /><description>formally known as `LoggerMessageFormat` now this message format is not only for logs but also for exceptions.

The format was enhanced to support `[]` place holders in addition to the tranditional `{}`. The `[]` place holders will automatically enclosed single parameters (i.e. non-array parameters) within brackets. For example:

pattern: "value []"
param: 34
output: "value [37]"

Note, array parameters will always be enclosed in brackets (regardless of whether `{}` or `[]` are used)
</description><key id="92774810">12008</key><summary>Introduced ESMessageFormat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label></labels><created>2015-07-03T02:07:35Z</created><updated>2016-03-10T11:05:17Z</updated><resolved>2016-03-10T11:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-07T08:13:08Z" id="119113644">I like this change. I'll merge this in later today.
</comment><comment author="jpountz" created="2015-07-07T22:42:12Z" id="119364958">I don't mind the renaming. However how much does it help to type `[]` instead of `[{}]`? I'm slightly concerned that we are making room for bugs without really improving the usability of the API?
</comment><comment author="rjernst" created="2015-07-08T04:47:38Z" id="119430440">I agree with @jpountz. I don't think saving 2 characters really is worth the complexity.
</comment><comment author="nik9000" created="2015-07-08T12:54:25Z" id="119568980">My fault I guess. I reviewed a previous commit that did a lot of
standardization on [{}] and said that it looked like a lot of repetition. I
suggested [] as a shorthand. I guess it wasn't a great suggestion. The
thing with seeing [{}] everywhere is that it looks like a standard
convention that'd be easy to forget at times. I thought [] would be a nice
shortening and a reminder. You don't see it in every project nor do you see
that [{}] standard either.

I still like the rename even if I have backed up a bit on the extra syntax.
On Jul 8, 2015 12:47 AM, "Ryan Ernst" notifications@github.com wrote:

&gt; I agree with @jpountz https://github.com/jpountz. I don't think saving
&gt; 2 characters really is worth the complexity.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/12008#issuecomment-119430440
&gt; .
</comment><comment author="s1monw" created="2015-07-08T12:55:42Z" id="119569396">&gt; I agree with @jpountz. I don't think saving 2 characters really is worth the complexity.

+1 lets keep it simple
</comment><comment author="martijnvg" created="2015-07-09T07:49:36Z" id="119860448">I had the same idea behind this PR as @nik9000, that it would be a nice shortcut, since `[{}]` is all over the place. So for now don't make this piece of code more complex than is needed.
</comment><comment author="nik9000" created="2015-07-27T17:03:35Z" id="125272730">I think the consensus is to drop the `[]` shortcut and change the name to `ElasticsearchMessageFormat`, right?
</comment><comment author="clintongormley" created="2015-09-15T09:58:52Z" id="140342290">@uboness are you planning on picking this up again?
</comment><comment author="nik9000" created="2015-09-15T12:37:53Z" id="140374267">&gt; @uboness are you planning on picking this up again?

Even if we don't get the `[]` format I think its an improvement.
</comment><comment author="clintongormley" created="2016-03-10T11:05:16Z" id="194792871">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scan/Scroll search request doesn't throw IndexMissingException when using pattern</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12007</link><project id="" key="" /><description>Hi,

```
GET /missing/_search?scroll=1m
{
    "query": {
        "match" : {
            "title" : "elasticsearch"
        }
    }
}
```

Leads to : 

```
{
   "error": "IndexMissingException[[missing] missing]",
   "status": 404
}
```

But 

```
GET /missing_*/_search?scroll=1m
{
    "query": {
        "match" : {
            "title" : "elasticsearch"
        }
    }
}
```

Leads to : 

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 0,
      "successful": 0,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": 0,
      "hits": []
   }
}
```
</description><key id="92750569">12007</key><summary>Scan/Scroll search request doesn't throw IndexMissingException when using pattern</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">l15k4</reporter><labels /><created>2015-07-02T22:40:27Z</created><updated>2015-07-03T08:17:47Z</updated><resolved>2015-07-03T00:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="l15k4" created="2015-07-03T00:31:22Z" id="118202188">Omg that makes sense, the pattern just doesn't match any index so it shouldn't throw `IndexMissingException` ... As initial 'Scroll' search request returns documents but 'Scan' does not, when scanning you cannot really tell whether scanning finished by just empty hits but also by `null` scrollId, so that `if (SearchType.Scan &amp;&amp; hits.empty &amp;&amp; sid == null)` condition is just for the possibility that `index pattern matching` wasn't successful .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Promote headers to first class citizens on exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12006</link><project id="" key="" /><description>This commit merges the pre-existing special exception that
allowed to associate headers with exceptions and the elasticsaerch
base class `ElasticsearchException` This allows for more generic use
of exceptions where plugins can associate meta-data with any elasticsearch
base exception to control behavior etc.

This also addds a generic SecurityException to allow plugins to pass on
information based on the RestStatus.
</description><key id="92741558">12006</key><summary>Promote headers to first class citizens on exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T21:40:17Z</created><updated>2015-07-03T06:59:47Z</updated><resolved>2015-07-03T06:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-02T21:41:05Z" id="118176478">@uboness can you take a look? This would also allow as a subsequent issue to render headers as json so 3rd party consumers can get structured information from apps?
</comment><comment author="uboness" created="2015-07-02T21:49:45Z" id="118178123">this is awesome! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ip_range aggregation with mask of 0.0.0.0/0 gets treated as 0.0.0.0/32</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12005</link><project id="" key="" /><description>An ip_range aggregation with mask of 0.0.0.0/0 is not being treated correctly, the prefix-length of 0 is being taken as 'unset' and is being set as /32

I am trying to get aggregated information on expanding scopes (some subnet in our network, our entire network, the internet...), so I want to use 0.0.0.0/0 ... a range of 0.0.0.0 to 255.255.255.255 does work (although is nit-pickingly incorrect as the range is exclusive and so the final address tested is 255.255.255.254, which might be surprising if you wanted information about broadcast packets.... but that is perhaps a separate bug report.

``` json
# Replace the template that logstash provides, so
# we add an 'ip' multifield type to the 'clientip'
# field.

PUT /_template/logstash
{
  "template": "logstash-*",
  "settings": {
    "index.refresh_interval": "5s"
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "message_field": {
            "mapping": {
              "index": "analyzed",
              "omit_norms": true,
              "type": "string"
            },
            "match_mapping_type": "string",
            "match": "message"
          }
        },
        {
          "ip_field": {
            "mapping": {
              "index": "not_analyzed",
              "omit_norms": true,
              "type": "string",
              "fields": {
                "num": {
                  "index": "not_analyzed",
                  "type": "string"
                }
              }
            },
            "match_mapping_type": "string",
            "match": "ip_*"
          }
        },
        {
          "string_fields": {
            "mapping": {
              "index": "analyzed",
              "omit_norms": true,
              "type": "string",
              "fields": {
                "raw": {
                  "index": "not_analyzed",
                  "ignore_above": 256,
                  "type": "string"
                }
              }
            },
            "match_mapping_type": "string",
            "match": "*"
          }
        }
      ],
      "properties": {
        "clientip": {
          "type": "string",
          "fields": {
            "num": {
              "index": "not_analyzed",
              "type": "ip"
            }
          }
        },
        "geoip": {
          "dynamic": true,
          "properties": {
            "location": {
              "type": "geo_point"
            }
          },
          "type": "object"
        },
        "@version": {
          "index": "not_analyzed",
          "type": "string"
        }
      },
      "_all": {
        "enabled": true,
        "omit_norms": true
      }
    }
  },
  "aliases": {}
}

# First, delete the current data, as we need to adjust
# the mapping

DELETE /logstash-test

# Now go ahead and index your logstash data...

PUT /logstash-test/logs/1
{
  "clientip": "12.34.0.1",
  "ip_source": "12.34.0.1",
  "message": "The very model of a modern major general",
  "somestring": "shoestring"
}

PUT /logstash-test/logs/2
{
  "clientip": "23.45.2.3",
  "ip_source": "23.45.2.3",
  "message": "Everyone needs a nemesis",
  "somestring": "stringthing"
}

# Check what we got back
#
# I was expecting to see:
#  - 'somestring' attribute should have a .raw subfield
#  - 'message' should not
#  - 'clientip' should have a .num subfield
#  - 'ip_source' should have a .num subfield
#
# ... but I don't see the subfields (not sure why)

GET /logstash-test/logs/1
GET /logstash-test/logs/2

# Check your mapping after you start indexing

GET logstash-test/_mapping/logs

# Do a search for it (remember it won't show up in
# a search until the index has passed its refresh
# interval)
#
# Remember to specify the subfield you need to work on

# I couldn't find an example of Kibana using a 'count'
# search-type, but it seems to use '"size": 0' instead.

### BUG!!! an ip_range with mask of 0.0.0.0/0 is not
# being treated correctly, the prefix-length of 0 is
# being taken as 'unset' and is being set as /32

GET logstash-test/_search
{
  "size": 0, 
  "aggs": {
    "ip_ranges": {
      "ip_range": {
        "field": "clientip.num",
        "ranges": [
          {
            "mask": "12.0.0.0/8"
          },
          {
            "mask": "0.0.0.0/1"
          },
          {
            "from": "0.0.0.0",
            "to": "255.255.255.255"
          },
          {
            "mask": "0.0.0.0/0"
          }
        ]
      }
    }
  }
}

# There is no IP address value that can legitimately be
# used as a "null" value for a CIDR prefix... 
# well, not in the IP address part; For IPv4, you could
# use /33 or such as an "null" value I suppose, if 
# you didn't want the overhead of adding an extra bit
# of information... so long as that implementation
# detail wasn't exposed to users.
```
</description><key id="92727186">12005</key><summary>ip_range aggregation with mask of 0.0.0.0/0 gets treated as 0.0.0.0/32</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cameronkerrnz</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-07-02T20:25:49Z</created><updated>2015-07-27T10:13:23Z</updated><resolved>2015-07-27T10:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cameronkerrnz" created="2015-07-02T22:50:41Z" id="118187530">This was seen in Elasticsearch 1.6.0
</comment><comment author="ruflin" created="2015-07-21T12:47:22Z" id="123293664">@cameronkerrnz Here is a potential fix: https://github.com/ruflin/elasticsearch/commit/3be98927a03fc7bd13b0792baddf1b7492f8855c

So far all tests are green, but I'm not sure if this could also have some unwanted side effects. Please have a look at it.
</comment><comment author="clintongormley" created="2015-07-23T10:01:34Z" id="124038722">thanks @ruflin - you want to open a PR?
</comment><comment author="ruflin" created="2015-07-23T19:34:04Z" id="124219254">Done: https://github.com/elastic/elasticsearch/pull/12430
</comment><comment author="jpountz" created="2015-07-27T10:13:23Z" id="125155540">Closed via #12430
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file></files><comments><comment>Merge pull request #12430 from ruflin/ip_range-mask-fix</comment></comments></commit></commits></item><item><title>Using putIfAbesnt to prevent a race condition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12004</link><project id="" key="" /><description>A race condition could occure in method snapshot of the class SnapshotDeletionPolicy.
This could be prevented with putIfAbsent.

I have created a pull request with my solution to this problem (#12003)
</description><key id="92713801">12004</key><summary>Using putIfAbesnt to prevent a race condition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wudi0910</reporter><labels><label>:Store</label><label>non-issue</label></labels><created>2015-07-02T19:18:03Z</created><updated>2015-07-09T14:06:54Z</updated><resolved>2015-07-09T14:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Using putIfAbsent to prevent race condition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12003</link><project id="" key="" /><description>It was possible to override a existing SnapshotHolder with a new one. If
a thread t1 calls the same function  after another thread t2 checks the
current value. Now t1 sets a new SnapshotHolder before the t2 could sets
a new SnapshotHolder. With putIfAbsent the sementic of the original code
segment will be executed atomically.
</description><key id="92712550">12003</key><summary>Using putIfAbsent to prevent race condition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">wudi0910</reporter><labels><label>:Engine</label><label>non-issue</label></labels><created>2015-07-02T19:13:15Z</created><updated>2015-07-07T12:40:11Z</updated><resolved>2015-07-07T12:39:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T16:18:47Z" id="118636127">@imotov please could you review
</comment><comment author="imotov" created="2015-07-06T20:22:57Z" id="118986244">@wudi0910 thanks for the PR. The private method `snapshot(SnapshotIndexCommit commit)` is only called in two places: [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/deletionpolicy/SnapshotDeletionPolicy.java#L117) and [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/deletionpolicy/SnapshotDeletionPolicy.java#L132) and in both places the call occurs under the same mutex. There is another place where the `snapshots` map can be modified, which is in the `close(long version)` method. However, snapshot holder removal there also occurs under the same mutex. So, unless I am missing something I don't really see how the race condition that you are describing can occur. Could you elaborate? 
</comment><comment author="wudi0910" created="2015-07-07T08:19:36Z" id="119114617">Under these circumstances, you're right. I misinterpreted that this method is called somewhere else . I'm sorry to have wasted your time.
</comment><comment author="imotov" created="2015-07-07T12:39:30Z" id="119191670">@wudi0910 no problem! I am going to close this PR then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add checksum to snapshot metadata files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12002</link><project id="" key="" /><description>This commit adds checksum to snapshot files that store global and index based metadata as well as shard information.

Closes #11589
</description><key id="92689773">12002</key><summary>Add checksum to snapshot metadata files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T17:26:24Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-15T23:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-07-06T16:15:29Z" id="118913341">@rmuir I pushed the fix for the checksum issue. Could you or @s1monw review when you have a chance?
</comment><comment author="imotov" created="2015-07-08T00:23:33Z" id="119380447">@s1monw I pushed the changes as we discussed and opened #12105 to track parsing exception changes.
</comment><comment author="dakrone" created="2015-07-10T16:39:14Z" id="120454801">Left some comments
</comment><comment author="imotov" created="2015-07-10T21:47:22Z" id="120537037">@dakrone thanks for the review. I pushed fixes.
</comment><comment author="dakrone" created="2015-07-14T15:03:10Z" id="121274836">Left a couple more comments, other than that I have a question, I see you creating and passing around a `ParseFieldMatcher` in the classes, but not using it anywhere, am I missing where you are using it?
</comment><comment author="imotov" created="2015-07-14T17:51:43Z" id="121322089">@dakrone I pushed the revision. Currently, `ParseFieldMatcher` is used in `BlobStoreIndexShardSnapshot`, `BlobStoreIndexShardSnapshots`. I could probably get rid of it in these classes but `ParseFieldMatcher`'s usage will be probably expanded in the future. So, it's nice to keep it handy.
</comment><comment author="dakrone" created="2015-07-14T23:03:09Z" id="121419829">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't add CWD to classpath when ES_CLASSPATH isn't set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12001</link><project id="" key="" /><description>See #12000 for what this does.

We may need equivalent fix for the .bat file, too
</description><key id="92688255">12001</key><summary>Don't add CWD to classpath when ES_CLASSPATH isn't set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T17:17:29Z</created><updated>2015-07-05T16:17:26Z</updated><resolved>2015-07-02T18:53:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-07-02T17:18:44Z" id="118097803">Looks right to me.
</comment><comment author="rmuir" created="2015-07-02T17:35:44Z" id="118103630">I added fix to the .bat file and it works on windows. I think this is ready.
</comment><comment author="nik9000" created="2015-07-02T17:43:37Z" id="118106851">&gt; I added fix to the .bat file and it works on windows. I think this is ready.

Can anyone check the windows change?
</comment><comment author="rmuir" created="2015-07-02T17:44:15Z" id="118107093">I ran it on a windows VM.
</comment><comment author="nik9000" created="2015-07-02T17:47:04Z" id="118108399">&gt; I ran it on a windows VM.

Cool. I saw the commit message and thought you needed someone. Sorry for the trouble.
</comment><comment author="rmuir" created="2015-07-02T17:47:33Z" id="118108621">Yeah, i edit and pushed from linux to do the minimal work in the windows vm. its too slow :)
</comment><comment author="nik9000" created="2015-07-02T17:48:48Z" id="118108934">&gt; Yeah, i edit and pushed from linux to do the minimal work in the windows vm. its too slow :)

Ah! Makes sense now.
</comment><comment author="rjernst" created="2015-07-02T18:51:21Z" id="118125040">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #12001 from rmuir/no_dont_do_that</comment></comments></commit></commits></item><item><title>elasticsearch.in.sh unintentionally adds CWD to classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12000</link><project id="" key="" /><description>it does this like so:

```
ES_CLASSPATH="$ES_CLASSPATH:..."
```

When ES_CLASSPATH is empty we get a classpath like this: 

```
:/wherever.jar:/whatever.jar
```

The empty path element gets interpreted as CWD.

I did fgrep -r ES_CLASSPATH and saw lots of logic in various packaging scripts, etc. We need to review how these scripts set the classpath and make sure they do not do this.
</description><key id="92684945">12000</key><summary>elasticsearch.in.sh unintentionally adds CWD to classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T16:59:01Z</created><updated>2015-07-02T18:53:37Z</updated><resolved>2015-07-02T18:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-02T16:59:31Z" id="118092839">Easy to reproduce, cd /, then run /path/to/bin/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard Started messages should be matched using an exact match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11999</link><project id="" key="" /><description>When a node sends a shard started message to the master, the master goes through the routing table looking for the shard to start. At the moment we validate the indexUUID, the node the shard is assigned to and the fact that the shard is initializing. This check goes wrong if a relocating replica shard finishes recovery just at the moment the source node  leaves the cluster. In this case the master will cancel the recovery and will likely assign a new initializing replica to the same target node. In this case the message from the relocation recovery can activate the new replica wrongfully.

Also, the logic for decided whether an incoming shard started message will be applied was split between ShardStateAction and the AllocationService.  
This commit does the following:
1) Let ShardStateAction only filter basic stuff like index existence and indexUUID.
2) Move the trickier shard started matching logic to the AllocationService and make it stricter
3) Unify ShardStateAction filtering logic for both shard started and shard failed.
4) Add unit tests for all of the above.

For an example test failure see: http://build-us-00.elastic.co/job/es_core_16_centos/388/
</description><key id="92683615">11999</key><summary>Shard Started messages should be matched using an exact match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>bug</label><label>resiliency</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T16:51:40Z</created><updated>2015-07-12T07:08:05Z</updated><resolved>2015-07-10T06:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-02T16:52:02Z" id="118091373">@kimchy can you take a look?
</comment><comment author="kimchy" created="2015-07-03T15:32:50Z" id="118375338">LGTM
</comment><comment author="kimchy" created="2015-07-03T15:33:53Z" id="118375811">@bleskes should we backport to 1.7 as well here?
</comment><comment author="bleskes" created="2015-07-06T09:35:14Z" id="118786762">@kimchy I think that makes sense. This is a bug fix and it's good to keep this logic similar as much as we can (give the other work around shard allocation goes into 1.x). I will give @s1monw some time to voice his opinion ... 
</comment><comment author="bleskes" created="2015-07-10T06:12:35Z" id="120243500">I pushed this to master. If I don't hear any objection by tomorrow, I'll back port to 1.7 as well...
</comment><comment author="bleskes" created="2015-07-12T07:08:05Z" id="120697620">pushed this to 1.7 as well.. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTest.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java</file></files><comments><comment>Allocation: Shard Started messages should be matched using an exact match</comment></comments></commit></commits></item><item><title>Teach /_cat/indices to Return the 'creation_date' Value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11998</link><project id="" key="" /><description>`/_cat/indices` currently allows returning an explicit set of headers in its output, such as health, status, index name, shard counts, etc. I'd love to see this endpoint support an additional header parameter: `?h=creation_date`.

This would be useful for enumerating indices by age.
</description><key id="92681382">11998</key><summary>Teach /_cat/indices to Return the 'creation_date' Value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RyanFrantz</reporter><labels /><created>2015-07-02T16:39:30Z</created><updated>2015-07-05T16:15:09Z</updated><resolved>2015-07-05T16:15:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T16:15:06Z" id="118635783">Closing as duplicate of #11524
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>handling queue ids vs message ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11997</link><project id="" key="" /><description>hey mates,

maybe some of you could help me with a problem i'm acutally trying to solve.
we're using logstash in combination with logstash-forwarder and elasticsearch to store the data. 

we're trying to analyze our internal mail routing. so we've got about 5 different kinds of log types and so we have 5 different queue ids (each system gives the incoming mail a different id) for one message id which is equal over the different systems. so e.g. we have 10 entries with the queue id 1234 but only one entry with the queue id 1234 and the message id xyz. but we need to combine / match / map every of these entries with the correct message id over all of these different log types.

our problem now is to match the data correct to our dashboard. 

i would be deeply grateful if some of you guys could show us a solution to solve this problem

-cheers 
</description><key id="92653478">11997</key><summary>handling queue ids vs message ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HoppeT</reporter><labels /><created>2015-07-02T14:37:13Z</created><updated>2015-07-05T16:13:58Z</updated><resolved>2015-07-05T16:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-05T16:13:57Z" id="118635751">Hi @HoppeT 

My initial thought would be to have a background job which pulls recent logs, groups them on message id, and the indexes a new log as a combined entry. I suggest asking this to the forum http://discuss.elastic.co/ as the github issues list is for bug reports and feature requests

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Strict json parsing improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11996</link><project id="" key="" /><description>We currently support the `index.query.parse.strict` setting that allows to turn on strict parsing, so that when deprecated keys are used elasticsearch returns an error. This was thought initialy for queries but got also expanded to anywhere we have json. Unfortunately the name of the setting doesn't reflect what it does anymore, as it can be used in the context of snapshots, aggregations, scripts etc. Let's discuss whether we should rename this setting of have separate settings for each feature. What do people think about this?

Also, I think it would be nice to improve this feature by collecting different errors and printing them all out at the same time rather than throwing exception at the first problem found. Or maybe add the possibility to enable logging deprecation usages rather than throwing exception (both can still be supported). Thoughts?
</description><key id="92649640">11996</key><summary>Strict json parsing improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label></labels><created>2015-07-02T14:18:49Z</created><updated>2015-07-10T10:31:06Z</updated><resolved>2015-07-10T10:30:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-03T07:53:46Z" id="118267805">+1 to this.

Personally I think it should be one setting for the search request in general since I don't see a reason why you would want to be notified of deprecated keys in your queries but not in your aggregations, etc.

It would be great to collate the error and print them all at once as this would save users lots of time when debugging syntax problems
</comment><comment author="clintongormley" created="2015-07-05T16:43:42Z" id="118637483">Actually, that setting was added for percolator queries and alias filters, where creating a query on an unmapped field led to it assuming the field was a string.  i think the setting was added because a user complained about the change.  Honestly, what they are doing is broken and I think we shouldn't support their particular use case. I do wonder if the need for this has been fixed by https://github.com/elastic/elasticsearch/pull/11930?  (at least for aliases if not for percolators?)

I think strict field lookups is distinct from warning/throwing exceptions for the use of deprecated functionality.  This issue deals with fieldname lookups: https://github.com/elastic/elasticsearch/issues/12016

But yes, I'm in favour of having a setting that controls whether the use of deprecated functionality throws an exception, warns to the deprecation log, or is ignored (see https://github.com/elastic/elasticsearch/issues/8963 for the original suggestion)
</comment><comment author="javanna" created="2015-07-06T07:07:28Z" id="118749117">Thanks for pointing this out @clintongormley I had a look at the codebase and this is what I found.

I do see in the history that `index.query.parse.strict` and `index.query.parse.allow_unmapped_fields` were introduced at the same time in the context of percolation and aliases. That said `index.query.parse.strict` was hooked into the existing `ParseField` infra as far as I can see, to enable/disable deprecation exceptions through it, which is the only thing that this setting does at the moment.

The error message that we are used to in the context of aliases and percolation "Strict field resolution and no field mapping can be found for the field with name..." depends on the value of the `index.query.parse.allow_unmapped_fields` setting though, nothing to do with the strict query parsing setting. Maybe @martijnvg can confirm on how we use these two settings.

I think it is worth at this point to introduce a new (more generic) setting to enable deprecation exceptions, which is not just a boolean but allows to either throw exception or log deprecation warning by using the deprecation logger that we now have.
</comment><comment author="martijnvg" created="2015-07-06T07:50:48Z" id="118761862">The `index.query.parse.allow_unmapped_fields` setting can fail a search request if no mapping for a field can be found. The percolator relies on this setting, because it parse the percolator queries at creation time and a mapping misconfiguration can lead to unexpected and difficult experience during percolating. Also aliases still uses the `index.query.parse.allow_unmapped_fields` setting. I did not disable this as part of #11930, even though aliases are now parsed at search time. It could be disabled, but I didn't do that, because it felt to me it is good to know that an alias filter is incorrect at creation time rather then at search time, since the alias filter isn't defined in the search request it self and this makes things confusing. It felt to me that this behaviour should be changed in a different change (#11806, but this was later superseded by #12016)

The `index.query.parse.strict` was always there and deals with deprecated json format. The setting was there before strict parsing was added to the percolator and alias fields. This setting was just changed into a constant as part of the strict parsing change. 

The `index.percolator.map_unmapped_fields_as_string` setting was added, because users in certain percolator scenarios (in cases when fields aren't known beforehand) did prefer flexibility over strictness and like the name suggests treats unmapped fields as string fields.

IMO all these settings are completely obsolete when #12016 gets added and the `index.query.parse.allow_unmapped_fields` and `index.percolator.map_unmapped_fields_as_string` should be removed as part of that change. 
</comment><comment author="clintongormley" created="2015-07-07T14:20:09Z" id="119219366">&gt; The index.query.parse.strict was always there and deals with deprecated json format. The setting was there before strict parsing was added to the percolator and alias fields. This setting was just changed into a constant as part of the strict parsing change.

Ah ok - i got the two mixed up.

&gt; IMO all these settings are completely obsolete when #12016 gets added and the index.query.parse.allow_unmapped_fields and index.percolator.map_unmapped_fields_as_string should be removed as part of that change.

I'm not sure I agree completely here (but maybe I'm missing something). As I understand it:
- alias filters are now parsed at execution time so they don't need the strict query parsing any more
- however percolators are still parsed only once, and so are susceptible to the same problems as before.
- the `map_unmapped_fields_as_string` setting was added for that one user's use case, which i still think is broken and i'm very tempted to remove it
</comment><comment author="martijnvg" created="2015-07-09T09:25:36Z" id="119887076">&gt; alias filters are now parsed at execution time so they don't need the strict query parsing any more

Ok, lets disable the strict parsing for alias filters.

&gt; however percolators are still parsed only once, and so are susceptible to the same problems as before.

True, we still need that, I made my conclusions too quick.

&gt; the map_unmapped_fields_as_string setting was added for that one user's use case, which i still think is broken and i'm very tempted to remove it

is there a bug with `map_unmapped_fields_as_string` setting?
</comment><comment author="martijnvg" created="2015-07-09T10:08:45Z" id="119897368">I opened #12150 for alias filters.
</comment><comment author="clintongormley" created="2015-07-10T08:37:51Z" id="120301553">&gt; is there a bug with map_unmapped_fields_as_string setting?

Not that there is a bug, just that their use case "assume all unmapped fields are strings" seems pretty broken.  Why would you create a percolator for an unknown field?

(While we're on it, a wildcarded field name in a percolator query suffers from a similar problem: it would  just take into account fields that exist when the query is instantiated)
</comment><comment author="javanna" created="2015-07-10T10:30:52Z" id="120365225">my proposals are now formalized as part of #8963 and we also have #12179 for percolator queries, I think we can close this issue then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replacing sigar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11995</link><project id="" key="" /><description>A first step in order to remove sigar (see #11034) 

This pull request removes all Sigar stuff and adds the ability to query multiple probes at once. It focused on `Process` stats/info for now and serves as an example for the other stats we have (Os, Fs, Network).
</description><key id="92640290">11995</key><summary>Replacing sigar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Stats</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T13:38:28Z</created><updated>2015-07-05T16:44:43Z</updated><resolved>2015-07-03T08:35:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-07-02T13:40:50Z" id="118036893">For the record, here are some data collected by the `JmxProcessProbe` and the `SigarProcessProbe` (not part of this pull request) on different platforms:

Empty values means "not supported".

###### Linux - Oracle Corporation Java HotSpot(TM) 64-Bit Server VM (1.8.0_20)

| Probe method | JmxProcessProbe | SigarProcessProbe |
| --- | --- | --- |
| pid | 6239 | 6239 |
| openFileDescriptor | 61 | 62 |
| maxFileDescriptor | 4096 |  |
| processCpuTime | 3580 | 3570 |
| processCpuLoad | 33 | 0 |
| processSystemTime | 16 | 170 |
| processUserTime | 0 | 3400 |
| residentMemorySize |  | 210898944 |
| sharedMemorySize |  | 15446016 |
| totalVirtualMemorySize | 7149735936 | 7149731840 |

###### Linux - Oracle Corporation OpenJDK 64-Bit Server VM (1.7.0_79)

| Probe method | JmxProcessProbe | SigarProcessProbe |
| --- | --- | --- |
| pid | 6131 | 6131 |
| openFileDescriptor | 62 | 63 |
| maxFileDescriptor | 4096 |  |
| processCpuTime | 2090 | 2080 |
| processCpuLoad | 50 | 0 |
| processSystemTime | 12 | 50 |
| processUserTime | 0 | 2030 |
| residentMemorySize |  | 144453632 |
| sharedMemorySize |  | 14565376 |
| totalVirtualMemorySize | 5920894976 | 5920890880 |

###### Windows 8.1 - Oracle Corporation Java HotSpot(TM) 64-Bit Server VM (1.8.0_25)

| Probe method | JmxProcessProbe | SigarProcessProbe |
| --- | --- | --- |
| pid | 2888 | 2888 |
| openFileDescriptor |  | 377 |
| maxFileDescriptor | 4096 |  |
| processCpuTime | 3656 | 3655 |
| processCpuLoad |  | 0 |
| processSystemTime | 0 | 437 |
| processUserTime | 0 | 3218 |
| residentMemorySize |  | 188260352 |
| sharedMemorySize |  | -1 |
| totalVirtualMemorySize | 378478592 | 2626973696 |
</comment><comment author="tlrx" created="2015-07-03T08:35:00Z" id="118276259">Closed in favor of #12010 , I keep the code here for the record.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>StackOverflowError when a SearchParseException is thrown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11994</link><project id="" key="" /><description>The following request produces a SearchParseException because there is no aggregator of type `foo`:

```
GET _search
{
  "aggs": {
    "foo": {
      "bar": {}
    }
  }
}
```

On the current master this is masked by a StackOverflowError (I presume as the stack trace fall off the top of the console buffer) will the follow (part) stacktrace showing the infinite loop:

```
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:312)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:312)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:312)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:312)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:312)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:312)
    at org.elasticsearch.ElasticsearchException.toXContent(ElasticsearchException.java:277)
```
</description><key id="92607289">11994</key><summary>StackOverflowError when a SearchParseException is thrown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Exceptions</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T10:43:14Z</created><updated>2015-07-03T11:51:54Z</updated><resolved>2015-07-03T11:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-07-02T10:46:00Z" id="117995726">SearchParseException extends SearchException which in turn extends ElasticsearchException and implements ElasticsearchWrapperException. Any exception which extends ElasticsearchException and implements ElasticsearchWrapperExceptionwill suffer this issue
</comment><comment author="colings86" created="2015-07-02T10:49:06Z" id="117996166">From what I can see, the following Exceptions are affected:
- BroadcastShardOperationFailedException
- IndexCreationException
- PercolateException
- RecoverFilesRecoveryException
- RemoteTransportException
- SearchException
- SearchContextException
- SendRequestTransportException
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/test/java/org/elasticsearch/ElasticsearchExceptionTests.java</file></files><comments><comment>Don't special-case on ElasticsearchWrapperException in toXContent</comment></comments></commit></commits></item><item><title>Add Mongolastic into Misc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11993</link><project id="" key="" /><description>Hi, 
I hope that the tool is eligible for the elasticsearch community in order to add it into Misc.
</description><key id="92601052">11993</key><summary>Add Mongolastic into Misc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ozlerhakan</reporter><labels /><created>2015-07-02T10:07:01Z</created><updated>2015-07-02T12:12:17Z</updated><resolved>2015-07-02T12:12:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-02T12:12:17Z" id="118011392">thanks @ozlerhakan - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11993 from ozlerhakan/patch-1</comment></comments></commit></commits></item><item><title>Remove PageCacheRecycler.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11992</link><project id="" key="" /><description>PageCacheRecycler is a sort of memory manager on top of the JVM that tries to
reuse fixed-sized pages of memory. I am unhappy about it because I don't think
it is elasticsearch's job to provide a memory management framework. Reusing
memory may be useful at times, but then it should be contained in the component
that needs to reuse memory.

By trying to be general-purpose, PageCacheRecycler also needs to allocate memory
to the JVM (10% by default), which is memory that the JVM cannot get back, even
in case of memory pressure.

Also allocations which are performed through PageCacheRecycler are only large
arrays, which should already be GC-friendly without recycling.
</description><key id="92594673">11992</key><summary>Remove PageCacheRecycler.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label></labels><created>2015-07-02T09:36:12Z</created><updated>2015-08-12T08:04:15Z</updated><resolved>2015-08-10T13:11:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-02T09:51:33Z" id="117985251">I am +1 on this I think it's the right thing todo
</comment><comment author="rjernst" created="2015-07-02T18:40:29Z" id="118120877">Big +1!
</comment><comment author="jpountz" created="2015-07-03T14:25:20Z" id="118363243">I created a (terrible) [benchmark](https://gist.github.com/jpountz/a08fb6f8c9c0e2d90b23) and compared memory usage profiles with jconsole. The above profile is the one with page cache recycling disabled and the below one has the recycler enabled.

Maybe it's because it did not run long enough (20 min each) but I can't see much that would convince me that one is better than the other. Maybe a slight difference is that with the cache recycler on, memory usage looks a bit higher, which I guess is related to the fact that it can reserve up to 10% of the JVM memory for recycling.

The below profile is with cache recycler ON and the above one is with cache recycler OFF.

![jconsole](https://cloud.githubusercontent.com/assets/299848/8500781/a5668b98-219f-11e5-93ec-4d6884495ef1.png)
</comment><comment author="jpountz" created="2015-07-06T10:45:46Z" id="118806338">I re-ran the benchmark with gc logging enabled this time in order to be able to perform a better comparison. And also with a fixed heap and ParNew/CMS instead of the default throughput collector.

I ran the experiment twice with the page cache recycler enabled and twice with the page cache recycler disabled, 20 min. every time. Unfortunately statistics about major collections were very inconsistent from run to run, so I only report here information about pauses in ParNew which looked consistent:

|  | Without PageCacheRecycler | With PageCacheRecycler |
| --- | --- | --- |
| Pause time in ParNew / min | 2.31 sec - 2.19 sec | 1.65 sec - 1.60 sec |
| Number of ParNew pauses / min | 20.8 - 21.4 | 12.47 - 11.60 |

The page cache recycler seems to actually help ParNew, but by less than one second per minute. I was hoping to see the impact with major garbage collections, but given that runs do not reproduce at all, I don't think we can draw any conclusions... In case someone would like to analyze gc logs, I added them to https://gist.github.com/jpountz/a08fb6f8c9c0e2d90b23
</comment><comment author="nik9000" created="2015-07-06T11:50:28Z" id="118830735">Parnew was always my problem anyway.
On Jul 6, 2015 6:45 AM, "Adrien Grand" notifications@github.com wrote:

&gt; I re-ran the benchmark with gc logging enabled this time in order to be
&gt; able to perform a better comparison. And also with a fixed heap and
&gt; ParNew/CMS instead of the default throughput collector.
&gt; 
&gt; I ran the experiment twice with the page cache recycler enabled and twice
&gt; with the page cache recycler disabled, 20 min. every time. Unfortunately
&gt; statistics about major collections were very inconsistent from run to run,
&gt; so I only report here information about pauses in ParNew which looked
&gt; consistent:
&gt;   Without PageCacheRecycler With PageCacheRecycler   Pause time in ParNew
&gt; / min 2.31 sec - 2.19 sec 1.65 sec - 1.60 sec  Number of ParNew pauses /
&gt; min 20.8 - 21.4 12.47 - 11.60
&gt; 
&gt; The page cache recycler seems to actually help ParNew, but by less than
&gt; one second per minute. I was hoping to see the impact with major garbage
&gt; collections, but given that runs do not reproduce at all, I don't think we
&gt; can draw any conclusions... In case someone would like to analyze gc logs,
&gt; I added them to https://gist.github.com/jpountz/a08fb6f8c9c0e2d90b23
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/11992#issuecomment-118806338
&gt; .
</comment><comment author="s1monw" created="2015-07-07T07:55:14Z" id="119109329">it's hard to see any real difference to make any decision. @kimchy any idea where we are moving towards?
</comment><comment author="nik9000" created="2015-07-27T17:01:52Z" id="125272365">I'm of the opinion that anything you can do to cause fewer ParNew allocations the better. Those stop the world and they only get longer the bigger your heap because of that card check phase. From that perspective this looks like a pretty bad change.
</comment><comment author="jpountz" created="2015-08-10T13:11:57Z" id="129438875">Closing then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion suggester payload is not being updated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11991</link><project id="" key="" /><description>I noticed a potential problem with the Java client and/or ES itself when updating completion suggestion payloads. My update to the payload is not being propagated to the suggestion index while using ES 1.6.0.

In order to illustrate the problem I've written a test case using the official driver and put it into a git repo: https://github.com/lenniboy/es-suggest-reproduction/tree/master/java

The actual code is repeated here:

``` java
import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequestBuilder;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.action.index.IndexRequestBuilder;
import org.elasticsearch.action.suggest.SuggestRequestBuilder;
import org.elasticsearch.action.suggest.SuggestResponse;
import org.elasticsearch.action.update.UpdateRequestBuilder;
import org.elasticsearch.client.Client;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.search.suggest.Suggest;
import org.elasticsearch.search.suggest.completion.CompletionSuggestion;
import org.elasticsearch.search.suggest.completion.CompletionSuggestionBuilder;
import org.junit.Test;

import java.util.Collections;
import java.util.Iterator;
import java.util.Map;

import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
import static org.hamcrest.CoreMatchers.is;
import static org.hamcrest.MatcherAssert.assertThat;


public class SuggestionsTest{

    String indexName = "suggestions-bug";
    String documentType = "band";
    String documentId = "1";

    @Test
    public void testSuggestions() throws Exception {

        Client client = new TransportClient()
                .addTransportAddress(new InetSocketTransportAddress("localhost", 9300));

        final IndicesExistsResponse res = client.admin().indices().prepareExists(indexName).execute().actionGet();
        if (res.isExists()) {
            final DeleteIndexRequestBuilder delIdx = client.admin().indices().prepareDelete(indexName);
            delIdx.execute().actionGet();
        }

        final CreateIndexRequestBuilder createIndexRequestBuilder = client.admin().indices().prepareCreate(indexName);

        final XContentBuilder mappingBuilder = jsonBuilder().prettyPrint()
                .startObject()
                    .startObject("properties")
                        .startObject("name")
                            .field("type", "string")
                        .endObject()
                        .startObject("suggest")
                            .field("type", "completion")
                            .field("payloads", true)
                        .endObject()
                    .endObject()
                .endObject();

        System.out.println("Adding mapping");
        System.out.println(mappingBuilder.string());
        createIndexRequestBuilder.addMapping(documentType, mappingBuilder);

        createIndexRequestBuilder.execute().actionGet();

        // Index document
        final IndexRequestBuilder indexRequestBuilder = client.prepareIndex(indexName, documentType, documentId);
        // build json object
        final XContentBuilder indexContentBuilder = jsonBuilder().prettyPrint()
                .startObject()
                    .field("name", "Queen")
                    .startObject("suggest")
                        .field("input", Collections.singletonList("Queen"))
                        .field("output", "Queen")
                        .startObject("payload")
                            .field("country", "GB")
                        .endObject()
                    .endObject()
                .endObject();

        System.out.println("Indexing document");
        System.out.println(indexContentBuilder.string());

        indexRequestBuilder.setSource(indexContentBuilder);
        indexRequestBuilder.execute().actionGet();

        // Update payload
        final UpdateRequestBuilder updateRequestBuilder = client.prepareUpdate(indexName, documentType, documentId);
        // build json object
        final XContentBuilder updateContentBuilder = jsonBuilder().prettyPrint()
                .startObject()
                    .startObject("doc")
                        .startObject("suggest")
                            .startObject("payload")
                                .field("country", "DE")
                            .endObject()
                        .endObject()
                    .endObject()
                .endObject();

        System.out.println("Updating document");
        System.out.println(updateContentBuilder.string());

        updateRequestBuilder.setSource(updateContentBuilder);
        updateRequestBuilder.execute().actionGet();

        GetResponse getResponse = client.prepareGet(indexName, documentType, documentId)
                .execute()
                .actionGet();

        System.out.println("Waiting one second for ES to finish indexing");
        Thread.sleep(1000);
        System.out.println("Fetching document. Response is");
        System.out.println(getResponse.getSourceAsString());

        Map suggest = (Map&lt;String, Map&lt;String, String&gt;&gt;)getResponse.getSourceAsMap().get("suggest");
        Map&lt;String, String&gt; payload = (Map&lt;String, String&gt;)suggest.get("payload");
        String getCountry = payload.get("country");

        System.out.println("The payload country in the document is " + getCountry);

        CompletionSuggestionBuilder suggestionsBuilder = new CompletionSuggestionBuilder("suggest");
        suggestionsBuilder.text("Queen");
        suggestionsBuilder.field("suggest");
        SuggestRequestBuilder suggestRequestBuilder = client.prepareSuggest(indexName).addSuggestion(suggestionsBuilder);
        SuggestResponse suggestResponse = suggestRequestBuilder.execute().actionGet();

        System.out.println("Suggestion response is");
        System.out.println(suggestResponse.getSuggest());

        CompletionSuggestion.Entry.Option option = (CompletionSuggestion.Entry.Option)
            suggestResponse.getSuggest().getSuggestion("suggest").getEntries().get(0).getOptions().get(0);
        String suggestCountry = (String) option.getPayloadAsMap().get("country");

        assertThat(getCountry, is(suggestCountry));
    }
}
```

So, you can see that I change the `payload.country` value from `GB` to `DE`. This is correctly applied to the "main" document but the suggestions index is not being updated. I cannot get the new payload into the suggestions index with out calling `_optimize?max_num_segments=1`.

Here comes the weird part. If you do the exact same steps through the HTTP API (Sense), it works!

```
DELETE /t 
PUT /t
PUT /t/_mapping/t
{
  "properties": {
    "name": {
      "type": "string"
    },
    "suggestions": {
      "type": "completion",
      "payloads": true
    }
  }
}

GET /t/_search
PUT /t/t/1
{
  "name": "Queen",
  "suggestions": {
    "input": [
      "Queen"
    ],
    "output": "Queen",
    "payload": {
      "country": "old"
    }
  }
}

POST /t/t/1/_update
{
  "doc": {
    "suggestions": {
      "payload": {
        "country": "new"
      }
    }
  }
}

GET /t/_search
GET /t/_suggest
{
  "s1": {
    "text": "Queen",
    "completion": {
      "field": "suggestions"
    }
  }
}
```

I am very puzzled by this.

Is the HTTP API doing something differently than the Java API? I presume that Java uses some binary protocol but could this be the culprit?

This was originally discovered in https://github.com/sksamuel/elastic4s/issues/343
</description><key id="92585194">11991</key><summary>Completion suggester payload is not being updated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leonardehrenfried</reporter><labels /><created>2015-07-02T08:51:02Z</created><updated>2015-07-02T12:07:12Z</updated><resolved>2015-07-02T12:07:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-02T12:07:11Z" id="118010699">Hi @lenniboy 

Great reproduction.  I think this is just a matter of timing.  The following demonstrates the same issue via the REST api:

```
DELETE t
PUT /t
{
  "mappings": {
    "t": {
      "properties": {
        "name": {
          "type": "string"
        },
        "suggestions": {
          "type": "completion",
          "payloads": true
        }
      }
    }
  }
}

POST /t/t/_bulk
{"index":{"_id":1}}
{"name":"Queen","suggestions":{"input":["Queen"],"output":"Queen","payload":{"country":"old"}}}
{"update":{"_id":1}}
{"doc":{"suggestions":{"payload":{"country":"new"}}}}

GET /t/_suggest
{
  "s1": {
    "text": "Queen",
    "completion": {
      "field": "suggestions"
    }
  }
}
```

In the current suggester, deleted documents are not taken into account, which I think is what you're seeing. The output is deduplicated, leaving the older version to take precedence.

Unfortunately, this is not something we can fix in the current suggester. 2.0 is going to have a completely new implementation (see #11740) which will take deletions into account.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[build] Update maven-invoker-plugin to 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11990</link><project id="" key="" /><description>Release Notes - Apache Maven Invoker Plugin - Version 2.0.0

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317525&amp;version=12332831

Important Note:

This Release is intended for the bug fix MINVOKER-187 and in this relationship
is was necessary to upgrade the JDK minimum to JDK 1.6. This was the reason for
the version upgrade to 2.0.0.

Bug:
- [MINVOKER-187] - Cloned IT project must be writable

Improvement:
- [MINVOKER-192] - Using fluido skin
</description><key id="92584929">11990</key><summary>[build] Update maven-invoker-plugin to 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T08:49:55Z</created><updated>2015-07-06T15:56:17Z</updated><resolved>2015-07-06T15:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-06T13:27:43Z" id="118853680">@rmuir @s1monw LGTY? :)
</comment><comment author="s1monw" created="2015-07-06T13:36:48Z" id="118856217">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] fix IndicesStoreIntegrationTests.indexCleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11989</link><project id="" key="" /><description>Despite many efforts to fix this test it still fails every now and then. The reason for failure does not seem to be the result of a bug but more a test issue. The test uses slow cluster state processing disruption to simulate cluster state processing delays when shards are deleted from a node due to relocation and this disruption sometimes seems to block more than just the needed cluster state update. 
Latest failure is here: http://build-us-00.elastic.co/job/es_core_1x_suse/958/
I will mute this until we have a better way to test this.
</description><key id="92577893">11989</key><summary>[CI] fix IndicesStoreIntegrationTests.indexCleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>jenkins</label></labels><created>2015-07-02T08:12:58Z</created><updated>2015-07-23T09:26:47Z</updated><resolved>2015-07-23T09:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file></files><comments><comment>[Test] make cluster state blocking more reliable in IndicesStoreIntegrationTests.indexCleanup()</comment></comments></commit></commits></item><item><title>Transport: Do not make the buffer skip while a stream is open.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11988</link><project id="" key="" /><description>This commit changes MessageChannelHandler to not skip the underlying
ChannelBuffer while a StreamInput is open on top of it. In case eg. compression
is enabled, this prevents failures due to the fact that the decompressed
stream input expects a certain structure that it can't verify if the position
of the underlying buffer is changed.
</description><key id="92576414">11988</key><summary>Transport: Do not make the buffer skip while a stream is open.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Network</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-02T08:03:59Z</created><updated>2015-07-02T08:36:13Z</updated><resolved>2015-07-02T08:36:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-02T08:19:37Z" id="117954507">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java</file></files><comments><comment>Merge pull request #11988 from jpountz/fix/zlib_skip_underlying</comment></comments></commit></commits></item><item><title>Provide access to filter cache use via stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11987</link><project id="" key="" /><description>Given [we track filter cache use](https://github.com/elastic/elasticsearch/issues/8449), is there anyway we can provide stats around what filter caches are in use and what that use looks like?
</description><key id="92543246">11987</key><summary>Provide access to filter cache use via stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2015-07-02T04:12:41Z</created><updated>2015-07-02T11:53:43Z</updated><resolved>2015-07-02T11:53:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-07-02T04:14:10Z" id="117900220">Was asked by @sl1pm4t in training :)
</comment><comment author="clintongormley" created="2015-07-02T11:53:42Z" id="118007978">Closing as duplicate of #9558
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature/query refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11986</link><project id="" key="" /><description /><key id="92521420">11986</key><summary>Feature/query refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iokays</reporter><labels /><created>2015-07-02T01:21:19Z</created><updated>2015-07-02T04:52:03Z</updated><resolved>2015-07-02T04:52:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-02T04:52:02Z" id="117905739">I guess you opened that by mistake ?

Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot info should contain version of elasticsearch that created the snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11985</link><project id="" key="" /><description>This information was stored with the snapshot but wasn't available on the interface. Knowing the version of elasticsearch that created the snapshot can be useful to determine the minimal version of the cluster that is required in order to restore this snapshot.

Closes #11980
</description><key id="92511465">11985</key><summary>Snapshot info should contain version of elasticsearch that created the snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T23:57:57Z</created><updated>2015-07-09T16:37:47Z</updated><resolved>2015-07-09T16:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-07-09T15:04:22Z" id="120018211">LGTM, I assume for backporting to 1.7.0 you are going to add version checks to the serialization?
</comment><comment author="imotov" created="2015-07-09T15:05:40Z" id="120019121">Thanks! Yes, I am actually thinking to open another PR for 1.7.0 backport, because of all extra checks that I will need to add.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add option to show aliases in _cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11984</link><project id="" key="" /><description>We already have `_cat/aliases` and it reports the indices associated with it. However, `_cat/indices` shows a lot of index-specific detail that would also benefit from showing any aliases associated with it.

This wouldn't necessarily need to be a default feature, but it would help to extend the flexibility of the single call so that one can associate things like store size with an alias in a single call. My big fear is that multiple aliases can exist, so this could really bloat the output if used in such cases.
</description><key id="92482311">11984</key><summary>Add option to show aliases in _cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:CAT API</label><label>enhancement</label></labels><created>2015-07-01T20:51:40Z</created><updated>2015-07-02T11:46:10Z</updated><resolved>2015-07-02T11:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-02T11:46:09Z" id="118006800">Hi @pickypg 

I don't think we should add this.

The problem is that multiple aliases would clash with the tabular output of cat. (See https://github.com/elastic/elasticsearch/pull/11791 for a similar issue that we're in the process of fixing). 

You can already use aliases with the indices stats API, which solves the problem by using structured output. Alternatively, you can use:

```
GET _cat/indices/{alias_name}
```

which will give you the data you're after for all of the associated indices.  I think that this already serves the purpose.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Grammar fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11983</link><project id="" key="" /><description /><key id="92478957">11983</key><summary>Grammar fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cdosborn</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-07-01T20:31:18Z</created><updated>2015-07-05T16:24:22Z</updated><resolved>2015-07-05T16:21:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-02T11:37:04Z" id="118005713">Hi @cdosborn 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="cdosborn" created="2015-07-02T21:10:54Z" id="118169566">Signed
</comment><comment author="clintongormley" created="2015-07-05T16:24:22Z" id="118636545">thanks @cdosborn - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11983 from cdosborn/patch-1</comment></comments></commit></commits></item><item><title>really ban exitVM with security policy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11982</link><project id="" key="" /><description>Today this is implicitly allowed. In tests actually we stop it, because Uwe already fixed this issue in TestSecurityManager. But nothing stops it when you are actually running ES.

See the notes in RuntimePermission[1]:

```
 Note: The "exitVM.*" permission is automatically granted to all code loaded from 
 the application class path, thus enabling applications to terminate themselves.
```
1. http://docs.oracle.com/javase/7/docs/api/java/lang/RuntimePermission.html
</description><key id="92469325">11982</key><summary>really ban exitVM with security policy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T19:42:50Z</created><updated>2015-07-02T11:48:59Z</updated><resolved>2015-07-01T21:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-07-01T19:54:25Z" id="117806717">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file></files><comments><comment>Merge pull request #11982 from rmuir/grr_bad_defaults</comment></comments></commit></commits></item><item><title>Parameterized exception messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11981</link><project id="" key="" /><description>Added dynamic arguments to `ElasticsearchException`, `ElasticsearchParseException` and `ElasticsearchTimeoutException`.

This helps keeping the exception messages clean and readable and promotes consistency around wrapping dynamic args with `[` and `]`.

This is just the start, we need to propagate this to all exceptions deriving from `ElasticsearchException`. Also, work started on standardizing on lower case logging &amp; exception messages. We need to be consistent here...
- Uses the same `LoggerMessageFormat` as used by our logging infrastructure.
</description><key id="92452728">11981</key><summary>Parameterized exception messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T18:20:54Z</created><updated>2015-07-02T11:48:30Z</updated><resolved>2015-07-01T21:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-01T18:51:05Z" id="117794028">LGTM thanks for doing this
</comment><comment author="nik9000" created="2015-07-01T19:47:24Z" id="117805268">Neat idea reusing the logging message formatter for exceptions. Nice for consistency. I see a lot of `[{}]` in there. Maybe make something convenient for that? Something along the lines of `[]` being interpreted like `[{}]`.
</comment><comment author="uboness" created="2015-07-01T21:17:30Z" id="117826818">&gt;  I see a lot of [{}] in there. Maybe make something convenient for that? Something along the lines of [] being interpreted like [{}].

now that's a neat idea as well :) I'll get this one in first and take it from there
</comment><comment author="nik9000" created="2015-07-01T21:30:45Z" id="117830753">&gt; now that's a neat idea as well :) I'll get this one in first and take it from there

Cool!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot Info doesn't indicate elastic search version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11980</link><project id="" key="" /><description>Calling 

GET /_snapshot/&lt;repo&gt;/&lt;snapshot&gt;

includes various information but not the elasticsearch version.

This is useful in a number of scenarios.  Although elasticsearch prevents restores between index versions if the snapshot.version &gt; cluster.version, this information would allow client side checks i.e. the client could avoid having to check compatibility by making a restore and noticing it fails.
</description><key id="92452333">11980</key><summary>Snapshot Info doesn't indicate elastic search version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">gingerwizard</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T18:19:16Z</created><updated>2015-07-09T16:37:46Z</updated><resolved>2015-07-09T16:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot info should contain version of elasticsearch that created the snapshot</comment></comments></commit></commits></item><item><title>Don't jarhell check system jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11979</link><project id="" key="" /><description>While this can and does find "interesting" stuff, its not our goal here.
</description><key id="92440493">11979</key><summary>Don't jarhell check system jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T17:27:18Z</created><updated>2015-07-02T11:31:06Z</updated><resolved>2015-07-01T18:53:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-07-01T18:46:47Z" id="117793195">I also added a commit here to not run the jar hell checks from IDEs. Instead it will warn you about anything crazy the IDE is doing but will not fail.
</comment><comment author="s1monw" created="2015-07-01T18:49:10Z" id="117793670">LGTM thx rob
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Merge pull request #11979 from rmuir/nosystem</comment></comments></commit></commits></item><item><title>Append the shard top docs in such a way to prevent AOOBE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11978</link><project id="" key="" /><description>This way a ArrayIndexOutOfBoundsException like is reported in #7926 is impossible to occur.
</description><key id="92438340">11978</key><summary>Append the shard top docs in such a way to prevent AOOBE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Scroll</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T17:15:00Z</created><updated>2015-07-08T11:57:17Z</updated><resolved>2015-07-06T17:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-06T10:23:46Z" id="118801783">LGTM.
</comment><comment author="jpountz" created="2015-07-06T15:39:17Z" id="118898994">Given that we are seeing such weird bugs, maybe we should check invariants more aggressively, such as ensuring that the same index of the `AtomicArray` is not set twice or that set is never called with `null`? What do you think? (Could be another PR)
</comment><comment author="martijnvg" created="2015-07-06T15:51:29Z" id="118906031">@jpountz I totally agree
</comment><comment author="jpountz" created="2015-07-06T17:13:44Z" id="118928011">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move short name access out of field type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11977</link><project id="" key="" /><description>Eventually, the field type should not need any names, because there
will be only one name which leads to finding it (the full name, which is
also the index name). However, the short or "simple" name (using java
terminology for class names) is needed just in a couple places, for
serialization.

This change moves the simple name out of MappedFieldType.Names, into
Mapper, and makes Mapper and FieldMapper abstract classes.
</description><key id="92433299">11977</key><summary>Move short name access out of field type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T16:52:10Z</created><updated>2015-07-05T16:15:45Z</updated><resolved>2015-07-02T17:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-02T17:00:15Z" id="118092984">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MetadataFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMetadataMapper.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file></files><comments><comment>Merge pull request #11977 from rjernst/pr/mapper-names</comment></comments></commit></commits></item><item><title>Fix typo in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11976</link><project id="" key="" /><description /><key id="92404351">11976</key><summary>Fix typo in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bocharsky-bw</reporter><labels><label>docs</label></labels><created>2015-07-01T14:48:48Z</created><updated>2015-07-08T16:00:33Z</updated><resolved>2015-07-08T16:00:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-02T11:02:10Z" id="117998970">thanks @bocharsky-bw - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add basic recovery prioritization to GatewayAllocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11975</link><project id="" key="" /><description>This commit adds logic to prefer shards with higher priority
or from newer indicse to be allocated first if they are unallocated post API.

This commit allows users to set `index.priority` to a non-negative integer to
prioritize index recovery for certain indices. This setting is dynamically updateable
and defaults to `0`. If two indices have the same priority this change takes the creation
date into account to prioritize shards from newer indices which is important in the time-based
indices usecase.

Closes #11787
</description><key id="92398695">11975</key><summary>Add basic recovery prioritization to GatewayAllocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>feature</label><label>release highlight</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T14:27:22Z</created><updated>2015-07-14T13:51:41Z</updated><resolved>2015-07-06T20:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-07-01T16:33:14Z" id="117740226">I may not be the drone you're looking for, but LGTM. Made a minor comment.
</comment><comment author="bleskes" created="2015-07-02T08:43:04Z" id="117960051">The change is good. I would to either have the unit tests modified to use the actual comparator and the standard index meta data &amp; settings or have an integration test. I think we need to make sure that the index constructs have effect.
</comment><comment author="s1monw" created="2015-07-03T14:49:40Z" id="118367139">&gt; The change is good. I would to either have the unit tests modified to use the actual comparator and the standard index meta data &amp; settings or have an integration test. I think we need to make sure that the index constructs have effect.

I do not understand what's wrong with this abstract class? Integration tests are going to be flaky and slow here. I don't see a good way to really _test_ this via an integration test
</comment><comment author="bleskes" created="2015-07-06T14:30:25Z" id="118873110">Talked this through with Simon. LGTM. (I misread the code thinking the test doesn't cover the usage of IndexMetaData.SETTING_PRIORITY  &amp; IndexMetaData.SETTING_CREATION_DATE but it does)
</comment><comment author="s1monw" created="2015-07-06T20:57:43Z" id="118995872">I will let this bake a bit and then backport to 1.7
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: unify boost and query name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11974</link><project id="" key="" /><description>Following the discussion in #11744, move boost and query _name to base class AbstractQueryBuilder with their getters and setters. Unify their serialization code and equals/hashcode handling in the base class too. This guarantess that every query supports both _name and boost and nothing needs to be done around those in subclasses besides properly parsing the fields in the parsers and printing them out as part of the doXContent method in the builders. More specifically, these are the performed changes:
- Introduced printBoostAndQueryName utility method in AbstractQueryBuilder that subclasses can use to print out _name and boost in their doXContent method.
- readFrom and writeTo are now final methods that take care of _name and boost serialization. Subclasses have to implement doReadFrom and doWriteTo instead.
- toQuery is a final method too that takes care of properly applying _name and boost to the lucene query. Subclasses have to implement doToQuery instead. The query returned will have boost and queryName applied automatically.
- Removed BoostableQueryBuilder interface, given that every query is boostable after this change. This won't have any negative effect on filters, as the boost simply gets ignored in that case.
- Extended equals and hashcode to handle queryName and boost automatically as well.
- Update the query test infra so that queryName and boost are tested automatically, and whenever they are forgotten in parser or doXContent tests fail, so this makes things a lot less error-prone
- Introduced DEFAULT_BOOST constant to make sure we don't repeat 1.0f all the time for default boost values.

SpanQueryBuilder is again a marker interface only. The convenient toQuery that allowed us to override the return type to SpanQuery cannot be supported anymore due to a clash with the toQuery implementation from AbstractQueryBuilder. We have to go back to castin lucene Query to SpanQuery when dealing with span queries unfortunately.

Note that this change touches not only the already refactored queries but also the untouched ones, by making sure that we parse _name and boost whenever we need to and that we print them out as part of QueryBuilder#doXContent. This will result in printing out the default boost all the time rather than skipping it in non refactored queries, something that we would have changed anyway as part of the query refactoring.

The following are the queries that support boost now while previously they didn't (parser now parses it and builder prints it out): and, exists, fquery, geo_bounding_box, geo_distance, geo_distance_range, geo_hash_cell, geo_polygon, indices, limit, missing, not, or, script, type.

The following are the queries that support _name now while previously they didn't (parser now parses it and builder prints it out): boosting, constant_score, function_score, limit, match_all,  type.

Range query parser supports now _name at the same level as boost too (_name is still supported on the outer object though for bw comp).

There are two exceptions that despite have getters and setters for queryName and boost don't really support boost and queryName: query filter and span multi term query. The reason for this is that they only support a single inner object which is another query that they wrap, no other elements.

Relates to #11744
Closes #10776
</description><key id="92369914">11974</key><summary>Query refactoring: unify boost and query name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>breaking</label><label>enhancement</label></labels><created>2015-07-01T12:31:10Z</created><updated>2015-07-01T16:00:03Z</updated><resolved>2015-07-01T16:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-01T13:21:48Z" id="117671387">This change is breaking for the java api, until we move to serialize queries in Streamable format rather than using their json representation. When using json (or smile etc.) the query builder prints out new fields (e.g. boost, _name) which the new version of the parser supports, but when the same request gets sent to an older node the parser will most likely throw error because it doesn't support boost (or _name).
</comment><comment author="cbuescher" created="2015-07-01T15:06:27Z" id="117710324">Did a round of review, LGTM.
</comment><comment author="javanna" created="2015-07-01T15:12:54Z" id="117711745">I pushed new commits that address your comments :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Query refactoring: unify boost and query name</comment></comments></commit></commits></item><item><title>Carry on rest status if exceptions are not serializable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11973</link><project id="" key="" /><description>Today we loose the RestStatus code for non-serializable exceptions.
This can be tricky if they are supposed to signal certain situations
like authentication errors etc. This commit adds support for carrying on
the exceptions in the NotSerializableExceptoinWrapper
</description><key id="92364504">11973</key><summary>Carry on rest status if exceptions are not serializable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T12:01:17Z</created><updated>2015-07-01T12:57:13Z</updated><resolved>2015-07-01T12:57:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-01T12:01:40Z" id="117633862">@kimchy regarding your request.. makes perfect sense
</comment><comment author="kimchy" created="2015-07-01T12:50:01Z" id="117648723">LGTM, thanks @s1monw !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't join master nodes or accept join requests of old and too new nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11972</link><project id="" key="" /><description>If the version of a node is lower than the minimum supported version or higher than the maximum hypothetical supported version, a node shouldn't be allowed to join and nodes should join that elected master node

Closes #11924
</description><key id="92358887">11972</key><summary>Don't join master nodes or accept join requests of old and too new nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T11:36:41Z</created><updated>2015-08-13T13:51:42Z</updated><resolved>2015-07-02T18:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-02T09:03:13Z" id="117968676">Left some comments..
</comment><comment author="martijnvg" created="2015-07-02T09:53:33Z" id="117985805">@bleskes thanks for the feedback! I updated the PR. Instead of moving the version check to findMaster(), I moved it to ElectMasterService#electMaster(), so the check doesn't need to get duplicated multiple times. 
</comment><comment author="bleskes" created="2015-07-02T10:00:19Z" id="117986886">Thx @martijnvg . I left some minor comments.
</comment><comment author="martijnvg" created="2015-07-02T10:15:30Z" id="117990424">@bleskes cool, I've updated the PR.
</comment><comment author="martijnvg" created="2015-07-02T11:19:47Z" id="118003551">@bleskes I change `ElectMasterService` instance to be managed by `ZenDiscovery`. I'm not happy about it, because it used by `ZenPingService` (which is injected by `ZenDiscovery` too). I made this work by making `ZenPingService#sortByMasterLikelihood()` static. 
</comment><comment author="martijnvg" created="2015-07-02T14:59:04Z" id="118059485">@bleskes I reverted the previous commit and let `ZenDiscovery` get the current version from the local node.
</comment><comment author="bleskes" created="2015-07-02T15:02:02Z" id="118060116">LGTM. Left a couple of trivial comments. No need for another review.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CLOSE_WAIT with hdfs repository plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11971</link><project id="" key="" /><description>when I use hdfs-repository plugin to create snapshot,there is a issue: Cannot assign requested address, there are too many CLOSE_WAIT
</description><key id="92346015">11971</key><summary>CLOSE_WAIT with hdfs repository plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JasonBian</reporter><labels /><created>2015-07-01T10:44:05Z</created><updated>2015-07-01T11:35:49Z</updated><resolved>2015-07-01T11:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-01T11:35:48Z" id="117621041">This issue was moved to elastic/elasticsearch-hadoop#492
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Makes SKIP Gap Policy work correctly for Bucket Script aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11970</link><project id="" key="" /><description>This change means that when the skip gap policy is used, the bucket script aggregation will skip executing the script on a bucket if any of the required bucket_paths are missing for the bucket. No aggregation will be added to the bucket, and the aggregation will move to the next bucket.
</description><key id="92322141">11970</key><summary>Aggregations: Makes SKIP Gap Policy work correctly for Bucket Script aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-07-01T09:01:49Z</created><updated>2015-07-01T09:19:32Z</updated><resolved>2015-07-01T09:19:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-01T09:06:55Z" id="117553788">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactoring of GeoBoundingBoxQueryBuilder and -Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11969</link><project id="" key="" /><description>... -Builder/-Parser. This add equals, hashcode, read/write methods, separates toQuery and JSON parsing and adds first tests.

Deprecates two types of initializing the bounding box: In our documentation we speak about specifying top/left and bottom/right corner of a bounding box. Here we also allow for top/right and bottom/left. This adds not only to the amount of code but also testing needed w/o too much benefit for the user other than more chances to confuse top/right/bottom/left/latitude/longitude IMHO.

Missing: validation, there are some easy opportunities for cleaning parser and builder.

The test path that checks the toQuery method if type is set to "indexed" fails at the moment due to missing Guice initialisations. I tried to add everything needed but felt like the result looked a lot like it would actually like to be an ElasticsearchSingleNodeTest so not sure how to proceed here?

Relates to #10217
</description><key id="92309171">11969</key><summary>Refactoring of GeoBoundingBoxQueryBuilder and -Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-07-01T08:11:26Z</created><updated>2015-09-20T19:00:05Z</updated><resolved>2015-09-20T19:00:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-07-01T08:14:54Z" id="117527164">This is very much work in progress. Posting the PR mostly to seek input on how to proceed with testing toQuery in the type="indexed" setting: 

Running as is it complains in the following assertion - trying to fix the assertion by providing the needed dependencies through Guice in our test setup felt like this would be easier to do in an ElasticsearchIntegration/SingleNodeTest. There must be some simpler option to be able to run the toQuery method in a test harness that doesn't need a cluster spun up which I don't see?

java.lang.AssertionError
    at __randomizedtesting.SeedInfo.seed([41C2A3231736F031:B639A11D66B535DB]:0)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.&lt;init&gt;(IndicesFieldDataCache.java:155)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache.buildIndexFieldDataCache(IndicesFieldDataCache.java:103)
    at org.elasticsearch.index.fielddata.IndexFieldDataService.getForField(IndexFieldDataService.java:268)
    at org.elasticsearch.index.query.QueryParseContext.getForField(QueryParseContext.java:183)
    at org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTest.createExpectedQuery(GeoBoundingBoxQueryBuilderTest.java:132)
</comment><comment author="javanna" created="2015-07-01T17:13:14Z" id="117755035">@MaineC this is one of the few cases that might make it very hard to test things without a node. I need to look into this deeper. Can you leave out for now that specific case in testing, we can look at it while the rest is ready.
</comment><comment author="MaineC" created="2015-07-01T18:06:21Z" id="117773879">Sure - no problem. Leaving it out is a matter of not setting the type to "indexed" when creating the test query builders. I'll also open an issue as a reminder to get back to this test if you don't mind.
</comment><comment author="MaineC" created="2015-07-02T10:04:31Z" id="117987866">Rebased, took the failing test out, added validation - some test setup for query creation still failing. Digging. Missing: cleanup.
</comment><comment author="MaineC" created="2015-07-06T19:39:31Z" id="118974650">Note: Keep #12016 in mind for the test setup.
</comment><comment author="MaineC" created="2015-07-07T09:57:25Z" id="119150527">I believe this is ready for a first round of feedback. Here's what I did:
- Split JSON parsing and Lucene query generation, add the usual suspects (equals, hashCode etc.) and a unit test.
- Deprecated initialising the bounding box with topRight and bottomLeft corners. In our documentation I only found references to topLeft and bottomRight corners. This leaves REST users two options: Either supply topLeft/bottomRight or supply top/left/bottom/right. Java API users are left with supplying topLeft and bottomRight. Correct me if I'm wrong but IIRC this reflects what is commonly used including what users might already know from Google Maps or Nokia Maps APIs.
- Instead of having a double array to hold the lat/lon pairs that define the bounding box and four constants that hold the indices into that array we now have two points of type GeoPoint to define the bounding box.
- Instead of having a variable of type String to hold either of two type values switched to using an enum instead.
- Returns MatchNoDocs in case no mappings could be found as proposed in #12016 (mostly to get the unit test to run, doesn't fully address #12016)

The previous bounding box test still passes, so does the newly added one (as well as anything that runs with mvn clean install).

Caveat: Neither test checks the type=memory code path.

@nik9000 I've been told that you are the Geo Magician - would be great if you could double check what I've done and point out any mistakes as well.

Note: I've left the refactoring steps in separate commits for now if you want to look at them separately to see which step introduced which change.
</comment><comment author="javanna" created="2015-07-07T10:00:48Z" id="119151271">wrong Nik @MaineC that should have been @nknize ;)
</comment><comment author="MaineC" created="2015-07-07T18:35:20Z" id="119295707">Too many Niks - thanks for correcting me. :)
</comment><comment author="MaineC" created="2015-07-09T08:59:02Z" id="119880335">Just skimmed (didn't look particularly deep at all) the changes with @cbuescher - it might make sense to move the geo point/ geo bounding box validation methods into a separate class so it can be used elsewhere. Same for the geo point/ box generation code in the test methods. Both - random point generation in the test and validation in the builder are candidates where I would appreciate input from @nknize - I believe all the rest should be standard query refactoring stuff.
</comment><comment author="cbuescher" created="2015-07-09T10:40:16Z" id="119906229">@MaineC thanks, left a few comments after round of review.
</comment><comment author="MaineC" created="2015-07-10T08:53:53Z" id="120304734">@cbuescher Thanks for your comments, adjusted the PR accordingly.
</comment><comment author="cbuescher" created="2015-07-10T10:39:02Z" id="120373021">I had another look, just two minor remarks. Looks good to me otherwise, @javanna mind to take another look? Also not sure if @nknize should have a look at the deprecation of the two alternative ways of specifying the corners and the validation.
</comment><comment author="MaineC" created="2015-07-13T08:25:23Z" id="120846786">Updated to latest comments, rebased, squashed, cleaned-up commit message.
</comment><comment author="MaineC" created="2015-07-13T12:15:13Z" id="120906726">I'm in the process of factoring some of the geo data generation in the test out into a separate class - need that for other geo queries as well, so unless you started already you might want to wait with doing one last review round until I'm through  with that.
</comment><comment author="MaineC" created="2015-07-13T19:27:05Z" id="121030756">Done factoring the geo data generation and validation out, should be ready for another review.

@cbuescher I reverted the check for emptiness in the constructor - IIRC we didn't have that in one of the previous queries as we didn't want to initialise the PROTOTYPE with some artificial string.
</comment><comment author="nknize" created="2015-07-17T05:38:19Z" id="122181743">Did a quick review and left a couple comments. I'll open a separate PR to merge [LUCENE-6547](https://issues.apache.org/jira/browse/LUCENE-6547) into our own lucene package to provide reusable code (e.g., bounding box) that will be included when I finish migrating ES to Lucene's new GeoPointField.
</comment><comment author="MaineC" created="2015-07-17T07:16:48Z" id="122200691">@nknize Thanks for your input. This is very valuable context. Will get back to when I'm done fixing your comments.
</comment><comment author="MaineC" created="2015-07-21T07:24:20Z" id="123197501">@nknize Can you have a second look?

Wrt. to reusing Lucene's geo implementations: What's the timeline for your changes - I'm thinking about waiting for it before merging this PR into the feature branch (meaning fix potential conflicts in the PR and get it reviewed instead of leaving this work to whoever merges master).
</comment><comment author="MaineC" created="2015-08-12T08:41:24Z" id="130219180">Rebased, added a few checks to Lucene Query generation. @nknize @cbuescher @javanna feel free to take another look.
</comment><comment author="cbuescher" created="2015-08-13T18:07:24Z" id="130783397">@MaineC Did another round of review, only have two questions for clarification and one minor duplication.
</comment><comment author="javanna" created="2015-08-19T12:39:13Z" id="132576791">I had a look at this too, looks good, left some comments
</comment><comment author="MaineC" created="2015-08-20T09:45:33Z" id="132957625">@javanna Thanks for your comments - I think I addressed all of them except for the one on mocking IndexFieldDataService which might take a bit more digging on my end. Will ping again when this remaining issue is addressed as well and the PR ready for another look. (Just noticed there's also one validation issue remaining).
</comment><comment author="MaineC" created="2015-08-27T08:31:45Z" id="135339633">Didn't merge/rebase yet, but figured out that mocking IndexFieldDataService is actually possible. See commit here: 

https://github.com/MaineC/elasticsearch/commit/bc560e3f906e90213e29d014ef32f4c1473f1e4e

For the record: Did the changes with Mockito as a POC, I'm happy to switch to any other solution.

Essentially I'm uncertain about where to put the initialisation I added here: https://github.com/MaineC/elasticsearch/commit/bc560e3f906e90213e29d014ef32f4c1473f1e4e#diff-a38e311dff061a2d8c3888dc0d1eb39dR129  I put this stuff there to see if mocking works at all. However it looks highly specific to GeoBoundingBoxQuery to me. I'm unclear about how best to move it down to the relevant test class. The bind(...) in line 163 is the essential part.

@cbuescher @javanna - do you have any advise?
</comment><comment author="javanna" created="2015-08-27T10:21:14Z" id="135373188">thanks for looking into expanding tests @MaineC . You can have a look at TermsQueryBuilderTest to see how we mocked TermsLookupService there. I don't think we need to do it in the base class given that we need that behaviour only in a specific test, at least for now. You can add a `@BeforeClass` or `@Before` method that does the mocking in your specific test. Also, I hope we can avoid having to inject mocks through guice. Seems that we could get around this but setting the mock to `IndexQueryParseService` and/or mock `QueryShardContext` too if needed. I would give this a try without mockito to be honest to see how hard it is. In general I would prefer making this stuff easier to test rather than introducing mockito as a dep (bigger discussion required to be honest) for this reason at this point.
</comment><comment author="MaineC" created="2015-09-01T09:29:24Z" id="136648374">@javanna Thanks for the pointer to the TermsQueryBuilderTest - I tried to follow the pattern there.

I didn't need to go as far as mocking out IndexQueryParseService or QueryShardContext after making IndexFieldDataService set-able in IndexQueryParseService as was done for the TermsLookupService. Not sure if there's implications to this change I currently don't see though.
</comment><comment author="javanna" created="2015-09-02T10:53:30Z" id="137028504">I did another round of review, left some comments. It is close though.
</comment><comment author="MaineC" created="2015-09-02T18:30:09Z" id="137201165">Thanks for your comments. Will integrate tomorrow morning.
</comment><comment author="s1monw" created="2015-09-04T09:03:03Z" id="137683951">I ran into this on #13333 

&gt; Running as is it complains in the following assertion - trying to fix the assertion by providing the needed dependencies through Guice in our test setup felt like this would be easier to do in an ElasticsearchIntegration/SingleNodeTest. There must be some simpler option to be able to run the toQuery method in a test harness that doesn't need a cluster spun up which I don't see?
&gt; 
&gt; java.lang.AssertionError
&gt; at __randomizedtesting.SeedInfo.seed([41C2A3231736F031:B639A11D66B535DB]:0)
&gt; at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.(IndicesFieldDataCache.java:155)
&gt; at 

and fixed it there by removing the cyclic dependency madness. I just wanna give you a headsup before you add some badass test mocking.  
</comment><comment author="MaineC" created="2015-09-04T09:19:53Z" id="137686382">&gt;  I just wanna give you a headsup before you add some badass test mocking

https://github.com/elastic/elasticsearch/pull/11969/files#diff-c34f179cb205333bd004736b1a8a71caR52 ... is the mocking currently under review including comments by @javanna 

https://github.com/MaineC/elasticsearch/commit/7fca091f97f0941b9ff5bab13833cab3efcfbed6#diff-a38e311dff061a2d8c3888dc0d1eb39dR140 ... nearly the same as above, but using Mockito for mocking (surprisingly to me about the same amount of code actually).

Note to myself - look back at https://github.com/elastic/elasticsearch/pull/12283 to see what exactly the problem with testing that one was. Exceptions I ran into there seemed pretty much the same as here.
</comment><comment author="MaineC" created="2015-09-08T08:32:34Z" id="138477424">Did another round of fixing. Left the mocking in place for now, though #13333 should fix the problem.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Numbers.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoExecType.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java</file></files><comments><comment>Merge pull request #11969 from MaineC/feature/geo-bounding-box-refactoring</comment></comments></commit></commits></item><item><title>Extraction of Log Messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11968</link><project id="" key="" /><description>where and how to write the custom filters to extract required fields in syslog message: 

message:Jun 30 21:35:52 192.168.100.244 local3: Cloudera|Navigator|1|type="HDFS",allowed="true",time="1435725371265",service="hdfs",user="oozie",ip="127.0.0.1",op="listStatus",src="/user/oozie/share/lib"

@timestamp      July 1st 2015, 10:06:12.937
t@version       1
t_id        AU5H5p4MXChH4k1HWWib
t_index     logstash-2015.07.01
t_source        {"message":"Jun 30 21:35:52 192.168.100.244 local3: Cloudera|Navigator|1|type=\"HDFS\",allowed=\"true\",time=\"1435725371265\",service=\"hdfs\",user=\"oozie\",ip=\"127.0.0.1\",op=\"listStatus\",src=\"/user/oozie/share/lib\"","@version":"1","@timestamp":"2015-07-01T04:36:12.937Z","type":"cloudera","file":"/var/log/navigator.log","host":"syslogserver","offset":"7809436"}
t_type      cloudera
tfile       /var/log/navigator.log
thost       syslogserver
tmessage        Jun 30 21:35:52 192.168.100.244 local3: Cloudera|Navigator|1|type="HDFS",allowed="true",time="1435725371265",service="hdfs",user="oozie",ip="127.0.0.1",op="listStatus",src="/user/oozie/share/lib"
toffset     7809436
ttype       cloudera

i want to extract message field ,
please anyone suggest me where and how to write extraction logic,and where the default filters in logstash are located
-thanks
</description><key id="92300406">11968</key><summary>Extraction of Log Messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">praveenaxxera</reporter><labels /><created>2015-07-01T07:29:54Z</created><updated>2015-07-01T07:35:04Z</updated><resolved>2015-07-01T07:35:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-01T07:35:03Z" id="117507510">You are asking about logstash in an elasticsearch issue system.
Not the right place, definitely.

Join us on discuss.elastic.co where we and the community can help you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>FSTs via completion suggester consuming all the heap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11967</link><project id="" key="" /><description>ES 1.6.0 with an index with 5 completion fields.  The heap usage on nodes hit 98%.  The heap itself is showing mostly segment readers with FSTs in them loaded via searches:

![image](https://cloud.githubusercontent.com/assets/7216393/8445344/b5e05680-1f50-11e5-83e6-aaa5a9c80509.png)

Currently, all FSTs for a field are loaded into the heap when the field is requested via search.  This makes me wonder if there's potentially a protection mechanism we can put in place to prevent the FSTs from consuming all of the heap.  Some kind of a cap and/or a way to evict them would be nice.
</description><key id="92233605">11967</key><summary>FSTs via completion suggester consuming all the heap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label></labels><created>2015-07-01T00:56:17Z</created><updated>2015-07-10T10:02:23Z</updated><resolved>2015-07-10T10:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-10T10:02:21Z" id="120332551">FSTs need to be in memory to work. We can't really evict them. It would also be difficult to add a circuit breaker to prevent loading the FST, because throwing an exception while loading the FST would make the entire segment unusable.

I don't think there is any way around this issue: you need to monitor the amount of memory used with the completion stats and plan around it.

Closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use consistent plural form of index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11966</link><project id="" key="" /><description>Indices or indexes; but please not a hodgepodge of both.
</description><key id="92229133">11966</key><summary>Use consistent plural form of index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">caldwecr</reporter><labels /><created>2015-07-01T00:11:37Z</created><updated>2015-07-01T08:51:58Z</updated><resolved>2015-07-01T08:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-01T08:51:47Z" id="117546414">thanks @caldwecr - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Use consistent plural form of index</comment></comments></commit></commits></item><item><title>Fix wrong reused file bytes in Recovery API reports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11965</link><project id="" key="" /><description>Calling the correct method, reusedBytes() when creating json output.
This fixes #11876.
</description><key id="92218136">11965</key><summary>Fix wrong reused file bytes in Recovery API reports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">szroland</reporter><labels><label>:Stats</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T22:44:46Z</created><updated>2015-07-14T13:46:34Z</updated><resolved>2015-07-01T07:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-07-01T07:00:10Z" id="117489850">Looks great. Thanks. I'll merge it in.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file></files><comments><comment>Recovery: Fix wrong reused file bytes in Recovery API reports</comment></comments></commit></commits></item><item><title>DateHistogram should be imported not Histogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11964</link><project id="" key="" /><description /><key id="92213296">11964</key><summary>DateHistogram should be imported not Histogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rmauge</reporter><labels><label>:Java API</label><label>docs</label><label>v1.6.1</label></labels><created>2015-06-30T22:14:34Z</created><updated>2015-07-01T07:56:37Z</updated><resolved>2015-07-01T07:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-30T22:18:41Z" id="117360777">Agreed! Could you sign the CLA?
</comment><comment author="rmauge" created="2015-06-30T22:28:02Z" id="117362261">Yes and done.
</comment><comment author="dadoonet" created="2015-07-01T07:56:25Z" id="117516597">Merged. Thanks!

Note that another change has been applied in master branch some months ago with this change https://github.com/elastic/elasticsearch/commit/5c44db50bc913b2ab8637bc181e12f181157cc4f#diff-5e52205b57abf7388fc3bb54f1cd86a7
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>detect jar hell before installing a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11963</link><project id="" key="" /><description>Currently ES fails on startup if jar hell is created. But its best to fail earlier, when the user tries to install the plugin, and keep the installation healthy.

Fixes #11946
</description><key id="92195994">11963</key><summary>detect jar hell before installing a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T20:45:07Z</created><updated>2015-07-01T08:59:52Z</updated><resolved>2015-07-01T04:00:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-30T21:01:29Z" id="117340798">left some minor comments LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Merge pull request #11963 from rmuir/prevent_hell</comment></comments></commit></commits></item><item><title>Rename "root" mappers to "metadata" mappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11962</link><project id="" key="" /><description>"Root" is a very confusing term for meta field mappers. This change
renames "RootMapper" to "MetadataFieldMapper" and simplifies
how metadata mappers are setup.

It also requires that metadata mappers are now a FieldMapper
(MetadataFieldMapper extends from AbstractFieldMapper). The only
use of a root mapper that wasn't a field mapper was the theoretical
"external" root mapper (just a test mapper). But it doesn't make
sense to not have an actual field, and this falls inline with
the hopefully eventual collapsing of AbstractFieldMapper/FieldMapper/Mapper.
</description><key id="92193423">11962</key><summary>Rename "root" mappers to "metadata" mappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T20:35:27Z</created><updated>2015-07-01T16:13:16Z</updated><resolved>2015-07-01T16:13:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-01T10:44:57Z" id="117598007">LGTM, I like sharing the same object hierarchy for metadata and regular mappers now!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MetadataFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/RootMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMetadataMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/RegisterExternalTypes.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/SimpleExternalMappingTests.java</file></files><comments><comment>Merge pull request #11962 from rjernst/pr/root-mappers-rename</comment></comments></commit></commits></item><item><title>Add missing format tick in 1.6 documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11961</link><project id="" key="" /><description>Formatting tick missing in `function-score-query` documents.
</description><key id="92192808">11961</key><summary>Add missing format tick in 1.6 documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmytheleaf</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-30T20:32:22Z</created><updated>2015-07-08T16:01:47Z</updated><resolved>2015-07-08T16:01:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-01T07:34:41Z" id="117507224">Hi @jimmytheleaf 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jimmytheleaf" created="2015-07-01T23:24:23Z" id="117848720">@clintongormley -- should be signed now. 
</comment><comment author="clintongormley" created="2015-07-02T11:51:21Z" id="118007513">thanks @jimmytheleaf - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reroute after node join is processed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11960</link><project id="" key="" /><description>#11776 has simplified our rerouting logic by removing a scheduled background reroute in favor of an explicit reroute during the cluster state processing of a node join (the only place where we didn't do it explicitly). While that change is conceptually good, it change semantics a bit in two ways:
- shard listing actions underpinning shard allocation do not have access to that new node yet (causing errors during shard allocation see #11923
- the very first cluster state published to a node already has shard assignments to it. This surfaced other issues we are working to fix separately
  
  This commit changes the reroute to be done post processing the initial join cluster state to side step these issues while we work on a longer term solution.
</description><key id="92184753">11960</key><summary>Reroute after node join is processed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T19:56:29Z</created><updated>2015-07-01T07:31:18Z</updated><resolved>2015-06-30T20:12:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-30T20:06:48Z" id="117324990">LGTM, thanks @bleskes!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file></files><comments><comment>ZenDiscovery: #11960 failed to remove eager reroute from node join</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file></files><comments><comment>Discovery: reroute after node join is processed</comment></comments></commit></commits></item><item><title>Filtered terms aggregation no more works from 1.5 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11959</link><project id="" key="" /><description>After upgrading ES from 1.4 to 1.5 some issue appears with terms aggregation.
On ES 1.4 this code is working perfect, beginning from 1.5 it returns empty buckets.

https://gist.github.com/Matrooskin/5c0bf81157038c6c3b6f
</description><key id="92174819">11959</key><summary>Filtered terms aggregation no more works from 1.5 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Matrooskin</reporter><labels /><created>2015-06-30T19:09:00Z</created><updated>2015-06-30T19:27:05Z</updated><resolved>2015-06-30T19:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T19:21:14Z" id="117311991">Hi @Matrooskin 

The problem is that you have a type name `nestedDocs` which is the same as a field name `objNestedDocs.nestedDocs`.  This ambiguity is causing the problem.  

We have fixed this in 2.0 with #8872.  In the meantime, all I can suggest to you is to change either the type or the field name to remove the ambiguity
</comment><comment author="Matrooskin" created="2015-06-30T19:27:04Z" id="117313415">Thank you for quick response. I will do that change.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>plugin exceptions that extend ElasticsearchException cannot be serialized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11958</link><project id="" key="" /><description>After #11910, "unknown" exceptions that extend `ElasticsearchException` are not serialized over the wire in their original form. These exceptions are wrapped in a `NotSerializableExceptionWrapper` and serialized this way. 

These exceptions should be serialized in their original form if they implement the constructor with `StreamInput` as the argument. In order to accomplish this, we could do one of the following:
1. Allow dynamic registering of exceptions to the `MAPPING` map in `ElasticsearchException`
2. Scan the classpath for classes that extend `ElasticsearchException`

I tend to lean towards number 1 to avoid scanning the classpath and everything that comes with it. There are also other approaches that could be taken. I'm marking this as a blocker as it breaks the functionality of some plugins.
</description><key id="92171812">11958</key><summary>plugin exceptions that extend ElasticsearchException cannot be serialized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels /><created>2015-06-30T18:56:19Z</created><updated>2015-07-09T18:24:02Z</updated><resolved>2015-07-09T18:24:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-07-09T18:24:01Z" id="120094650">closing as some improvements have been made such as copying the status and we can use one of the many exceptions defined instead.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add simple plugins smoke tester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11957</link><project id="" key="" /><description>At first I tried to fold this into dev-tools/build_release.py but I think we should (separately) refactor that into "smoke testing" from "making a release"?

So for now I just made a standalone script: this was easier, and I think we can refactor later.

It just runs `mvn package` and then installs ES to a tmp dir, installs each plugin using `bin/plugin`, then starts ES and lists the plugins to see they are all there.

It currently fails with this:

```
14:03 $ rm /tmp/elasticsearch_smoke_test_plugins.log; python3 -u dev-tools/smoke_test_plugins.py 
Logging to /tmp/elasticsearch_smoke_test_plugins.log
Build release bits...
Find plugins:
  install plugin analysis-icu...
  install plugin analysis-kuromoji...
  install plugin analysis-phonetic...
  install plugin analysis-smartcn...
  install plugin analysis-stempel...
  install plugin cloud-aws...
  install plugin cloud-azure...
  install plugin cloud-gce...
  install plugin delete-by-query...
  install plugin lang-javascript...
  install plugin lang-python...
Start Elasticsearch
ES: [2015-06-30 14:05:05,486][INFO ][node                     ] [smoke_tester] version[2.0.0-SNAPSHOT], pid[10429], build[a847dd2/2015-06-30T18:04:00Z]
ES: [2015-06-30 14:05:05,486][INFO ][node                     ] [smoke_tester] initializing ...
ES: [2015-06-30 14:05:05,550][INFO ][plugins                  ] [smoke_tester] loaded [lang-python, analysis-kuromoji, analysis-smartcn, cloud-gce, analysis-stempel, cloud-aws, delete-by-query, analysis-phonetic, cloud-azure, lang-javascript, analysis-icu], sites []
ES: [2015-06-30 14:05:05,570][INFO ][env                      ] [smoke_tester] using [1] data paths, mounts [[/ (/dev/mapper/haswell--vg-root)]], net usable_space [210.6gb], net total_space [465gb], spins? [no], types [btrfs]
ES: {2.0.0-SNAPSHOT}: Initialization Failed ...
ES: - ExecutionError[java.lang.NoClassDefFoundError: org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer]
ES:     NoClassDefFoundError[org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer]
ES:         ClassNotFoundException[org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer]
ES: **process exit**

Traceback (most recent call last):
  File "dev-tools/smoke_test_plugins.py", line 145, in &lt;module&gt;
    raise RuntimeError('ES failed to start')
RuntimeError: ES failed to start
```
</description><key id="92163974">11957</key><summary>Add simple plugins smoke tester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T18:21:39Z</created><updated>2015-08-13T14:28:13Z</updated><resolved>2015-06-30T18:34:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-30T18:23:16Z" id="117290646">+1, pls push this! we can dig into any issues as a followup.
</comment><comment author="rmuir" created="2015-06-30T18:24:57Z" id="117291017">the first part of debugging whatever is happening there, will be to ensure we get a proper goddamn stacktrace!
</comment><comment author="dadoonet" created="2015-06-30T19:09:25Z" id="117308311">IMHO with the distribution modules coming, smoke tests should be considered as integration tests for each artifact type we generate.

Something we should found in src/it/resources for example in each module...
</comment><comment author="rmuir" created="2015-06-30T19:10:43Z" id="117308789">That won't work in this case @dadoonet 

Here we are actually testing what comes out of the maven-assembly plugin, and what happens when you run bin/elasticsearch (shell scripts etc). All of these contribute to jar hell in their own special ways. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11957 from mikemccand/smoke_test_plugins</comment></comments></commit></commits></item><item><title>Enable "Spatial Join" Aggregations Behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11956</link><project id="" key="" /><description>Sorry for the horrible title. Since a geohash aggregation is not possible with geo_shape type items in an index (which makes sense), I would like to be able to produce a count of features (geoshapes) that intersect another "layer" of features (which can be in the form of another query or a different index).
Here is a sql equivalent to illustrate what I'm trying to do.

SELECT
a.id, 
a.thePolygonGeom,
COUNT(b.id) as cnt
FROM a 
INNER JOIN b
ON st_intersects(a.geom, b.geom) 
GROUP BY a.id,a.thePolygonGeom;

in this example, table A would be the geohash grid.

This way I could load an index with a pregenerated geohash grid shapefile, and dynamically intersect other geoshapes for stats at different levels.

let me know if this is a dumb idea, or if it's already possible.
</description><key id="92154368">11956</key><summary>Enable "Spatial Join" Aggregations Behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">giaconiamark</reporter><labels><label>:Geo</label></labels><created>2015-06-30T17:39:59Z</created><updated>2015-07-10T14:26:59Z</updated><resolved>2015-07-10T10:06:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-01T08:21:33Z" id="117530844">Hi @giaconiamark 

An inner join like the one you describe would not perform well at all.  We don't support even simple term-based distributed joins because the performance would be terrible (and wouldn't scale).  That said, using a list of shapes in a "geo-terms" aggregation, with intersecting shapes collected in each bucket, might be an interesting idea.

@nknize @jpountz any thoughts?
</comment><comment author="giaconiamark" created="2015-07-01T11:23:29Z" id="117617047">Yes, the ladder is something like I need... I need buckets with an itemid and count of shapes that intersected that shape, grouped by type. What I think it amounts to is a bucket aggregation, but with a collection of shapes as a filter, and it would return buckets containing the shape and id of each shape in the collection.
The purpose (use case) for this is to understand density of shapes.
I have a cluster with well over a billion shapes, and need to know basic things like "where are the most roads?" and "where do I not have buildings" etc.
I think each item in the returned json array would look something like this psuedo
"buckets:".....

"bucket":{
  "join_shape":{..polygon of coords...},
  "intersectingFeatureCountsByType:{  
    "road":234},  
    "river":190,
    "building":756,
    "etc...":869678}
}
an array of these would allow me to do very interesting visualizations across the globe of what I have where...likely wouldn't ever produce more than a few hundred polygons (it would be based on zoom level).

right now I accomplish this manually by generating my own grid of polygons, and doing a bbox intersects with each box to get a count of features that overlap on my webserver.
It should act very similar to what a geohash agg currently does, except this works off of another index of shapes or a query for shapes.

Hope this helps
</comment><comment author="colings86" created="2015-07-10T10:06:14Z" id="120337615">@giaconiamark I think you should be able to achieve this already with the filters aggregation? You can create a filter containing geo polygon filter(s) for each of your 'layers' and use this to create buckets containing documents with geo_shape's that intersect each 'layer'.
</comment><comment author="giaconiamark" created="2015-07-10T14:26:59Z" id="120422084">thanks guys, I'll give that a shot
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Updated the translog docs to reflect the new behaviour/settings </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11955</link><project id="" key="" /><description>Closes #11287
</description><key id="92149672">11955</key><summary>Docs: Updated the translog docs to reflect the new behaviour/settings </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T17:20:39Z</created><updated>2015-07-10T12:06:05Z</updated><resolved>2015-07-07T13:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-07T13:37:30Z" id="119204198">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11955 from clintongormley/translog_docs</comment></comments></commit></commits></item><item><title>Change `percolator.getTime` -&gt; `percolator.time`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11954</link><project id="" key="" /><description /><key id="92141286">11954</key><summary>Change `percolator.getTime` -&gt; `percolator.time`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T16:46:22Z</created><updated>2015-07-02T11:28:33Z</updated><resolved>2015-07-01T17:08:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-30T17:35:09Z" id="117273633">I feel a bit bad about changing the response format. Would be nice to implement something like https://github.com/elastic/elasticsearch/issues/11184 before doing such changes?
</comment><comment author="kimchy" created="2015-06-30T17:58:45Z" id="117283635">I think it is ok for this change...
</comment><comment author="martijnvg" created="2015-06-30T20:09:01Z" id="117326151">the name is just wrong, but it was never noticed... also it is a small change and only gets added to the next major version and therefore I think versioning is not necessary.
</comment><comment author="jpountz" created="2015-07-01T10:45:30Z" id="117598373">LGTM then :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java</file></files><comments><comment>Merge pull request #11954 from martijnvg/percolator/stats/typo</comment></comments></commit></commits></item><item><title> Save repository to it's own sources.list file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11953</link><project id="" key="" /><description>Best to put repo definitions in a separate file from that of the core OS's sources.list.

Plus if someone has to type this command manually and skips the `-a` flag they won't clobber their sources.list. 

Also makes it easy to swap/replace sources.list without having to worry about packages that were added there. 
</description><key id="92136117">11953</key><summary> Save repository to it's own sources.list file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jhr007</reporter><labels><label>docs</label></labels><created>2015-06-30T16:22:41Z</created><updated>2015-07-02T11:21:22Z</updated><resolved>2015-07-02T11:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jhr007" created="2015-06-30T17:05:48Z" id="117259116">CLA signed
</comment><comment author="clintongormley" created="2015-07-01T08:14:31Z" id="117526762">Hi @jhr007 

Thanks for the PR.  I'm unsure whether to proceed or not because I'm not an ubuntu/debian user myself.  I'd like other debian/ubuntu users to chime in on whether this is a good idea or not.  Also, do you need to delete the old branch files when upgrading?
</comment><comment author="nik9000" created="2015-07-01T14:09:40Z" id="117688561">I'm a Debian/Ubuntu user. Its more normal to use source.list.d like @jhr007 is proposing.

You shouldn't have to delete the old branch file. Its ok to have multiple versions of the same package available for installation. Its good, even. That way you can force installation of a certain version.

Some folks have a repository manager that only allows a single version of the package to be in it at a time but those folks aren't going to follow these instructions anyway.
</comment><comment author="jhr007" created="2015-07-01T14:10:15Z" id="117688888">I don't think multiple files or repos really matter to apt, magic prevails and apt just understands. 

I tested by adding a .list file pointing to 1.5, w/1.6 installed. Nothing conflicted on `apt-get update`, `apt-cache showpkg` shows 1.5 is available as version to install. I'm gonna assume that since apt sees 1.5 as an available install, when I add the 1.7 repository apt will show that elasticsearch can be upgraded. 

Just tested with two copies of my 1.6.list files and `apt-get update` fails with:

&gt; W: Duplicate sources.list entry http://packages.elastic.co/elasticsearch/1.6/debian/ stable/main amd64 Packages (/var/lib/apt/lists/packages.elastic.co_elasticsearch_1.6_debian_dists_stable_main_binary-amd64_Packages)
&gt; W: You may want to run apt-get update to correct these problems
</comment><comment author="jhr007" created="2015-07-01T15:30:53Z" id="117716031">@nik9000 I think removing the -a would make instructions a lot less forgiving, especially for beginners. 

Compare worst cases and let me know what you think: 
- Without -a: User overwrites something important and loses time finding original copy. 
- With -a: User adds an extra line to another file. or User has duplicate repos, and gets the `apt-get update` error mentioned above. 
</comment><comment author="nik9000" created="2015-07-01T15:37:09Z" id="117718567">&gt; With -a: User adds an extra line to another file. or User has duplicate repos, and gets the apt-get update error mentioned above.

I'm sold. Could you add the error that you quoted above to the docs and a quick explanation/workaround? The folks following these instruction might not be super familiar with apt.
</comment><comment author="clintongormley" created="2015-07-02T11:21:22Z" id="118003766">thanks @jhr007 - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Save debian repository to it's own sources.list file</comment></comments></commit></commits></item><item><title>[Docs] Embedded node client should disable HTTP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11952</link><project id="" key="" /><description>The Java API documentation talks about the creation of an embedded node client, but it does not talk about the negatives. One oft-surprising negative is that, because it's a full node, it also provides an HTTP port for access.

For embedded client nodes, this should be disabled by default. For standalone client nodes (the main use case for client nodes), this should be on by default.

``` java
Node node =
    nodeBuilder()
        .settings(ImmutableSettings.settingsBuilder().put("http.enabled", false))
        .client(true)
    .node();

Client client = node.client();
```
</description><key id="92133975">11952</key><summary>[Docs] Embedded node client should disable HTTP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>docs</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-06-30T16:12:05Z</created><updated>2015-07-31T03:13:22Z</updated><resolved>2015-07-31T03:13:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-30T16:38:35Z" id="117248928">@pickypg you wrote almost everything ! Why not sending a PR ?

Wanna do it ?
</comment><comment author="pickypg" created="2015-07-31T03:13:22Z" id="126554605">Closed by 8efa18e6167e276dd220b60c34b516d05252bc22 (master), 2461e913b717cf2c06f58a615ae8aa65a7a70a7f (1.7), d4b7e12d1cf57899db990690413d49a26fd37370 (1.6)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Adds a new GapPolicy - NONE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11951</link><project id="" key="" /><description>This is to allow the skip gap policy to always skip execution if values are missing. The none gap policy will set the value of the missing metric to NaN. In practice the skip and non gap policies work the same in all pipeline aggregators except the `bucket_script` agg, where `skip` will not execute the script for buckets where any of the input metrics are missing, and `none` will pass Double.NaN into the skip as the value of these missing metrics
</description><key id="92127907">11951</key><summary>Aggregations: Adds a new GapPolicy - NONE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T15:42:22Z</created><updated>2015-07-10T12:11:30Z</updated><resolved>2015-07-01T09:47:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-30T17:46:58Z" id="117278523">I'm not too happy with having both `none` and `skip` given how close they are. I think we need to pick one.
</comment><comment author="clintongormley" created="2015-07-01T08:26:46Z" id="117533830">@jpountz If we have to choose one then it should be `skip`, as this will be the most common use case for scripting.  However, this will prevent users of `bucket_scripts` from processing buckets with truly null values (distinct from zeroes).

We don't know yet whether this second use case is valid or not.  I'd suggest leaving this PR as stalled for now, and we can always revisit it if users need it.
</comment><comment author="colings86" created="2015-07-01T08:29:15Z" id="117535932">@clintongormley we cannot leave this PR as stalled as skip does not work as we intend it to for the bucket_script aggregation currently (it passes NaN into the script rather than skipping execution). Would you rather that I change this PR to just make bucket_script not execute the script if any of the variables are missing in the bucket?
</comment><comment author="colings86" created="2015-07-01T09:47:36Z" id="117567608">Skip gap policy has been fixed in https://github.com/elastic/elasticsearch/pull/11970. Closing this for now as we need to think about whether other gap policies are required
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Intermittent delays when performing search queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11950</link><project id="" key="" /><description>We have a strange problem whereby most queries take a very short amount of time to execute (&lt;50ms), but intermittently they will take almost _exactly_ 21 seconds.

While trying to find a solution to the problem, I stumbled across an unanswered [stack overflow question](http://stackoverflow.com/questions/29552070/intermittent-delay-on-searching-elasticsearch-using-nest) describing almost identical behaviour (and use case).

**Our use case**

We have a small micro service API (using [NancyFx](http://nancyfx.org/)) which performs relatively simple queries and filters. We instantiate our ElasticClient at start up, and inject it where required.

We enabled the elasticsearch slow query log on the index, and this shows that queries never take more than ~130ms to execute (and that is seldom). It also doesn't align with these 21s "blips" that we see in the logs.

At the moment I wonder if it is caused by the client connections to the production cluster being dropped, however, the really odd thing is that this delay is 21 seconds. I would expect the time taken to reconnect to be more random. We have a bizarre spike if we graph the request times. 95% &lt; 1s, 97.5% &lt; 2s, and then nothing until this spike of the last 2.5% of requests at 21s.

Any ideas on things to try would be appreciated. 
</description><key id="92122728">11950</key><summary>Intermittent delays when performing search queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikezopa</reporter><labels><label>feedback_needed</label></labels><created>2015-06-30T15:20:56Z</created><updated>2015-07-01T08:37:33Z</updated><resolved>2015-07-01T08:37:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-01T07:42:57Z" id="117513122">Hi @mikezopa 

I don't know what is going on, but my hunch is that this is related to where you are hosting Elasticsearch.  You don't mention whether you're using an HTTP or Java client, where it's hosted etc... Possibilities include: 
- slow DNS resolution
- slow TCP connections
- unused small VM giving resources to other VMs on the same box
- other?
</comment><comment author="mikezopa" created="2015-07-01T08:16:02Z" id="117528082">Hi @clintongormley,

Thanks for the reply! Here's some more information about the configuration:

We are using the HTTP client, communicating over TCP 9200. We have 2 elasticsearch server nodes in a clustered configuration (both VMs running with 2 cores @ 2.5GHz and 12GB RAM). This is all hosted in a data centre, and our client (the micro service I mentioned) is on yet another VM also within the same DC. These VMs are either on the same server, or a different rack server, but in the same rack. Just checked with the infrastructure guys and we aren't oversubscribed on any resources on these production boxes.

With regards to the network connectivity, they are co-located so I would be surprised if this were the issue. DNS resolution is also not an issue as the DNS server is next to them as well. 

I was wondering if it might be a connection pooling issue? We are using the ElasticClient with the `SniffingConnectionPool`. I must admit my hunch was towards the client (mainly because of the server logs showing great performance on the main cluster).
</comment><comment author="mikezopa" created="2015-07-01T08:37:33Z" id="117539614">Hi @clintongormley,

Really sorry, I just realised I have opened this issue against Elasticsearch instead of the .NET elasticsearch client! My mistake!

Going to close this now as I have ruled out the environment.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cut over to writeable for TransportAddress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11949</link><project id="" key="" /><description>This will allow us to move away from reflection hacks to serialize
transport exceptions.
</description><key id="92111532">11949</key><summary>Cut over to writeable for TransportAddress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T14:37:05Z</created><updated>2015-07-01T08:43:49Z</updated><resolved>2015-06-30T19:00:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-30T14:58:50Z" id="117218395">left a minor comment, LGTM though
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Adds other bucket to filters aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11948</link><project id="" key="" /><description>The filters aggregation now has an option to add an 'other' bucket which will, when turned on, contain all documents which do not match any of the defined filters. There is also an option to change the name of the 'other' bucket from the default of '_other_'

Closes #11289
</description><key id="92110655">11948</key><summary>Aggregations: Adds other bucket to filters aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T14:33:30Z</created><updated>2015-07-01T09:44:58Z</updated><resolved>2015-07-01T09:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-30T16:42:04Z" id="117250120">I left some comments. Also I'm wondering about the API: we need to convey two pieces of information:
- whether the other bucket should be computed
- what key should be used for the other bucket

Right now it's in a single field that can take either a string or boolean value, but I have vague memories that it can be an issue for some clients ( @clintongormley perl maybe?). Maybe we could make it always a string and use `null` in order to not compute the other bucket (but maybe this could cause issues too?). Or maybe we need two fields?
</comment><comment author="colings86" created="2015-06-30T16:50:18Z" id="117253013">The issue with using just the name, with null meaning 'don't enable', is that for anonymous filters (non-keyed response) you would be forced to name something which is then never shown to you. I can break it out into two fields `other_bucket: true|false` and `other_bucket_key: "name"` but I would like to make it so if only `other_bucket_key` is specified then the `other_bucket` is implicitly enabled, even if `other_bucket` is not present.
</comment><comment author="jpountz" created="2015-06-30T17:19:26Z" id="117266028">I see, so in practice it could be either:
- `other_bucket: false` if you don't want the other bucket to be computed or
- `other_bucket_key: "something"` otherwise?
</comment><comment author="colings86" created="2015-07-01T08:05:24Z" id="117518227">Thinking about it, I actually don't like the idea of having the other bucket on by default. I think there are many use cases where you wouldn't want the other bucket on, and it seems like a weird default to have. I would prefer that the default setting was `other_bucket: false` and the settings to turn it on would be either of:
- `other_bucket: true` to use the default key or if your are using anonymous filters 
- `other_bucket_key: "something"` to use a custom key
</comment><comment author="clintongormley" created="2015-07-01T09:12:25Z" id="117555949">I agree with not enabling the other bucket by default.  Simplest thing is: only enable it if the user passes `other_bucket`, which accepts the name the bucket should use.
</comment><comment author="colings86" created="2015-07-01T09:15:41Z" id="117556577">@clintongormley As I said above, this causes weirdness when you use anonymous filters. You are required to name your other bucket but that name is not used in the response. This is why the original commit used `other_bucket: true|false|"name"` as the options, but @jpountz said that might cause problems for some clients
</comment><comment author="clintongormley" created="2015-07-01T09:27:13Z" id="117560543">&gt; anonymous filters (non-keyed response) you would be forced to name something which is then never shown to you

ah ok - missed that bit.  hmm I think Perl's the only one that would have issues (but they are not insurmountable).  The `other_bucket` and `other_bucket_key` option is more verbose but cleaner. Practically, most people would never need to change the name... I'm leaning towards having the two options.
</comment><comment author="colings86" created="2015-07-01T09:28:16Z" id="117560728">@clintongormley ok great. I think having the two options works.

@jpountz I've updated the PR with your comments, could you take another look?
</comment><comment author="jpountz" created="2015-07-01T09:36:33Z" id="117563038">@colings86 I left two minor comments
</comment><comment author="colings86" created="2015-07-01T09:40:53Z" id="117564412">@jpountz Thanks for the review. I pushed another update
</comment><comment author="jpountz" created="2015-07-01T09:41:33Z" id="117564498">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use of pre and post filters throws "TokenStream contract violation: close() call missing"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11947</link><project id="" key="" /><description>This happens when using the standard analyzer as a pre and post filter after a direct candidate generator (ES 1.6). When this query was run in a multi-node environment, the error seemed to persist and required a rolling restart.  For example: 

``` json
{ "suggest" : {
    "text" : "pizeo",
    "simple_phrase" : {
      "phrase" : {
        "analyzer" : "english",
        "field" : "synopses",
        "direct_generator" : [ {
          "field" : "synopses",
          "suggest_mode" : "always",
          "min_word_length" : 1
        }, {
          "field" : "synopses",
          "suggest_mode" : "always",
          "min_word_length" : 1,
          "pre_filter" : "standard",
          "post_filter" : "standard"
        } ]
      }
    }
  }
}
```

Will throw this error:

``` json
{
   "took": 21,
   "timed_out": false,
   "_shards": {
      "total": 4,
      "successful": 1,
      "failed": 3,
      "failures": [
         {
            "index": "project",
            "shard": 0,
            "status": 500,
            "reason": "IllegalStateException[TokenStream contract violation: close() call missing]"
         },
         {
            "index": "project",
            "shard": 2,
            "status": 500,
            "reason": "IllegalStateException[TokenStream contract violation: close() call missing]"
         },
         {
            "index": "project",
            "shard": 3,
            "status": 500,
            "reason": "IllegalStateException[TokenStream contract violation: close() call missing]"
         }
      ]
   },
   "suggest": {
      "simple_phrase": [
         {
            "text": "pizo",
            "offset": 0,
            "length": 4,
            "options": []
         }
      ]
   }
}
```

[2015-06-30 10:16:05,888][DEBUG][action.search.type       ] [Wiz Kid] [project][0], node[OhG1Vc65RPyjCcSlXDbCJQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7bd06ad7] lastShard [true]
java.lang.IllegalStateException: TokenStream contract violation: close() call missing
    at org.apache.lucene.analysis.Tokenizer.setReader(Tokenizer.java:90)
    at org.apache.lucene.analysis.Analyzer$TokenStreamComponents.setReader(Analyzer.java:323)
    at org.apache.lucene.analysis.standard.StandardAnalyzer$1.setReader(StandardAnalyzer.java:133)
    at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:147)
    at org.elasticsearch.search.suggest.SuggestUtils.analyze(SuggestUtils.java:119)
    at org.elasticsearch.search.suggest.SuggestUtils.analyze(SuggestUtils.java:115)
    at org.elasticsearch.search.suggest.phrase.DirectCandidateGenerator.preFilter(DirectCandidateGenerator.java:133)
    at org.elasticsearch.search.suggest.phrase.DirectCandidateGenerator.frequency(DirectCandidateGenerator.java:92)
    at org.elasticsearch.search.suggest.phrase.DirectCandidateGenerator$2.nextToken(DirectCandidateGenerator.java:155)
    at org.elasticsearch.search.suggest.SuggestUtils.analyze(SuggestUtils.java:130)
    at org.elasticsearch.search.suggest.SuggestUtils.analyze(SuggestUtils.java:122)
    at org.elasticsearch.search.suggest.SuggestUtils.analyze(SuggestUtils.java:115)
    at org.elasticsearch.search.suggest.phrase.DirectCandidateGenerator.postFilter(DirectCandidateGenerator.java:148)
    at org.elasticsearch.search.suggest.phrase.DirectCandidateGenerator.drawCandidates(DirectCandidateGenerator.java:122)
    at org.elasticsearch.search.suggest.phrase.MultiCandidateGeneratorWrapper.drawCandidates(MultiCandidateGeneratorWrapper.java:52)
    at org.elasticsearch.search.suggest.phrase.NoisyChannelSpellChecker.getCorrections(NoisyChannelSpellChecker.java:116)
    at org.elasticsearch.search.suggest.phrase.PhraseSuggester.innerExecute(PhraseSuggester.java:98)
    at org.elasticsearch.search.suggest.phrase.PhraseSuggester.innerExecute(PhraseSuggester.java:54)
    at org.elasticsearch.search.suggest.Suggester.execute(Suggester.java:43)
    at org.elasticsearch.search.suggest.SuggestPhase.execute(SuggestPhase.java:85)
    at org.elasticsearch.search.suggest.SuggestPhase.execute(SuggestPhase.java:74)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:170)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:289)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:300)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</description><key id="92107669">11947</key><summary>Use of pre and post filters throws "TokenStream contract violation: close() call missing"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">jhariani</reporter><labels><label>:Suggesters</label><label>bug</label></labels><created>2015-06-30T14:21:30Z</created><updated>2015-11-17T16:35:56Z</updated><resolved>2015-10-02T13:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-01T07:37:57Z" id="117509518">@areek please could you investigate
</comment><comment author="vreon" created="2015-08-03T23:29:32Z" id="127430890">We ran into this issue as well (also 1.6).
</comment><comment author="aparo" created="2015-09-30T10:02:23Z" id="144344842">I ran in this issue with 2.0-beta2. With a similar configuration of @jhariani.
The shards are random failing executing many times the same call.
The fails are related to preFilter call in o.e.search.suggest.phrase.DirectCandidateGenerator.preFilter(DirectCandidateGenerator.java:115)
</comment><comment author="mikemccand" created="2015-09-30T10:31:51Z" id="144349570">This was supposed to have been fixed in d97afd52f3e743547277c8fea6fb02fbe2bdc1ff ... and that fix is in 2.0-beta2 ... @aparo can you post more details about the exception?  Was there an initial exception before you hit the `close() call missing`?
</comment><comment author="jelmerk" created="2015-11-13T16:06:27Z" id="156474439">We are still seeing this issue on elastic 1.7.1 when using the following generator in a phrase suggester

```
   {
      "field" : "title.reversed",
      "prefix_length" : 2,
      "pre_filter" : "reverser",
      "post_filter" : "reverser"
    }
```

Where reverser looks like this :

```
    "reverser": {
      "tokenizer": "standard",
      "filter": [
        "lowercase",
        "dutch_stopwords",
        "asciifolding",
        "reverse"
      ]
    }
```

if I take out the pre_filter things it does not fail
</comment><comment author="clintongormley" created="2015-11-17T16:35:56Z" id="157424415">@jelmerk the fix is in 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PluginManager should not install a plugin, if it would create jar hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11946</link><project id="" key="" /><description>We now fail on ES startup if we detect jar hell (which we should keep), but I think we can probably factor out the logic to a method that takes URL[]. pluginmanager could then pass classpath of current plugins, and add the "candidate" classpath from the plugin to-be-installed.

Ideally we do all of this before touching the filesystem, so the plugin installation just fails but the overall ES installation stays working.

I will try to make a prototype.
</description><key id="92101440">11946</key><summary>PluginManager should not install a plugin, if it would create jar hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>v2.0.0-beta1</label></labels><created>2015-06-30T13:58:56Z</created><updated>2015-07-01T04:00:13Z</updated><resolved>2015-07-01T04:00:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow shards to be allocated if leftover shard from different index exists.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11945</link><project id="" key="" /><description>If an index name is reused but a leftover shard still exists on any node
we fail repeatedly to allocate the shard since we now check the index UUID
before reusing data. This commit allows to recover even if there is such a
leftover shard by trying to archive the shard into a seperate directory.

Closes #10677
</description><key id="92100915">11945</key><summary>Allow shards to be allocated if leftover shard from different index exists.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>won't fix</label></labels><created>2015-06-30T13:57:01Z</created><updated>2015-07-14T14:54:40Z</updated><resolved>2015-07-08T16:02:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-01T10:24:49Z" id="117583951">@bleskes can you take a look if you have some time
</comment><comment author="bleskes" created="2015-07-02T09:33:12Z" id="117976434">If I understand correctly, our primary concern here is to not delete old indices by mistake because a node joins a cluster it wasn't supposed to or some other weird case. In this case we shouldn't archive into the same index folder as we do now, but rather into another index level folder. If we keep it locally we might run the risk of cleaning up the archived shard folder by mistake when all shards of the active index are moved to another node.

The flip of this is that if we put it into another index folder, we should change the meta data as well (to make it consistent) which also means it will be picked up by the dangling indices, surfacing it up to the user and also allowing them to deal with it using the API (delete it).
</comment><comment author="s1monw" created="2015-07-02T09:53:22Z" id="117985732">you are right, I think the simplest solution is to just delete the stuff there and be done with it. its such a conrner case I don't want to overdesign for cornercases and bogus like this.
</comment><comment author="bleskes" created="2015-07-02T10:06:08Z" id="117988106">++ to maintain old behavior and clean it up for now.

&gt; On 02 Jul 2015, at 11:53, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; you are right, I think the simplest solution is to just delete the stuff there and be done with it. its such a conrner case I don't want to overdesign for cornercases and bogus like this.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[build] include in plugins only needed jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11944</link><project id="" key="" /><description>We don't shade anymore elasticsearch dependencies, so plugins might include jars in the distribution ZIP file which might not be needed anymore.

For example, `elasticsearch-cloud-aws` comes with:

```
Archive:  cloud-aws/target/releases/elasticsearch-cloud-aws-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
  1920788  05-18-15 09:42   aws-java-sdk-ec2-1.9.34.jar
   503963  05-18-15 09:42   aws-java-sdk-core-1.9.34.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   915096  01-19-15 09:24   jackson-databind-2.3.2.jar
   252288  05-18-15 09:42   aws-java-sdk-kms-1.9.34.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
    35058  01-19-15 09:24   jackson-annotations-2.3.0.jar
   229998  05-29-15 12:28   jackson-core-2.5.3.jar
   589289  01-19-15 09:24   joda-time-2.7.jar
   562858  05-18-15 09:42   aws-java-sdk-s3-1.9.34.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
    44854  06-12-15 19:22   elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar
 --------                   -------
  6221815                   13 files
```

A lot of those files are already distributed with elasticsearch itself so classes are available within the classloader.

Closes #11647.

If accepted, I'll most likely squash all commits in a single one.
</description><key id="92073002">11944</key><summary>[build] include in plugins only needed jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T11:54:34Z</created><updated>2015-07-05T17:20:36Z</updated><resolved>2015-07-01T19:39:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-30T13:00:31Z" id="117169062">looks good to me, thanks for tackling this!

Hopefully we get a chance to tackle https://github.com/elastic/elasticsearch/issues/11935 and we will have a running "test" in jenkins for these issues.
</comment><comment author="dadoonet" created="2015-07-01T19:39:10Z" id="117802983">Closed by e429b8d19053674f3739121aac79b56745ab4084
</comment><comment author="apatrida" created="2015-07-04T15:28:56Z" id="118523334">You sure you didn't delete one too many?  Creating an S3 repo with master (as-of today) of elastic search tree, with the cloud-was plugin from the same tree results in missing classes:

```
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.s3.S3Repository
  while locating org.elasticsearch.repositories.Repository

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:140)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:368)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:55)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:110)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
    at com.amazonaws.AmazonWebServiceClient.&lt;init&gt;(AmazonWebServiceClient.java:129)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:432)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:414)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.getClient(InternalAwsS3Service.java:153)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.client(InternalAwsS3Service.java:82)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:125)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: org.apache.http.protocol.HttpContext
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 37 more

```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Makes ValueFormat and ValueFormatter never null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11943</link><project id="" key="" /><description>This allows a lot of null checks to be removed where we were always falling back to the ValueFormat.RAW anyway. Now the format is set to ValueFormat.RAW when no alternative is suitable.

Closes #10594
</description><key id="92065317">11943</key><summary>Aggregations: Makes ValueFormat and ValueFormatter never null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T11:24:34Z</created><updated>2015-07-01T07:58:21Z</updated><resolved>2015-07-01T07:58:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-30T17:51:32Z" id="117281550">Wonderful! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NEST Multiple aggregations does not work.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11942</link><project id="" key="" /><description>I am trying to retrieve multiple aggregations through search with the following snippet of code:

```
        var searchDescriptor = new SearchDescriptor&lt;Foo&gt;();
        searchDescriptor
            .Take(50)
            .From(0)
            .Query(q =&gt; q
                .Filtered(filtered =&gt; filtered
                    .Query(fq =&gt; fq
                        .QueryString(qs =&gt; qs
                            .Query(query)
                        )
                    )
                    .Filter(filter =&gt; filter
                        .And(fs.ToArray())
                    )
                )
            )
            .Aggregations(aggs =&gt; aggs
                .Terms("categories", ct =&gt; ct
                    .Field("categories")
                    .Size(0)
                )
                .Terms("activities", at =&gt; at
                    .Field("activities")
                    .Size(0)
                )
                .GeoHash("locations", l =&gt; l
                    .Field(f =&gt; f.Location)
                    .GeoHashPrecision(GeoHashPrecision.Precision6)
                )
                .GeoBounds("bounds", b =&gt; b
                    .Field(f =&gt; f.Location)
                )
            );
```

When getting the corresponding json for this query and double check it with Sense I get all the aggregations, but the search response from NEST only contains the locations and the bounds aggregations.

Is this an issue or have I misunderstood something?
</description><key id="92053691">11942</key><summary>NEST Multiple aggregations does not work.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Ekenstein</reporter><labels /><created>2015-06-30T10:22:25Z</created><updated>2015-06-30T11:43:34Z</updated><resolved>2015-06-30T11:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T11:43:33Z" id="117142680">This issue was moved to elastic/elasticsearch-net#1474
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Pipeline Aggregation to filter buckets based on a script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11941</link><project id="" key="" /><description>This aggregation will use a script and the other aggregations within the bucket to determine whether the bucket is to be kept or discarded in the final output
</description><key id="92047580">11941</key><summary>Aggregations: Pipeline Aggregation to filter buckets based on a script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T09:54:38Z</created><updated>2015-07-07T10:46:47Z</updated><resolved>2015-07-07T08:56:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-07T07:02:14Z" id="119095287">LGTM. I left minor comments but feel free to address them without further review.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decode URL.getPath before resolving a real file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11940</link><project id="" key="" /><description>URL with spaces are URL-Encoded and need to be decoded before
they can be used to open files etc.
</description><key id="92034738">11940</key><summary>Decode URL.getPath before resolving a real file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T09:18:05Z</created><updated>2015-06-30T12:06:02Z</updated><resolved>2015-06-30T11:36:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-30T09:18:52Z" id="117069897">I ran into this running tests from intellij 

```
java.lang.RuntimeException: found jar hell in test classpath
at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:58)
at org.elasticsearch.test.ElasticsearchTestCase.&lt;clinit&gt;(ElasticsearchTestCase.java:102)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:340)
at com.carrotsearch.randomizedtesting.RandomizedRunner$1.run(RandomizedRunner.java:562)
Caused by: java.io.FileNotFoundException: /Applications/IntelliJ%20IDEA%2014.app/Contents/lib/idea_rt.jar (No such file or directory)
```
</comment><comment author="javanna" created="2015-06-30T11:10:21Z" id="117127464">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: improve query validation errors messages by providing more context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11939</link><project id="" key="" /><description>If you have a compound query composed of different levels of nested queries, which is very common, and one single query fails validation, we currently print out a generic validation error but it is hard to identify which query is the "broken" one from the error message. We should improve this, probably by keeping track of where a query is positioned in the query tree and outputting a more specific error message.
</description><key id="92032685">11939</key><summary>Query refactoring: improve query validation errors messages by providing more context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>enhancement</label></labels><created>2015-06-30T09:11:23Z</created><updated>2015-09-22T12:01:40Z</updated><resolved>2015-09-22T12:01:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-22T12:01:40Z" id="142267156">Since we're moving away from validate() method this can be closed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Release: Build two RPMS, signed and unsigned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11938</link><project id="" key="" /><description>In order to support older RPM based distributions like CentOS5,
we should have one RPM available, which is not signed.

This commit creates an unsigned RPM first, then moves it over to
target/releases during the build, then builds a signed RPM.

The unsigned one is uploaded via S3, where as the signed one is
used for the repositories.

In addition, you can now build an RPM without having to specify
any gpg credentials due to offloading this into a maven profile
that is only activated when specifying `rpm.sign` property.

Closes #11587
</description><key id="92013478">11938</key><summary>Release: Build two RPMS, signed and unsigned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T07:29:30Z</created><updated>2015-08-18T17:08:39Z</updated><resolved>2015-06-30T12:26:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-30T09:19:13Z" id="117070143">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed broken link to github repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11937</link><project id="" key="" /><description /><key id="92003886">11937</key><summary>Fixed broken link to github repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peyerc</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-30T06:36:46Z</created><updated>2016-03-03T19:06:21Z</updated><resolved>2016-03-03T19:06:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T11:35:28Z" id="117138463">Hi @srf-peyerc 

Thanks for the PR. Please could I ask you to sign the CLA so that i can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-03-03T19:06:21Z" id="191917384">No CLA signed, so treating this as a feature request, I opened https://github.com/elastic/elasticsearch/pull/16939 for this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>jar hell check should fail, if jars require higher java version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11936</link><project id="" key="" /><description>This could happen e.g. if a plugin requires java 8, but the user is on java 7. Instead of strange classfile format errors, we can try for a nice exception.

In order to be efficient, we just check jar manifest metadata for this, if available. This is nonstandard but e.g. all lucene and ES jars have it. 

Otherwise we'd have to do something that would be prohibitively slow (like looking at bytes of classes). So its a best effort.
</description><key id="91993014">11936</key><summary>jar hell check should fail, if jars require higher java version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T05:52:20Z</created><updated>2015-06-30T11:34:17Z</updated><resolved>2015-06-30T06:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-30T05:54:50Z" id="116986608">note we also have other useful metadata in plugin jars like ES and lucene versions. I want to do that on separate issues. The java version is just the most basic check.
</comment><comment author="rjernst" created="2015-06-30T06:02:44Z" id="116990667">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file></files><comments><comment>Merge pull request #11936 from rmuir/jdkhell</comment></comments></commit></commits></item><item><title>smoketester should install ALL plugins/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11935</link><project id="" key="" /><description>`dev-tools/build_release.py` should install all the plugins from plugins/ into elasticsearch before it starts up. 

This will allow detection of packaging bugs (maven assembly logic) such as https://github.com/elastic/elasticsearch/issues/11647. If the plugin's zip has conflicting jars, they are detected as jar hell on startup.

It would be great if it also asserted that all the plugins expected to be loaded in fact were.

I'm not a python guru, but this should really prevent a lot of release bugs.
Then we can fix #11647 and have confidence in the packaging.
</description><key id="91985015">11935</key><summary>smoketester should install ALL plugins/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T05:14:47Z</created><updated>2015-08-13T16:29:30Z</updated><resolved>2015-06-30T18:34:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-30T13:37:24Z" id="117187234">+1
</comment><comment author="mikemccand" created="2015-06-30T14:49:52Z" id="117214670">@s1monw I have a rough start at this, as a standalone smoke_test_plugins.py tool ...
</comment><comment author="s1monw" created="2015-06-30T14:51:43Z" id="117215208">@mikemccand feel free to take this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11957 from mikemccand/smoke_test_plugins</comment></comments></commit></commits></item><item><title>Add jitter in AWS API retries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11934</link><project id="" key="" /><description>Currently if a large number of nodes need to hit the AWS endpoint for whatever reason (eg discovery), they can be impacted by [API throttling].(http://docs.aws.amazon.com/AWSEC2/latest/APIReference/query-api-troubleshooting.html#api-request-rate).

We will [retry on a failure](https://github.com/elastic/elasticsearch-cloud-aws/blob/master/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java#L86), but if there are a large number of nodes trying to hit the API again, they can be throttled for a second time.
Rinse. Repeat.
This could mean that for larger number of nodes, they end up in a loop as they can never get a response from the API and never actually form the cluster.

It'd be great if we could add a jitter aspect to the retry so the nodes don't all end up retrying at the same time.
</description><key id="91983369">11934</key><summary>Add jitter in AWS API retries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Plugin Cloud AWS</label><label>adoptme</label><label>feature</label></labels><created>2015-06-30T05:04:58Z</created><updated>2016-07-29T08:37:44Z</updated><resolved>2015-08-03T16:26:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-07-01T20:15:31Z" id="117812454">I don't think we need to add this jitter logic. The exponential backoff policy with max 3 times retry is already in default retry logic, which is the default configuration for S3 client and EC2 client.
</comment><comment author="clintongormley" created="2015-07-10T10:07:55Z" id="120340031">Adding a bit of randomization is pretty easy, and certainly won't hurt. I think we should do it
</comment><comment author="xuzha" created="2015-07-24T06:42:20Z" id="124353985">Do you mean we add extra randomization and plus exponential back-off latency? 

Please correct me if I'm wrong, I'm pretty to new to this. From my understanding, for right now we are using default exponential backoff policy, which is not configurable for our aws plugin users. But for S3 client, we make max number of retries configurable, and we also pass this number into our [S3 client](https://github.com/elastic/elasticsearch/blob/4c981ff4bfc250080d521af105b5e8589c9fc517/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L126) when we creating it ( I don't think we should do it )

So for example, if user put 3 for max number of tries into the setting, aws java client will do 3 times retries for us (hopefully) . AND PLUS, during doing snapshot, we would do another 3 times retries at our side if we got retryable exception, which is 9 times retryies (3 exponential retries ) \* 3. 

I think we probably need make max number of retries configurable for our EC2 client. 
</comment><comment author="markwalkom" created="2015-07-24T06:44:31Z" id="124356914">We should add randomisation, yes.
</comment><comment author="clintongormley" created="2015-07-27T10:57:25Z" id="125165372">&gt; Do you mean we add extra randomization and plus exponential back-off latency?

The problem with the backoff is that all requests can be backed off and then retried at exactly the same time.  with a little randomization, it'll spread the retries out a bit.
</comment><comment author="jpountz" created="2015-08-03T16:26:31Z" id="127307109">Actually, there is already some randomization happening with the default client configuration, see https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedRetryPolicies.java#L232
</comment><comment author="petemounce" created="2016-07-29T08:37:44Z" id="236125727">https://www.awsarchitectureblog.com/2015/03/backoff.html is an interesting analysis on different retry strategies.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail engine without marking it as corrupt when recovering on SharedFS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11933</link><project id="" key="" /><description>Currently when an engine is failed, it is marked as corrupted regardless of
the failure type. This change marks the engine as corrupted only when the failure
is caused by an actual index corruption. When an engine is failed for other
reasons, the engine is only closed without removing the shard state.

closes #11788
</description><key id="91971084">11933</key><summary>Fail engine without marking it as corrupt when recovering on SharedFS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Shadow Replicas</label><label>bug</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T03:55:25Z</created><updated>2015-07-07T21:47:53Z</updated><resolved>2015-07-07T21:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-30T07:29:04Z" id="117036683">LGTM
</comment><comment author="bleskes" created="2015-06-30T11:46:59Z" id="117143449">I went through this and I think things are a bit confusing. We now sometimes leave a corruption marker  the engine's index.engine.Engine#failEngine and we sometimes do it in .IndexShard.ShardEngineFailListener#onFailedEngine (which does not override the previous one, but you have to check store.markStoreCorrupted() to know that). 

Also the listener signature is a bit confusing when used out side of the indexShard (do we need to mark it as corrupted in the IndicesClusterService class or not?) :

```
onFailedEngine(ShardId shardId, String reason, boolean markAsCorrupted, @Nullable Throwable failure)
```

I suggest the following:

Change `index.engine.Engine#failEngine(java.lang.String, boolean, java.lang.Throwable)` to always listen to it's markAsCorrupted parameter (don't introspect the exception type).  

Change `index.engine.Engine#failEngine(java.lang.String, java.lang.Throwable)` to introspect the throwable it got and set the markAsCorrupted flag accordingly when calling the failEngine overload.

Remove the logic to set the corruption marker in `index.shard.IndexShard.ShardEngineFailListener` listener and also the `markAsCorrupted` from the listener.
</comment><comment author="areek" created="2015-06-30T20:36:04Z" id="117334208">Updated PR: 
- `IndexShard/Engine#failEngine(String reason, Throwable failure)` marks the engine as corrupted
- `IndexShard/Engine#failEngine(String reason, boolean markAsCorrupted, Throwable failure)` allows optional corruption marking 
- `failEngine(String reason, boolean markAsCorrupted, Throwable failure)` is only used in `SharedFSRecoverySourceHandler`
</comment><comment author="bleskes" created="2015-07-01T09:18:18Z" id="117557296">left some minor comments... thx @areek 
</comment><comment author="areek" created="2015-07-01T17:41:20Z" id="117762427">Updated PR:
- incorporate feedback
- hitting OOM will not mark the engine as corrupt
</comment><comment author="bleskes" created="2015-07-02T08:30:46Z" id="117956354">LGTM!
</comment><comment author="kimchy" created="2015-07-03T15:52:48Z" id="118379742">I have been thinking about it, what happens for cases where we are running out of disk space and such? The shard is not corrupted in such cases. Also, the commit now is different than what is described, right? We now almost always mark the shard as corrupted for any failure?
</comment><comment author="s1monw" created="2015-07-06T19:11:07Z" id="118966364">&gt; I have been thinking about it, what happens for cases where we are running out of disk space and such? The shard is not corrupted in such cases. Also, the commit now is different than what is described, right? We now almost always mark the shard as corrupted for any failure?

so I think it's good practice to tombstone a failed engine in general. I guess it's more a naming question rather than if we should actually should do it. I guess we should rename `Store.markAsCorrupted` into `Store#markAsFailed`. We can extend this entire procedure an make the store marker a real structure and add a reason and if it's fatal. so I think it's the right thing todo.

regarding the out of disk problem I guess we should fallback to deleting the shard state in that case.
</comment><comment author="kimchy" created="2015-07-07T13:59:26Z" id="119210624">I like the idea of marking a shard as failed and keeping a tombstone for the reason of failure, not calling it corruption. I guess we will need to know if it was because of corruption or not, we can have it as a woolen flag on mark as failed so we can keep it around when we load it?

I wonder if we should really remove the shard state at all? if it is marked, it is marked as failed, and can be a reason of corruption. If someone is running with 0 replicas, and they run out of disk space, the shard should be able to recover without external intervention?
</comment><comment author="areek" created="2015-07-07T16:01:33Z" id="119248926">Why not mark shard as failed for all IOE and ISE thrown by the engine, except for "Out of disk space" exception? For OOM and "Out of disk space" exception, close the engine without removing the shard state, this way if there is any TransportException causing an engine failure (like recovery failure on SharedFS), it will just close the engine instead of marking it as failed. Thoughts?
</comment><comment author="s1monw" created="2015-07-07T16:25:47Z" id="119260253">I think we should for now only mark the shard as fail (corrupted) if it is really corrupted. the problem of having a broken engine even after a simple IO exception is from the days where we had no checkpoints in translog etc. so I think we can simply remove the boolean entirely and only mark as corrupted if it's really corrupted. all the otehr cases it's fine to just fail the engine.
</comment><comment author="kimchy" created="2015-07-07T17:27:20Z" id="119275836">@s1monw +1, I think this change is great, just remove the boolean corrupt flag, and use the throwable to decide if we want to write a corruption marker or not. We can do the other mark failed suggestions in a different change.
</comment><comment author="areek" created="2015-07-07T17:46:09Z" id="119280805">Updated PR:
- mark engine as corrupted only when the index is corrupted
- never delete shard state in case of engine failure, close the engine instead
- close the engine if there is an "out of disk space" exception
</comment><comment author="bleskes" created="2015-07-07T18:21:22Z" id="119291010">LGTM. Thanks @areek 
</comment><comment author="areek" created="2015-07-07T21:47:52Z" id="119352234">merged to master (https://github.com/elastic/elasticsearch/commit/4849e762750766b5f2b51de7d10f9cb0bcb0a504) and 1.x (https://github.com/elastic/elasticsearch/commit/cfd8909676ab4a2be94e6c554df9d211b42db2fb)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail startup (and tests) on jar hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11932</link><project id="" key="" /><description>Note its imperfect, there is currently some intentional "hell" done by ES:
- specifying elasticsearch.jar twice, in order to override a joda-time class
- the overriding of said joda-time class
- some clashes with log4j/log4j-extras

But I removed all the jar hell in the test classpath and so on.

Closes #11926
</description><key id="91966927">11932</key><summary>Fail startup (and tests) on jar hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-30T03:18:33Z</created><updated>2016-01-25T07:25:06Z</updated><resolved>2015-06-30T04:58:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-30T03:26:02Z" id="116932884">On my machine, the scan takes 18-20ms on average: so the cost is negligible, it just scans the entry names from the zip...
</comment><comment author="rjernst" created="2015-06-30T04:18:19Z" id="116948643">LGTM
</comment><comment author="synhershko" created="2015-12-29T14:18:56Z" id="167798978">While I admire the effort and understand the drive behind this, this causes Elasticsearch hell for us on several projects due to issues that are beyond our control. This renders using the ES jar impossible for even the minimal use of TransportClient, and blocks us from fully moving to ES 2.x.

Here is a concrete example, one of many I'm afraid.

We are using Elasticsearch from a Storm bolt, and are using Maven as our build system. storm-core has a dependency in kryo, which in turn has some issues with shaded dependencies (https://github.com/EsotericSoftware/kryo/issues/189). This may or may not have been fixed there, but the latest storm-core still uses the affected version (http://mvnrepository.com/artifact/org.apache.storm/storm-core/0.10.0).

As a result, we are seeing the following:

```
class: com.esotericsoftware.reflectasm.ConstructorAccess
jar1: /home/dan/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar
jar2: /home/dan/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar
```

Due to:

```
[INFO] +- org.apache.storm:storm-core:jar:0.10.0:compile
[INFO] |  +- com.esotericsoftware.kryo:kryo:jar:2.21:compile
[INFO] |  |  +- com.esotericsoftware.reflectasm:reflectasm:jar:shaded:1.07:compile
[INFO] |  |  |  \- org.ow2.asm:asm:jar:4.0:compile
[INFO] |  |  \- com.esotericsoftware.minlog:minlog:jar:1.2:compile
```

And all we need the ES jar for is for TransportClient, not going to run a full blown server / cluster there. And tests (Unit and ITs).

So after fixing all the Jar Hell issues that are indeed under our control (&gt; 1 week worth of work), we are now stuck with Jar Hell issues that are beyond our control.

It appears you yourself experienced the same: https://github.com/elastic/elasticsearch/blob/c813d21ffb8a1fc10e8ad50314b5b53cb40e7aaa/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java#L274. So guess what, the list of exceptions to the rule need to be extended, and the least you could do is allow this. Otherwise, again, the Java API will just be useless by projects with large codebases.

I'm aware of @rmuir's stand on the topic, but this practically renders the ES Java API useless in any large scale project, exactly because the current Jar Hell check bites way too deep. It at least needs to ignore mismatches in shaded dependencies or the like in certain scenarios, which I agree needs to be well defined.

Please allow customisations of exclusions, or disabling Jar Hell altogether At Your Own Risk(TM) at least while running ES as a library (as opposed to as a server instance).
</comment><comment author="jpountz" created="2015-12-31T15:59:42Z" id="168214384">&gt;  I don't really care about your personal fucking opinion
&gt; Yes users are absolute idiots

Can we please avoid such aggressivity and only discuss facts and technical arguments.
</comment><comment author="rjernst" created="2015-12-31T19:20:40Z" id="168236814">Running ES either embedded, or as a java client does not run the jarhell check. The issue here is about tests. But testing transport client with an in memory ES is not a good test! If you want to test your use of transport client, run a real integration test, where ES is started in a separate process. That is the only real test, anything else is a fantasyland which does not actually confirm that the system works as expected. And regarding unit tests, you are free to not use the ES test framework. It is intended to test ES components, where jarhell is not allowed.
</comment><comment author="mikemccand" created="2016-01-01T08:53:41Z" id="168294349">Please keep the discussions here civil and technical!  Don't attack the people raising ideas or asking questions; answer with technical counter arguments instead.
</comment><comment author="s1monw" created="2016-01-04T13:55:14Z" id="168682091">@synhershko thanks for bringing this up, I think lots of users might run into problems with our strict checks and I this we should clarify why we are doing the checks and who they are supposed to work.

First lemme clarify your statement:

&gt;  This renders using the ES jar impossible for even the minimal use of TransportClient, and blocks us from fully moving to ES 2.x.

as @rjernst already mentioned this is not true. The JarHell checks are run doing Bootstrap which is run in 2 ways:
- if ES is started with the commandline as a service
- if the `ESTestCase` is  initialized in a static block. 

if you run ES embedded or as a transport client this will not be executed. Hence you statement is not correct and will cause a lot of confusion.

If you use the ES test framework you will run into the JarHell checks and that is unfortunate. I think we might be able to help here and maybe move the checks under a special flag only for tests where you can disable the JarHell check for testing in `BootstrapForTesting` with a system property but not in the actual `Bootstrap` that we use to run ES. I can totally see this as a compromise. A switch that allows you to not run without the jarhell check in production is on my end not considered as an option since we have to maintain strictness on the server end. If your project can't use the test framework I think we can help with a system property but for the production case that's absolutely required. I still want to run this idea passed @rmuir and others since rob has put a lot of effort into locking the daemon down to pave the way for a painless future. 

@synhershko please lemme know if that would help you along the lines... 
</comment><comment author="synhershko" created="2016-01-04T16:59:59Z" id="168734290">Thanks @s1monw.

I stand corrected with regards to the usage of TransportClient. The blocker for us was building unit tests and integration tests with the excellent ES test suite. We have a rather complex system and ES is being used in various parts of it. We want to be able to both unit- and integration- test those components, and creating a test-local cluster is the only way we could do this efficiently, mocking an ES cluster with test data. Maintaining a test cluster is not an option in that case, and we are fully aware of the consequences of running in a non-clean environment. For us they are negligible, given our use cases.

We spent about 2 weeks cleaning up jar hell issues, and ended up complaining about it because we hit walls due to misconfigurations of dependencies which are out of our reach. Since then we were able to bypass Jar Hell with a nice and elegant hack, without forking.

Your suggestion is exactly what we were after, and I think I explicitly stated that above. I never wanted to discard strictness for service / server mode, only for test scope in clients due to various circumstances which are beyond our control. Quick googling on this will show we are not the only ones with this kind of issues.

I hope this clarifies things further, and thank you for chiming in.
</comment><comment author="rjernst" created="2016-01-04T18:47:49Z" id="168765075">@synhershko See the [client qa test](https://github.com/elastic/elasticsearch/tree/2.x/qa/smoke-test-client) for an example of how to create a real integration test. This would be much better than using ESIntegTestCase, as it will test against a real ES node, not an in memory one that could be mucked up by jarhell. What does ESTestCase get you that you need for your own unit tests, that you would not get by using either junit directly, or LuceneTestCase?
</comment><comment author="synhershko" created="2016-01-05T10:12:31Z" id="168963123">@rjernst like I said, maintaining an external cluster for tests is an overkill for us. A test-scope cluster is exactly what we need, and if we get to a point where we see failures that we can attribute to jar hell or the like we will revisit this decision. I've been doing this for a while and it hasn't happened yet.

ESSmokeClientTestCase gives me nothing that I can't achieve with ESIntegTestCase with less headaches. LuceneTestCase is irrelevant for our purposes.
</comment><comment author="s1monw" created="2016-01-05T15:03:02Z" id="169026516">&gt; @rjernst like I said, maintaining an external cluster for tests is an overkill for us. A test-scope cluster is exactly what we need, and if we get to a point where we see failures that we can attribute to jar hell or the like we will revisit this decision. I've been doing this for a while and it hasn't happened yet.

@synhershko either way we might even go and make that mandatory in upcoming version since it is the right way to test the integration of elasticsearch. Now that in 3.0 you can reset cluster settings there is no real need for Test scope in 99% of the cases so in upcoming version this might be the way to go. For instance all our REST tests run against such a cluster, they are way more reliable than all other tests.
</comment><comment author="synhershko" created="2016-01-05T16:52:49Z" id="169059195">@s1monw I'd really appreciate if you wouldn't go that way. Having test-scoped, in-memory clusters is perfect for various tests which make light use of Elasticsearch and therefore aren't exposed (or don't care) about the various failures you are afraid of. This is exactly what we are using it for, and would really like to see this functionality stay as it greatly simplifies and speeds-up our testing infrastructure. Not many databases / data-stores provide volatile versions of themselves for testing purposes, and those which do are much easier to work with in dev environments.
</comment><comment author="rjernst" created="2016-01-05T21:07:55Z" id="169134304">&gt; I'd really appreciate if you wouldn't go that way. Having test-scoped, in-memory clusters is perfect for various tests which make light use of Elasticsearch and therefore aren't exposed (or don't care) about the various failures you are afraid of.

We have been gradually moving away from this for a reason. It doesn't really test integration, it is really just a complicated unit test. It is much better to have real unit tests (which are actually light weight, ESIntegTestCase is so far from light weight it is not funny), and test integration with a real node. The difference in time between an in memory cluster for tests, and actually spinning up an ES node (which the infrastructure provides in both maven for 2.x and gradle for master), isn't that much, and the latter _actually_ checks it works against elasticsearch.
</comment><comment author="s1monw" created="2016-01-17T18:52:45Z" id="172364980">@synhershko here is a workaround for tests https://github.com/elastic/elasticsearch/pull/16042
</comment><comment author="synhershko" created="2016-01-17T18:54:35Z" id="172365188">Looks good, thanks @s1monw . I did some homework wrt to the tests framework the past week, I'll post about it later today or tomorrow.
</comment><comment author="synhershko" created="2016-01-25T07:25:06Z" id="174425117">Took longer than than expected, but here is my take on testing with Elasticsearch.

There aren't many technologies which provide a good testing framework bundled with the tech, and even fewer data store technologies which provide in-memory testing capabilities. But those which do provide this, are by far easier to get things right with.

Worth noting that the advice given in the discussion here is different than what's advertised in [the docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/integration-tests.html). With regard to the [ESSmokeClientTestCase class](https://github.com/elastic/elasticsearch/blob/2.x/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java), it is already implemented as [ExternalTestCluster](https://github.com/elastic/elasticsearch/blob/master/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java) in the aforementioned Java Testing Framework. And then there is also the InternalTestCluster implementation which spins an in-memory, test-scoped cluster for a test to run with.

As a long time user and an Elasticsearch consultant I've seen pretty much every use case you can imagine, and at the end of the day when it comes to testing there are those systems with those Units that can only be really tested when they are paired with Elasticsearch. And for the cases where you are actually running on the JVM, you are particularly lucky.

Think about the Storm use case, where somewhere in the topology there are bolts that issue a query (we are not using es-hadoop). The only real way of testing those bolts and their logic is to have a contained Elasticsearch cluster, with contained index, issue the query based on the inputs, and compare the results. This is a Unit Test per se, one that can only be conducted on a clean index with a known set of documents, which is presumably populated at the beginning of each test method / class.

Now, sure, one could spin his own cluster somewhere and run those tests each against it's own index. But as everyone who had to deal with CI and deployments knows - infrastructure is a bitch. We all want to avoid work there. Every test will have to run on at least 2 different environments (dev local, and CI, and maybe via production probes etc) and requiring an Elasticsearch cluster just for testing on each is an overkill. Ideally, each test should be able to run it's own ES cluster if needed, at least as a fallback, so at least the local dev's experience is flawless. This is uber important in large teams, where many technologies are in use.

There's also questions of security and costs involved.

This isn't about testing ES plugins or cluster configurations, and I couldn't care less about performance or the like. Most of the time it's also not about integration tests. It is just about having (even a minimal) version of Elasticsearch available, with the least headache possible, to perform what we call Unit Tests, but ones that mean nothing without Elasticsearch playing along for basic functionality. For those cases, shard allocation, performance etc aren't a concern.

To facilitate that, we ended up doing the following:
1. In our testing framework (which happens to be testing our classes and storm topologies we built with it) we took dependency of elasticsearch and the integration tests package.
2. An abstract class `BaseTestWithElasticsearch` uses as our base class for unit tests requiring Elasticsearch. The `initialiseElasticsearch()` and `shutdownElasticsearch()` are executed at the beginning and end of each test suite / class using the runner's attributes (historically we use TestNG, so `@BeforeClass` and `@AfterClass`). Using TestNG wasn't possible before because of the randomizedtesting framework used internally by ES testing.
3. Initialising Elasticsearch is done by first trying to figure out if there is a cluster available (mostly via configs and env vars) and if there isn't, we initialise an instance of `InternalTestCluster`. Since the original class has a lot of mocks and randomised testing stuff that we don't really need, we copied it over and stripped it of that complexity. Wasn't too hard to do.
4. If an external, real ES cluster is indeed available, we initialise an instance of `ExternalTestCluster` with the URLs to initialise TransportClients with. We use the original class in that case.
5. Since for some of our internal tests we need to inherit from another class to facilitate Storm topology testing etc, we also enable using `BaseTestWithElasticsearch` via composition. That is not currently possible via the ES testing infrastructure and provides great aid for our infrastructure.
6. The `BaseTestWithElasticsearch` class is where we have all the REALLY USEFUL syntactic sugar around indexing, index creation, maintenance etc. Save lots of code when writing tests and populating initial test data, or validating state.

Wherever we need to test cluster configurations, real integration etc, we obviously use real cluster deployments. This isn't the case in many of our tests, and exactly why we could use such support from the official ES project. As it's already there, it might be worth knowing that there are people who find it useful before deciding to remove it or bluntly declaring that if you use it you are doing something wrong.

We will work on extracting this from our code base and open-sourcing it soon in case someone else finds it useful.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/JarHell.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorTests.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file><file>core/src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java</file><file>core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>core/src/test/java/org/elasticsearch/test/XContentTestUtils.java</file><file>core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportTests.java</file></files><comments><comment>Merge pull request #11932 from rmuir/jarhell</comment></comments></commit></commits></item><item><title>(::regression from ES 1.3.5 ) Elasticsearch 1.6.0 - "unmapped_type"/"ignore_unmapped" does not work for Sorting with nested filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11931</link><project id="" key="" /><description>"unmapped_type"/"ignore_unmapped" does not work when used with sorting with nested filter. This example will make it clear:

Create two indexes - **testindex1** and **testindex2**:

``` json
curl -XPUT localhost:9200/testindex1 -d '{"mappings":{"type1":{"properties":{"firstname":{"type":"string"},"servers":{"type":"nested","properties":{"name":{"type":"string"},"location":{"type":"nested","properties":{"name":{"type":"string"}}}}}}}}}'

curl -XPUT localhost:9200/testindex2 -d '{"mappings":{"type1":{"properties":{"firstname":{"type":"string"},"computers":{"type":"nested","properties":{"name":{"type":"string"},"location":{"type":"nested","properties":{"name":{"type":"string"}}}}}}}}}'
```

The only difference between these two indexes is - **testindex1** has "server" field and **textindex2** has "computers" field. 
Now let's insert test data in both the indexes:

``` json
curl -XPUT localhost:9200/testindex1/type1/1 -d '{"firstname":"servertom","servers":[{"name":"server1","location":[{"name":"location1"},{"name":"location2"}]},{"name":"server2","location":[{"name":"location1"}]}]}'
curl -XPUT localhost:9200/testindex1/type1/2 -d '{"firstname":"serverjerry","servers":[{"name":"server2","location":[{"name":"location5"}]}]}'
```

``` json
curl -XPUT localhost:9200/testindex2/type1/1 -d '{"firstname":"computertom","computers":[{"name":"computer1","location":[{"name":"location1"},{"name":"location2"}]},{"name":"computer2","location":[{"name":"location1"}]}]}'
curl -XPUT localhost:9200/testindex2/type1/2 -d '{"firstname":"computerjerry","computers":[{"name":"computer2","location":[{"name":"location5"}]}]}'
```

Case 1 - Regular sort (without nested filter)  + "unmapped_type"/"ignore_unmapped" works fine. No issues here. This can be seen by following queries:

``` json
curl -XPOST 'localhost:9200/testindex2/_search?pretty' -d '{"fields":["firstname"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","unmapped_type":"string"}}]}'
curl -XPOST 'localhost:9200/testindex2/_search?pretty' -d '{"fields":["firstname"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","ignore_unmapped": "true"}}]}'
```

Both the queries above run on **"testindex2"** which does not have "servers.location.name" field.  Both the queries run fine as we have used - "unmapped_type"/"ignore_unmapped", however sort field has some integer value which is not expected:

``` json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : null,
    "hits" : [ {
      "_index" : "testindex2",
      "_type" : "type1",
      "_id" : "1",
      "_score" : null,
      "fields" : {
        "firstname" : [ "tom" ]
      },
      "sort" : [ -9223372036854775808 ]
    }, {
      "_index" : "testindex2",
      "_type" : "type1",
      "_id" : "2",
      "_score" : null,
      "fields" : {
        "firstname" : [ "tom" ]
      },
      "sort" : [ -9223372036854775808 ]
    } ]
  }
}
```

Case 2 - Nested filter sort on **testindex1** works fine as  **"servers.location.name"** field is present in this index:

``` json
curl -XPOST 'localhost:9200/testindex1/_search?pretty' -d '{"fields":["firstname","servers.location.name"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","unmapped_type":"string","nested_filter":{"term":{"servers.location.name":"location5"}},"nested_path":"servers"}}]}'
curl -XPOST 'localhost:9200/testindex1/_search?pretty' -d '{"fields":["firstname","servers.location.name"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","ignore_unmapped": "true","nested_filter":{"term":{"servers.location.name":"location5"}},"nested_path":"servers"}}]}'
```

Case 3 - Nested filter sort on **testindex2** doesn't work even if "unmapped_type"/"ignore_unmapped" is used. Following examples throw error:

``` json
curl -XPOST 'localhost:9200/testindex2/_search?pretty' -d '{"fields":["firstname","servers.location.name"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","unmapped_type":"string","nested_filter":{"term":{"servers.location.name":"location5"}},"nested_path":"servers"}}]}'
curl -XPOST 'localhost:9200/testindex2/_search?pretty' -d '{"fields":["firstname","servers.location.name"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","ignore_unmapped": "true","nested_filter":{"term":{"servers.location.name":"location5"}},"nested_path":"servers"}}]}'
```

Surprisingly, Issue with "ignore_unmapped" example in Case 3 is not observed in older version of elasticsearch - 1.1.0 and the query works fine. 
</description><key id="91935509">11931</key><summary>(::regression from ES 1.3.5 ) Elasticsearch 1.6.0 - "unmapped_type"/"ignore_unmapped" does not work for Sorting with nested filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">navneet83</reporter><labels /><created>2015-06-29T23:36:11Z</created><updated>2015-06-30T11:25:58Z</updated><resolved>2015-06-30T11:25:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="navneet83" created="2015-06-30T06:19:33Z" id="117004963">This issue is reproducible in ES 1.4, 1.5 and 1.6.

In ES 1.3.5, "unmapped_type" is not supported. I tried with "ignore_unmapped" examples above. Surprisingly second query in Case3 above works fine in ES 1.3.5. So this is definitely a case of regression. Something broke between 1.3.5 and 1.4.
</comment><comment author="navneet83" created="2015-06-30T06:46:52Z" id="117020878">Exception : 
[2015-06-29 23:18:46,442][DEBUG][action.search.type       ] [Smuggler] All shards failed for phase: [query]
org.elasticsearch.search.SearchParseException: [testindex2][4]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{"fields":["firstname","servers.location.name"],"query":{"match_all":{}},"sort":[{"servers.location.name":{"order":"desc","ignore_unmapped": "true","nested_filter":{"term":{"servers.location.name":"location5"}},"nested_path":"servers"}}]}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:735)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:560)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:532)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:294)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.query.QueryParsingException: [testindex2] [nested] failed to find nested object under path [servers]
    at org.elasticsearch.index.query.support.NestedInnerQueryParseSupport.setPath(NestedInnerQueryParseSupport.java:163)
    at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:173)
    at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:86)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:719)
    ... 9 more
</comment><comment author="clintongormley" created="2015-06-30T11:25:57Z" id="117132130">Hi @navneet83 

Thanks for the detailed reproduction!

&gt; Both the queries above run on "testindex2" which does not have "servers.location.name" field. Both the queries run fine as we have used - "unmapped_type"/"ignore_unmapped", however sort field has some integer value which is not expected

`ignore_unmapped` is deprecated in favour of `unmapped_type` for this very reason: we need to know the datatype of the sort value up front.  Also see #9155 

&gt; Case 3 - Nested filter sort on testindex2 doesn't work even if "unmapped_type"/"ignore_unmapped" is used. Following examples throw error:

Yes, this is a known issue with eg nested, parent-child, geo, function_score, which require the field to be mapped.  We're discussing what approach to take to solve this in #11806 

I think both issues are already covered in other tickets, so I'm going to close this one.  Thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parse aliases at search time and never cache parsed alias filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11930</link><project id="" key="" /><description>This cleans up the alias code. The time spend on alias filter parsing is significantly lower compared to the execution time of a search request. Pre parsing alias filters and caching the parsed version at alias creation time isn't worth the effort (code complexity and memory).

Also:
- This allows parent/child queries to be specified again in alias filters.
- The work around for resolving `now` in queries doesn't need to be used for aliases, becuase alias filters are parsed at search time. However it can't be removed, because the percolator relies on it.

PR for #10485
</description><key id="91933213">11930</key><summary>Parse aliases at search time and never cache parsed alias filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aliases</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T23:18:21Z</created><updated>2015-07-01T19:21:53Z</updated><resolved>2015-07-01T19:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T10:59:23Z" id="117125331">@jpountz please could you review
</comment><comment author="jpountz" created="2015-06-30T12:00:18Z" id="117150251">Just discussed it with @martijnvg. With this pull request, we make use of two hacks in order to make sure that the parsed filter is what we expect:
- new flags on QueryParseContext
- build a first "dummy" query that only rewrites to the correct query

I like parsing aliases filters as late as possible, but I'm not happy with having both these hacks. It seems to me that we could make things work without adding more methods to QueryParseContext and just use the same rewrite hack that we use for date range queries.
</comment><comment author="martijnvg" created="2015-06-30T16:39:15Z" id="117249264">@jpountz I updated the PR and used the dummy query hack in all places where last minute query parsing needs to happen.
</comment><comment author="jpountz" created="2015-06-30T17:18:11Z" id="117265219">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The query parse context should be fetched from the IndexQueryParseService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11929</link><project id="" key="" /><description>instead of maintaining a thread local cache in the PercolatorQueriesRegistry.

Before PercolatorQueriesRegistry had its own cache, because all the queries had to forcefully opt out of caching. Nowadays in master small segments are never cached by the query cache, so the reason for the dedicated cache is no longer valid.
</description><key id="91926093">11929</key><summary>The query parse context should be fetched from the IndexQueryParseService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T22:33:18Z</created><updated>2015-06-30T11:39:39Z</updated><resolved>2015-06-30T09:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-29T23:33:09Z" id="116879128">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file></files><comments><comment>Merge pull request #11929 from martijnvg/percolator/drop__own_qpc_cache</comment></comments></commit></commits></item><item><title>Create Snapshot: remove _create from POST path to match PUT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11928</link><project id="" key="" /><description>As discussed. This fixes #11897.
</description><key id="91902478">11928</key><summary>Create Snapshot: remove _create from POST path to match PUT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">szroland</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T20:31:28Z</created><updated>2015-06-30T10:16:23Z</updated><resolved>2015-06-30T10:16:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T09:58:48Z" id="117104425">@bleskes please take a look
</comment><comment author="bleskes" created="2015-06-30T10:15:57Z" id="117113203">LGTM. I'll merge this in. I'll pull this into 1.x &amp; 1.6 (but leave the _create end point) so we can fix #11876 
</comment><comment author="bleskes" created="2015-06-30T10:16:08Z" id="117113243">and of course, many thanks @szroland 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file></files><comments><comment>Merge pull request #11928 from szroland/#11897</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file></files><comments><comment>Merge pull request #11928 from szroland/#11897</comment></comments></commit></commits></item><item><title>Build: If SHA files have changed, explain how to update them in the license check exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11927</link><project id="" key="" /><description>If SHA files have changed or been added or removed (ie the dependencies have changed), the 
license check utility now throws an exception explaining what command to run in order to update
the list of SHA files.
</description><key id="91886032">11927</key><summary>Build: If SHA files have changed, explain how to update them in the license check exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T19:22:09Z</created><updated>2015-08-13T14:28:19Z</updated><resolved>2015-06-30T09:29:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-29T19:59:00Z" id="116818998">LGTM. I will leave disgust with perl comments for another issue. ;)
</comment><comment author="s1monw" created="2015-06-29T20:12:15Z" id="116826152">LGTM too thanks @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11927 from clintongormley/license_check_usage</comment></comments></commit></commits></item><item><title>fail startup on jar hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11926</link><project id="" key="" /><description>After plugins have been loaded on initial startup, the classpath is complete, and we should inspect for jar hell and fail with a good error message if it exists. Otherwise behavior will be "undefined" with confusing error messages to users.

I will try to work up a prototype.
</description><key id="91879635">11926</key><summary>fail startup on jar hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T18:49:04Z</created><updated>2015-08-13T11:44:52Z</updated><resolved>2015-08-13T11:44:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-29T19:26:22Z" id="116806248">++
</comment><comment author="clintongormley" created="2015-08-13T11:44:52Z" id="130635218">Closed by #11932
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve repository verification failure message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11925</link><project id="" key="" /><description>Closes #11922
</description><key id="91876217">11925</key><summary>Improve repository verification failure message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T18:33:43Z</created><updated>2015-07-01T01:17:43Z</updated><resolved>2015-07-01T01:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-29T18:57:06Z" id="116796201">Left one minor comment. LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch 2.0 joins 1.6 cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11924</link><project id="" key="" /><description>Start a node with 1.6.0, then a node from master.

Check the cluster health on master - it reports two nodes.
</description><key id="91873840">11924</key><summary>Elasticsearch 2.0 joins 1.6 cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Discovery</label><label>blocker</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T18:25:04Z</created><updated>2015-07-02T18:53:18Z</updated><resolved>2015-07-02T18:53:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-06-29T18:25:37Z" id="116785279">That's unlikely to work well....
</comment><comment author="martijnvg" created="2015-06-30T13:41:38Z" id="117188073">I tried to reproduce this, but I couldn't get the 1.6 node to join the node from master. Maybe it just happened based on a particular commit? Regardless of this, it still feels good to have have validation during pinging that ignores ping responses (and log a warning) of nodes with a version lower than the minimum supported version. (in the case that ping requests/responses happen to serialize successfully between versions that shouldn't be compatible )
</comment><comment author="martijnvg" created="2015-07-01T08:35:21Z" id="117538717">I managed to reproduce this with unicast discovery.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTest.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPingTests.java</file></files><comments><comment>zen: Don't join master nodes or accept join requests of old and too new nodes.</comment></comments></commit></commits></item><item><title>NoSuchNodeException during startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11923</link><project id="" key="" /><description>When adding a new node to the cluster, master throws a series of NoSuchNodeException exceptions until the new node is ready:

```
[2015-06-29 20:14:27,958][WARN ][gateway                  ] [foo] [t][3]: failed to list shard for shard_store on node [g51KTc9wQZWlLiwiYFkcgg]
FailedNodeException[Failed node [g51KTc9wQZWlLiwiYFkcgg]]; nested: NoSuchNodeException[No such node [g51KTc9wQZWlLiwiYFkcgg]];
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:179)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:131)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$100(TransportNodesAction.java:91)
    at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:65)
    at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:42)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.list(TransportNodesListShardStoreMetaData.java:82)
    at org.elasticsearch.gateway.AsyncShardFetch.asyncFetch(AsyncShardFetch.java:267)
    at org.elasticsearch.gateway.AsyncShardFetch.fetchData(AsyncShardFetch.java:117)
    at org.elasticsearch.gateway.GatewayAllocator.allocateUnassigned(GatewayAllocator.java:406)
    at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:72)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:179)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:159)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:145)
    at org.elasticsearch.discovery.zen.ZenDiscovery$11.execute(ZenDiscovery.java:937)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: NoSuchNodeException[No such node [g51KTc9wQZWlLiwiYFkcgg]]
    ... 20 more
```
</description><key id="91871582">11923</key><summary>NoSuchNodeException during startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Cluster</label><label>adoptme</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T18:16:17Z</created><updated>2015-06-30T20:20:10Z</updated><resolved>2015-06-30T20:20:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-29T21:30:29Z" id="116851548">@kimchy can you take a look at this?
</comment><comment author="kimchy" created="2015-06-29T21:42:46Z" id="116856088">I think I know what happens, now that we reroute within the same cluster state when we add nodes, it means that they will be part of the cluster state being built. When we go and list the started shards, we use the existing cluster state that hasn't yet been updated to find the relevant nodes, and they will not be there since they are just being added... .
</comment><comment author="bleskes" created="2015-06-30T20:20:04Z" id="117329709">closed with #11960 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file></files><comments><comment>Discovery: reroute after node join is processed</comment></comments></commit></commits></item><item><title>Failed repository verification message can be confusing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11922</link><project id="" key="" /><description>During repository verification the master node writes a single test file into repository and then all data nodes try to read this file back. If this operation fails, the error message `store location [......] is not shared between node [the data node name] and the master node]`. The message can be very confusing in situation when there is only one node in the cluster and the error is caused by wrong S3 permissions. 
</description><key id="91866992">11922</key><summary>Failed repository verification message can be confusing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2015-06-29T17:53:37Z</created><updated>2015-07-01T01:17:42Z</updated><resolved>2015-07-01T01:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file></files><comments><comment>Improve repository verification failure message</comment></comments></commit></commits></item><item><title>Docs: Documented delayed allocation settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11921</link><project id="" key="" /><description>Relates to: #11712
</description><key id="91866126">11921</key><summary>Docs: Documented delayed allocation settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T17:48:59Z</created><updated>2015-06-30T12:03:52Z</updated><resolved>2015-06-30T11:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-29T17:49:07Z" id="116773919">@kimchy please review
</comment><comment author="clintongormley" created="2015-06-30T11:54:19Z" id="117146078">Updated to address comments. Merging to master
</comment><comment author="clintongormley" created="2015-06-30T12:03:52Z" id="117151840">Backported to 1.x in https://github.com/elastic/elasticsearch/commit/985f8ea7a9bdf8b39c47a81759041df593d30fc3
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11921 from clintongormley/delayed_alloc_docs</comment></comments></commit></commits></item><item><title>[Docs] Explain Error Handling in TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11920</link><project id="" key="" /><description>The handling of the internal transport connection needs to be clarified in the documentation while using the `TransportClient`. Specifically, it would be good to document what happens to connections:
- During the initial connection.
  - Connections are tried in parallel.
- When sniffing the connection.
  - The cluster state is fetched to find connections.
  - Does this happen more than once (e.g., to catch new nodes or nodes that were down)?
- When a connection fails.
  - There is an active/inactive list controlled by `client.transport.nodes_sampler_interval` (`5s` by default).
  - What does this mean for sniffed nodes?
- When a request fails.
  - If the connection fails, then it will automatically try the next node.
</description><key id="91854804">11920</key><summary>[Docs] Explain Error Handling in TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>adoptme</label><label>docs</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-06-29T16:48:07Z</created><updated>2015-12-02T22:29:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T09:54:08Z" id="117103400">Agreed - want to send a PR?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Slowlog does NOT log timeouts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11919</link><project id="" key="" /><description>We are getting multiple timeouts on our ElasticSearch server, but it looks like these timeouts are NOT reported in slowlog.
For example, on 2015-06-27 we got 223 timeout errors.
Slowlog shows only 58 records on that day.
It looks like none of these records match with each other (we used time of log and content of the query to try to match slowlog records with timeout reports from our client code).

Every timeout is about 100s.
Slowlog limits are set between 1 and 10 seconds, so in theory

Here's slowlog config settings section from elasticsearch.yml

```
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.fetch.warn: 1s
index.indexing.slowlog.threshold.index.warn: 10s
monitor.jvm.gc.young.warn: 1000ms
monitor.jvm.gc.old.warn: 10s
```

We are using ElasticSearch 1.6.0 on Windows Server 2012 R2 (Microsoft Windows [Version 6.3.9600]).
We set mlockall to true (preventing any Elasticsearch memory from being swapped out).

Is slowlog designed not to log queries that timed out?

Update:
Created separate "ElasticSearch memory leaks" issue: https://github.com/elastic/elasticsearch/issues/12255
</description><key id="91853359">11919</key><summary>Slowlog does NOT log timeouts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dennisgorelik</reporter><labels><label>feedback_needed</label></labels><created>2015-06-29T16:41:52Z</created><updated>2016-03-30T08:20:47Z</updated><resolved>2015-08-18T11:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-30T09:53:26Z" id="117103033">I don't know what is causing your timeouts, but the slowlog does record slow events, eg (you need to pass `--script.inline on` on the command line for this to work):

```
DELETE t

PUT t
{
  "settings": {
    "index.search.slowlog.threshold.query.warn": "1s"
  }
}

PUT t/t/1
{}

GET _search
{
  "script_fields": {
    "FIELD": {
      "script": "sleep(3000); return 'foo'"
    }
  }
}
```

The above request will show up every time you run it, as will this one:

```
GET _search
{
  "aggs": {
    "foo": {
      "terms": {
        "script": "sleep(3000); return 'foo'"  
      }
    }
  }
}
```

And why are your queries taking 100s?  Are you requesting 10M docs? Something else is going on.
</comment><comment author="dennisgorelik" created="2015-07-06T05:11:57Z" id="118721560">Here's some side story that would probably involve into separate "ElasticSearch memory leaks" issue when we have more data:
7 days ago (June 29 2015) we moved our ElasticSearch database into Linux (CentOs 7) on 32GB machine dedicated to ElasticSearch.

Initially we got no timeouts, but after 5 days without ElasticSearch reboot we started to get timeouts.
It looks like number of timeouts grows ~exponentially with times since last reboot:

```
Days 1 ... 5: 0 (no) timeouts.
Day 6: 3 timeouts.
Day 7: 5 timeouts.
```

That pattern of growing number timeouts is the same as when we were running ElasticSearch on Windows box with 2 times less memory allocated to ElasticSearch (16GB in Windows vs 32GB on Linux).
Windows machine produced more timeout errors, but also clearly showed the trend of increasing number of crashes with time since last reboot.

Answering your question about query size:
None of these failing queries that we run on that ElasticSearch server exceed few kilobytes.
Normally they should execute in well below 1s (e.g. ~50ms).
But several times per day these queries time out at 100s (see above).
To put things in perspective, on our ElasticSearch server we are running about 200,000 ElasticSearch queries per day.

In the next comment I'm going to add some details about the key issue here: slowlog is not recording timeouts.
</comment><comment author="dennisgorelik" created="2015-07-06T05:46:50Z" id="118731935">@clintongormley

Now about slowlog not recording timeouts.
1) In my previous comment I described that we got 8 timeout errors on our new Linux ElasticSearch 1.6.0 box (3 timeouts on day 6 and 5 timeouts on day 7).

2) Slowlog recorded only 3 timeouts out of 8 (1 timeouts on day 6 and 2 timeouts on day 7).
Slowlog also recorded some other long-running queries which did not result in timeouts.

3) On our new Linux box we have the same slowlog settings that I already described in the main post here: from 1s (fetch) to 10s(query).

4) We ran your suggested query with artificial delay in fetch ("script_fields-FIELD-script").
Our slowlog recorded that slow query (as it should because our slowlog fetch trigger is set to 1s).

5) We tried to run up to 10 queries with artificial delay in parallel (10 async calls from our client).
Slowlog recorded all of them.

6) We tried to run up to 10 queries in parallel and set artificial delay to 105000ms, so we got 10 parallel timeouts.
Slowlog recorded all of them.

My only guess so far is that slowlog misses timeouts that happen due to poor query execution (as opposing to fetch).
But I do not know how to reliably reproduce poor ElasticSearch query execution (when slowlog fails).

Any idea how to make queries that ran slow once (as an exception due to poor ElasticSearch server performance) - ran slow consistently?
</comment><comment author="dennisgorelik" created="2015-07-07T06:07:22Z" id="119082625">As expected, today (2015-07-06) as we got more ElasticSearch timeouts than yesterday.
Number of ElasticSearch slowlog records: 22.
Number of timeouts recorded by our client code: 34.
Number of timeouts recorded by ElasticSearch slowlog: 1 (out of 22).
So pretty much all timeouts went unrecorded by slowlog.

About 2 minutes after getting timeouts we turned on BigDesk monitoring on ElasticSearch.
BigDesk did NOT show any lack of resources (Heap Mem ~6Gb, CPU used less than 20%, Search requests per second less than 100).

We are going to reboot our ElasticSearch server and see what would happen (probably no timeouts for about 5 days since reboot).
</comment><comment author="clintongormley" created="2015-07-15T11:06:54Z" id="121583581">These two lines make me think you're hiding problems:

```
monitor.jvm.gc.young.warn: 1000ms
monitor.jvm.gc.old.warn: 10s
```

Those are very slow times for GC.  I'd undset those lines and look for slow GC messages in your logs.  Also check `GET _nodes/process` to see if mlockall is actually being set properly or not.
</comment><comment author="dennisgorelik" created="2015-07-15T17:53:01Z" id="121692466">What does "undset" mean?
Are you suggesting to reduce slowlog warning threshholds?
What  values do you suggest?

Note that we do get slowlog records occasionally. They just do not match with our timeouts.
</comment><comment author="dennisgorelik" created="2015-07-16T05:13:19Z" id="121831373">I just checked: mlockall is set to true.

```
GET _nodes/process
```

```
{
   "cluster_name": "ccccccccc",
   "nodes": {
      "pNrRIW49SEanDr-0289lAQ": {
         "name": "Node1",
         "transport_address": "inet[/xxx.xxx.xxx.xxx:9300]",
         "host": "hhhhhhh",
         "ip": "xxx.xxx.xxx.xxx",
         "version": "1.6.0",
         "build": "cdd3ac4",
         "http_address": "inet[/xxx.xxx.xxx.xxx:9200]",
         "attributes": {
            "master": "true"
         },
         "process": {
            "refresh_interval_in_millis": 1000,
            "id": 8156,
            "max_file_descriptors": 65535,
            "mlockall": true
         }
      }
   }
}
```
</comment><comment author="clintongormley" created="2015-07-17T13:29:05Z" id="122274869">"undset" means "unset" :)  ie remove those lines and go back to the defaults. Let's see what shows up in your logs
</comment><comment author="dennisgorelik" created="2015-07-18T06:11:42Z" id="122489444">We commented out slowlog GC settings.
Here are log results we are getting (PjfSearch.log.2015-07-17)

```
[2015-07-17 13:40:08,119][INFO ][monitor.jvm              ] [Node1] [gc][young][18149][1529] duration [769ms], collections [1]/[1.3s], total [769ms]/[4.3m], memory [6.7gb]-&gt;[5.7gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[655.9kb]/[1.1gb]}{[survivor] [54.2mb]-&gt;[55mb]/[149.7mb]}{[old] [5.6gb]-&gt;[5.6gb]/[14.5gb]}
[2015-07-17 14:30:17,403][WARN ][monitor.jvm              ] [Node1] [gc][young][21149][1746] duration [1.2s], collections [1]/[1.7s], total [1.2s]/[5m], memory [8.1gb]-&gt;[7.1gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[595.5kb]/[1.1gb]}{[survivor] [53mb]-&gt;[61.6mb]/[149.7mb]}{[old] [7gb]-&gt;[7gb]/[14.5gb]}
[2015-07-17 14:30:40,864][WARN ][monitor.jvm              ] [Node1] [gc][young][21171][1749] duration [1.3s], collections [1]/[2.3s], total [1.3s]/[5m], memory [8gb]-&gt;[7.1gb]/[15.8gb], all_pools {[young] [980.1mb]-&gt;[642.7kb]/[1.1gb]}{[survivor] [60.9mb]-&gt;[60.8mb]/[149.7mb]}{[old] [7gb]-&gt;[7gb]/[14.5gb]}
[2015-07-17 14:35:01,755][INFO ][monitor.jvm              ] [Node1] [gc][young][21431][1770] duration [836ms], collections [1]/[1s], total [836ms]/[5.1m], memory [8.3gb]-&gt;[7.2gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[5.9mb]/[1.1gb]}{[survivor] [60mb]-&gt;[58.2mb]/[149.7mb]}{[old] [7.1gb]-&gt;[7.1gb]/[14.5gb]}
[2015-07-17 14:46:04,340][INFO ][monitor.jvm              ] [Node1] [gc][young][22090][1828] duration [793ms], collections [1]/[1s], total [793ms]/[5.4m], memory [8.7gb]-&gt;[7.6gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[59.5mb]/[1.1gb]}{[survivor] [67mb]-&gt;[74.5mb]/[149.7mb]}{[old] [7.5gb]-&gt;[7.5gb]/[14.5gb]}
[2015-07-17 14:46:14,349][INFO ][monitor.jvm              ] [Node1] [gc][young][22100][1829] duration [738ms], collections [1]/[1s], total [738ms]/[5.4m], memory [8.7gb]-&gt;[7.6gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[15.2mb]/[1.1gb]}{[survivor] [74.5mb]-&gt;[70.9mb]/[149.7mb]}{[old] [7.5gb]-&gt;[7.5gb]/[14.5gb]}
[2015-07-17 16:38:25,231][WARN ][monitor.jvm              ] [Node1] [gc][young][28784][2438] duration [1s], collections [1]/[1s], total [1s]/[8.6m], memory [6.9gb]-&gt;[5.7gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[608.2kb]/[1.1gb]}{[survivor] [68.9mb]-&gt;[66.2mb]/[149.7mb]}{[old] [5.6gb]-&gt;[5.6gb]/[14.5gb]}
[2015-07-17 16:39:16,276][INFO ][monitor.jvm              ] [Node1] [gc][young][28835][2441] duration [950ms], collections [1]/[1s], total [950ms]/[8.6m], memory [6.9gb]-&gt;[5.7gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[4.8mb]/[1.1gb]}{[survivor] [67.5mb]-&gt;[65.7mb]/[149.7mb]}{[old] [5.6gb]-&gt;[5.7gb]/[14.5gb]}
[2015-07-17 17:37:57,460][WARN ][monitor.jvm              ] [Node1] [gc][young][32344][2759] duration [1s], collections [1]/[1.6s], total [1s]/[9.6m], memory [8.7gb]-&gt;[7.6gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[1.1mb]/[1.1gb]}{[survivor] [48.7mb]-&gt;[49.6mb]/[149.7mb]}{[old] [7.5gb]-&gt;[7.5gb]/[14.5gb]}
[2015-07-17 17:43:03,431][INFO ][monitor.jvm              ] [Node1] [gc][young][32649][2787] duration [742ms], collections [1]/[1s], total [742ms]/[9.7m], memory [8.9gb]-&gt;[7.8gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[188.7kb]/[1.1gb]}{[survivor] [63.4mb]-&gt;[59.6mb]/[149.7mb]}{[old] [7.7gb]-&gt;[7.7gb]/[14.5gb]}
[2015-07-17 17:44:13,429][INFO ][monitor.jvm              ] [Node1] [gc][young][32718][2793] duration [878ms], collections [1]/[1.5s], total [878ms]/[9.8m], memory [8.8gb]-&gt;[7.8gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[1.9mb]/[1.1gb]}{[survivor] [64.5mb]-&gt;[66.7mb]/[149.7mb]}{[old] [7.7gb]-&gt;[7.7gb]/[14.5gb]}
[2015-07-17 18:33:36,104][WARN ][monitor.jvm              ] [Node1] [gc][young][35667][3047] duration [2.2s], collections [1]/[3.1s], total [2.2s]/[10.8m], memory [10.3gb]-&gt;[9.3gb]/[15.8gb], all_pools {[young] [1021.1mb]-&gt;[1mb]/[1.1gb]}{[survivor] [58.7mb]-&gt;[64.4mb]/[149.7mb]}{[old] [9.3gb]-&gt;[9.3gb]/[14.5gb]}
[2015-07-17 18:35:17,368][WARN ][monitor.jvm              ] [Node1] [gc][young][35766][3054] duration [2.1s], collections [1]/[2.9s], total [2.1s]/[10.9m], memory [10.4gb]-&gt;[9.4gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[599.7kb]/[1.1gb]}{[survivor] [62.2mb]-&gt;[64.4mb]/[149.7mb]}{[old] [9.3gb]-&gt;[9.3gb]/[14.5gb]}
[2015-07-17 19:34:45,083][INFO ][monitor.jvm              ] [Node1] [gc][young][39305][3324] duration [759ms], collections [1]/[1.5s], total [759ms]/[12.1m], memory [6.9gb]-&gt;[5.8gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[4.6mb]/[1.1gb]}{[survivor] [54.2mb]-&gt;[57.4mb]/[149.7mb]}{[old] [5.7gb]-&gt;[5.7gb]/[14.5gb]}
[2015-07-17 20:22:23,246][WARN ][monitor.jvm              ] [Node1] [gc][young][42154][3508] duration [2.1s], collections [1]/[2.7s], total [2.1s]/[12.7m], memory [7.9gb]-&gt;[6.9gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[5mb]/[1.1gb]}{[survivor] [64.8mb]-&gt;[67.6mb]/[149.7mb]}{[old] [6.8gb]-&gt;[6.9gb]/[14.5gb]}
[2015-07-17 20:24:38,625][WARN ][monitor.jvm              ] [Node1] [gc][young][42288][3520] duration [1.7s], collections [1]/[2s], total [1.7s]/[12.8m], memory [8.1gb]-&gt;[7gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[799kb]/[1.1gb]}{[survivor] [62.5mb]-&gt;[62.4mb]/[149.7mb]}{[old] [6.9gb]-&gt;[6.9gb]/[14.5gb]}
[2015-07-17 20:51:55,654][WARN ][monitor.jvm              ] [Node1] [gc][young][43918][3664] duration [1.4s], collections [1]/[1.9s], total [1.4s]/[13.3m], memory [8.8gb]-&gt;[7.8gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[678.2kb]/[1.1gb]}{[survivor] [62.7mb]-&gt;[55.6mb]/[149.7mb]}{[old] [7.8gb]-&gt;[7.8gb]/[14.5gb]}
[2015-07-17 20:57:28,428][WARN ][monitor.jvm              ] [Node1] [gc][young][44249][3692] duration [1s], collections [1]/[1.6s], total [1s]/[13.4m], memory [9.1gb]-&gt;[8gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[741.4kb]/[1.1gb]}{[survivor] [55.9mb]-&gt;[55mb]/[149.7mb]}{[old] [7.9gb]-&gt;[7.9gb]/[14.5gb]}
[2015-07-17 21:00:04,808][WARN ][monitor.jvm              ] [Node1] [gc][young][44404][3710] duration [1.2s], collections [1]/[1.4s], total [1.2s]/[13.5m], memory [9.2gb]-&gt;[8.1gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[682.4kb]/[1.1gb]}{[survivor] [58.7mb]-&gt;[54.4mb]/[149.7mb]}{[old] [8gb]-&gt;[8gb]/[14.5gb]}
[2015-07-17 21:04:16,142][WARN ][monitor.jvm              ] [Node1] [gc][young][44654][3715] duration [1s], collections [1]/[1.4s], total [1s]/[13.5m], memory [9.2gb]-&gt;[8.1gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[830.2kb]/[1.1gb]}{[survivor] [64.6mb]-&gt;[62.6mb]/[149.7mb]}{[old] [8.1gb]-&gt;[8.1gb]/[14.5gb]}
[2015-07-17 21:12:49,400][WARN ][monitor.jvm              ] [Node1] [gc][young][45164][3761] duration [1.2s], collections [1]/[2s], total [1.2s]/[13.7m], memory [9.5gb]-&gt;[8.4gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[2mb]/[1.1gb]}{[survivor] [66mb]-&gt;[64.7mb]/[149.7mb]}{[old] [8.3gb]-&gt;[8.4gb]/[14.5gb]}
[2015-07-17 23:40:27,883][INFO ][monitor.jvm              ] [Node1] [gc][young][53981][4480] duration [802ms], collections [1]/[1.5s], total [802ms]/[16.7m], memory [8.9gb]-&gt;[7.9gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[817.4kb]/[1.1gb]}{[survivor] [59.6mb]-&gt;[63.9mb]/[149.7mb]}{[old] [7.8gb]-&gt;[7.8gb]/[14.5gb]}
[2015-07-17 23:41:18,012][WARN ][monitor.jvm              ] [Node1] [gc][young][54030][4485] duration [1s], collections [1]/[2s], total [1s]/[16.7m], memory [8.9gb]-&gt;[7.9gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[4.3mb]/[1.1gb]}{[survivor] [65.1mb]-&gt;[64.5mb]/[149.7mb]}{[old] [7.8gb]-&gt;[7.8gb]/[14.5gb]}
```

Note that we already saw similar log entries (including sub-second entries) even before we commented out 1s threshold in slowlog GC settings.

We also do NOT experience timeouts now, because ElasticSearch service runs for less than 5 days (so timeout issues did not start yet).
</comment><comment author="clintongormley" created="2015-07-19T11:14:52Z" id="122649907">So, as I suspected, you have very long GCs.  GC time may not counted as part of query execution time as it can happen in between phases, hence the missing slowlog entries.

What is curious is that GC is slow even though you half your heap is empty.  This leads me back to wondering about whether `mlockall` is actually working, which leads me to questions about your environment:
- what JVM are you using?
- are you setting any JVM flags?
- what OS are you using?
- are you running on bare metal, or are you using containers or VMs?
- what CPUs do you have?
- can you provide a [diagnostics dump](https://github.com/elastic/elasticsearch-support-diagnostics)
- does the problem disappear if you disable swap completely?
</comment><comment author="clintongormley" created="2015-07-19T11:15:14Z" id="122649920">Also, what indexing and search requests are you running?
</comment><comment author="dennisgorelik" created="2015-07-20T02:45:10Z" id="122736262">@clintongormley

&gt; it can happen in between phases, hence the missing slowlog entries

Are you saying that if we have sequence of:
- Query time 100ms
- Garbage collector 200s (which means timeout for us).
- Fetch time 100ms.
  Then slowlog will NOT record anything?
  Is it designed behavior?

Does not look right to me. No matter the phase, all slow times must be recorded, otherwise slowlog has very limited use.
</comment><comment author="dennisgorelik" created="2015-07-20T02:54:08Z" id="122736704">&gt; what JVM are you using?

From bigdesk info:

```
JVM

VM name: Java HotSpot(TM) 64-Bit Server VM
VM vendor: Oracle Corporation
VM version: 25.45-b02

Uptime: 2.5d
Java version: 1.8.0_45
PID: 15693
```

&gt; are you setting any JVM flags?

Only these:

```
ES_JAVA_OPTS='-Djna.tmpdir=/es_tmp'
```

&gt; what OS are you using?

CentOS 7

&gt; are you running on bare metal, or are you using containers or VMs?

Bare metal.

&gt; what CPUs do you have?

```
OS

CPU vendor: Intel
CPU model: Xeon (2600 MHz)
CPU total logical cores: 24
CPU cache: 15kb

Uptime: 29.4m
Refresh interval: 1sms
Total mem: 31.2gb (33531912192 b)
Total swap: 1023.9mb (1073737728 b)
```
</comment><comment author="dennisgorelik" created="2015-07-20T03:03:14Z" id="122739889">&gt; what indexing and search requests are you running?

I'm not sure what you're asking here.
Would that bigdesk data help?

```
Indices

Docs count: 2361020
Docs deleted: 429086

Flush: 1197, 19s
Refresh: 18256, 10.4m

Size: 10.1gb
```

Most of indexed documents are jobs (~1KB in size on average).
</comment><comment author="dennisgorelik" created="2015-07-20T03:04:28Z" id="122740656">We'll try to disable Linux swap, but bigdesk reports only ~1GB swap.
</comment><comment author="clintongormley" created="2015-07-20T10:34:04Z" id="122843016">&gt; what indexing and search requests are you running?

I just mean real examples of the types of queries and indexing request you're running. If you're using bulk indexing, how many docs do you send in one request?  with search, are you doing deep paging (eg `from:10000` or `size:10000`)

&gt; We'll try to disable Linux swap, but bigdesk reports only ~1GB swap.

If any of the heap lands up in swap it can have a huge effect on GC, which is why I suggest disabling it to see.

The [diagnostic dump](https://github.com/elastic/elasticsearch-support-diagnostics) would also be very useful.
</comment><comment author="dennisgorelik" created="2015-07-21T07:06:26Z" id="123193670">See below typical ES queries that we run:
1) The most frequently run query runs about 150,000 times per day.
The query is a batch with 2 "function_score" queries

```
{}
{   "fields": [],   "size": 7,  "query": {      "function_score": {         "functions": [          {               "script_score": {                   "script": "pow(0.99, (1437445335545.0 - doc['PostedDate'].value)/86400000)"             }           },          {               "field_value_factor": {                 "field": "PenaltyFactor",                   "factor": 1.0}          },          {               "weight": 0.015625          }],         "query" : {"filtered" : {       "query" : {         "query_string" : {      "fields" : ["JobTitle","JobDescription","CompanyName","Salary"],        "query" : "MARKETING^1 OR STRATEGY^0.612 OR ACCESS^0.552 OR SALES^0.382 OR IT^0.348",       "analyzer" : "pjfqueryanalyzer"}        },"filter" : {"bool" : {"must" : [{"query":{    "term" : {      "Country": "US" }}}],"must_not" : [{ "term" : {"JobId" : 150107695}}]}}}},          "score_mode": "multiply",           "boost_mode": "multiply"        }   }}
{}
{   "fields": [],   "size": 200,    "query": {      "function_score": {         "functions": [                      {               "field_value_factor": {                 "field": "PenaltyFactor",                   "factor": 1.0}          },          {               "weight": 0.015625          }],         "query" : {"filtered" : {       "query" : {         "query_string" : {      "fields" : ["JobTitle","JobDescription","CompanyName","Salary"],        "query" : "MARKETING^1 OR STRATEGY^0.612 OR ACCESS^0.552 OR SALES^0.382 OR IT^0.348",       "analyzer" : "pjfqueryanalyzer"}        },"filter" : {"bool" : {"must" : [{"query":{    "term" : {      "Country": "US" }}}],"must_not" : [{ "term" : {"JobId" : 150107695}}]}}}},          "score_mode": "multiply",           "boost_mode": "multiply"        }   }}
```

2) Simple search queries -- about 40,000 per day.

```
{"fields" : ["JobId"],
"sort" : [
        {"PostedDate" : {"order" : "desc"}}
],
"from" : 0, "size" : 10,
"query" : {
"filtered" : {
        "query" : {
            "query_string" : {
        "fields" : ["JobTitle","JobDescription","CompanyName","Salary"],
        "query" : "TEST",
        "analyzer" : "pjfqueryanalyzer"
}
        },
"filter" : {
"match_all" : {}
}
}
}
}
```

3)  Bulk insert: up to 10000 docs per batch.
Overall it inserts about 150,000 documents per day.

4) We have other queries, but they are less frequent and are similar to the queries above.

&gt; are you doing deep paging

No.
</comment><comment author="dennisgorelik" created="2015-07-21T07:25:59Z" id="123197822">We ran another experiment yesterday:
Added percolator queries to our dedicated ElasticSearch server.

Prior to yesterday we were running percolator queries on a separate Virtual Machine (2GB RAM, 4 virtual CPU cores, CentOS 7).
Percolation performance on VM was pretty robust.
CPU at heavy load was around 90%, but 4 percolation queues were running at a good pace (about 1 percolation per second per queue).
After we moved percolator to dedicated ElasticSearch server, percolation queries became about 40% slower.
But what was worse, our most frequently running "function_score" queries (see #1 in my previous comment) became about 100 times slower (execution time increased from about 200ms to about 20s).
Meanwhile CPU utilization was never above 60% and there was lots of available RAM (the total RAM on our dedicated box is 32GB).
Somehow ElasticSearch on dedicated server was not able to use available hardware resources, even though we have "standalone" job percolator index that is not used for regular ES queries.

Could it relate to slowlog issue?
</comment><comment author="clintongormley" created="2015-07-23T09:36:22Z" id="124034069">All of this seems to be tied to the slow GC.  Usually you see logs of slow old generation GC.  Slow young gen GC seems to be related to kernel bugs, CPU bugs or VM bugs.  May be worth comparing the setup of your external percolator machine and this dedicated cluster, and try finding the differences, eg kernel, JVM version, CPU etc.

Also, I'm still waiting for two things:
- the effect of disabling swap
- the diagnostic dump
</comment><comment author="clintongormley" created="2015-07-23T18:34:43Z" id="124202420">A colleague gave me an idea: Centos has transparent hugepages enabled by default.  This can interfere with memory handling.

Try disabling them and see if it improves things.  See https://www.centos.org/forums/viewtopic.php?f=47&amp;t=49428
</comment><comment author="dennisgorelik" created="2015-07-24T02:12:23Z" id="124293194">&gt; the effect of disabling swap

It's been 4th day since last reboot (we had to reboot because disabling swap requires it).
We have much lower number of Garbage Collector records now.

```
[2015-07-20 12:33:25,312][INFO ][monitor.jvm              ] [Node1] [gc][young][42980][86773] duration [850ms], collections [1]/[1s], total [850ms]/[1.3h], memory [8.7gb]-&gt;[7.6gb]/[15.8gb], all_pools {[young] [1.1gb]-&gt;[643.1kb]/[1.1gb]}{[survivor] [55.7mb]-&gt;[58.1mb]/[149.7mb]}{[old] [7.6gb]-&gt;[7.6gb]/[14.5gb]}
[2015-07-20 12:34:07,670][INFO ][monitor.jvm              ] [Node1] [gc][young][43022][86781] duration [799ms], collections [1]/[1.1s], total [799ms]/[1.3h], memory [8.7gb]-&gt;[7.7gb]/[15.8gb], all_pools {[young] [1gb]-&gt;[597.7kb]/[1.1gb]}{[survivor] [56.8mb]-&gt;[56.2mb]/[149.7mb]}{[old] [7.6gb]-&gt;[7.6gb]/[14.5gb]}
```

We do not have any timeouts yet in our client app (but historically timeouts started from day 5 since reboot).

We still have some slowlog records (about 10 day), but that number is much lower now than it was before and it does NOT grow with the "number of days since last reboot".
Slowlog records duration stays low (e.g. "took[19.2s]") and does not grow into 60+ seconds range.

So ElasticSearch performance looks good so far.
</comment><comment author="dennisgorelik" created="2015-07-24T02:28:10Z" id="124294903">&gt; comparing the setup of your external percolator machine and this dedicated cluster, and try finding the differences, eg kernel, JVM version, CPU etc.

I can think of only these differences:
1) VM vs dedicated.
2) Different version of ElasticSearch:
VM: v. 1.5.2
Dedicated: v. 1.6.0
3) RAM
VM: 2GB RAM
Dedicated: 32 GB RAM

CPU on both machines is the same: Xeon (2600 MHz)
VM: 20kb CPU cache
Dedicated: 15kb CPU cache

Kernel version is almost the same:
VM:

```
3.10.0-229.1.2.el7.x86_64
```

Dedicated :

```
3.10.0-229.7.2.el7.x86_64
```

VM does percolation 40% faster than dedicated server (on the same number of queues).

All comparisons were made after we disabled Linux swap on Dedicated server.

VM reported no Garbage Collector errors in almost 3 months (heavy percolation use, but no direct searches).
VM has 4 slowlog records in these 3 months (which is much better than dedicated service performance).
</comment><comment author="dennisgorelik" created="2015-07-24T02:32:33Z" id="124295360">&gt; the diagnostic dump

We installed "elastic-support-diagnostic" plugin and recorded statistics.

```
/usr/share/elasticsearch/bin/plugin --install elasticsearch/elasticsearch-support-diagnostics
```

Should I email the diagnostic dump to clint@traveljury.com ?
</comment><comment author="dennisgorelik" created="2015-07-24T02:50:37Z" id="124298191">&gt; Centos has transparent hugepages enabled by default

Are you suggesting disable THP in addition to disabling Linux memory swap?
Or enable Linux memory swap again, disable THP and see if we still have no errors?
</comment><comment author="dennisgorelik" created="2015-07-25T23:46:15Z" id="124916315">Unfortunately disabling Linux swap did NOT help.
At the end of day 5 (since last reboot) we started getting timeouts again.
At the day 6 number of timeouts became significant enough so we had to reboot ElasticSearch again.

We did not try disabling transparent hugepages, but it probably would not help against timeouts (5+ days after last ElasticticSearch reboot).
The reason I think that is that we had the same 5+ days timeouts issue with our Windows ElasticSearch installation.
These empirical observations point to ElasticSearch itself as a responsible party for these timeouts.
Most likely cause - memory leak (otherwise timeouts would not go away after ElasticSearch reboot).

Disabling transparent hugepages still may (or may not) help us to combine percolator functionality with regular searches and make using server resources efficiently.
Though I am a little bit skeptical at this point about positive outcome.
Running ElasticSearch in smaller virtual machines seems to be more promising option.
</comment><comment author="pickypg" created="2015-07-26T16:29:42Z" id="125013645">@dennisgorelik I noticed in one of your normal queries that you're using a script. On the node with timeouts, can you run:

``` bash
$ curl -XGET 'localhost:9200/_cat/fielddata?v&amp;fields=*'
```

and

``` bash
$ curl -XGET localhost:9200/_cat/plugins?v
```

There's two things that come to mind when I look at your GCs:
1. Your memory seems to have settled at just over 50% of the total heap, which isn't giving a dramatic amount of headroom to do other stuff. I expect that this article will be useful to you here: https://elastic.co/blog/support-in-the-wild-my-biggest-elasticsearch-problem-at-scale
2. Your young GCs are indeed slow, but if you look at the young versus old, you can see that it's actually aging the memory rather than freeing it. This means that whatever is happening is probably happening all at once.
</comment><comment author="clintongormley" created="2015-07-26T19:41:17Z" id="125031879">&gt; Should I email the diagnostic dump to clint@traveljury.com ?

yes please
</comment><comment author="dennisgorelik" created="2015-07-27T03:45:45Z" id="125076215">@pickypg

&gt; localhost:9200/_cat/fielddata?v&amp;fields=*

```
5KUus70-Taix-_Tokoea_Q pjfsearch xxx.xxx.xxx.xxx Node1  38mb        9mb 2.5mb         2.5mb   23.8mb 
```

(That's the snapshot at ~1 day since last ElasticSearch reboot.

Does it help to determine anything?

&gt; /_cat/plugins?v

```
Node1 bigdesk   NA      s    /_plugin/bigdesk/ 
```

&gt; settled at just over 50% of the total heap

Do you mean that normal heap usage should be less than 50%?

Do you think that some of our requests require some memory buffer of more than 50% of the heap?
(That seems unlikely, considering that we have a lot of RAM and our queries are not really that big).

&gt; it's actually aging the memory rather than freeing it

From what information do you see that young memory is not freeing?
</comment><comment author="clintongormley" created="2015-07-27T09:20:13Z" id="125139770">&gt; Do you mean that normal heap usage should be less than 50%?

No.  It's normal for old gen GC to kick in at 75% and drop the heap to 50%.  

&gt; From what information do you see that young memory is not freeing?

Your young gen GCs are really slow, and the young heap usage never drops by much.

@pickypg's mention of scripts made me take another look at your scripts:

&gt;    "pow(0.99, (1437445335545.0 - doc['PostedDate'].value)/86400000)" 

You have values like `1437445335545.0` in your scripts.  Do these values change on most requests?  If so, you recompile your scripts every time you run them.  This could well be the problem you're suffering. Script compilation is heavy and can generate a lot of garbage.  

Change your scripts to use params instead, and see if that helps, eg:

```
      "script_score": {
        "script": "pow(0.99, (my_param - doc['PostedDate'].value)/86400000)"
      },
      "params": {
        "my_param": 1437445335545
      }
```
</comment><comment author="dennisgorelik" created="2015-07-28T08:24:21Z" id="125498488">&gt;  It's normal for old gen GC to kick in at 75% and drop the heap to 50%.

That is exactly what we had (and still have) in our ElasticSearch.
Having "GC usage at just over 50%" was a temporary snapshot.

&gt; Do these values change on most requests?

Yes: these values change almost always.

&gt; you recompile your scripts every time you run them. This could well be the problem you're suffering. 

You are right.
We changed that query to parameterized version and that immediately brought great improvements:
1) In our development environment (ElasticSearch on Windows without running any other concurrent tasks) query execution time dropped from 20ms down to 4ms (5x improvement!).
2) In our production environment we now are able to run our percolation queries with our direct search queries (the queries that we just parameterized).

We do not have timeouts yet, but when running percolation queries and direct queries concurrently, direct queries execute much longer (e.g. 10s instead of 120ms).
We have that direct queries slowness in spite of CPU utilization of being never more than 60% (and plenty of RAM).

Even though it looks likely that we fixed (parametrized) query that was causing memory leaks, we still did not test it empirically: It's been less than a day since last ElasticSearch reboot.
I will report in about a week if we would get timeouts or not.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Load plugins into classpath in bootstrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11918</link><project id="" key="" /><description>This can be done on startup, then we restore `setAccessible(false)` and engage security.

This also removes uber-dangerous global sun.misc permission (so only special jars have it).

Its not a true fix to #11917 but its progress and improves security.

Note: I had to remove some "integration-like" tests. In this simpler world all plugins are "classpath plugins" so testing is already more realistic (and we can followup by removing that confusing parameter) If we want some more integration tests around it I think those should be in a separate place? Anyway i feel bad about not being able to preserve all the tests, but I think its important to lock this down.
</description><key id="91847378">11918</key><summary>Load plugins into classpath in bootstrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T16:14:44Z</created><updated>2015-06-29T20:18:51Z</updated><resolved>2015-06-29T18:45:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-29T16:34:01Z" id="116751794">I discussed with @uschindler (he inspected source code), we don't need the try-finally block since the Method does not escape. I will simplify this one.
</comment><comment author="uschindler" created="2015-06-29T16:35:50Z" id="116752682">Hi,
you dont need to do setAccessible. The JVM allows setAccessible only per-instance (the Javadoc is a bit vague about this). The instances are never cached. Each call to Class.getMethod() returns a _new_ instance (the internal cached one is cloned/copied). You can follow this in the JDK source code (grepcode.com). In fact the Method instances cannot really be cached for outside callers, because they are @CallerSensitive
Uwe
</comment><comment author="rmuir" created="2015-06-29T16:36:30Z" id="116752986">Thanks for the investigation! I cleaned this up
</comment><comment author="uschindler" created="2015-06-29T16:48:39Z" id="116757795">By the way here is the Method.copy() which is used by Java's cache. This one has the nice statement that confirms all of this: http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/8u40-b25/java/lang/reflect/Method.java#Method.copy%28%29

``` java
        // This routine enables sharing of MethodAccessor objects
        // among Method objects which refer to the same underlying
        // method in the VM. (All of this contortion is only necessary
        // because of the "accessibility" bit in AccessibleObject,
        // which implicitly requires that new java.lang.reflect
        // objects be fabricated for each reflective call on Class
        // objects.)
```
</comment><comment author="rjernst" created="2015-06-29T18:14:09Z" id="116781288">LGTM
</comment><comment author="uschindler" created="2015-06-29T18:41:23Z" id="116792818">We should really fix the addURL shit in later versions. It is horrible to add URLs to SystemClassLoader. The correct way to do this would be to make Bootstrap to create a _new_ ClassLoader with all URLs and let the application classloader (which is != SystemClassloader, so this is also a security issue!!!) just be parent.
</comment><comment author="rmuir" created="2015-06-29T18:45:34Z" id="116793747">@uschindler yes, the long term fix is https://github.com/elastic/elasticsearch/issues/11917

This issue is to improve security around this more, by doing this up-front in bootstrap. Its just a step. It removes access to sun.misc.Unsafe for example, except where explicitly granted.

If i go off on tangents and try to fix everything about some of this stuff, then we will make no progress. It remains the same stuff as it was before.
</comment><comment author="uschindler" created="2015-06-29T19:38:10Z" id="116812410">OK, thanks! I just wanted to point out that modifying the system classloader is a huge security issue. Nobody ever should do that! It would be OK if it s the application classloader, but this variant is a desaster! Really! That sucks horribly. I have to say that Apache Solr does a much better job here! :-)
</comment><comment author="uschindler" created="2015-06-29T19:43:00Z" id="116813958">Who wrote that code should be nailed against a wall and the Forbidden Policeman would probe his gun at him/her :-)
</comment><comment author="uschindler" created="2015-06-29T19:57:40Z" id="116818350">Was a joke, of course :-) Something for @UweSays :-)
</comment><comment author="uschindler" created="2015-06-29T20:06:57Z" id="116823318">I would also see this as huge security issue in any previous Elasticsearch version! You should patch earlier versions to not use System classloader and instead (if you dont fix it completely like in 2.0), change the code to use: Bootstrap.class.getClassLoader() instead of ClassLoader.getSystemClassLoader so it uses the application classloader and does not modify the JVM internals bypassing any security checks!
</comment><comment author="rmuir" created="2015-06-29T20:11:10Z" id="116825716">Hi Uwe, again the code was simply moved here, so securitymanager can blockaccess to `sun.misc`. That's all this issue is about. I did not write this code here.

If you want to improve the code, please spend efforts helping me with https://github.com/elastic/elasticsearch/issues/11917, then its gone completely. We don't need to improve it. We need to remove it.
</comment><comment author="uschindler" created="2015-06-29T20:14:05Z" id="116827024">I was talking about 1.6 and earlier! I will open issue. This cannot stay like this!
</comment><comment author="rmuir" created="2015-06-29T20:15:59Z" id="116827609">1.6 isn't even on my radar. it doesn't even run with a security manager
</comment><comment author="uschindler" created="2015-06-29T20:18:51Z" id="116828913">It still breaks the whole JVM internals!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginLuceneCheckerTests.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginServiceTests.java</file></files><comments><comment>Merge pull request #11918 from rmuir/plugin_manager_classpath</comment></comments></commit></commits></item><item><title>bin/elasticsearch should include plugins in classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11917</link><project id="" key="" /><description>Currently, instead java code adds jars dynamically to the system classloader with some reflection hacks. Really the classpath should be thought of as immutable, and if it was set correctly to begin with, we wouldn't need this.
</description><key id="91834756">11917</key><summary>bin/elasticsearch should include plugins in classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T15:18:31Z</created><updated>2015-07-22T15:32:24Z</updated><resolved>2015-07-22T15:32:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-29T15:19:01Z" id="116728275">See #11898 for additional background/discussion.
</comment><comment author="rmuir" created="2015-06-29T15:44:45Z" id="116737661">FYI I'm currently trying to factor out this classpath-loading stuff into bootstrap. I think its clean. This would be a good intermediate solution and gets the logic closer to where it should be, and lets us just set classpath but then setAccessible(false) and engage security after.
</comment><comment author="rmuir" created="2015-07-22T15:32:24Z" id="123759675">This is fixed with child classloaders for plugins in https://github.com/elastic/elasticsearch/pull/12367
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The "size" and "from" of root of search JSON do not work with aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11916</link><project id="" key="" /><description>Hey,

I'm observing some strange behavior with aggregation. I'm using ES 1.6.

```
{
  "status" : 200,
  "name" : "Honey Lemon",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.6.0",
    "build_hash" : "cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0",
    "build_timestamp" : "2015-06-09T13:36:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

I've built search JSON where I used "from", "size" and aggregation. So, it looks like following code:

```
{
  "from": 0,
  "size": 5,
  "sort" : [
       { "profile" : {"order" : "asc"}}
  ],
  "aggs": {
    "genders": {
      "terms": {
        "field": "custom_field",
        "order" : { "_term" : "asc" }
      },
      "aggs": {
                "my_top_hits": {
                    "top_hits": {
                       "from" : 0,
                        "sort": [
                            {
                                "count": {
                                    "order": "asc"
                                }
                            }
                        ],
                      "size" : 4  // I know, I can increase the value. But it's not solves issue anyway
                    }
                }
            }
    }
  }
}
```

My expectations are, only 5 first documents of index are used for aggregation... However, I was surprised.. Aggregation results include all documents from index.

I tried to add query tag in search JSON.. And I see, aggregation works only with part of documents that are "provided"/"filtered" by query..
I think, "from" and "size" of root search JSON should cut list of documents for aggregations, similar with 

Task that I'm trying to solve is grouping documents by some field. Something like GROUP BY of MySQL. Some example from extjs could be find here http://docs.sencha.com/extjs/4.2.3/extjs-build/examples/build/KitchenSink/ext-theme-neptune/#grouped-grid

My idea was, to use "from"/"size", for paging via aggregated data. In other words, I would be "ask" aggregation process only required for myself part of data. 
For example, now I could be use "size" of terms aggregation.. but it means, I'll have N buckets with length &lt;= the "size". But it's worse way..Because to show table I 'll use only number required for site documents. But, also.. site will received a lot of data that are not required. It can impact customer's PC (I mean memory) and enforce to wait data foo long time if I use big data

Some questions are opened for myself:
1. Is it feature or bug? ( If it's bug I can take a look.. would be great if you recommend where in code I should start)
2. Possible, u know other way to group data by field.
3. Could be results of several metric aggregation are joined to one array ?
</description><key id="91827502">11916</key><summary>The "size" and "from" of root of search JSON do not work with aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rusboy</reporter><labels /><created>2015-06-29T14:53:21Z</created><updated>2015-06-30T09:40:28Z</updated><resolved>2015-06-30T09:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rusboy" created="2015-06-29T20:25:05Z" id="116831796">hm.. seems I found workaround solution for myself..
it looks so, 

```
{
  "sort" : [
      { "custom-field1" : {"order" : "asc"}},
      { "count"   : {"order" : "asc"}}
  ],
  "aggs": {
      "group-by-custom-field1": {
          "terms": {
             "field": "custom-field1",
             "size" : 0
           },
           "aggs": {
              "group-by-count" : {
                  "terms": {
                      "field": "count",
                       "size" : 0
                   }
               }
           }
      }
  }
}
```

Sort rows by grouped fields and calculate by aggregation count of values in each group. The example for case when user groups results by 2 fields (custom-field1 and count)

Anyway, I would be interested to know answers on my question above.
Thanks!
</comment><comment author="clintongormley" created="2015-06-30T09:40:27Z" id="117094582">Hi @rusboy 

It is a feature, not a bug.  The typical use case is: calculate analytics over all matching documents, and give me the top N documents as hits.

To achieve what you want (ie to only aggregate on the page of results returned in hits) you would need to do this in two steps:
- run a query with `from`/`size`
- run a second request with aggs, where the query matches just the doc ids from the first query
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Refactoring: Make parsing nested queries always return query builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11915</link><project id="" key="" /><description>Currently parsing inner queries can return `null` which leads to unnecessary complicated
null checks further up the query hierarchy. By introducing a special EmptyQueryBuilder
that can be used to signal that this query clause should be ignored upstream where possible,
we can avoid additional null checks in parent query builders and still allow for this query
to be ignored when creating the lucene query later.

This new builder returns `null` when calling its `toQuery()` method, so at this point
we still need to handle it, but having an explicit object for the intermediate query
representation makes it easier to differentiate between cases where inner query clauses are
empty (legal) or are not set at all (illegal).

This PR goes agains the query-refactoring branch.
</description><key id="91824522">11915</key><summary>Query Refactoring: Make parsing nested queries always return query builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-29T14:41:19Z</created><updated>2016-03-11T11:51:22Z</updated><resolved>2015-07-03T07:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-06-29T19:08:36Z" id="116799609">+1 from my side to the idea and the amount of handling special cases that this PR removes.
</comment><comment author="javanna" created="2015-06-30T08:39:28Z" id="117056815">I like this a lot, left a few comments
</comment><comment author="cbuescher" created="2015-06-30T10:45:26Z" id="117121690">@javanna addressed your comments, have a second look if you like.
</comment><comment author="cbuescher" created="2015-06-30T13:54:42Z" id="117192485">I agree that inner queries that are currently mandatory in the query DSL and are checked for in the parser should also be enforced to be set in the builder. We can do so by making all those cases mandatory arguments in the constructors now that we introduce the EmptyQueryBuilder as a stand-in for empty inner queries. I think from the cases you mentioned, only BoostingQueryBuilder needs an additional 2-arg constructor with null-checks then. 
</comment><comment author="cbuescher" created="2015-07-01T10:30:31Z" id="117586119">@javanna Rebased and re-checked all the builders touched so far for breaking changes regarding inner queries. Other than the additional null-checks in constructors we're pretty consistend with the current behavior on master. Changed BoosingQueryBuilder, removing setters for inner queries and instead forcing them to be non-null in constructor, also adding this to the changes doc. Also extended handling of empty collection of inner clauses in Bool/And/OrQueryBuilder.
</comment><comment author="javanna" created="2015-07-01T16:30:32Z" id="117739098">I did another round, left a few comments
</comment><comment author="cbuescher" created="2015-07-01T21:57:58Z" id="117835745">Rebased onto the boost/queryName refactoring, had to adapt a few tests for that and also added junit tests that check constructors and setters throw exceptions where they check for `null`.
</comment><comment author="javanna" created="2015-07-02T07:34:28Z" id="117942897">looks good, left a few comments, actually more questions than comments.
</comment><comment author="cbuescher" created="2015-07-02T13:51:09Z" id="118039271">Removed early return when creating BooleanQuery in BoolQueryBuilder and updated a few comments. 
</comment><comment author="javanna" created="2015-07-02T14:24:19Z" id="118049314">thanks @cbuescher left another comment :)
</comment><comment author="javanna" created="2015-07-03T07:23:18Z" id="118260676">LGTM we have to fix the bool query and probably move some logic around isFilter to the toQuery method, but let's do it in a separate issue.
</comment><comment author="cbuescher" created="2015-07-03T07:55:14Z" id="118268171">Thanks, will merge after rebase &amp; squash and open separate issue for the BoolQueryBuilder problem mentioned above.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Merge pull request #11915 from cbuescher/feature/query-refactoring-emptyquerybuilder</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Query Refactoring: Make sure that parsing nested queries always returns an query builder</comment></comments></commit></commits></item><item><title>Rename MetaData.uuid -&gt; MetaData.clusterUUID and IndexMetaData.uuid-&gt; IndexMetaData.indexUUID</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11914</link><project id="" key="" /><description>As explained in #11831, we currently have uuid fields on the cluster state, meta data and index metadata. The latter two are persistent across changes are being effectively used as a persistent uuid for this cluster and a persistent uuid for an index. The first (ClusterState.uuid) is ephemeral and changes with every change to the cluster state. This is confusing,

UPDATE:
We settled on having the following:

-&gt; ClusterState.uuid -&gt; stateUUID (transient)
-&gt; MetaData.uuid -&gt; clusterUUID (persistent)
-&gt; IndexMetaData.uuid -&gt; indexUUID (persistent).
</description><key id="91817479">11914</key><summary>Rename MetaData.uuid -&gt; MetaData.clusterUUID and IndexMetaData.uuid-&gt; IndexMetaData.indexUUID</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T14:18:18Z</created><updated>2015-07-14T11:58:50Z</updated><resolved>2015-07-13T18:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-29T14:40:50Z" id="116708977">Sorry I misunderstood your proposal. I thought you want to rename MetaData.uuid into MetaData.metaDataUUID. In my opinion renaming MetaData.uuid into MetaData.ClusterUUID (persistent) while keeping ClusterState.uuid (transient) is extremely confusing and is much worse than what we have today. I don't think we should get this in until we fix #11831.
</comment><comment author="bleskes" created="2015-07-02T10:10:57Z" id="117989706">@imotov I update the PR, can you take a second look?
</comment><comment author="imotov" created="2015-07-02T17:44:57Z" id="118107434">Left a couple of minor comments. Otherwise LGTM.
</comment><comment author="bleskes" created="2015-07-10T06:17:42Z" id="120244619">@imotov thx. I pushed another commit.
</comment><comment author="imotov" created="2015-07-12T13:01:40Z" id="120719258">One minor comment. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/VersionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Rename MetaData.uuid -&gt; MetaData.clusterUUID and IndexMetaData.uuid-&gt; IndexMetaData.indexUUID</comment></comments></commit></commits></item><item><title>Add mocking for securitymanager environment.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11913</link><project id="" key="" /><description>This is a standalone wrapper around Mockito, it allows us to use
it without granting dangerous permissions to all of our code.

It just presents the same binary API, to be a drop-in replacement, wrapping all calls in AccessController. 

See #11898 

Let me know about naming and placement of this hack. I don't think it would change so often, only when we bump the Mockito version. It doesn't have any elasticsearch dependencies or anything like that.
</description><key id="91804970">11913</key><summary>Add mocking for securitymanager environment.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T13:35:19Z</created><updated>2015-06-29T14:33:10Z</updated><resolved>2015-06-29T14:27:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-29T13:37:14Z" id="116662226">LGTM
</comment><comment author="nik9000" created="2015-06-29T14:32:13Z" id="116705376">Is this something you can upstream?
</comment><comment author="rmuir" created="2015-06-29T14:33:10Z" id="116705741">Maybe, one issue is that it would be a risk, if you used this in a production (non-test) environment. Personally I think thats stupid, but I see the argument of someone accidentally leaving it in classpath.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>securemock/src/main/java/org/elasticsearch/mock/Mockito.java</file></files><comments><comment>Merge pull request #11913 from rmuir/securemock</comment></comments></commit></commits></item><item><title>Update post-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11912</link><project id="" key="" /><description>Removing useless comma
</description><key id="91802210">11912</key><summary>Update post-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rusboy</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-29T13:22:46Z</created><updated>2015-07-01T07:33:05Z</updated><resolved>2015-07-01T07:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-29T13:49:48Z" id="116666411">Hi @rusboy 

Thanks for the PR.  Please can I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2015-07-01T07:33:05Z" id="117505808">thanks @rusboy - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update post-filter.asciidoc</comment></comments></commit></commits></item><item><title>Don't convert possibly corrupted bytes to UTF-8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11911</link><project id="" key="" /><description>If the translog UUID is corrupted we should not convert it
to UTF-8 since it might be invalid. Instead we should compare
the UTF-8 byte representation directly.

this failed on jenkins last weekend here http://build-us-00.elastic.co/job/es_core_master_debian/5924/
the seed fails without this fix:

```
mvn test -Pdev -Dtests.seed=EDA6944EE3C55BD1 -Dtests.class=org.elasticsearch.index.store.CorruptedTranslogTests -Dtests.method="testCorruptTranslogFiles" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.locale=sr_RS -Dtests.timezone=WET
```
</description><key id="91776884">11911</key><summary>Don't convert possibly corrupted bytes to UTF-8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T11:19:30Z</created><updated>2015-06-29T13:47:03Z</updated><resolved>2015-06-29T13:00:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-29T11:39:51Z" id="116623721">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java</file></files><comments><comment>Merge pull request #11911 from s1monw/no_utf_uuid</comment></comments></commit></commits></item><item><title>Ban java serialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11910</link><project id="" key="" /><description>We had several problems with Java Serialization in the past. At some point
in the Java 1.7.x series JDKs were not compatible anymore when java
serialization (ObjectStream) was used to exchange objects. In elasticsearch
we used this to serialize exceptions across the wire which caused several problems
with incompatible JDKs. While causing lot of trouble this essentially prevented
users from moving forward and upgrade their JVMs. To prevent these kind of issues
this commit removes the dependency on java serialization entirely and bans the
usage of ObjectOutputStream and ObjectInputStream entirely.

Yet, we can't fully serialize all exception anymore such that this commit
is best effort and adds hand written serialization to all elasticsearch exceptions
as well to a selected set of JDK and Lucene exceptions. (see StreamOutput#writeThrowable /
StreamInput.readThrowable). Stacktraces should be preserved for all exceptions while
several names might be replaced with ElasticsearchException if there is no mapping for
the given exception.
</description><key id="91775233">11910</key><summary>Ban java serialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>:Network</label><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T11:10:12Z</created><updated>2015-11-05T13:07:24Z</updated><resolved>2015-06-30T12:52:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-29T13:02:09Z" id="116650942">Thanks for adding to forbidden apis! I like being more robust here, we don't want to discourage upgrading the JVM.
</comment><comment author="rjernst" created="2015-06-29T19:53:37Z" id="116816461">LGTM, one small question.
</comment><comment author="s1monw" created="2015-06-29T21:24:43Z" id="116850481">I added a bunch of unittests for all the custom serialization exceptions I think we are pretty close here... I would like to move forward with this and push this tomorrow unless anybody objects.
</comment><comment author="kimchy" created="2015-06-30T15:57:38Z" id="117237655">@s1monw small comment since I pulled this change and noticed, the check in StreamOutput#writeThrowable for `AlreadyClosedException` is not going to execute since it extends IllegalStateException which is checked before.
</comment><comment author="s1monw" created="2015-06-30T20:05:22Z" id="117324392">&gt; @s1monw small comment since I pulled this change and noticed, the check in StreamOutput#writeThrowable for AlreadyClosedException is not going to execute since it extends IllegalStateException which is checked before.

pushed a fix - I had a test for it but it was jagged!
</comment><comment author="vvcephei" created="2015-09-30T17:37:38Z" id="144486063">hey @s1monw, I guess you missed the discussion of this: https://github.com/vvcephei/es-rest-client-java in https://github.com/elastic/elasticsearch/issues/7743, and I missed this PR!

Someone just told me about this project, so I haven't looked at it yet, but I'm super stoked you're doing this. If it helps you, my project has delete, get, index, and search fully implemented. We are working on deserializing exceptions now.

Are you planning to support 1.x versions at all? If not, I may just re-label my project as a 1.x client. If you are going to support 1.x apis, I'd be interested in seeing if I can help you out at all.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `min` score mode to `nested` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11909</link><project id="" key="" /><description>This score mode was added with the Lucene 5.2 release, but the `nested` query parser hasn't been changed to use it.
</description><key id="91767224">11909</key><summary>Add `min` score mode to `nested` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T10:27:41Z</created><updated>2015-06-30T09:41:17Z</updated><resolved>2015-06-29T15:48:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-29T12:45:12Z" id="116644469">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file></files><comments><comment>Merge pull request #11909 from martijnvg/nested_query_min_score_mode</comment></comments></commit></commits></item><item><title>Docs: clarification of allocation awareness w.r.t. rack failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11908</link><project id="" key="" /><description /><key id="91754814">11908</key><summary>Docs: clarification of allocation awareness w.r.t. rack failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T09:34:49Z</created><updated>2015-06-29T09:58:01Z</updated><resolved>2015-06-29T09:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-29T09:51:32Z" id="116587001">tiny comment, but LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: clarification of allocation awareness w.r.t. rack failures</comment></comments></commit></commits></item><item><title>ES eventually falls in timeouts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11907</link><project id="" key="" /><description>I have server with 32Gb RAM under Windows Server 2012 R2 Standard.
I have installed on server:
1) ES 1.6.0 - takes 8Gb of RAM, configured as single node.
ES configuration:

```
cluster.name: xxxxx
bootstrap.mlockall: true
network.host: xxxxx
transport.tcp.port: xxxxx
http.port: xxxxx
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.fetch.warn: 1s
index.indexing.slowlog.threshold.index.warn: 10s
monitor.jvm.gc.young.warn: 1000ms
monitor.jvm.gc.old.warn: 10s
script.inline: on
script.indexed: on
```

2) MS SQL Server 2008R2 - takes 16Gb of RAM.
When I just started ES - all is fine, works without any timeouts.
But after 3-4 days I'm starting get timeouts during different ES operations (search, msearch, bulk insert).
During getting timeouts BigDesk shows that do not exist any problems with HeapMemory (~5-6 Gb out of 8Gb is used), CPU (cpu usage ~30-40%), Search requests per second ( 50-80 requests per second).
I turned on slow logging and noticed that number of records in elasticsearch_index_search_slowlog.\* files usually grows day by day and some queries are executed in 1-2 minutes.
The volume of indexed documents is the same in 3-4 days approximately, so I do not think that it could be a reason.
If I restart ES - it helps, but situation is repeated in 3-4 days.
Any hints what could it be?
On what should I pay attention in order to recognize possible problems?
</description><key id="91735076">11907</key><summary>ES eventually falls in timeouts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreyAfanasyev</reporter><labels /><created>2015-06-29T08:23:57Z</created><updated>2015-06-29T08:46:33Z</updated><resolved>2015-06-29T08:28:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-29T08:28:36Z" id="116521425">Hi @AndreyAfanasyev 

The best place to ask questions like this is in the forum: https://discuss.elastic.co/. This list is for bug reports and feature requests.  That said, looking at your config, your slow GC warning level is way too high.  I'd expect slow GCs to be around 500ms, which leads me to think that you are suffering from slow GCs because you are not using mlockall.  

See https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory
</comment><comment author="AndreyAfanasyev" created="2015-06-29T08:43:22Z" id="116527630">@clintongormley,
I already use mlockall setting:

```
bootstrap.mlockall: true
```

in my config.

Please see /_nodes/process?pretty response:

```
{
  "cluster_name" : "elasticsearch",
  "nodes" : {
    "suIdQ4kFRUWJ-gCaZRKTKA" : {
      "name" : "Xxxxx",
      "transport_address" : "inet[/xxxxxxxxxxxxxxxx]",
      "host" : "xxxxx",
      "ip" : "xxxxxxxxxxxxxxx",
      "version" : "1.6.0",
      "build" : "cdd3ac4",
      "http_address" : "inet[/xxxxxxxxxxxxxxx]",
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 6548,
        "max_file_descriptors" : -1,
        "mlockall" : true
      }
    }
  }
}
```

What should I do else to setup mlockall properly under Windows Server?
</comment><comment author="clintongormley" created="2015-06-29T08:46:33Z" id="116529027">OK.  The best place to ask these questions is still the forum
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Testing: Fix running of rest tests against external cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11906</link><project id="" key="" /><description>The TransportClient specified in the ExternalCluster was missing a
`path.home` setting and thus would not start.
</description><key id="91729134">11906</key><summary>Testing: Fix running of rest tests against external cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-06-29T07:55:55Z</created><updated>2015-06-30T08:15:44Z</updated><resolved>2015-06-30T06:48:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-29T16:47:52Z" id="116757485">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support aliases for type </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11905</link><project id="" key="" /><description>If i'm not wrong, aliases are supported only at the index level now. It would be helpful in certain use cases like ours if alias can be made for type too.

For e.x our environment has 1000 of apps and each app has 100's of documents. Right now we are storing them as different types under same index. 

But while reindexing each app, We have to cleanup the documents first (because field mapping may change). Due to this we see zero results for a brief period of time.  Alias support for type would be useful here.
</description><key id="91687844">11905</key><summary>Support aliases for type </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msathis</reporter><labels /><created>2015-06-29T03:53:52Z</created><updated>2015-07-01T15:42:04Z</updated><resolved>2015-06-29T12:29:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-29T09:34:27Z" id="116572466">You can use an alias with a filter for this.
</comment><comment author="msathis" created="2015-06-29T10:02:10Z" id="116590279">@markwalkom Filtering means you won't be able to change the mapping. To solve that, We may index into different type, but there is no way to rename it back to original name after indexing done. :(
</comment><comment author="clintongormley" created="2015-06-29T12:29:12Z" id="116637033">@msathis we have removed the ability to delete type mappings in master because it can lead to corrupt data. (https://github.com/elastic/elasticsearch/pull/10231)  If you need to make changes like this, you need to reindex to a new index.
</comment><comment author="msathis" created="2015-06-29T13:07:00Z" id="116652772">@clintongormley So your suggestion is to make 10,000 indexes, if i have 10,000 apps? Unfortunately you didn't point me to a convincing alternative solution before closing the issue.
</comment><comment author="msathis" created="2015-06-30T03:54:40Z" id="116937542">Hi @clintongormley , You haven't cited a reason for closing this. I still feel, it's good to have feature as there are no efficient alternatives present. Let me explain my use case for this in few sentences,
1. We have ten thousands of apps. Each app has 100's of documents and indexed as a type.
2. Each app has differently structured documents and so different field mappings as well. These mapping may change at any time. When mapping changes, we have to reindex the app.

Right now we are deleting the type first and then reindexing to the same type. Due to this we see "zero results" or "fewer results" for one or two seconds (during indexing period). We want to avoid this.

If alias for type existed, i could've indexed into different type and then made alias for actual type name.

 Is there a better way to achieve this with current elasticsearch itself? 

Thanks.
</comment><comment author="clintongormley" created="2015-06-30T11:28:21Z" id="117134300">Hi @msathis 

I've closed this ticket because: (1) we no longer support delete-mapping and (2) we're trying to simplify mappings, rather than make them more complicated (and thus more prone to bugs).

You can read more about the changes coming in mappings here: https://www.elastic.co/blog/great-mapping-refactoring along with some suggestions for modelling your data.
</comment><comment author="msathis" created="2015-07-01T08:05:08Z" id="117518119">Thanks @clintongormley . This makes sense from elasticsearch point of view. Do you see any performance implications in having 10,000 indices compared to 10,000 types in one index?
</comment><comment author="clintongormley" created="2015-07-01T09:09:10Z" id="117554620">@msathis yes :)  unless you have a LOT of nodes...  I'd suggest name-spacing your fields instead, eg `app1.foo` or something like that, but really depends on your needs.  Best place to discuss design issues like this is the forum: http://discuss.elastic.co/
</comment><comment author="msathis" created="2015-07-01T15:42:04Z" id="117720165">Ok. Thanks @clintongormley. I will post a question there. :) 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot and restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11904</link><project id="" key="" /><description>Hi, i have a snapshot from a cluster,
then how to restore it in a different and new cluster with same configures.

I can create snapshot repo but it don't have any snapshot in the new cluster.

Can i restore the new cluster from snapshot?
</description><key id="91608615">11904</key><summary>Snapshot and restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cnsinger</reporter><labels /><created>2015-06-28T15:20:52Z</created><updated>2015-06-28T15:33:40Z</updated><resolved>2015-06-28T15:33:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-28T15:33:39Z" id="116292519">Yes you can but unless it's an issue, please join us on discuss.elastic.co.
We can help you there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Treat path object as a simple value instead of Iterable in XContentBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11903</link><project id="" key="" /><description>Treat path object as a simple value objects instead of Iterable in XContentBuilder, using toString() to create String representation.

This addresses #11771
</description><key id="91529204">11903</key><summary>Treat path object as a simple value instead of Iterable in XContentBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">szroland</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-27T23:36:07Z</created><updated>2015-07-10T08:56:55Z</updated><resolved>2015-07-10T08:56:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-28T02:43:48Z" id="116186471">LGTM (after writing it, I decided that I wanted to see another two tests to see/test it in action in a more realistic way)
</comment><comment author="pickypg" created="2015-06-29T17:38:13Z" id="116770926">LGTM
</comment><comment author="clintongormley" created="2015-06-30T13:00:27Z" id="117169037">@javanna please could you review and pull in if ok?
</comment><comment author="szroland" created="2015-07-08T21:36:11Z" id="119738206">I updated the PR as per Adrien's recommendation, instead of dedicated methods, the existing ones handling Iterables are amended to check for a Path argument specifically.
</comment><comment author="jpountz" created="2015-07-09T06:54:46Z" id="119845561">Thanks @szroland , this looks good to me now. I was going to merge and ran tests but had a failure in `testHandlingOfPath_absolute` and `testHandlingOfPath_relative`. Here is the error for `testHandlingOfPath_absolute` for instance:

```
java.lang.AssertionError: 
Expected: "{\"file\":\"\/tmp\/org.elasticsearch.common.xcontent.builder.XContentBuilderTests_1C03416FCE58429B-001\/tempDir-001\"}"
     but: was "{\"file\":\"/tmp/org.elasticsearch.common.xcontent.builder.XContentBuilderTests_1C03416FCE58429B-001/tempDir-001\"}"
    at __randomizedtesting.SeedInfo.seed([1C03416FCE58429B:54DEB5C2CC66D9A2]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.common.xcontent.builder.XContentBuilderTests.checkPathSerialization(XContentBuilderTests.java:290)
    at org.elasticsearch.common.xcontent.builder.XContentBuilderTests.testHandlingOfPath_absolute(XContentBuilderTests.java:283)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1627)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:872)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:886)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:845)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:747)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:792)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)

```

Maybe the test should be changed in such a way that we verify that serializing a Path is the same as serializing its toString() representation if we don't want to rely on escaping rules?
</comment><comment author="szroland" created="2015-07-09T07:38:05Z" id="119857297">Yes, well, this is kind of why I wanted to avoid using full paths in the tests. I used org.apache.commons.lang3.StringEscapeUtils to do the escaping, but reading its javadoc it does escape forward slash claiming that is the spec, where apparently XContentBuilder/Jackson don't do that. This didn't come out on Windows as the separator is backslash, which is escaped by both.

Your idea works. Let me update the test tonight.
</comment><comment author="szroland" created="2015-07-10T05:26:07Z" id="120232605">Fixed the tests in the above commit, they should no longer depend on json escaping behavior.
</comment><comment author="jpountz" created="2015-07-10T08:56:47Z" id="120305154">I merged manually, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix wrong reused file bytes in Recovery API reports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11902</link><project id="" key="" /><description>Calling the correct method, `reusedBytes()` when creating json output.
This fixes #11876.
</description><key id="91523515">11902</key><summary>Fix wrong reused file bytes in Recovery API reports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">szroland</reporter><labels /><created>2015-06-27T22:20:12Z</created><updated>2015-06-30T21:58:45Z</updated><resolved>2015-06-30T21:56:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-28T02:58:56Z" id="116186772">I think that we should keep the POST option in the REST spec and add it to the code rather than removing it from what we deem to be the code contract (a non-backwards compatible change, even though we don't technically do it). We can create a separate issue for that and fix/remove it there, which should simplify your this pull request.
</comment><comment author="bleskes" created="2015-06-28T09:12:36Z" id="116233070">@pickypg the code actually supports PUT without a _create and POST with :

```
        controller.registerHandler(PUT, "/_snapshot/{repository}/{snapshot}", this);
        controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}/_create", this);
```

@szroland I agree this is confusing. Can you separate the snapshot create change into another PR/issue so we can discuss things over there? It's different from the RecoveryState change which is good and should go in as is ... 
</comment><comment author="szroland" created="2015-06-28T12:39:25Z" id="116271781">@pickypg, I know that the code does support the POST with _create. But the semantics of the original spec, at least as implemented in the Rest Test infrastructure, is as if the controller registered all these handlers (since the json format cannot limit a HTTP method to a particular path):

```
   controller.registerHandler(PUT, "/_snapshot/{repository}/{snapshot}", this);
   controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}", this);

   controller.registerHandler(PUT, "/_snapshot/{repository}/{snapshot}/_create", this);
   controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}/_create", this);
```

So in this sense, the spec was actually incorrect. With this approach, the portion it describes is correct, only it does not fully describe the available endpoints. As I commented on the main issue, I did open a separate issue to resolve this particular problem:  #11897.

@bleskes, while I could pull that out to a separate PR, that would break the test. Without this change it is not possible to use the snapshot.create API in a Rest Test. I think it is a better option overall to at least temporarily fix the spec, make it work in tests, and in the context of a dedicated issue, #11897, figure out what the best way to resolve this for good is. Whatever the outcome of that is, though, this test should continue to work fine as is.

As I noted in #11897, the same thing (omitting the POST / _create endpoint) has been done with other APIs as well. Also the spec currently is not comprehensive anyway, for instance it misses many of the documented arguments on snapshot restore, whereas POST / _create does not seem to be mentioned in the documentation as an option.
</comment><comment author="bleskes" created="2015-06-29T08:34:44Z" id="116524025">@szroland fair enough. I would still prefer to separate the two changes for future reference. I just left a comment on #11897 +1'ing one of your suggestions. Let's get #11897 fixed first and then pull the RecoveryState fix immediately afterwards. Makes sense?
</comment><comment author="szroland" created="2015-06-29T20:46:03Z" id="116841142">Sure, I added the pull request over at #11897, if that looks good and gets merged I can fix this PR up
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rethink string versus not_analyzed string mappings and support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11901</link><project id="" key="" /><description>### Problem

In Elasticsearch, you can currently support indexing your string data as either `analyzed` strings, which is great for unstructured, full text search, or as `not_analyzed` strings, which is great for structured search (e.g., _exact_ matches). However, there is frequently an in-between where you want exact matches, but you want them to ignore case or accented characters (`AA == aa == Ââ`). This forces the use of analyzers for normalization.
### Partial Workaround

For those scenarios, you are currently forced to use the analyzed string variant with a specific analyzer. This generally leads to users forgetting to disable a lot of things like norms, positions, and frequencies. Even if you do happen to do all of that right, you simply cannot take advantage of doc values.
### Potential Solution

It would be interesting to potentially rethink strings and how they're mapped. In the case of analyzed strings, there really isn't much need for improvement except potentially the naming of it. For not_analyzed strings, there's a lot of room for improvement.
#### Mockup

``` json
PUT /my-index
{
  "mappings" : {
    "my-type" : {
      "properties" : {
        "full_text" : {
          "type" : "string",
          "analyzer" : "standard"
        },
        "constant_string" : {
          "type" : "constant_string",
          "filter" : [ "lowercase", "trim" ],
          "char_filter" : [ "..." ]
        }
      }
    }
  }
}
```

Note: the difference is that analyzed strings stay "string" and not_analyzed strings become "constant_string". It's unlikely that we could easily change from "string" for analyzed text, but if we could, then perhaps we could switch analyzed strings to be `text` and not_analyzed strings to be just `string`.

This avoids a lot of questions and regular problems. It also provides the exact same functionality as we have today, if you choose to not supply a filter or char_filter for constant_strings, but it also provides more flexibility in that users can finally use doc values with filtered text that still can be reasonably sorted and aggregated in a normalized format, without the ability to confusingly tokenize the string and unnecessarily use norms, position, or frequency data.
</description><key id="91511789">11901</key><summary>Rethink string versus not_analyzed string mappings and support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-06-27T20:43:54Z</created><updated>2015-11-21T16:23:18Z</updated><resolved>2015-11-21T16:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-27T21:25:55Z" id="116145393">I think we should try prototyping a new mapper for `small_string` or whatever? We can figure out the naming afterwards and rename.

I think also it could have a bare minimum of options to start: it doesn't need to support `indexOptions` for frequencies, positions, offsets (and corresponding vector options), doesn't need an option to turn on norms, it doesn't need to support confusing options like `boost` that won't work, etc.
</comment><comment author="nik9000" created="2015-06-27T22:29:12Z" id="116163512">Call it keyword? Token filter is still needed but i feel like the word
token isn't needed any more but don't know a better name.

Should elasticsearch default strings to this when automatically creating
the mapping in any case?  Like maybe if they are short?  Seems funky.
On Jun 27, 2015 5:26 PM, "Robert Muir" notifications@github.com wrote:

&gt; I think we should try prototyping a new mapper for small_string or
&gt; whatever? We can figure out the naming afterwards and rename.
&gt; 
&gt; I think also it could have a bare minimum of options to start: it doesn't
&gt; need to support indexOptions for frequencies, positions, offsets (and
&gt; corresponding vector options), doesn't need an option to turn on norms, it
&gt; doesn't need to support confusing options like boost that won't work, etc.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11901#issuecomment-116145393
&gt; .
</comment><comment author="rmuir" created="2015-06-27T22:39:16Z" id="116163825">i don't like keyword because thats not what is happening. the idea is this field can change, but in a restricted way: one token.

i dont think the defaults should change. no magic! this just makes not-analyzed strings a proper type of its own: gives you an option to normalize short strings (e.g. case-insensitive sorting and aggregations and other basic stuff). 

the real challenge here will be the bikeshedding of the naming...

keep in mind when painting your shed, that the current naming is horrible. Because having two radically different `string` configurations and defaults based on `not_analyzed` is totally counterintuitive.
</comment><comment author="nik9000" created="2015-06-27T22:48:31Z" id="116164169">Yeah. It's only semi-intuitive if you're familiar with lucene. But bike
shedding is fun and I can do it on mobile to kill time.

Call it token? Like, instead of calling it string. As in "here is a token,
no need to tokenize that again. I already did."

I think both char and token filters could still be useful here but would
bring more confusion.
On Jun 27, 2015 6:39 PM, "Robert Muir" notifications@github.com wrote:

&gt; i don't like keyword because thats not what is happening. the idea is this
&gt; field can change, but in a restricted way: one token.
&gt; 
&gt; i dont think the defaults should change. no magic! this just makes
&gt; not-analyzed strings a proper type of its own: gives you an option to
&gt; normalize short strings (e.g. case-insensitive sorting and aggregations and
&gt; other basic stuff).
&gt; 
&gt; the real challenge here will be the bikeshedding of the naming...
&gt; 
&gt; keep in mind when painting your shed, that the current naming is horrible.
&gt; Because having two radically different string configurations and defaults
&gt; based on not_analyzed is totally counterintuitive.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11901#issuecomment-116163825
&gt; .
</comment><comment author="rmuir" created="2015-06-27T23:10:16Z" id="116164897">yeah, personally i'd prefer `string` and `text` :)

I don't mind `token` or `small_string` or similar though. I think it helps the user understand its one token and not be surprised when they configure all the text of moby dick this way and indexwriter gets angry about it.

yeah, i don't have an opinion on charfilters. usually i just see it as a way to tweak the tokenizer. I do think the tokenizer should be limited to or implicitly `keyword` and that the filters should only generate one token. 

We annotate those "safe" analysis factories with an interface in lucene, so we could do some static checking, and of course we should check incrementToken() doesn't return true again at runtime.
</comment><comment author="pickypg" created="2015-06-27T23:14:09Z" id="116165074">I like both `small_string` and `token`. From a new user's perspective, I feel like `small_string` may be easier to figure out, but I think `token` is much clearer in terms of its literal effect.
</comment><comment author="clintongormley" created="2015-06-28T11:44:41Z" id="116262774">What about the path hierarchy tokeniser? I would think that a path field would still be a good candidate for doc values even though it produces multiple tokens?

On naming, I prefer `string` and `text` too, but we should probably abandon `string` for bwc sanity. Another suggestion is `enum`.

Regarding OOB defaults, I agree that we should stick with analysing strings by default. Unfortunately this means that doc values are disabled by default. @jpountz made an intriguing suggestion:

For dynamically mapped string fields, should we add an analysed `text` field with field data losing disabled
</comment><comment author="clintongormley" created="2015-06-28T11:49:10Z" id="116263016">Bah... Finger slipped on the mobile... To continue:

For dynamically mapped string fields, should we add an analysed text field with field data loading disabled, and a `_raw` multi field of type `enum` which is not analysed and has doc values enabled.

It'd mean indexing more data by default but would be a good OOB experience, and users can always specify more optimal mappings themselves. We'd need to make the `_raw` name reserved, but I doubt that'll cause many issues.
</comment><comment author="rmuir" created="2015-06-28T12:07:17Z" id="116265296">personally i think the scope of this issue should be contained: to the simple case of one token initially.

Stuff like path tokenization starts to walk into the world of traps and more rare esoteric use cases. I don't see for example why someone would need to aggregate on this (sorry). And changing what dynamic mappings does could also be another followup issue? Again there are tradeoffs (like disk space).

This issue can be contained, at minimum to a new mapper type that lets you normalize (e.g. lowercase/asciifold) a string... and it has clear use cases like case-insensitive sort. It doesn't really have tradeoffs, its just an option you didnt have before. Personally I think this is enough to chew on at once.
</comment><comment author="clintongormley" created="2015-06-28T12:18:15Z" id="116268293">Agreed ++
</comment><comment author="clintongormley" created="2015-07-10T10:19:10Z" id="120353213">Discussed this in FixItFriday: Let's go with `keyword` (this string can only use a keyword analyzer) and deal with any confusion in the first line of the docs.
</comment><comment author="colings86" created="2015-07-10T10:19:49Z" id="120353959">Can only use a keyword tokenizer you mean? They will need to use other token filters to support lowercasing etc.
</comment><comment author="rjernst" created="2015-07-10T16:27:41Z" id="120451708">+1 to `keyword`! We can just not support the `tokenizer` setting, so it is always `keyword_tokenizer`. As for filters, how should we limit these to filters that do not produce new tokens? Should we start with a static list of supported filters?
</comment><comment author="rmuir" created="2015-07-10T16:31:23Z" id="120452768">lucene already marks these filters with an interface. it needs to be used here.
</comment><comment author="rjernst" created="2015-07-10T16:33:25Z" id="120453347">Ah right, perfect. I can take a shot at this new mapper then.
</comment><comment author="nik9000" created="2015-07-10T16:40:15Z" id="120455025">Yup. It's pretty simple to check the token count on the way out of indexing
a document for that extra bit of sanity.
On Jul 10, 2015 12:33 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; Ah right, perfect. I can take a shot at this new mapper then.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11901#issuecomment-120453347
&gt; .
</comment><comment author="clintongormley" created="2015-11-21T16:23:18Z" id="158659822">Closing in favour of https://github.com/elastic/elasticsearch/issues/12394
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>core-types.asciidoc, fielddata.asciidoc: replace "faceting" with "aggregating"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11900</link><project id="" key="" /><description>Replaced "faceting" with "aggregating" because facets were deprecated. It seems to me that this change makes reading more clear, pointing to appropriate Elasticsearch module (instead of using the term 'faceting' that refers to the deprecated module - this can confuse reader).
</description><key id="91498990">11900</key><summary>core-types.asciidoc, fielddata.asciidoc: replace "faceting" with "aggregating"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels><label>docs</label></labels><created>2015-06-27T18:18:08Z</created><updated>2015-06-30T09:45:09Z</updated><resolved>2015-06-29T12:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="golubev" created="2015-06-27T18:20:33Z" id="116114154">My company - LUN UA LLC - signed the CCLA and had listed me as a contributor. And I've signed the ICCLA under that one above.
</comment><comment author="clintongormley" created="2015-06-29T12:15:17Z" id="116631041">thanks @golubev - merged
</comment><comment author="golubev" created="2015-06-30T09:45:06Z" id="117097988">Thanks, @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error : curl(56)  Recv failure: Connection reset by peer </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11899</link><project id="" key="" /><description>I am trying to load my ES db using bulk API, but I am getting this error
"curl: (56) Recv failure: Connection reset by peer"

The structure of json documents in my json file is correct 
But my file size is 750MB 

Please suggest me some solution 

Thanks
</description><key id="91435630">11899</key><summary>Error : curl(56)  Recv failure: Connection reset by peer </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rpalsaxena</reporter><labels /><created>2015-06-27T09:03:47Z</created><updated>2015-07-03T08:34:23Z</updated><resolved>2015-06-27T10:00:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-27T10:00:08Z" id="116000588">Please use discuss.elastic.co

Try to reduce your bulk size. I think I don't insert more than 10k docs per bulk (small docs). So, some mb at most per bulk.
</comment><comment author="rpalsaxena" created="2015-07-03T08:11:31Z" id="118270453">But elasticsearch is made for handling big size data some GBs of data , even wiki and github are also using it .

I think reducing the input size is not a good solution , but I somehow managed to insert the data using python ..

Even then if someone can suggest me best possible solution then kindly comment 
</comment><comment author="dadoonet" created="2015-07-03T08:34:23Z" id="118275996">Yes. I can confirm that elasticsearch can handle billions of documents, trillions of documents.
It does not mean that you can pass all those docs within one single bulk request.

Have a look BTW at `http.max_content_length`. By default it's limited to 100mb (on purpose).

https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>steps to remove dangerous security permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11898</link><project id="" key="" /><description>These two "unsafe-like" permissions (`sun.misc` and `reflectionFactoryAccess`) should be more contained. Removing any of these permissions is like climbing mount everest, but I think we can make some progress on it.

This patch:
- Add simple infra to do per-jar permissions.
- Remove code copy of jsr166e and use one from maven.
- Grant `sun.misc` access specifically to lucene-core.jar and jsr166e.jar.
- Remove `reflectionFactoryAccess` and grant only to mockito (see below)

Caveats:
- I couldn't yet remove `sun.misc` completely. 4 plugin tests fail, this is because PluginManager currently does evil hacks to add jars directly to the system classloader. This should be avoided (instead elasticsearch.bat/sh should simply setup correct classpath, keep this immutable!!!). But this patch is progress, because there is now just that one thing left to fix.
- Plugins using mockito need to change maven configuration to use https://github.com/rmuir/securemock (can we bring this into an ES repo?). Code changes are not needed. I want to encourage use of mocking. This library is just a binary-compat replacement for Mockito (it wraps it). This scheme was tested with all plugins using mockito.
</description><key id="91424409">11898</key><summary>steps to remove dangerous security permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-27T06:32:32Z</created><updated>2015-06-30T09:40:47Z</updated><resolved>2015-06-29T15:13:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-27T06:34:31Z" id="115975029">@s1monw @uboness appreciate your thoughts on this.
</comment><comment author="dadoonet" created="2015-06-27T08:10:49Z" id="115983674">&gt; This should be avoided (instead elasticsearch.bat/sh should simply setup correct classpath, keep this immutable!!!).

I just love this idea. If we won't never use hot reload, that's just a fantastic simplification.

&gt; I want to encourage use of mocking

Big +1. I really like to have unit tests in plugins and not (less) integration tests
</comment><comment author="s1monw" created="2015-06-29T06:59:26Z" id="116485007">@rmuir even though that this is not perfect this is a massive step in the right direction. Before I say anything else this LGTM and should go in as it is. We can move secure mock in our codebase on core even to make things simpler?

For PluginManager I think we should make classpath handling fixes a blocker for 2.0 and fix it the right way but lets move on here... I am leaning towards making the pluginmanager a sep module anyway but that is a different story.
</comment><comment author="uboness" created="2015-06-29T09:28:06Z" id="116566634">fully agree with @s1monw here... would love to see the secure mock in core! 

+1 on a blocker for fixing the plugin manager. Just as a side note... moving to CP building on the script level would be great, we just need to make sure all works well with all the packages as well (deb, rpm, etc..)
</comment><comment author="rmuir" created="2015-06-29T12:33:31Z" id="116638917">I'll spin off a separate issue for the mocking, and then come back to this one.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/jsr166e/DoubleAdder.java</file><file>core/src/main/java/jsr166e/DoubleMaxUpdater.java</file><file>core/src/main/java/jsr166e/LongAdder.java</file><file>core/src/main/java/jsr166e/LongMaxUpdater.java</file><file>core/src/main/java/jsr166e/Striped64.java</file><file>core/src/main/java/org/elasticsearch/bootstrap/Security.java</file><file>core/src/main/java/org/elasticsearch/common/metrics/CounterMetric.java</file><file>core/src/main/java/org/elasticsearch/common/metrics/EWMA.java</file><file>core/src/main/java/org/elasticsearch/common/metrics/MeanMetric.java</file><file>core/src/main/java/org/elasticsearch/common/metrics/MeterMetric.java</file><file>core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java</file></files><comments><comment>Merge pull request #11898 from rmuir/lockdown</comment></comments></commit></commits></item><item><title>Issues with the json description of Snapshot Create API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11897</link><project id="" key="" /><description>In general, Rest test code picks the HTTP method and path randomly from the API json spec. Not sure what other uses of this specification exist, but this indicates an assumption that if there are multiple methods available, they are all available on all urls (that match the parameters) with the same semantics.

However, this breaks for snapshot create, where the POST and PUT URLs in the API are different, there is an extra `_create` for POST only:

```
        controller.registerHandler(PUT, "/_snapshot/{repository}/{snapshot}", this);
        controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}/_create", this);
```

The API spec in json:

```
    "methods": ["PUT", "POST"],
    "paths": [
      "/_snapshot/{repository}/{snapshot}", 
      "/_snapshot/{repository}/{snapshot}/_create"],
```

So if you tried to use this API in rest tests (which might explain why no such tests currently exist), sometimes RestClient actually picks the wrong combination and does a PUT to `/_snapshot/{repository}/{snapshot}/_create` which happens to get parsed as a create index request (and fails because of the invalid index name `_snapshot`, which could indicate some issue in the dispatch code as well).

So there are a couple of things that could be done:
- Change the Snapshot Create API to have PUT and POST both on all the same paths, in line with other APIs (e.g. add PUT and POST to both, assuming request dispatching could support that, or drop "_create" from the POST path)
- Change the json API spec format to allow the linking of method to path
- Create 2 separate json files for snapshot create, one describing the PUT api and the other the POST api (you would lose the random selection between the methods in tests, and would pick e.g. between snapshot.create or snapshot.create.post in your test descriptor)

I guess the best course of action depends on what uses of the json specification there are, what it is really intended for.

Dropping POST/_create, at least from the spec file, seems to be an interesting option, since I see this has been done on other APIs as well (e.g. search_template and put_script), which have a POST/_create mapping in code but not in the json spec. I guess the issue might be that the path also matches the create index action path `/{index}/{type}/{id}/_create`, not sure if the dispatch logic deterministically considers the create index option last (e.g. prioritizes more specific paths over less specific ones).
</description><key id="91380572">11897</key><summary>Issues with the json description of Snapshot Create API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">szroland</reporter><labels /><created>2015-06-26T22:50:47Z</created><updated>2015-06-30T10:16:23Z</updated><resolved>2015-06-30T10:16:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-29T08:33:15Z" id="116523293">&gt; Change the Snapshot Create API to have PUT and POST both on all the same paths, in line with other APIs (e.g. add PUT and POST to both, assuming request dispatching could support that, or drop "_create" from the POST path)

Let's do this. As you say it's consistent with other api (for example, search templates)
</comment><comment author="clintongormley" created="2015-06-29T12:22:21Z" id="116634233">`.../_create` isn't even documented.  I'd go for just dropping that URL from the spec.
</comment><comment author="bleskes" created="2015-06-29T13:05:06Z" id="116652339">@clintongormley works for me, but let's remove it from the code as well (2.0 only). We should still support POST (at least in the code level) for  `/_snapshot/{repository}/{snapshot}` .
</comment><comment author="clintongormley" created="2015-06-29T13:12:02Z" id="116654409">To be clear: support PUT and POST, but drop the _create endpoint.
</comment><comment author="szroland" created="2015-06-29T20:33:06Z" id="116835433">Added a pull request, please see #11928
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file></files><comments><comment>Merge pull request #11928 from szroland/#11897</comment></comments></commit></commits></item><item><title>Refactoring of RegexpQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11896</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="91330757">11896</key><summary>Refactoring of RegexpQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-26T18:44:39Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-23T15:48:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-07-01T16:59:31Z" id="117751346">heya @alexksikes sorry may I ask you to rebase please? #11974 changed things quite a bit, for the better!
</comment><comment author="alexksikes" created="2015-07-02T00:44:57Z" id="117862364">@javanna OK you can go ahead, it's rebased. Thank you.
</comment><comment author="javanna" created="2015-07-02T07:47:31Z" id="117947222">I did a first round of review, left a few comments
</comment><comment author="javanna" created="2015-07-06T08:16:29Z" id="118769276">left  a few comments
</comment><comment author="alexksikes" created="2015-07-07T08:08:46Z" id="119112866">@javanna thanks for the review, I updated the PR.
</comment><comment author="javanna" created="2015-07-07T08:41:44Z" id="119123243">I did another round of review, maybe @cbuescher wants to have a quick look too?
</comment><comment author="cbuescher" created="2015-07-07T09:49:17Z" id="119144410">Did a quick round of review as well. Beeing late to the party, I first found the change from allowed regex in Builder from String to general Object confusing, also introducing much complexity that wasn't there before. I tried to follow the argument in the comments, especially seeing that `value` was `Object` and also parsed as that in the current parser code. However I feel that using numbers/date values as regex is a little bit strange, would be interested in the reason why this is allowed in the parser (it apparently was introduced long ago with Commit 22077d1c).
Looking at how `value = parser.objectBytes();` currenty cannot return Date object, I think this doesn't have to be supported since any value will be converted using `toString()` later anyway.
</comment><comment author="javanna" created="2015-07-07T10:04:43Z" id="119151877">I share your concerns @cbuescher around supporting Object rather than String, but thought that we already support objects everywhere (parser + mappers) so this really seemed like a leftover in the java api and it makes sense to fix it. let's ask for a second opinion from @jpountz ? :)
</comment><comment author="alexksikes" created="2015-07-13T07:48:56Z" id="120840927">@javanna Thanks for the review. I updated the PR accordingly. 
</comment><comment author="javanna" created="2015-07-13T08:03:53Z" id="120843183">did another round, left a few comments
</comment><comment author="alexksikes" created="2015-07-13T08:25:30Z" id="120846824">@javanna I updated the PR accordingly. Thank you.
</comment><comment author="javanna" created="2015-07-13T08:29:43Z" id="120847726">I think this is ready (besides my last small comment above), but it needs some rebasing before it can get in
</comment><comment author="alexksikes" created="2015-07-13T09:18:20Z" id="120864480">@javanna It's rebased
</comment><comment author="javanna" created="2015-07-13T09:21:48Z" id="120865927">I think we may need to rebase again after #12200 I would wait for that one.
</comment><comment author="alexksikes" created="2015-07-23T15:08:48Z" id="124135047">@javanna It's rebased you can take a look now.
</comment><comment author="javanna" created="2015-07-23T15:16:17Z" id="124136928">left two minor comments, LGTM though
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java</file></files><comments><comment>Refactoring of RegexpQuery</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file></files><comments><comment>RegexpQueryParser takes a String as value like its Builder</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file></files><comments><comment>Properly fix the default regex flag to ALL for RegexpQueryBuilder and Parser</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file></files><comments><comment>fix RegexpQueryBuilder#maxDeterminizedStates</comment></comments></commit></commits></item><item><title>Use only Jackson in favor of XContent?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11895</link><project id="" key="" /><description>I've been using the Java API recently to dynamically generate different types of index mappings for particular use cases and indices.  I found myself implementing POJO's that simulate what XContent parses and using Jackson to produce JSON for the ElasticSearchClient.

I see that XContent uses parts of Jackson under the covers, but I'm a little lost on why most of the endpoints don't just parse POJO objects that may be shared with the Java API.  Not to mention, it seems like some of the other data formats that Jackson has would be useful, such as Avro.

*Edited this after some more investigation.
</description><key id="91325830">11895</key><summary>Use only Jackson in favor of XContent?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mtraynham</reporter><labels><label>:Java API</label></labels><created>2015-06-26T18:18:55Z</created><updated>2015-07-10T14:47:32Z</updated><resolved>2015-07-10T11:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-27T20:53:27Z" id="116140851">Not for or against this, but I wanted to [link to the Avro page](https://github.com/FasterXML/jackson-dataformat-avro#schema-not-optional). One key takeaway that does make it less attractive for me:

&gt; Avro is strongly Schema-based, and all use requires an Avro Schema. Since there is little metadata in encoded in Avro data, it is not possible to know anything about structure of data without Schema.

This makes it inherently harder to implement than the rest of the formats that we do support.
</comment><comment author="mtraynham" created="2015-06-28T00:08:45Z" id="116167075">@pickypg That is very good point! Somewhat of a pitfall to Avro.  For RPC, both parties must know the schema to understand the transmitted Avro data; unlike file based serialization that stores a copy of the schema at the top.  Although, an Avro schema is somewhat similar to an ElasticSearch index mapping and it might be possible to infer that schema (or store it on the index mapping).  I only bring it up because you could get a lot more over wire in less time.

This kind of diverges from my curiosity though, which was more about why XContent responses are built using array/object/field builders and not just POJO's that could be shared through the Java API, like with index mappings or field types or using Jackson directly.
</comment><comment author="spinscale" created="2015-07-10T11:00:30Z" id="120385415">one of the reasons, that we do not use POJOs, is that this would require serialization/deserialization up front and possibly create big objects in memory. This would not be GC friendly.

For example, take an elasticsearch query. Instead of having to pass the pojo to the different query parser (one for a filtered query, one for a geo shape query, etc.. one for each type), we can just pass the xcontent parser and read through it step by step.

As you already mentioned, the `XContent` stuff is just a small layer on top of JSON to simplify the handling of streaming data. Due to this, it is not really easy to create POJOs out of this at this layer and you need to do that in your client code.

As you can see in our `pom.xml` dependencies, we already use the `jackson-dataformat-smile`, `jackson-dataformat-yaml` and `jackson-dataformat-cbor` libraries. Because of the `xContent` abstraction it is really easy for us to support different formats and also extract the format being used in the data sent to elasticsearch (one of the reasons why we can index data in YAML format as well as read JSON configuration files). You may be able to extend the xcontentparsers to also support avro without the need to bind to a POJO by using the `jackson-dataformat-avro` library...

I will close this ticket though, as we will not change our approach here. Hope this helps.

**Update**: One last thing I forgot to mention... we can also do things like exiting early (from example if there is an error how the query was written, not a JSON error, but a semantically one), which would not work in case you create a POJO up front.
</comment><comment author="mtraynham" created="2015-07-10T14:47:32Z" id="120426928">@spinscale Hey thanks for addressing me.  I do understand the benefits of the streaming aspect and the gains that may be had with query parsing/responses.  In terms of fairly static endpoints that don't receive requests very often, and the schema may be somewhat small and addressable (specifically index mappings), it just seems a bit overkill.  The Java API does have fluid interfaces for handling these types of requests, but it simply builds XContent behind the scenes.  In my case, we load a mapping and add/remove fields as we see fit.  So I found myself implementing POJO's to simulate what XContent would generate.  I couldn't find a good way to read a static index mapping from disk, nor an easy way to modify the fields that were on that mapping using XContent.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[doc] Reorganize and clean Java documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11894</link><project id="" key="" /><description>This commit reorganizes the docs to make Java API docs looking more like the REST docs.
Also, with 2.0.0, FilterBuilders don't exist anymore but only QueryBuilders.
## Remove removed queries/filters
- Remove Constant Score Query with filter
- Remove Fuzzy Like This (Field) Query (flt and flt_field)
- Remove FilterBuilders
## Move filters to queries
- Move And Filter to And Query
- Move Bool Filter to Bool Query
- Move Exists Filter to Exists Query
- Move Geo Bounding Box Filter to Geo Bounding Box Query
- Move Geo Distance Filter to Geo Distance Query
- Move Geo Distance Range Filter to Geo Distance Range Query
- Move Geo Polygon Filter to Geo Polygon Query
- Move Geo Shape Filter to Geo Shape Query
- Move Has Child Filter by Has Child Query
- Move Has Parent Filter by Has Parent Query
- Move Ids Filter by Ids Query
- Move Limit Filter to Limit Query
- Move MatchAll Filter to MatchAll Query
- Move Missing Filter to Missing Query
- Move Nested Filter to Nested Query
- Move Not Filter to Not Query
- Move Or Filter to Or Query
- Move Range Filter to Range Query
- Move Ids Filter to Ids Query
- Move Term Filter to Term Query
- Move Terms Filter to Terms Query
- Move Type Filter to Type Query
## Add missing queries
- Add Common Terms Query
- Add Filtered Query
- Add Function Score Query
- Add Geohash Cell Query
- Add Regexp Query
- Add Script Query
- Add Simple Query String Query
- Add Span Containing Query
- Add Span Multi Term Query
- Add Span Within Query
## Reorganize the documentation
- Organize by full text queries
- Organize by term level queries
- Organize by compound queries
- Organize by joining queries
- Organize by geo queries
- Organize by specialized queries
- Organize by span queries
- Move Boosting Query
- Move DisMax Query
- Move Fuzzy Query
- Move Indices Query
- Move Match Query
- Move Mlt Query
- Move Multi Match Query
- Move Prefix Query
- Move Query String Query
- Move Span First Query
- Move Span Near Query
- Move Span Not Query
- Move Span Or Query
- Move Span Term Query
- Move Template Query
- Move Wildcard Query

Also closes #7826
Related to https://github.com/elastic/elasticsearch/pull/11477#issuecomment-114745934
</description><key id="91294093">11894</key><summary>[doc] Reorganize and clean Java documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-26T16:38:47Z</created><updated>2015-07-01T20:36:11Z</updated><resolved>2015-07-01T20:35:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-26T16:39:46Z" id="115749138">@clintongormley WIP here. I just need to run the doc generation and check that everything is still ok.
In the mean time, could you tell me what you think about it?

Thanks!
</comment><comment author="dadoonet" created="2015-06-30T12:37:57Z" id="117162033">@clintongormley I pushed a new change which fix errors while building docs. So it's now ready for review...

When I rendered html docs, I did not get the smart and cool menu you made for the REST doc. Could you tell me if I need to change something on my end?
</comment><comment author="clintongormley" created="2015-06-30T12:52:28Z" id="117165512">&gt; When I rendered html docs, I did not get the smart and cool menu you made for the REST doc. Could you tell me if I need to change something on my end?

You just need to build with `--open --web` to view it via a web server

I had a brief skim. In general looks good, although I'd make it a bit more structured, eg make Match All, Full text queries, Term level queries etc use `=== Header` instead of `== Header`.

Same thing could be done for Index, delete, get, update, bulk.  
</comment><comment author="dadoonet" created="2015-06-30T13:41:07Z" id="117187971">I meant that I get this:

![term level queries google chrome aujourd hui at 15 35 03](https://cloud.githubusercontent.com/assets/274222/8432099/4002ac7c-1f3e-11e5-9ba3-e1635374e10f.png)

 instead of this

![queries google chrome aujourd hui at 15 35 18](https://cloud.githubusercontent.com/assets/274222/8432105/4c2ecba2-1f3e-11e5-9a1e-7e1e1888fde4.png)

Opening with `--open --web` gives the same result.

&gt; I'd make it a bit more structured

Ok. Will update. Thanks!
</comment><comment author="dadoonet" created="2015-06-30T15:10:52Z" id="117221594">@clintongormley I think it's ok now in term of layout. Thanks for the comment. It's more readable now.
May be you could then check how to render a tree menu as you did for the reference guide but for  the java guide?

@rjernst Wanna review the Java content itself?
</comment><comment author="rjernst" created="2015-06-30T17:28:56Z" id="117270903">Java content looks fine to me. However, long term I think we should work towards having the client classes cleanly separated from the rest of the server, and making sure they are all documented, so that javadocs can be all the documentation we need for the java api.
</comment><comment author="dadoonet" created="2015-06-30T21:59:47Z" id="117356969">You have no idea how much I agree with you!
</comment><comment author="clintongormley" created="2015-07-01T08:50:01Z" id="117545399">@dadoonet ah ok - add `--chunk 1` when building.  You need to add that option here as well: https://github.com/elastic/docs/blob/master/conf.yaml#L136

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename caches.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11893</link><project id="" key="" /><description>In order to be more consistent with what they do, the query cache has been
renamed to request cache and the filter cache has been renamed to query
cache.

A known issue is that package/logger names do no longer match settings names,
please speak up if you think this is an issue.

Here are the settings for which I kept backward compatibility. Note that they
are a bit different from what was discussed on #11569 but putting `cache` before
the name of what is cached has the benefit of making these settings consistent
with the fielddata cache whose size is configured by
`indices.fielddata.cache.size`:
- index.cache.query.enable -&gt; index.requests.cache.enable
- indices.cache.query.size -&gt; indices.requests.cache.size
- indices.cache.filter.size -&gt; indices.queries.cache.size

Close #11569
</description><key id="91293189">11893</key><summary>Rename caches.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-26T16:33:31Z</created><updated>2015-06-29T08:21:21Z</updated><resolved>2015-06-29T08:21:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-26T16:35:52Z" id="115748329">this is so awesome @jpountz, thank you for doing this, its big for our ability to explain how 2.0 works now. LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/index/IndexQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Fix missing renamings.</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/filter/FilterCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/index/IndexQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/none/NoneQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/RequestCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineSearcherFactory.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java</file><file>core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file><file>core/src/test/java/org/elasticsearch/test/search/MockSearchService.java</file></files><comments><comment>Merge pull request #11893 from jpountz/fix/rename_cache</comment></comments></commit></commits></item><item><title>Request Timeout Error after multiple search hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11892</link><project id="" key="" /><description>Hi,

I have nodejs script in which I am using same es client instance to search for more than 50 times in a loop. After first 20 search the es client throws exception [Error: Request Timeout after 30000ms]

Any help would be highly appreciated.

Thanks in advance!
</description><key id="91285683">11892</key><summary>Request Timeout Error after multiple search hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranm</reporter><labels /><created>2015-06-26T15:53:49Z</created><updated>2015-06-26T16:19:14Z</updated><resolved>2015-06-26T16:19:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imranm" created="2015-06-26T15:55:37Z" id="115739393">``` javascript
var es = new elasticsearch.Client({ host: ES_HOST, requestTimeout: 30000, maxSockets: 20 });
```
</comment><comment author="clintongormley" created="2015-06-26T16:19:11Z" id="115744972">This issue was moved to elastic/elasticsearch-js#230
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`inner_hits` total count is not being calculated correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11891</link><project id="" key="" /><description>The `inner_hits` counter `total` is not returning the correct number of elements inside `inner_hits.&lt;inner_hits_name&gt;.hits.hits` array. 
</description><key id="91285244">11891</key><summary>`inner_hits` total count is not being calculated correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RanadeepPolavarapu</reporter><labels><label>feedback_needed</label></labels><created>2015-06-26T15:52:11Z</created><updated>2015-06-29T13:34:20Z</updated><resolved>2015-06-29T13:34:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-26T16:17:40Z" id="115744709">@RanadeepPolavarapu could you provide a recreation so we can see what you're seeing?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>change CORS allow origin default to allow no origins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11890</link><project id="" key="" /><description>Today, we disable CORS by default, but if a user simply enables CORS their instance of
elasticsearch will allow cross origin requests from anywhere, as the default value for allowed
origins is `*`.

This changes the default to be `null` so that no origins are allowed and the user must explicitly
specify the origins they wish to allow requests from. The documentation also mentions that there
is a security risk in using `*` as the value.

Closes #11169
</description><key id="91275969">11890</key><summary>change CORS allow origin default to allow no origins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-26T15:14:22Z</created><updated>2015-07-14T16:28:19Z</updated><resolved>2015-07-10T18:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T16:13:45Z" id="119643394">Should there be a note in the migration guide about this change?
</comment><comment author="jaymode" created="2015-07-08T19:24:52Z" id="119704958">Good catch @jpountz! Added the migration docs and fixed the typo 
</comment><comment author="jpountz" created="2015-07-08T20:13:51Z" id="119717995">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java</file></files><comments><comment>Merge pull request #11890 from jaymode/cors</comment></comments></commit></commits></item><item><title>Query refactoring: validate inner queries whenever supported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11889</link><project id="" key="" /><description>Added validation for all inner queries that any already refactored query may hold. Added also tests around this. At the end of the refactoring validate will be called by SearchRequest#validate during TransportSearchAction execution, which will call validate against the top level query builder that will need to go and validate its data plus all of its inner queries, and so on.
</description><key id="91254965">11889</key><summary>Query refactoring: validate inner queries whenever supported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-26T13:40:59Z</created><updated>2015-06-30T12:28:30Z</updated><resolved>2015-06-30T12:28:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-26T15:35:49Z" id="115733824">I went through the changes quickly, it is great that we can validate recursively that way. LGTM besides the small remark which at the moment doesn't seem solvable, unless you have an idea. Tracking the path in the query tree as state in some collector object we pass in with the topmost query might be possible, but maybe overkill at this point.
</comment><comment author="javanna" created="2015-06-30T09:11:58Z" id="117066652">&gt; LGTM besides the small remark which at the moment doesn't seem solvable, unless you have an idea.

I would leave keeping track of the query tree etc. for later, it is a nice to have though, opened #11939 for that.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java</file></files><comments><comment>Query refactoring: validate inner queries whenever supported</comment></comments></commit></commits></item><item><title>Problem with cardinality count..</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11888</link><project id="" key="" /><description>Hi,
Am using cardinality to get the count of unique values of a field...
Here is my Query..
GET hahn_pcdb/hahn_pcdb_type/_search
{
   "size": 0,
   "aggs": {
      "category_name": {
         "terms": {
            "field": "CATEGORY_NAME",
            "size": 0
         },
         "aggs": {
            "part_number": {
               "cardinality": {
                  "field": "PART_NUMBER"
               }
            }
         }
      }
   }
}

But when am checking the count of them in MySql getting variation in both distinct count..
Can anyone trace this...

And here are my mappings..

"hahn_pcdb_type": {
            "properties": {
               "BRAND_NAME": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "raw": {
                        "type": "string"
                     }
                  }
               },
               "CATEGORY_ID": {
                  "type": "string"
               },
               "CATEGORY_NAME": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "raw": {
                        "type": "string"
                     }
                  }
               },
               "GROUP_ID": {
                  "type": "string"
               },
               "GROUP_NAME": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "raw": {
                        "type": "string"
                     }
                  }
               },
               "LINE_CODE": {
                  "type": "string"
               },
               "PART_NUMBER": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "PART_TYPE_DESC": {
                  "type": "string",
                  "index": "not_analyzed",
                  "fields": {
                     "raw": {
                        "type": "string"
                     }
                  }
               },
               "PART_TYPE_ID": {
                  "type": "string"
               }
            }
         }
      }
   }
</description><key id="91230894">11888</key><summary>Problem with cardinality count..</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HemaAnusha</reporter><labels /><created>2015-06-26T11:42:33Z</created><updated>2015-06-26T15:48:04Z</updated><resolved>2015-06-26T15:48:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marg51" created="2015-06-26T12:11:03Z" id="115657173">Hi @HemaAnusha 
Cardinality is approximate, the behaviour you're seeing is expected. 
See https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html#_counts_are_approximate for more informations
</comment><comment author="HemaAnusha" created="2015-06-26T12:58:45Z" id="115671666">Hi Marg,
Thanks for your response, I used the precision threshold: 40000 which is giving the accurate count, even that is also not the exact count but near to that, the accuracy increased.
So how can i achieve exact count of unique values, is there any way ?
</comment><comment author="clintongormley" created="2015-06-26T15:48:03Z" id="115737435">Hi @HemaAnusha 

This issues list is for bug reports and features request. A better place to ask such questions is in the forum: https://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Give _uid doc values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11887</link><project id="" key="" /><description>We already use fielddata on the `_uid` field today in order to implement random sorting. However, given that doc values are disabled on `_uid`, this will use an insane amount of memory in order to load information in memory given that this field only has unique values.

Having better fielddata for `_uid` would also be useful in order to have more consistent sort order when paginating or hitting different replicas: we could always add a tie-break on the value of the `_uid` field.

I think we have several options:
- Option 1: Add SORTED doc values to `_uid`
- Option 2: Add BINARY doc values to `_uid`
- Option 3: Add SORTED doc values to `_type` and `_id`
- Option 4: Add SORTED doc values to `_type` and BINARY to `_id` 

Option 2 would probably be wasteful in terms of disk space given that we don't have good compression available for binary doc values (and it's hard to implement given that the values can store pretty much anything).

Options 3 and 4 have the benefit of not having to duplicate information if we also want to have doc values on `_type` and `_id`: we could even build a BINARY fielddata view for `_uid`.

Then the other question is whether we should rather use sorted or binary doc values, the former being better for sorting (useful for the consistent sorting use-case) and the latter being better for value lookups (useful for random sorting).
</description><key id="91227042">11887</key><summary>Give _uid doc values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2015-06-26T11:18:31Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-12-01T16:55:56Z" id="161030076">My vote would be for number three or four:

&gt; Option 3: Add SORTED doc values to _type and _id
&gt; Option 4: Add SORTED doc values to _type and BINARY to _id

With #14783 we already enable doc values for `_type`, so it makes sense to individually call out the `_id` as well. This also allows changes to happen to `_type` without necessarily breaking `_id`.

In my experience, most users do not use random sorting, but sorting on `_id` is not very common either. With only sorting in mind, I would expect to see `_id` used for non-random sorting a lot more than for random sorting. However, for other use cases, such as referencing the `_id` in other scenarios that use fielddata (e.g., rarely, but sometimes in aggregations, as well as scripts), it may tip it in favor of being binary.
</comment><comment author="rmuir" created="2015-12-01T17:12:00Z" id="161036151">There is a _huge_ difference between `_type` and `_id` when it comes to the expense of docvalues.

`_type` is a lowish cardinality field. This means if you have only 2 unique values `foo` and `bar`, lucene will deduplicate this and write 1 bit per document. If you only have 1 unique `type` (also common), we will write 0 bits per document, each segment just has in the metadata "all docs have value `foo`: it costs nothing). So for 10M documents with 2 types, docvalues for this field costs a little over a megabyte.

On the other hand unique ids are high cardinality by definition: deduplication does nothing. Either choice is extremely costly in comparison. Lets consider 10M documents with ids of length in bytes 16 each and make some guesses:
- BINARY might you ~ 160MB for the bytes (10M \* 16). That is how binary works: its just a straightforward encoding of what you gave it. If the IDs do not have a fixed length, but are instead variable length, then there are additional costs.
- SORTED might cost you ~160MB for the bytes (10M \* 16) and additional 30MB (10M \* 24bpv) for ordinals. The bytes are prefix compressed in this case, because access by ordinal is more important, but for randomish ids this compression will probably not be very efficient. Access to the bytes is also slower, that is the downside of prefix compression (which likely does not help).

I just want to make it clear this is apples and oranges. The fact we turned on docvalues for type is irrelevant when it comes to unique ids. We need very strong use cases and features IMO if we are going to incur this cost.
</comment><comment author="pickypg" created="2015-12-01T17:18:05Z" id="161037816">Very good info @rmuir, as usual. It makes me think that `_id` supporting doc values should exist (particularly in light of #15155), but it should be opt-in.
</comment><comment author="mikemccand" created="2015-12-01T17:20:21Z" id="161038415">I think it's actually 20 bytes for ES's auto-generated IDs (15 fully binary bytes for the Flake ID, and 20 bytes once it's Base64 encoded) ... but, yeah, this would be a big cost ...
</comment><comment author="rmuir" created="2015-12-01T17:23:51Z" id="161039291">Why do we base64? This probably bloats the terms dict today.
</comment><comment author="rjernst" created="2015-12-01T17:30:59Z" id="161041057">&gt; It makes me think that _id supporting doc values should exist 

Why can't a user store this in their own field if they want to do something crazy with it? I don't think we should add back configurability for metadata fields, even if it is just one. It was a lot of work to remove that (#8143), and these are our fields, for internal use by elasticsearch. Edge cases like described in  #15155 can be handled by a user field with doc values enabled, if they want to do such a crazy thing.
</comment><comment author="pickypg" created="2015-12-01T17:34:55Z" id="161042056">But edge cases like #15155 cannot be handled without some other special handling because it's the access of the `_id` that is the slowdown. Adding a doc value field does not bypass that cost.
</comment><comment author="eeeebbbbrrrr" created="2015-12-21T23:31:28Z" id="166457908">Hi all!  @pickypg linked this issue to me because he knows it's near and dear to my heart.

My exact use case (shameless plug:  @zombodb:  https://github.com/zombodb/zombodb) is actually what y'all are describing as an "edge case" in #15155 -- that is, ES is being used as a searching index only (ie, store=false, _source=disabled), and an external "source of truth" (Postgres) is used to provide document data back to the user.

While @zombodb might be unique in implementation, I doubt its general approach of providing `_id` values and using them to later lookup records in an external source is.

An implementation detail is that @zombodb, through a REST endpoint plugin, uses the SCAN+SCROLL API to retrieve all matching `_id` values, re-encodes them as 6byte pairs, and streams them back as a binary blob.

Against ES v1.7 (and 1.6 and 1.5), benchmarking has shown that the overhead of simply retrieving the `_id` value completely swamps searching and even the String--&gt;byte encoding ZDB does, so I'm excited y'all are looking at ways to make this better.

(as an aside, I've actually spent quite a bit of time debugging this (against 1.5), and found that _if_ a parent&lt;--&gt;child mapping exists, using its cache to lookup the `_id` by ordinal (bypassing Lucene, decompressing, and decoding the `_id`) is nearly an order of magnitude faster.  I gave some patches to @pickypg awhile back through my employer's support agreement, but we all kinda decided it wasn't worth the effort of integrating into ES because v2.0 was near and changed everything.)

The idea that such things can "be handled by a user field with doc values enabled" isn't really true, as @pickypg pointed out, because ES is still doing all the work to retrieve the `_id` value for each hit.

So a half-baked idea would be:  What if retrieving the `_id` could be disabled on a search-by-search basis?  Instead, the search request would specify a "user field with doc values enabled" that is a copy of the `_id` value.  Maybe more generally, the ability to elide returning all the fields that are deemed "for internal use by elasticsearch"? 
</comment><comment author="eeeebbbbrrrr" created="2015-12-22T22:35:17Z" id="166752409">So I experimented with this idea (disabling returning _id and _type) against v1.7 (I'm not in a position to work with v2.x yet).

All I did was quickly hack `FetchPhase.java` to set the `fieldsVisitor` to null and then guard against that in the places it's used, and hardcoded both the "type" and "id" properties of the `SearchHit` to the empty string.

I then setup a little benchmark using @zombodb.

With a query that returns 14k documents, retrieving all the "ids" in a SCAN+SCROLL loop:

Stock ES: 17 per second
Hacked Version: 120 per second

Of course, all the ids were blank, so it's not very useful!

I then added a `doc_values=true` field to the index that contains a copy of the `_id` field.  Against the hacked version, I was able to sustain 104 per second.  That's about a 6x gain.  There's definitely quite a bit of overhead in uid decoding.

In case you care how I hacked FetchPhase.java:  https://gist.github.com/eeeebbbbrrrr/9af88e6dc88943450c73
</comment><comment author="rmuir" created="2015-12-23T12:58:06Z" id="166889526">&gt; With a query that returns 14k documents

You should just return the top-N instead. That is what lucene is designed to do.
</comment><comment author="eeeebbbbrrrr" created="2015-12-23T17:20:04Z" id="166948302">&gt; You should just return the top-N instead. That is what lucene is designed to do.

The point is that there's room for significant improvement around how `_uid` is handled.  I was trying to show what the overhead is -- and on my test data on my laptop, it's about 6x.  If a reasonable way to improve this can be found, everyone wins.
</comment><comment author="rmuir" created="2015-12-23T17:51:31Z" id="166956217">Well, lucene just isn't designed to return 14k documents, and by the way docvalues aren't designed for that either. for such huge numbers then a database is a better solution, as it is designed for those use cases.

Just like you wouldn't move your house with a sports car: its a faster vehicle, but its gonna be slower overall.
</comment><comment author="eeeebbbbrrrr" created="2015-12-23T18:20:08Z" id="166962708">&gt; Just like you wouldn't move your house with a sports car: its a faster vehicle, but its gonna be slower overall.

I don't know how this is relevant. 

If y'all make progress towards improving `_uid` in whatever way, I'd be happy to help test and benchmark changes.
</comment><comment author="shamak" created="2016-02-27T00:38:21Z" id="189538989">Hey, I stumbled upon this issue while I was trying to do something similar in Elasticsearch. I aimed (ambitiously) to retrieve ~1million documents in under 1 second based on a simple filter query. I noticed the unzipping of the '_id' field was taking a while (~8seconds) using the hot_threads API:

```
org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:179)
org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
org.apache.lucene.store.DataInput.readVInt(DataInput.java:122)
org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsRe
der.java:249) org.apache.lucene.index.SegmentReader.document(SegmentReader.java:335)
org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:427)
org.elasticsearch.search.fetch.FetchPhase.createSearchHit(FetchPhase.java:219)
org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:184)
org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:401)
org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:833)
org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:824)
```

So I wrote a plugin to stop retrieving the '_id' field, and just retrieve a secondary, integer, doc_values field from the document, specified in the query. I thought this would be super quick but suprisingly, it took almost the same amount of time and now, the hot_threads API showed:

```
 org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:179)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:122)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:249)
       org.apache.lucene.index.SegmentReader.document(SegmentReader.java:335)
       org.elasticsearch.search.lookup.SourceLookup.loadSourceIfNeeded(SourceLookup.java:70)
       org.elasticsearch.search.lookup.SourceLookup.extractRawValues(SourceLookup.java:145)
       plugin.retrievedocvalues.search.fetch.CustomFetchPhase.createSearchHit(CustomFetchPhase.java:256)
       plugin.retrievedocvalues.search.fetch.CustomFetchPhase.execute(CustomFetchPhase.java:189)
       plugin.retrievedocvalues.search.CustomSearchService.executeFetchPhase(CustomSearchService.java:500)
```

The query I'm using is against a custom endpoint and the body is:

```
 { "sort": "_doc",
    "_source": false,
   "fields": ["foo"]
    "size": 1000000,
    "filter": {
        "bool": {
            "should": [
                {
                    "term": {
                        "foo": "bar"
                    }
                },
                {
                    "term": {
                        "baz": "qux"
                    }
                }
            ]
        }
    }
```

The field 'foo' is an integer field which has doc_values enabled on ES version 1.7.1. The weird thing is the aggregation on the field is super quick, but retrieving the data itself is slow.

I guess the underlying point is it may not be that much faster to enable doc_values on the '_id' field since I can't see much of an improvement, unless I'm missing something which someone here could point out?
</comment><comment author="bleskes" created="2016-02-29T13:05:25Z" id="190202209">@shamak you can use `fielddata_fields` in your search request to retrieve field values from doc values (or in memory field data). Fields are meant to get stored fields with a fall back to _source (which was [removed](15017) in 5.x as it is confusing):

```
GET _search
{
  "fielddata_fields": [ "fieldname"]
}
```

Note though that getting 10K docs should be done with a scroll rather than getting so many docs at one.

Since we now promote doc values as a possible data storage (next to _source and stored fields) , I wonder if we should support a `doc_value_fields` entry in the search response. I think more and more people will expect it to be there. /cc @clintongormley @jpountz 
</comment><comment author="clintongormley" created="2016-02-29T14:36:29Z" id="190234373">&gt; Since we now promote doc values as a possible data storage (next to _source and stored fields) , I wonder if we should support a doc_value_fields entry in the search response. I think more and more people will expect it to be there. /cc @clintongormley @jpountz

That's essentially what `fielddata_fields` is.  We were talking about not using the doc values terminology in favour of in-memory vs on-disk fielddata, although I don't think that's the right tradeoff either.  The "fielddata" term has history, and referring to doc values as "on-disk" does them a disservice given that they're usually cached in RAM.

So yes, maybe we should add `doc_values_fields` (or just `doc_values`?) as a synonym for `fielddata_fields`?
</comment><comment author="jpountz" created="2016-03-18T10:47:48Z" id="198302459">Something else we could consider would be to only store the id and type in doc values and not in stored fields in order to not incur a large increase of index size. The benefit is that we would not need any new option on the mappings. However the fetch phase would have to 3 random seeks instead of 1, which could hurt if the index size is much larger than the fs cache.
</comment><comment author="nik9000" created="2016-03-18T14:52:47Z" id="198395438">&gt; However the fetch phase would have to 3 random seeks instead of 1, which could hurt if the index size is much larger than the fs cache.

I suppose then disabling _source would entirely skip stored fields which is kind of cool.

I suspect the _type is going to be cached super fast, especially if we ever decide to sort by _type. Many many use cases use a single type per index so the type lookup is just metadata. Either way I suspect you'd see closer to 2 seeks than 3. Even still, 2 is much worse than 1.

Another question: do we really need to return the _id and _type all the time? I know I typically just wanted some portion of the _source. Usually, like, two or three fields from _source and a couple of highlights. Anyway, maybe we should allow those to be disabled.
</comment><comment author="pickypg" created="2016-03-18T15:26:10Z" id="198408478">I like the idea of _not_ always returning those fields as it's unnecessary information in a lot of cases, especially for the single `_type` use case. We call it metadata, so maybe we should treat it like metadata and only return it when requested (defaulting to true).
</comment><comment author="jpountz" created="2016-03-18T20:54:32Z" id="198538157">I am fine with allowing some of those meta fields to not be returned, but I tend to like that they are returned by default: it is easy to forget that some things are not available if they are not returned by default, and it makes reindexing easier as you don't have to think about fields that you might need for reindexing: everything is there by default.
</comment><comment author="jimczi" created="2016-06-01T08:58:20Z" id="222933618">I made some tests to check the cost of adding the docvalues to the _id field. I tried to index 1M documents with one field (_id) and different configurations.
I tested 3 configurations:
- _id with index=true and stored=true
- _id with index=true, stored=false and binary doc values.
- _id with index=true, stored=false and sorted doc values.
  For the generation of the _id I tried all the configurations with UUIDs.base64UUID and UUIDs.randomBase64UUID. 

### base64UUID

| Configuration | Size | Docs/s | Random access (docs/s) | Sequential access(docs/s) |
| :-: | :-: | :-: | :-: | :-: |
| Stored | 12 MB | 372,000 | 532,000 | 716,000 |
| BinaryDV | 26 MB | 378,000 | 9,009,000 | 40,000,000 |
| SortedDV | 13 MB | 255,000 | 4,608,000 | 16,129,000 |

The binary doc values doubles the size of the index because they don't use any compression. They are very fast for accessing any values and the indexation speed is almost the same as the stored field. 
The sorted doc values have almost the same size than the stored field, this is due to the prefix compression that they use to store the values.  They are also quite fast to access any values but the indexation is slower ( ˜= 30% slower).

### randomBase64UUID

| Configuration | Size | Docs/s | Random access (docs/s) | Sequential access(docs/s) |
| :-: | :-: | :-: | :-: | :-: |
| Stored | 49 MB | 332,000 | 719,000 | 1,751,000 |
| BinaryDV | 46 MB | 358,000 | 8,695,000 | 38,461,000 |
| SortedDV | 48MB | 246,000 | 5,524,000 | 9,523,000 |

For the random id case, the size of the index is almost the same for the 3 configurations but the sorted doc values are still slower to index the data.

I ran some benchmark and the extra cost during the indexation for the sorted doc values is the sorting of the dictionary (in this case we need to do it twice, one for the terms dictionary of the postings and one for the sorted doc values).
Since each _id is unique I tried to add a way to search on the sorted doc values directly, to do so I just added a file that contains the docID for each _id. It's an extra cost of 4 bytes per documents (it's faster for random access to use a full int instead of a vint or a block compression) and to search a docID it needs to retrieve the ordinal of the term first and then seek/read the docID. For existing _id the search is faster than the one that uses the postings but it can be slower when the _id does not exist and does not share a prefix with the existing ones (the latter case is optimized in the terms dictionary of the postings).
I don't know if this can be something we want to explore but I wanted to propose at least one option if the extra cost of adding doc values to the _id field is prohibitive.
</comment><comment author="jpountz" created="2016-06-01T22:55:36Z" id="223149265">Thanks for testing! The hybrid postings/doc-values idea sounds appealing, but it might be challenging to expose it cleanly? (I haven't thought much about it). Otherwise I am wondering how much [LUCENE-7299](https://issues.apache.org/jira/browse/LUCENE-7299)  would close the gap in terms of indexing speed with SORTED_SET doc values and also that maybe we should implement tome simple compression on binary doc values for such cases (eg. based on the most common ngrams).
</comment><comment author="rmuir" created="2016-06-02T13:27:19Z" id="223290808">I don't think the idea of trying to use the postings dictionary for the term dictionary will work well (besides practical concerns). It will simply be too slow.

The problem is, they are different data structures (it is like trie versus tree, but the difference is important).

The terms dictionary is optimized for lookup by "String", but the docvalues dictionary is optimized for lookup by ordinal.

The docvalues lookup by term is much slower than the postings one, because its not optimized for that. The inverse is true for lookup by ordinal: the entire datastructure is built around doing this with as little overhead as possible: it can do random access within a block, etc. 

Given that even a vint for prefix/suffix length is too costly for that case, I don't think we should introduce a branch per-byte with something like n-gram compression. I have run the numbers for that on several datasets (real data: not artificial crap like IDs) and it only saves something like 25% space for that datastructure, depending on the text: in many cases lower than that.

Its important to keep seek-by-ord fast at the moment, because too much code uses sorted/sorted_set docvalues in an abusive fashion with a seek-by-ord for every document, to lookup the text. Elasticsearch has gotten a little better by incorporating things like global ordinals, but it still has bad guys like its scripting support. There are similar cases for other lucene users and even in some lucene modules itself. Historically, people wrote code expecting this to be "ok" and "fast" with fieldcache/fielddata, because that did no compression at all: not even prefix compression. A lot of this code was just ported to docvalues without addressing this, so we still have to keep it fast.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Store filter cache statistics at the shard level instead of index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11886</link><project id="" key="" /><description>Today we keep track of how often filters are used at the index level in order
to decide whether they should be cached or not. This is an issue if you have
several shards of the same index on the same node as it will multiply statistics
by the number of shards that you have for this index on the node, which defeats
the purpose of waiting for a filter to be reused before caching it.
</description><key id="91223038">11886</key><summary>Store filter cache statistics at the shard level instead of index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-26T10:52:51Z</created><updated>2015-06-29T11:42:22Z</updated><resolved>2015-06-29T11:35:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-29T08:49:36Z" id="116530788">I'm good with the change, I just wonder if we should use Guice to instantiate it. We don't expose custom classes setting anyway. Can we instantiate it in the IndexShard constructor? This means that we could pass in some settings in the future (like the minimum segment size etc.)
</comment><comment author="jpountz" created="2015-06-29T10:59:38Z" id="116607409">Thanks @bleskes you made a very good point. I rebased against master which changed a bit after the cache renamings, and applied your suggestion.
</comment><comment author="bleskes" created="2015-06-29T11:42:22Z" id="116624379">for completeness - LGTM :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Merge pull request #11886 from jpountz/fix/shard_filter_cache_stats</comment></comments></commit></commits></item><item><title>Refactors FilteredQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11885</link><project id="" key="" /><description>Separates JSON parsing from Lucene query creation, adds support for streaming, hashCode and equals as well as unit tests.

Relates to #10217 

@cbuescher assigning to you for a first round of reviews. Not particularly happy with the createExpectedQuery implementation at the moment.
</description><key id="91187392">11885</key><summary>Refactors FilteredQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-26T08:19:05Z</created><updated>2015-07-08T08:59:06Z</updated><resolved>2015-07-08T08:58:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-26T14:42:59Z" id="115711787">I  went through it and it looks like there only a few minor changes to address. Left a few comments.
</comment><comment author="MaineC" created="2015-06-26T18:47:34Z" id="115829214">Thanks for your comments. Looking into the changes needed.
</comment><comment author="MaineC" created="2015-06-29T19:43:40Z" id="116814132">@cbuescher I believe I addressed your comments. Added call to AbstractQueryBuilder#doXContentInnerBuilder in doXContent - happy to remove if #11915 comes in before this one.
</comment><comment author="cbuescher" created="2015-06-30T09:31:57Z" id="117083949">Left one very minor comment, otherwise LGTM. Some of the code concerning null-checks for inner queries can be changed once #11915 is on feature branch. @javanna can you also have a look?
</comment><comment author="MaineC" created="2015-07-01T09:37:09Z" id="117563356">Left some explanations and updated the PR.
</comment><comment author="javanna" created="2015-07-01T16:57:56Z" id="117751001">I did another round, looks good but I would need to have another look after you rebased I think, #11974 changed things quite a bit (for the better!)
</comment><comment author="MaineC" created="2015-07-02T08:50:47Z" id="117962817">Did another round after re-basing.

@jpountz if you have a few free cycles it would be good if you could take a final look at the way the Lucene query is generated as well as I restructured it quite a bit.
</comment><comment author="jpountz" created="2015-07-02T09:39:00Z" id="117978807">&gt; @jpountz if you have a few free cycles it would be good if you could take a final look at the way the Lucene query is generated as well as I restructured it quite a bit.

This looks good to me!
</comment><comment author="MaineC" created="2015-07-02T19:02:16Z" id="118130035">@jpountz Thanks for the feedback.
</comment><comment author="javanna" created="2015-07-06T12:27:22Z" id="118839706">left a few comments, @cbuescher please have a look too :)
</comment><comment author="cbuescher" created="2015-07-06T13:27:06Z" id="118853543">Also went through the changes once again, would prefer null-checks in constructors as well, left some more remarks.
</comment><comment author="MaineC" created="2015-07-07T08:51:37Z" id="119127029">Went through your comments and updated the PR (noticed only after pushing that one of the questions I had left was answered by both of you in the mean time - you are too fast answering questions). Updated code is ready for another look.
</comment><comment author="cbuescher" created="2015-07-07T09:04:19Z" id="119131757">@MaineC left a few comments, sorry I didn't realize I should not to that on the individual commits but rather on the overall change. Will do so next time.
</comment><comment author="javanna" created="2015-07-07T10:29:58Z" id="119163500">I did another round of review, left some comments. I'd need @cbuescher to have another look too though  :)
</comment><comment author="cbuescher" created="2015-07-07T21:02:26Z" id="119339849">Also did another round of reviews and left some comments, mostly minor stuff. Agree that the query null-check in the parser doesn't need to be there, happy to clear this up more if there are questions remaining.
</comment><comment author="MaineC" created="2015-07-08T08:37:51Z" id="119497760">Ready for another look.
</comment><comment author="javanna" created="2015-07-08T08:51:54Z" id="119503252">LGTM besides the two minor comments I left
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11885 from MaineC/feature/filter-query-builder-refactoring</comment></comments></commit></commits></item><item><title>Logging: Allow to hardcode logging factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11884</link><project id="" key="" /><description>Due to the mechanism to automatically select an existing logging
factory on startup this might clash with packages that bridge to
existing logging mechanisms.

In order to be more explicit this commit allows to set the
es.logger.impl property to explicitely configure a JDK, log4j or slf4j
logging factory.

Closes #11423
</description><key id="91178386">11884</key><summary>Logging: Allow to hardcode logging factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Logging</label><label>enhancement</label></labels><created>2015-06-26T07:20:34Z</created><updated>2016-02-26T21:44:33Z</updated><resolved>2016-02-26T21:44:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-06-26T08:10:20Z" id="115568385">thinking about this some more, we could even stop trying to do things automatically, document this new setting and just go with log4j by default...
</comment><comment author="jpountz" created="2015-07-08T16:14:52Z" id="119643948">&gt; thinking about this some more, we could even stop trying to do things automatically, document this new setting and just go with log4j by default...

+1 !
</comment><comment author="spinscale" created="2015-07-10T12:46:56Z" id="120401590">updated the PR, added documentation for this settings, removed the automagic portion of this code
</comment><comment author="nik9000" created="2015-09-22T14:22:51Z" id="142303687">I'm trying to figure out what the bridging problem is.

Say you want to use Logback. You'd do this by adding slf4j and log4j-over-slf4j and jcl-over-slf4j to the classpath and elasticsearch would think its logging to log4j because that is the first class it finds. Is the trouble that it'd then try to configure log4j which would fail completely because log4j isn't there?
</comment><comment author="rmuir" created="2015-09-22T14:47:57Z" id="142310718">I am confused by the logging setup myself. Is this another thing where we have conflicting uses of "client" vs "server"?  I am honestly just asking the question, it seems like we can just pick one library that we use, and that is that?

I would love if logging is simpler, I always give up completely if an issue involves logging, I try to avoid use of logging whenever possible in my code, etc because logging is so overengineered and complex.
</comment><comment author="nik9000" created="2015-09-22T14:57:17Z" id="142313421">&gt; I am confused by the logging setup myself. Is this another thing where we have conflicting uses of "client" vs "server"? I am honestly just asking the question, it seems like we can just pick one library that we use, and that is that?

I think so. In the server case right now we support log4j and that is it.

In the client use case we're going to have to conform to whatever the client application wants to use and log4j isn't really a common choice any more. Maybe the problem vanishes in a puff of smoke if we have a java client library.
</comment><comment author="spinscale" created="2015-09-23T08:01:53Z" id="142524407">Yes, this is rather for the client use-case, when someone wants to use another logging framework, which is used by the application. The problem would indeed vanish with a dedicated client.

@nik9000 the current problem is, that you might accidentally use log4j because your bridging library uses the same class. Instead of magically trying to load classes this approach favors explicit configuration. Hope, that explanation makes sense.
</comment><comment author="nik9000" created="2015-09-23T12:27:02Z" id="142584009">&gt; @nik9000 the current problem is, that you might accidentally use log4j because your bridging library uses the same class. Instead of magically trying to load classes this approach favors explicit configuration. Hope, that explanation makes sense.

It makes sense and I'm all for getting it configured for cleanliness but I wanted to be clear on the problem. Is the problem that people using Elasticsearch as a client and using the log4j bridge are triggering elasticsearch to try and configure log4j?
</comment><comment author="rmuir" created="2015-09-23T12:47:12Z" id="142587499">&gt; Yes, this is rather for the client use-case, when someone wants to use another logging framework, which is used by the application. The problem would indeed vanish with a dedicated client.

Can't we still just use one library? We should put the onus on the client to deal with this. There are logging libraries that intercept the api from whichever one we pick and convert it to their logging library of choice.

That is how this should work.
</comment><comment author="nik9000" created="2015-09-23T13:11:11Z" id="142594421">&gt; Can't we still just use one library? We should put the onus on the client to deal with this. There are logging libraries that intercept the api from whichever one we pick and convert it to their logging library of choice.
&gt; 
&gt; That is how this should work.

I think the trouble is the server trying to configure Log4j. We honestly shouldn't do that when elasticsearch is used as a client. If we didn't then we could indeed just target log4j and drop support for the others. People would just use a bridge if they don't like log4j.
</comment><comment author="rmuir" created="2015-09-23T13:39:16Z" id="142604051">&gt; I think the trouble is the server trying to configure Log4j. We honestly shouldn't do that when elasticsearch is used as a client. If we didn't then we could indeed just target log4j and drop support for the others. People would just use a bridge if they don't like log4j.

So lets fix it? This should only happen from Bootstrap.
</comment><comment author="nik9000" created="2016-02-10T15:01:45Z" id="182410337">@spinscale and @rmuir I think this is fixed in master - logging is only set up by Bootstrap and PluginCli.
</comment><comment author="spinscale" created="2016-02-26T21:44:07Z" id="189492896">closing due to #16703
</comment><comment author="nik9000" created="2016-02-26T21:44:33Z" id="189492986">&gt; closing due to #16703

Cool.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Corrupted replica shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11883</link><project id="" key="" /><description>We're using Elasticsearch 1.3.4. Every now and then we run into corrupted replica shard issues on our production cluster. Below is one of the error messages we see in the log file.

```
[MasterNode] [index10][1] received shard failed for [index10][1], node[BujRysY1Sh-g1ZCFjA5Nag], [R], s[INITIALIZING], indexUUID [SEEuraAWTwObNVug0lz4sA], reason [Failed to start shard, message [CorruptIndexException[[index10][1] Corrupted index [corrupted_2rbhI3j1SY-WsNsOgdsjug] caused by: CorruptIndexException[codec footer mismatch: actual footer=1291846144 vs expected footer=-1071082520 (resource: MMapIndexInput(path="&lt;path to one of the .doc file&gt;"))]]]]
```

The replica shard remains in the corrupted state indefinitely even when the corresponding primary shard is in `STARTED` state. Today we workaround this issue by setting the number of replica shards of the affected index to `0` and then back to the original number. Though this fixes the corruption, it has other non-trivial side-effects:
- Replicas of even the unaffected shards get re-created but take a lot of time to get to the `STARTED` state.
- As all replica shards are re-created, filter and fielddata caches for those shards is reset, causing slower queries.

I want to understand if there is a better way to fix such issues or is this fixed in higher versions of Elasticsearch.

Ideally, if a replica shard is corrupted and the primary shard is not, it should recover from the primary shard because the corruption will never self-heal, no?

If that is not possible, is it possible to delete the corrupted replica shard from file system and use the `_cluser/reroute` API to allocate it back? Even better, allocation with `_cluster/reroute` should first check if the shard is corrupted and if yes, delete it and finally allocate.
</description><key id="91176098">11883</key><summary>Corrupted replica shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels /><created>2015-06-26T07:03:40Z</created><updated>2015-06-26T15:40:09Z</updated><resolved>2015-06-26T15:40:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-26T15:40:08Z" id="115735096">Hi @bittusarkar 

This and many other corruption issues have been fixed in later versions. I highly recommend that you upgrade.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TTL recalculated on upsert</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11882</link><project id="" key="" /><description>We are currently running into an issue with ttl and upserts (using doc_as_upsert).  It looks like the ttl is being recalulated when we perform an upsert that does not update _ttl or _timestamp.

Here is the command to create the mapping we are using to test this:

```
curl -XPUT localhost:9200/timetolive -d'{
"mappings": {
    "test": {
        "_ttl": {
            "enabled" : true
        },
        "_timestamp": {
            "enabled" : true, 
            "format":"basic_date_time",
            "path":"post_date"
        },

        "properties": {
            "post_date": {
                "type": "date", 
                "format":"basic_date_time"
            },
            "text": {
                "type": "string"
            }
        }
    }
}
}'
```

Then insert a document with a ttl and a timestamp:

```
curl -XPUT localhost:9200/timetolive/test/1 -d'{
    "text" : "Initial test", 
    "post_date" : "20150625T130000.000000-0700", 
    "_ttl": "20s"
}'
```

Next, update only the text field on the document: 

```
curl -XPOST localhost:9200/timetolive/test/1/_update -d'{
    "doc" : {
        "text" : "Upsert text"
    },
    "doc_as_upsert" : true 
}'
```

After the upsert, the ttl has now been extended.  It looks like is taking the calculated time to live (_ttl + timestamp - now) from the initial request and adding it to the stored timestamp when the upsert comes in. 

We are running ElasticSearch version 1.5.2.
</description><key id="91078816">11882</key><summary>TTL recalculated on upsert</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ronlepper</reporter><labels><label>:Mapping</label><label>bug</label><label>discuss</label></labels><created>2015-06-25T21:21:15Z</created><updated>2016-05-12T12:55:44Z</updated><resolved>2016-05-12T12:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-27T21:14:29Z" id="116144181">The problem is that the partial update will fetch the current document, which contains `"_ttl" : "20s"`, and update just the `text` field. Once done, it will reindex the _whole_ document and the `"_ttl" : "20s"` ends up being reset as a result.

This is why the TTL ends up restarting and, without special-casing TTL in the partial update process, which may be possible, there is not a very good solution to it.

Regardless, I would encourage you to look at using time-based indices rather than TTL-based documents. It's _much_ easier to delete an entire index than it is to have the automatic background query deleting documents, which itself creates load on your index/node/cluster. Naturally this isn't really doable with a `20s` TTL, but hopefully that was just an example value for the demonstration of the issue.
</comment><comment author="ronlepper" created="2015-07-06T19:22:51Z" id="118969500">Unfortunately, the time-based indices won't work for our use case.  The expiration date for each document has the potential to change so we'd need to delete from one index and insert into another index.  Since the documents being indexed are coming in through logstash, we'd rather use the update api and update the expiration date for the document based off the id.

It seems like most of our use cases can be handled with an update script to set the ttl properly, except when we want to set a ttl with an expiration to having no expiration.  If we set the ttl value to null, it will always use the existing calculated expiration time.  From the org.elasticsearch.action.update.UpdateHelper.java class, it looks like whenever ttl is null, it will attempt to pull the existing ttl expiration. Ideally, I would need some constant that could be used to reset a document to an infinite expiration.  

From line 222 of UpdateHelper.java

```
if (ttl == null) {
    ttl = getResult.getFields().containsKey(TTLFieldMapper.NAME) ?    (Long)getResult.field(TTLFieldMapper.NAME).getValue() : null;
    if (ttl != null) {
            ttl = ttl - TimeValue.nsecToMSec(System.nanoTime() - getDateNS); // It is an approximation of exact TTL value, could be improved
     }
}
```
</comment><comment author="pickypg" created="2015-07-06T20:48:28Z" id="118993939">Hi @ronlepper, that's fine and often times unavoidable. However, I'm not convinced that the approach that you're suggesting is superior to the default reset option given the possibility (or likelihood?) of update-after-dead actions, which is particularly expensive alongside TTL functionality.

Ignoring that behavior you _can_ use a Groovy script to imitate the functionality that you are looking for as part of an update (don't use dynamic scripting, but this is the example script to maintain it):

``` json
{
  "script" : "ctx._source.text = text; ctx._source._ttl = ctx._ttl",
  "params" : {
    "text" : "Upsert text"
  },
  "upsert" : {
    "text" : "Upsert text",
    "_ttl" : "20s"
  }
}
```

This uses the "live" TTL value and sets it over the current source value's, which means that it maintains the current TTL rather than resetting it. Note: If you use a script, then [you cannot use the `doc` (use script `params`)](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#_updates_with_a_partial_document).

I do wonder if another flag should be added for updates that allow automatic continuation of the document's TTL (effectively doing the `ctx._source._ttl = ctx._ttl` step of the above script), but I definitely don't think that it should be default.
</comment><comment author="ronlepper" created="2015-07-06T21:21:43Z" id="119001553">Hi @pickypg, thanks for the feedback.  I've implemented a similar script to the one you mention above and had some luck with it for most of my cases. The one case that I can't seem to get working though is removing the expiration.  For example, if I had a document that has a ttl value set and  I want to update the document to have no ttl based expiration.  Setting ctx._ttl to null or ctx._source._ttl to null in the script will just result in the UpdateHelper.java class pulling the current calulated ttl value.  Any suggestions for that case?
</comment><comment author="pickypg" created="2015-07-06T23:32:19Z" id="119027241">@ronlepper The only way to "unset" the `_ttl` is to reset or to completely replace (reindex) the document. You _can_ set it to `null`, but we're not checking for it, which I do consider to be a bug.

``` json
{
  "script" : "ctx_source._ttl = null"
}
```

This does change the `_ttl` to `null`, but the code that you spotted assumes that `null` means that it's not set rather than that it's being _unset_. We need to check for the key if it's `null`, then remove it in that case.
</comment><comment author="nside" created="2016-04-29T21:38:27Z" id="215888469">I'm hitting a similar bug. Basically the _ttl doesn't get reset in the bulk update but does in the atomic one.
Here's a script to reproduce

```
import requests, time

def assert_success(r):
  assert r.json()['acknowledged']

INDEX = 'http://localhost:9200/test_ttl'
print 'delete index'
requests.delete(INDEX)
print 'create index'
r = requests.put(INDEX, data="""{
  "mappings": {
    "my_type": {
      "_ttl": {
        "enabled": true,
        "default": "5m"
      }
    }
  }
}""")

assert_success(r)

print 'read index'
assert requests.get(INDEX).json()['test_ttl']['mappings']['my_type']['_ttl']['default'] == 300000

print 'insert record'
r = requests.put(INDEX + '/my_type/1', data='{"hello": 1}')

print 'read record'
ttl1 = requests.get(INDEX + '/my_type/1').json()['_ttl']

time.sleep(1)
print 'read record'
ttl2 = requests.get(INDEX + '/my_type/1').json()['_ttl']

assert ttl2 &lt; ttl1

print 'update record by id'
r = requests.put(INDEX + '/my_type/1', data='{"hello": 2}')

ttl3 = requests.get(INDEX + '/my_type/1').json()['_ttl']

assert ttl3 &gt; ttl2, "TTL wasn't reset"

print 'update record bulk'
r = requests.put(INDEX + '/my_type/1/_bulk', data="""{ "update" : {"_id" : "1"}}
{"doc": {"hello" : 3} } }""")
print r.content

ttl4 = requests.get(INDEX + '/my_type/1').json()['_ttl']
assert ttl4 &gt; ttl3, "TTL wasn't reset %f %f" % (ttl4, ttl3)
```
</comment><comment author="clintongormley" created="2016-05-12T12:55:44Z" id="218748463">Closing in favour of #18280
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add cost minimizer to tune `moving_avg` parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11881</link><project id="" key="" /><description>Introduces an optimizer that will fit a model to the underlying data by automatically tuning the model's parameters.

Not all models can be optimized, and only Holt-Winters is optimized by default.  I think only HW is useful for prediction (since the other models just predict constant "lines"), and the main utility of fitting the model to the data is to generate accurate predictions.  The other moving average are more useful for their smoothing capabilities than their predictive capabilities (again, in my opinion :) )

To make the optimizer semi-generic, models have moved from explicit local variables (`alpha`, `beta`, etc) to an array of doubles.   Each model knows what is supposed to be in each position.  The optimizer doesn't actually care what the values represents, and can generically work with the array.  This seemed like the least painful way to accomplish it ... open to suggestions!

Still a WIP:
- Needs tests, and current tests were temporarily disabled because of changes
- Lots of edge cases that can currently explode, e.g not enough data to train on
- The architecture is a bit wonky.  E.g. we rely on the model to extract settings (because only it knows what kind of settings are available for that model), but the settings manipulation is all done "outside" the model by the Aggregator and Optimizer
- Need to run it through a profiler

Putting this up so I can ping a few people for eyeballs on certain design decisions, does not need a review yet! :)
</description><key id="91074313">11881</key><summary>Add cost minimizer to tune `moving_avg` parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-25T20:57:47Z</created><updated>2015-07-09T15:50:52Z</updated><resolved>2015-07-08T20:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-25T21:11:06Z" id="115399342">I haven't looked at the code yet but I am not sure about the name of the parameter: "optimize". Maybe we should try to find a less cool-sounding name, who would ever want to build an aggregation with `optimize: false`? I don't have good suggestions however...
</comment><comment author="polyfractal" created="2015-06-25T23:44:42Z" id="115431799">:)  

That was actually something I wanted to ask about.  I'm leaning towards `fit`, `fit_data` or something along that lines.  It's more accurate about what's going on, and less cool-sounding for exactly the reason you mentioned 
</comment><comment author="clintongormley" created="2015-06-26T11:47:43Z" id="115654342">`auto_tune`?
</comment><comment author="polyfractal" created="2015-06-26T13:55:02Z" id="115698165">_&lt; insert joke about auto-tuning songs &gt;_

Some more options: `minimize_cost`, `minimize`, `fit_model`

Another note, just so I don't forget, @jpountz raised the question if this should only be available on HoltWinters (since practically speaking, it is the only one that will _probably_ benefit).  The existing code has a method which returns true/false if a model can be optimized.  So we could simply disable the option for all other models.

Alternatively, we could roll the optimization stuff directly into the HW model, which would remove a lot of the constraints placed on the rest of the code (coefficients can go back to be explicit variables instead of double arrays, etc).

I'm inclined to allow the user the option to "optimize" EWMA and Holt-Linear, because they have parameters that can be tuned.  And just advise them it's probably not what they want in the docs, and leave the defaults to be not-optimized.
</comment><comment author="polyfractal" created="2015-07-01T20:54:29Z" id="117821301">@colings86 Ok, I think this is ready for a first pass review.  Thanks!
</comment><comment author="colings86" created="2015-07-02T09:48:56Z" id="117983555">@polyfractal I left some comments. I haven't looked at the `minimize()` method or the `SimulatedAnnealingMinimizer` class yet as I want to understand a bit more about the theory first
</comment><comment author="polyfractal" created="2015-07-02T22:00:54Z" id="118179704">Pushed a commit that replaces the primitive array with a wrapper object.  The rest we should chat about, since I think they're are a set of pros/cons to each approach. :)
</comment><comment author="polyfractal" created="2015-07-07T15:47:19Z" id="119244634">@colings86 Fixed up, ready for another look.  Thanks!
</comment><comment author="colings86" created="2015-07-08T10:00:14Z" id="119529057">@polyfractal left some more comments, most of them are pretty minor. Looking really close now though.
</comment><comment author="polyfractal" created="2015-07-08T19:01:49Z" id="119697571">@colings86 All cleaned up, rebased against master (and squashed to make the rebase easier), docs added.
</comment><comment author="colings86" created="2015-07-08T19:11:54Z" id="119699599">Left a few very minor comments but LGTM so feel free to merge without further review
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file></files><comments><comment>Merge pull request #11881 from polyfractal/feature/movavg_optim2</comment></comments></commit></commits></item><item><title>Properly support named queries for both nested and parent child inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11880</link><project id="" key="" /><description>Before inner_hits existed named queries had support to also verify if inner queries of nested query matched with returned documents. This logic was broken and became obsolete from the moment inner hits got released. #10694 fixed named queries for nested docs in the top_hits agg, but it didn't fix the named query support for nested inner hits. This commit fixes that and on top of this also adds support for parent/child inner hits.

Left over of issue #10661
</description><key id="91072732">11880</key><summary>Properly support named queries for both nested and parent child inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-25T20:52:21Z</created><updated>2015-07-09T09:21:15Z</updated><resolved>2015-07-09T09:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T16:17:45Z" id="119645223">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Batman to names.txt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11879</link><project id="" key="" /><description>If there's an orphan-maker batman is an eventuality.
</description><key id="91029151">11879</key><summary>Add Batman to names.txt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JonMR</reporter><labels /><created>2015-06-25T17:38:17Z</created><updated>2015-06-26T16:47:53Z</updated><resolved>2015-06-26T12:40:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-26T12:40:34Z" id="115665657">woah woah woah what are you doing????  

I Marvel at this attempt to infiltrate DC superheroes into our list :)
</comment><comment author="JonMR" created="2015-06-26T16:47:53Z" id="115752865">Why so serious???
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve mapping documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11878</link><project id="" key="" /><description>I've been trying to configure mapping, the docs are leading me round and round in circles. A quick search finds 9 different pages in the docs related to mapping (I'm sure there are more), information is spread out all over the place.

https://www.elastic.co/guide/en/elasticsearch/guide/current/custom-dynamic-mapping.html  
https://www.elastic.co/guide/en/elasticsearch/guide/current/default-mapping.html
https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping-intro.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-dynamic-mapping.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-object-type.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-root-object-type.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html
</description><key id="91000245">11878</key><summary>Improve mapping documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">j0hnsmith</reporter><labels><label>docs</label></labels><created>2015-06-25T15:43:18Z</created><updated>2015-08-15T13:59:17Z</updated><resolved>2015-08-15T13:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="j0hnsmith" created="2015-06-25T16:13:46Z" id="115306895">Mapping for indices that don't yet exist can be configured via index templates, this is what I was looking for, I don't believe they're mentioned on any of those 9 pages.
</comment><comment author="clintongormley" created="2015-08-15T13:59:17Z" id="131384473">I've completely rewritten the mapping docs in master - should be much easier to understand now.

https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dynamic mapping with automatic index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11877</link><project id="" key="" /><description>The docs suggest that it's possible to create a mapping for a type that doesn't yet exist.

https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-creation

&gt; The index operation automatically creates an index if it has not been created before (check out the create index API for manually creating an index), and also automatically creates a dynamic type mapping for the specific type if one has not yet been created (check out the put mapping API for manually creating a type mapping).

When I try this I get `IndexMissingException`.

```
$ curl -XPUT 'http://elasticsearch:9200/log-*/_mapping/log_entry' -d '
{
    "log_entry" : {
        "properties" : {
            "category" : {"type" : "string", "index": "not_analyzed"},
            "message" : {"type" : "string", "store" : true }
        }
    }
}'
```

Every example of the Put Mapping api I can find in the docs includes a concrete index, how do you specify a dynamic type mapping without referencing a concrete index?
</description><key id="90998080">11877</key><summary>Dynamic mapping with automatic index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">j0hnsmith</reporter><labels /><created>2015-06-25T15:35:58Z</created><updated>2015-06-25T16:16:06Z</updated><resolved>2015-06-25T16:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-25T16:02:37Z" id="115304590">Please use discuss.elastic.co for questions.

If you want to add a mapping to an index, you need to create an index.
If you want to create a mapping as a template for the time an index is being created, then use an index template.
If you want to create an index and its mapping at the same time, have a look at the create index API.

Not an issue. Closing.
</comment><comment author="j0hnsmith" created="2015-06-25T16:10:31Z" id="115306257">I've just found index templates in the docs after at least an hour reading through them specifically trying to find out how to do it. 

The bug is that the docs say you can do it but don't say how (or link anywhere that does). I read at least 10 pages of docs relating to mapping, not one of them mentioned index templates.
</comment><comment author="dadoonet" created="2015-06-25T16:16:05Z" id="115307380">This? https://www.elastic.co/guide/en/elasticsearch/guide/current/index-templates.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recovery API reports wrong reused file bytes during snapshot restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11876</link><project id="" key="" /><description>The snapshot restore procedure copies over all the bytes. The output however is: (note `reused_in_bytes` == `total_in_bytes`):

```
 "index": {
               "size": {
                  "total_in_bytes": 4390745019,
                  "reused_in_bytes": 4390745019,
                  "recovered_in_bytes": 31053339,
                  "percent": "0.7%"
               },
               "files": {
                  "total": 136,
                  "reused": 0,
                  "recovered": 3,
                  "percent": "2.2%"
               },
               "total_time_in_millis": 131455,
               "source_throttle_time_in_millis": 0,
               "target_throttle_time_in_millis": 0
```
</description><key id="90979833">11876</key><summary>Recovery API reports wrong reused file bytes during snapshot restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>bug</label></labels><created>2015-06-25T14:26:00Z</created><updated>2015-07-01T07:09:40Z</updated><resolved>2015-07-01T07:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="szroland" created="2015-06-25T22:43:51Z" id="115422089">This seems to be a simple typo in `org.elasticsearch.indices.recovery.RecoveryState toXContent()`:

```
            builder.byteSizeField(Fields.TOTAL_IN_BYTES, Fields.TOTAL, totalBytes());
            builder.byteSizeField(Fields.REUSED_IN_BYTES, Fields.REUSED, totalBytes());
```

The result of `totalBytes()` is used for both fields when serializing, instead of calling `recoveredBytes()` in the second case. The values produced by the latter seem to be tested, so I guess this is just an issue of serization, which may not be explicilty tested.

I guess I could fix this, and maybe add a test, if no one else is working on this.
</comment><comment author="clintongormley" created="2015-06-26T13:26:58Z" id="115685006">@szroland please do
</comment><comment author="szroland" created="2015-06-26T22:57:54Z" id="115912112">Well, as expected the code wasn't too hard to fix, but while creating the test, I actually ran into an issue with how the Snapshot Create API is described in the rest API json spec file (or rather how that spec is treated in rest tests) vs. how it is actually mapped in its rest controller, see  #11897. 

Practically if you use snapshot create in a rest test, it sometimes works and sometimes fails, depending on which HTTP method / path combination is picked by the RestClient from the API spec.
</comment><comment author="szroland" created="2015-06-27T22:24:14Z" id="116163356">@clintongormley, for now I choose to remove the POST option from the snapshot create API json spec file, which makes it possible to create rest tests that include this API.

I think how that spec file and API should look like can be discussed and implemented in the context of #11897. The test case created here should simply continue to work whatever the outcome of that will be.

Let me know what you think.
</comment><comment author="bleskes" created="2015-06-30T10:34:23Z" id="117118115">@szroland now that #11928 is taken care of, care to extract the fix for this one and make a PR?
</comment><comment author="szroland" created="2015-06-30T22:48:14Z" id="117365748">I created the new pull request, see #11965 above
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file></files><comments><comment>Recovery: Fix wrong reused file bytes in Recovery API reports</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file></files><comments><comment>Recovery: Fix wrong reused file bytes in Recovery API reports</comment></comments></commit></commits></item><item><title>Make translog file name parsing strict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11875</link><project id="" key="" /><description>Today we are very lenient in parsing the translog files. This is
actually not necessary since we have a clear run once upgrade path.
All files are converted into the new file name pattern such that we
only need to look at old file patterns in the context of the upgrade.

This commit makes parsing really strict with the exceptoin of the upgrade path.

this is a followup from https://github.com/elastic/elasticsearch/pull/11860
</description><key id="90970495">11875</key><summary>Make translog file name parsing strict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-25T13:49:20Z</created><updated>2015-08-13T14:28:28Z</updated><resolved>2015-06-25T14:31:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-25T14:06:15Z" id="115269409">left some minor comments. LGTM (imho no need for another review)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Update joda url date-format.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11874</link><project id="" key="" /><description>Joda documentation moved from http://joda-time.sourceforge.net/ to http://www.joda.org/joda-time/. Updated the links in the documentation accordingly.
</description><key id="90927998">11874</key><summary>Docs: Update joda url date-format.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label></labels><created>2015-06-25T10:51:03Z</created><updated>2015-08-08T07:16:12Z</updated><resolved>2015-06-26T07:43:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-25T23:11:17Z" id="115427058">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Random crashes even when idle (on Mac)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11873</link><project id="" key="" /><description>I have an Elasticsearch engine running on my Macbook and it is crashing randomly, even when in idle mode. Below please find the error log. (This is my first time posting here so apologies if I included more/less info than required)

```
  A fatal error has been detected by the Java Runtime Environment:

   SIGSEGV (0xb) at pc=0x0000000125b14d1c, pid=36072, tid=58371

  JRE version: Java(TM) SE Runtime Environment (8.0_05-b13) (build 1.8.0_05-b13)
  Java VM: Java HotSpot(TM) 64-Bit Server VM (25.5-b02 mixed mode bsd-amd64 compressed oops)
  Problematic frame:
  C  [libnet.dylib+0x9d1c]  Java_java_net_SocketInputStream_socketRead0+0x23d

  Core dump written. Default location: /cores/core or core.36072

  If you would like to submit a bug report, please visit:
    http://bugreport.sun.com/bugreport/crash.jsp
  The crash happened outside the Java Virtual Machine in native code.
  See problematic frame for where to report the bug.


---------------  T H R E A D  ---------------

Current thread (0x00007fcf1a22d000):  JavaThread "elasticsearch[Arides][keep_alive]" daemon [_thread_in_native, id=58371, stack(0x000000012cfce000,0x000000012d0ce000)]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x0000000f1a22d1e0

Registers:
RAX=0x000000000000019d, RBX=0x000000012d0bce60, RCX=0x0000000000000000, RDX=0x0000000125b3ca40
RSP=0x000000012d0bce50, RBP=0x000000012d0cce90, RSI=0x0000080000000800, RDI=0x0000070000000803
R8 =0x000000012d0bcdb0, R9 =0x0000000000000000, R10=0x0000000000000000, R11=0x0000000000000246
R12=0x000000000000019d, R13=0x0000000000002000, R14=0x0000000f1a22d1e0, R15=0x000000000000ea60
RIP=0x0000000125b14d1c, EFLAGS=0x0000000000010202, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x000000012d0bce50)
0x000000012d0bce50:   000000012d0cceb8 000000008a02db4e
0x000000012d0bce60:   312e312f50545448 0d4b4f2030303220
0x000000012d0bce70:   746e65746e6f430a 61203a657079542d
0x000000012d0bce80:   69746163696c7070 3b6e6f736a2f6e6f
0x000000012d0bce90:   7465737261686320 0a0d382d4654553d
0x000000012d0bcea0:   2d746e65746e6f43 203a6874676e654c
0x000000012d0bceb0:   7b0a0d0a0d363233 746174732220200a
0x000000012d0bcec0:   3032203a20227375 616e2220200a2c30
0x000000012d0bced0:   4122203a2022656d 0a2c227365646972
0x000000012d0bcee0:   7473756c63222020 22656d616e5f7265
0x000000012d0bcef0:   7069727422203a20 762220200a2c2278
0x000000012d0bcf00:   20226e6f69737265 202020200a7b203a
0x000000012d0bcf10:   227265626d756e22 2e352e3122203a20
0x000000012d0bcf20:   202020200a2c2232 685f646c69756222
0x000000012d0bcf30:   22203a2022687361 3836383966663236
0x000000012d0bcf40:   3463306138633462 6262656230363835
0x000000012d0bcf50:   3839313265393532 6331626138373730
0x000000012d0bcf60:   22202020200a2c22 69745f646c697562
0x000000012d0bcf70:   22706d617473656d 3531303222203a20
0x000000012d0bcf80:   305437322d34302d 5a36303a31323a39
0x000000012d0bcf90:   22202020200a2c22 6e735f646c697562
0x000000012d0bcfa0:   2022746f68737061 2c65736c6166203a
0x000000012d0bcfb0:   756c22202020200a 7265765f656e6563
0x000000012d0bcfc0:   203a20226e6f6973 22342e30312e3422
0x000000012d0bcfd0:   20200a2c7d20200a 656e696c67617422
0x000000012d0bcfe0:   756f5922203a2022 66202c776f6e4b20
0x000000012d0bcff0:   637261655320726f 6331620a7d0a2268
0x000000012d0bd000:   222020202e7a4f30 69745f646c697562
0x000000012d0bd010:   22706d617473656d 3531303222203a20
0x000000012d0bd020:   305437322d34302d 5a36303a22a60048
0x000000012d0bd030:   222020202c7ce1d0 6e735f6422c4f320
0x000000012d0bd040:   2022746f22a5ffc8 2c65736c2c7cc090 

Instructions: (pc=0x0000000125b14d1c)
0x0000000125b14cfc:   48 8d 35 a5 35 00 00 48 8d 15 f7 36 00 00 eb e6
0x0000000125b14d0c:   48 8d 35 e3 3a 00 00 48 8d 15 00 3f 00 00 eb d6
0x0000000125b14d1c:   49 8b 06 4c 89 f7 48 8b b5 c0 ff fe ff 8b 95 cc
0x0000000125b14d2c:   ff fe ff 44 89 e1 49 89 d8 ff 90 80 06 00 00 48 

Register to memory mapping:

RAX=0x000000000000019d is an unknown value
RBX=0x000000012d0bce60 is pointing into the stack for thread: 0x00007fcf1a22d000
RCX=0x0000000000000000 is an unknown value
RDX=0x0000000125b3ca40 is an unknown value
RSP=0x000000012d0bce50 is pointing into the stack for thread: 0x00007fcf1a22d000
RBP=0x000000012d0cce90 is pointing into the stack for thread: 0x00007fcf1a22d000
RSI=0x0000080000000800 is an unknown value
RDI=0x0000070000000803 is an unknown value
R8 =0x000000012d0bcdb0 is pointing into the stack for thread: 0x00007fcf1a22d000
R9 =0x0000000000000000 is an unknown value
R10=0x0000000000000000 is an unknown value
R11=0x0000000000000246 is an unknown value
R12=0x000000000000019d is an unknown value
R13=0x0000000000002000 is an unknown value
R14=0x0000000f1a22d1e0 is an unknown value
R15=0x000000000000ea60 is an unknown value


Stack: [0x000000012cfce000,0x000000012d0ce000],  sp=0x000000012d0bce50,  free space=955k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libnet.dylib+0x9d1c]  Java_java_net_SocketInputStream_socketRead0+0x23d
J 9529  java.net.SocketInputStream.socketRead0(Ljava/io/FileDescriptor;[BIII)I (0 bytes) @ 0x00000001105318e1 [0x0000000110531800+0xe1]
J 9528 C1 java.net.SocketInputStream.read([BIII)I (290 bytes) @ 0x0000000110540e24 [0x00000001105408c0+0x564]
J 9311 C1 java.net.SocketInputStream.read([BII)I (15 bytes) @ 0x000000011041067c [0x00000001104105c0+0xbc]
J 8997 C1 java.io.BufferedInputStream.fill()V (233 bytes) @ 0x00000001102b7c0c [0x00000001102b7580+0x68c]
J 24 C1 java.io.BufferedInputStream.read1([BII)I (108 bytes) @ 0x000000010ebd612c [0x000000010ebd5f60+0x1cc]
J 23 C1 java.io.BufferedInputStream.read([BII)I (113 bytes) @ 0x000000010ebd4fec [0x000000010ebd4d60+0x28c]
J 9668 C1 sun.net.www.http.HttpClient.parseHTTPHeader(Lsun/net/www/MessageHeader;Lsun/net/ProgressSource;Lsun/net/www/protocol/http/HttpURLConnection;)Z (944 bytes) @ 0x000000010fcbcdf4 [0x000000010fcbcb60+0x294]
J 9497 C1 sun.net.www.http.HttpClient.parseHTTP(Lsun/net/www/MessageHeader;Lsun/net/ProgressSource;Lsun/net/www/protocol/http/HttpURLConnection;)Z (188 bytes) @ 0x0000000110518a64 [0x0000000110518740+0x324]
J 8319 C1 sun.net.www.protocol.http.HttpURLConnection.getInputStream0()Ljava/io/InputStream; (2019 bytes) @ 0x0000000110077c0c [0x0000000110076d40+0xecc]
J 8151 C1 sun.net.www.protocol.http.HttpURLConnection.getInputStream()Ljava/io/InputStream; (56 bytes) @ 0x000000010f8aa4e4 [0x000000010f8a9fe0+0x504]
j  org.elasticsearch.marvel.agent.exporter.ESExporter$ConnectionKeepAliveWorker.run()V+120
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.dylib+0x2c5186]
V  [libjvm.dylib+0x2c5914]
V  [libjvm.dylib+0x2c5ac0]
V  [libjvm.dylib+0x3152a7]
V  [libjvm.dylib+0x51ccf5]
V  [libjvm.dylib+0x51e432]
V  [libjvm.dylib+0x445426]
C  [libsystem_pthread.dylib+0x3268]  _pthread_body+0x83
C  [libsystem_pthread.dylib+0x31e5]  _pthread_body+0x0
C  [libsystem_pthread.dylib+0x141d]  thread_start+0xd

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
J 9529  java.net.SocketInputStream.socketRead0(Ljava/io/FileDescriptor;[BIII)I (0 bytes) @ 0x0000000110531867 [0x0000000110531800+0x67]
J 9528 C1 java.net.SocketInputStream.read([BIII)I (290 bytes) @ 0x0000000110540e24 [0x00000001105408c0+0x564]
J 9311 C1 java.net.SocketInputStream.read([BII)I (15 bytes) @ 0x000000011041067c [0x00000001104105c0+0xbc]
J 8997 C1 java.io.BufferedInputStream.fill()V (233 bytes) @ 0x00000001102b7c0c [0x00000001102b7580+0x68c]
J 24 C1 java.io.BufferedInputStream.read1([BII)I (108 bytes) @ 0x000000010ebd612c [0x000000010ebd5f60+0x1cc]
J 23 C1 java.io.BufferedInputStream.read([BII)I (113 bytes) @ 0x000000010ebd4fec [0x000000010ebd4d60+0x28c]
J 9668 C1 sun.net.www.http.HttpClient.parseHTTPHeader(Lsun/net/www/MessageHeader;Lsun/net/ProgressSource;Lsun/net/www/protocol/http/HttpURLConnection;)Z (944 bytes) @ 0x000000010fcbcdf4 [0x000000010fcbcb60+0x294]
J 9497 C1 sun.net.www.http.HttpClient.parseHTTP(Lsun/net/www/MessageHeader;Lsun/net/ProgressSource;Lsun/net/www/protocol/http/HttpURLConnection;)Z (188 bytes) @ 0x0000000110518a64 [0x0000000110518740+0x324]
J 8319 C1 sun.net.www.protocol.http.HttpURLConnection.getInputStream0()Ljava/io/InputStream; (2019 bytes) @ 0x0000000110077c0c [0x0000000110076d40+0xecc]
J 8151 C1 sun.net.www.protocol.http.HttpURLConnection.getInputStream()Ljava/io/InputStream; (56 bytes) @ 0x000000010f8aa4e4 [0x000000010f8a9fe0+0x504]
j  org.elasticsearch.marvel.agent.exporter.ESExporter$ConnectionKeepAliveWorker.run()V+120
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x00007fcf144dd800 JavaThread "elasticsearch[Arides][merge][T#1]" daemon [_thread_blocked, id=45355, stack(0x0000000127bed000,0x0000000127ced000)]
  0x00007fcf1a237800 JavaThread "elasticsearch[Arides][bulk][T#8]" daemon [_thread_blocked, id=67075, stack(0x000000012e248000,0x000000012e348000)]
  0x00007fcf1b0b1800 JavaThread "elasticsearch[Arides][bulk][T#7]" daemon [_thread_blocked, id=66563, stack(0x000000012d955000,0x000000012da55000)]
  0x00007fcf1a149800 JavaThread "elasticsearch[Arides][bulk][T#6]" daemon [_thread_blocked, id=65547, stack(0x000000012d852000,0x000000012d952000)]
  0x00007fcf1a808000 JavaThread "elasticsearch[Arides][bulk][T#5]" daemon [_thread_blocked, id=66055, stack(0x000000012d2d7000,0x000000012d3d7000)]
  0x00007fcf14be1000 JavaThread "elasticsearch[Arides][flush][T#1]" daemon [_thread_blocked, id=64523, stack(0x000000012af4e000,0x000000012b04e000)]
  0x00007fcf150e2000 JavaThread "elasticsearch[Arides][bulk][T#4]" daemon [_thread_in_Java, id=65031, stack(0x000000012cd65000,0x000000012ce65000)]
  0x00007fcf19836800 JavaThread "elasticsearch[Arides][bulk][T#3]" daemon [_thread_blocked, id=64003, stack(0x000000012d5dd000,0x000000012d6dd000)]
  0x00007fcf18802000 JavaThread "elasticsearch[Arides][listener][T#4]" daemon [_thread_blocked, id=63491, stack(0x000000012ddb0000,0x000000012deb0000)]
  0x00007fcf16185000 JavaThread "elasticsearch[Arides][management][T#5]" daemon [_thread_blocked, id=62979, stack(0x000000012dcad000,0x000000012ddad000)]
  0x00007fcf14627000 JavaThread "elasticsearch[Arides][management][T#4]" daemon [_thread_blocked, id=62467, stack(0x000000012dbaa000,0x000000012dcaa000)]
  0x00007fcf188c9800 JavaThread "elasticsearch[Arides][management][T#3]" daemon [_thread_blocked, id=61955, stack(0x000000012daa7000,0x000000012dba7000)]
  0x00007fcf19c7a000 JavaThread "elasticsearch[Arides][management][T#2]" daemon [_thread_blocked, id=61443, stack(0x000000012d74f000,0x000000012d84f000)]
  0x00007fcf172cd800 JavaThread "elasticsearch[Arides][bulk][T#2]" daemon [_thread_blocked, id=60931, stack(0x000000012d1d4000,0x000000012d2d4000)]
  0x00007fcf170d1800 JavaThread "elasticsearch[Arides][refresh][T#1]" daemon [_thread_blocked, id=60419, stack(0x000000012ad3b000,0x000000012ae3b000)]
  0x00007fcf19b56000 JavaThread "elasticsearch[Arides][listener][T#3]" daemon [_thread_blocked, id=59907, stack(0x000000012d0d1000,0x000000012d1d1000)]
  0x00007fcf19bb7000 JavaThread "elasticsearch[Arides][bulk][T#1]" daemon [_thread_blocked, id=59395, stack(0x000000012c9a9000,0x000000012caa9000)]
  0x00007fcf171ef800 JavaThread "elasticsearch[Arides][listener][T#2]" daemon [_thread_blocked, id=58883, stack(0x00000001269d1000,0x0000000126ad1000)]
=&gt;0x00007fcf1a22d000 JavaThread "elasticsearch[Arides][keep_alive]" daemon [_thread_in_native, id=58371, stack(0x000000012cfce000,0x000000012d0ce000)]
  0x00007fcf139dc800 JavaThread "Keep-Alive-Timer" daemon [_thread_blocked, id=57355, stack(0x000000012cecb000,0x000000012cfcb000)]
  0x00007fcf1787d000 JavaThread "elasticsearch[Arides][listener][T#1]" daemon [_thread_blocked, id=57863, stack(0x000000012cc62000,0x000000012cd62000)]
  0x00007fcf14b4c000 JavaThread "elasticsearch[Arides][warmer][T#1]" daemon [_thread_blocked, id=56835, stack(0x000000012a796000,0x000000012a896000)]
  0x00007fcf18969800 JavaThread "DestroyJavaVM" [_thread_blocked, id=3847, stack(0x000000010ca7e000,0x000000010cb7e000)]
  0x00007fcf14dc9800 JavaThread "elasticsearch[keepAlive/1.5.2]" [_thread_blocked, id=55811, stack(0x000000012ae4b000,0x000000012af4b000)]
  0x00007fcf18917800 JavaThread "elasticsearch[Arides][http_server_boss][T#1]{New I/O server boss #51}" daemon [_thread_in_native, id=55299, stack(0x000000012c2a6000,0x000000012c3a6000)]
  0x00007fcf18917000 JavaThread "elasticsearch[Arides][http_server_worker][T#16]{New I/O worker #50}" daemon [_thread_in_native, id=54787, stack(0x000000012c1a3000,0x000000012c2a3000)]
  0x00007fcf13d9c000 JavaThread "elasticsearch[Arides][http_server_worker][T#15]{New I/O worker #49}" daemon [_thread_in_native, id=54275, stack(0x000000012c0a0000,0x000000012c1a0000)]
  0x00007fcf18b09000 JavaThread "elasticsearch[Arides][http_server_worker][T#14]{New I/O worker #48}" daemon [_thread_in_native, id=53763, stack(0x000000012bf9d000,0x000000012c09d000)]
  0x00007fcf18131800 JavaThread "elasticsearch[Arides][http_server_worker][T#13]{New I/O worker #47}" daemon [_thread_in_native, id=53251, stack(0x000000012be9a000,0x000000012bf9a000)]
  0x00007fcf13d9b800 JavaThread "elasticsearch[Arides][http_server_worker][T#12]{New I/O worker #46}" daemon [_thread_in_native, id=52739, stack(0x000000012bd97000,0x000000012be97000)]
  0x00007fcf18130800 JavaThread "elasticsearch[Arides][http_server_worker][T#11]{New I/O worker #45}" daemon [_thread_in_native, id=52227, stack(0x000000012bc94000,0x000000012bd94000)]
  0x00007fcf14581800 JavaThread "elasticsearch[Arides][http_server_worker][T#10]{New I/O worker #44}" daemon [_thread_in_native, id=51715, stack(0x000000012bb91000,0x000000012bc91000)]
  0x00007fcf13815000 JavaThread "elasticsearch[Arides][http_server_worker][T#9]{New I/O worker #43}" daemon [_thread_in_native, id=51203, stack(0x000000012ba8e000,0x000000012bb8e000)]
  0x00007fcf18329800 JavaThread "elasticsearch[Arides][http_server_worker][T#8]{New I/O worker #42}" daemon [_thread_in_native, id=50691, stack(0x000000012b98b000,0x000000012ba8b000)]
  0x00007fcf18adf000 JavaThread "elasticsearch[Arides][http_server_worker][T#7]{New I/O worker #41}" daemon [_thread_in_native, id=50179, stack(0x000000012b888000,0x000000012b988000)]
  0x00007fcf13d77000 JavaThread "elasticsearch[Arides][http_server_worker][T#6]{New I/O worker #40}" daemon [_thread_in_native, id=49667, stack(0x000000012b785000,0x000000012b885000)]
  0x00007fcf1783e000 JavaThread "elasticsearch[Arides][http_server_worker][T#5]{New I/O worker #39}" daemon [_thread_in_native, id=49155, stack(0x000000012b682000,0x000000012b782000)]
  0x00007fcf188b3000 JavaThread "elasticsearch[Arides][http_server_worker][T#4]{New I/O worker #38}" daemon [_thread_in_native, id=48643, stack(0x000000012b57f000,0x000000012b67f000)]
  0x00007fcf1381b000 JavaThread "elasticsearch[Arides][http_server_worker][T#3]{New I/O worker #37}" daemon [_thread_in_native, id=48131, stack(0x000000012b47c000,0x000000012b57c000)]
  0x00007fcf13813000 JavaThread "elasticsearch[Arides][http_server_worker][T#2]{New I/O worker #36}" daemon [_thread_in_native, id=47619, stack(0x000000012b1be000,0x000000012b2be000)]
  0x00007fcf14a1d800 JavaThread "elasticsearch[Arides][http_server_worker][T#1]{New I/O worker #35}" daemon [_thread_in_native, id=17159, stack(0x000000012b0bb000,0x000000012b1bb000)]
  0x00007fcf1456f000 JavaThread "elasticsearch[Arides][management][T#1]" daemon [_thread_blocked, id=17671, stack(0x00000001278eb000,0x00000001279eb000)]
  0x00007fcf1456c000 JavaThread "elasticsearch[Arides][riverClusterService#updateTask][T#1]" daemon [_thread_blocked, id=16139, stack(0x00000001237a6000,0x00000001238a6000)]
  0x00007fcf17828800 JavaThread "elasticsearch[Arides][transport_client_timer][T#1]{Hashed wheel timer #1}" daemon [_thread_blocked, id=45827, stack(0x0000000127cf0000,0x0000000127df0000)]
  0x00007fcf1783d000 JavaThread "elasticsearch[Arides][generic][T#1]" daemon [_thread_blocked, id=44803, stack(0x0000000127586000,0x0000000127686000)]
  0x00007fcf1838d000 JavaThread "elasticsearch[Arides][clusterService#updateTask][T#1]" daemon [_thread_blocked, id=44291, stack(0x0000000127483000,0x0000000127583000)]
  0x00007fcf151c1000 JavaThread "elasticsearch[#shared#][discovery#multicast#receiver][T#1]" daemon [_thread_in_native, id=43779, stack(0x00000001266ba000,0x00000001267ba000)]
  0x00007fcf144e1000 JavaThread "elasticsearch[Arides][[http_server_boss.default]][T#1]{New I/O server boss #34}" daemon [_thread_in_native, id=43267, stack(0x000000012a28d000,0x000000012a38d000)]
  0x00007fcf168f7800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#16]{New I/O worker #33}" daemon [_thread_in_native, id=42755, stack(0x000000012a18a000,0x000000012a28a000)]
  0x00007fcf168f4800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#15]{New I/O worker #32}" daemon [_thread_in_native, id=42243, stack(0x000000012a087000,0x000000012a187000)]
  0x00007fcf169d6000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#14]{New I/O worker #31}" daemon [_thread_in_native, id=41731, stack(0x0000000129f84000,0x000000012a084000)]
  0x00007fcf168f2000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#13]{New I/O worker #30}" daemon [_thread_in_native, id=41219, stack(0x0000000129e81000,0x0000000129f81000)]
  0x00007fcf168ed800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#12]{New I/O worker #29}" daemon [_thread_in_native, id=40707, stack(0x0000000129d7e000,0x0000000129e7e000)]
  0x00007fcf17109800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#11]{New I/O worker #28}" daemon [_thread_in_native, id=40195, stack(0x0000000129c7b000,0x0000000129d7b000)]
  0x00007fcf17032800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#10]{New I/O worker #27}" daemon [_thread_in_native, id=39683, stack(0x0000000129b78000,0x0000000129c78000)]
  0x00007fcf17063800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#9]{New I/O worker #26}" daemon [_thread_in_native, id=39171, stack(0x0000000129a75000,0x0000000129b75000)]
  0x00007fcf170a8800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#8]{New I/O worker #25}" daemon [_thread_in_native, id=38659, stack(0x0000000129972000,0x0000000129a72000)]
  0x00007fcf16249800 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#7]{New I/O worker #24}" daemon [_thread_in_native, id=38147, stack(0x000000012986f000,0x000000012996f000)]
  0x00007fcf16247000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#6]{New I/O worker #23}" daemon [_thread_in_native, id=37635, stack(0x000000012976c000,0x000000012986c000)]
  0x00007fcf168eb000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#5]{New I/O worker #22}" daemon [_thread_in_native, id=37123, stack(0x0000000129669000,0x0000000129769000)]
  0x00007fcf16949000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#4]{New I/O worker #21}" daemon [_thread_in_native, id=36611, stack(0x0000000127aea000,0x0000000127bea000)]
  0x00007fcf16948000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#3]{New I/O worker #20}" daemon [_thread_in_native, id=36099, stack(0x000000012680c000,0x000000012690c000)]
  0x00007fcf14aeb000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#2]{New I/O worker #19}" daemon [_thread_in_native, id=35587, stack(0x0000000125d13000,0x0000000125e13000)]
  0x00007fcf14aea000 JavaThread "elasticsearch[Arides][[http_server_worker.default]][T#1]{New I/O worker #18}" daemon [_thread_in_native, id=35075, stack(0x0000000125b6e000,0x0000000125c6e000)]
  0x00007fcf13947000 JavaThread "elasticsearch[Arides][transport_client_boss][T#1]{New I/O boss #17}" daemon [_thread_in_native, id=34563, stack(0x0000000129566000,0x0000000129666000)]
  0x00007fcf16243000 JavaThread "elasticsearch[Arides][transport_client_worker][T#16]{New I/O worker #16}" daemon [_thread_in_native, id=34051, stack(0x0000000129463000,0x0000000129563000)]
  0x00007fcf16240800 JavaThread "elasticsearch[Arides][transport_client_worker][T#15]{New I/O worker #15}" daemon [_thread_in_native, id=33539, stack(0x0000000129360000,0x0000000129460000)]
  0x00007fcf1623d800 JavaThread "elasticsearch[Arides][transport_client_worker][T#14]{New I/O worker #14}" daemon [_thread_in_native, id=33027, stack(0x000000012925d000,0x000000012935d000)]
  0x00007fcf1622b000 JavaThread "elasticsearch[Arides][transport_client_worker][T#13]{New I/O worker #13}" daemon [_thread_in_native, id=32515, stack(0x000000012915a000,0x000000012925a000)]
  0x00007fcf1611f000 JavaThread "elasticsearch[Arides][transport_client_worker][T#12]{New I/O worker #12}" daemon [_thread_in_native, id=32003, stack(0x0000000129057000,0x0000000129157000)]
  0x00007fcf1611c800 JavaThread "elasticsearch[Arides][transport_client_worker][T#11]{New I/O worker #11}" daemon [_thread_in_native, id=31491, stack(0x0000000128f54000,0x0000000129054000)]
  0x00007fcf16119800 JavaThread "elasticsearch[Arides][transport_client_worker][T#10]{New I/O worker #10}" daemon [_thread_in_native, id=30979, stack(0x0000000128e51000,0x0000000128f51000)]
  0x00007fcf16117000 JavaThread "elasticsearch[Arides][transport_client_worker][T#9]{New I/O worker #9}" daemon [_thread_in_native, id=30467, stack(0x0000000128d4e000,0x0000000128e4e000)]
  0x00007fcf160ba000 JavaThread "elasticsearch[Arides][transport_client_worker][T#8]{New I/O worker #8}" daemon [_thread_in_native, id=29955, stack(0x0000000128c4b000,0x0000000128d4b000)]
  0x00007fcf160b9000 JavaThread "elasticsearch[Arides][transport_client_worker][T#7]{New I/O worker #7}" daemon [_thread_in_native, id=29443, stack(0x0000000128b48000,0x0000000128c48000)]
  0x00007fcf160d8800 JavaThread "elasticsearch[Arides][transport_client_worker][T#6]{New I/O worker #6}" daemon [_thread_in_native, id=28931, stack(0x0000000128a45000,0x0000000128b45000)]
  0x00007fcf160d8000 JavaThread "elasticsearch[Arides][transport_client_worker][T#5]{New I/O worker #5}" daemon [_thread_in_native, id=28419, stack(0x0000000128942000,0x0000000128a42000)]
  0x00007fcf16082800 JavaThread "elasticsearch[Arides][transport_client_worker][T#4]{New I/O worker #4}" daemon [_thread_in_native, id=27907, stack(0x000000012883f000,0x000000012893f000)]
  0x00007fcf16081800 JavaThread "elasticsearch[Arides][transport_client_worker][T#3]{New I/O worker #3}" daemon [_thread_in_native, id=27395, stack(0x000000012873c000,0x000000012883c000)]
  0x00007fcf14302000 JavaThread "elasticsearch[Arides][transport_client_worker][T#2]{New I/O worker #2}" daemon [_thread_in_native, id=26883, stack(0x0000000128639000,0x0000000128739000)]
  0x00007fcf14539800 JavaThread "elasticsearch[Arides][transport_client_worker][T#1]{New I/O worker #1}" daemon [_thread_in_native, id=26371, stack(0x00000001277e8000,0x00000001278e8000)]
  0x00007fcf14d70000 JavaThread "elasticsearch[Arides][[ttl_expire]]" daemon [_thread_blocked, id=25859, stack(0x0000000128536000,0x0000000128636000)]
  0x00007fcf168fc800 JavaThread "elasticsearch[Arides][master_mapping_updater]" [_thread_blocked, id=25347, stack(0x0000000128433000,0x0000000128533000)]
  0x00007fcf1606f000 JavaThread "elasticsearch[Arides][marvel.exporters]" daemon [_thread_in_native, id=24331, stack(0x0000000128330000,0x0000000128430000)]
  0x00007fcf149c7800 JavaThread "elasticsearch[Arides][scheduler][T#1]" daemon [_thread_blocked, id=24839, stack(0x000000012822d000,0x000000012832d000)]
  0x00007fcf144c9000 JavaThread "elasticsearch[Arides][[timer]]" daemon [_thread_blocked, id=23811, stack(0x0000000126b2f000,0x0000000126c2f000)]
  0x00007fcf15074800 JavaThread "Service Thread" daemon [_thread_blocked, id=22787, stack(0x0000000125692000,0x0000000125792000)]
  0x00007fcf1409c000 JavaThread "C1 CompilerThread3" daemon [_thread_blocked, id=22275, stack(0x000000012558f000,0x000000012568f000)]
  0x00007fcf1409b800 JavaThread "C2 CompilerThread2" daemon [_thread_blocked, id=21763, stack(0x000000012548c000,0x000000012558c000)]
  0x00007fcf14808800 JavaThread "C2 CompilerThread1" daemon [_thread_in_native, id=21251, stack(0x0000000125389000,0x0000000125489000)]
  0x00007fcf1409a800 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=20739, stack(0x0000000125286000,0x0000000125386000)]
  0x00007fcf1381c000 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=20227, stack(0x0000000125183000,0x0000000125283000)]
  0x00007fcf1406a800 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=16395, stack(0x0000000125080000,0x0000000125180000)]
  0x00007fcf14069000 JavaThread "Finalizer" daemon [_thread_blocked, id=15107, stack(0x000000012365d000,0x000000012375d000)]
  0x00007fcf14068000 JavaThread "Reference Handler" daemon [_thread_blocked, id=14595, stack(0x000000012355a000,0x000000012365a000)]

Other Threads:
  0x00007fcf14063800 VMThread [stack: 0x0000000123457000,0x0000000123557000] [id=14083]
  0x00007fcf1409d000 WatcherThread [stack: 0x0000000125795000,0x0000000125895000] [id=23299]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap:
 par new generation   total 314560K, used 191197K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K,  64% used [0x0000000780000000, 0x000000078afbaf80, 0x0000000791110000)
  from space 34944K,  32% used [0x0000000793330000, 0x0000000793e2c680, 0x0000000795550000)
  to   space 34944K,   0% used [0x0000000791110000, 0x0000000791110000, 0x0000000793330000)
 concurrent mark-sweep generation total 699072K, used 439853K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38173K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4688K, capacity 4748K, committed 4864K, reserved 1048576K

Card table byte_map: [0x000000011e698000,0x000000011e899000] byte_map_base: 0x000000011aa98000

Marking Bits: (CMSBitMap*) 0x00007fcf15059cc8
 Bits: [0x000000011f1ef000, 0x000000011fc99c00)

Mod Union Table: (CMSBitMap*) 0x00007fcf15059d88
 Bits: [0x000000011fc9a000, 0x000000011fcc4ab0)

Polling page: 0x000000010cbc2000

CodeCache: size=245760Kb used=28086Kb max_used=28099Kb free=217673Kb
 bounds [0x000000010eac0000, 0x0000000110660000, 0x000000011dac0000]
 total_blobs=8539 nmethods=8001 adapters=452
 compilation: enabled

Compilation events (10 events):
Event: 673.575 Thread 0x00007fcf1409c000 nmethod 9870 0x0000000110649390 code [0x00000001106494e0, 0x00000001106495f0]
Event: 674.268 Thread 0x00007fcf1409c000 9871       3       org.apache.lucene.codecs.DocValuesConsumer::&lt;init&gt; (5 bytes)
Event: 674.269 Thread 0x00007fcf1409c000 nmethod 9871 0x0000000110649650 code [0x00000001106497a0, 0x0000000110649950]
Event: 674.273 Thread 0x00007fcf1409b800 9872       4       org.apache.lucene.util.packed.PackedInts$Format::longCount (86 bytes)
Event: 674.275 Thread 0x00007fcf1409b800 nmethod 9872 0x000000011064a8d0 code [0x000000011064aa80, 0x000000011064acd8]
Event: 677.744 Thread 0x00007fcf14808800 9873       4       org.elasticsearch.common.jackson.core.json.UTF8JsonGenerator::writeStartObject (72 bytes)
Event: 677.751 Thread 0x00007fcf14808800 nmethod 9873 0x0000000110649c10 code [0x0000000110649de0, 0x000000011064a360]
Event: 678.388 Thread 0x00007fcf1409c000 9874   !   3       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector::run (462 bytes)
Event: 678.391 Thread 0x00007fcf1409c000 nmethod 9874 0x000000011064df50 code [0x000000011064e480, 0x0000000110652408]
Event: 683.764 Thread 0x00007fcf14808800 9875       4       org.elasticsearch.common.bytes.ChannelBufferBytesReference::slice (19 bytes)

GC Heap History (10 events):
Event: 477.411 GC heap before
{Heap before GC invocations=6204 (full 4):
 par new generation   total 314560K, used 283430K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K, 100% used [0x0000000780000000, 0x0000000791110000, 0x0000000791110000)
  from space 34944K,  10% used [0x0000000791110000, 0x00000007914c9a70, 0x0000000793330000)
  to   space 34944K,   0% used [0x0000000793330000, 0x0000000793330000, 0x0000000795550000)
 concurrent mark-sweep generation total 699072K, used 439827K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38136K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4687K, capacity 4748K, committed 4864K, reserved 1048576K
Event: 477.416 GC heap after
Heap after GC invocations=6205 (full 4):
 par new generation   total 314560K, used 8635K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K,   0% used [0x0000000780000000, 0x0000000780000000, 0x0000000791110000)
  from space 34944K,  24% used [0x0000000793330000, 0x0000000793b9ef20, 0x0000000795550000)
  to   space 34944K,   0% used [0x0000000791110000, 0x0000000791110000, 0x0000000793330000)
 concurrent mark-sweep generation total 699072K, used 439832K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38136K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4687K, capacity 4748K, committed 4864K, reserved 1048576K
}
Event: 518.575 GC heap before
{Heap before GC invocations=6205 (full 4):
 par new generation   total 314560K, used 288251K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K, 100% used [0x0000000780000000, 0x0000000791110000, 0x0000000791110000)
  from space 34944K,  24% used [0x0000000793330000, 0x0000000793b9ef20, 0x0000000795550000)
  to   space 34944K,   0% used [0x0000000791110000, 0x0000000791110000, 0x0000000793330000)
 concurrent mark-sweep generation total 699072K, used 439832K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38141K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4687K, capacity 4748K, committed 4864K, reserved 1048576K
Event: 518.578 GC heap after
Heap after GC invocations=6206 (full 4):
 par new generation   total 314560K, used 5465K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K,   0% used [0x0000000780000000, 0x0000000780000000, 0x0000000791110000)
  from space 34944K,  15% used [0x0000000791110000, 0x0000000791666628, 0x0000000793330000)
  to   space 34944K,   0% used [0x0000000793330000, 0x0000000793330000, 0x0000000795550000)
 concurrent mark-sweep generation total 699072K, used 439837K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38141K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4687K, capacity 4748K, committed 4864K, reserved 1048576K
}
Event: 561.554 GC heap before
{Heap before GC invocations=6206 (full 4):
 par new generation   total 314560K, used 285081K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K, 100% used [0x0000000780000000, 0x0000000791110000, 0x0000000791110000)
  from space 34944K,  15% used [0x0000000791110000, 0x0000000791666628, 0x0000000793330000)
  to   space 34944K,   0% used [0x0000000793330000, 0x0000000793330000, 0x0000000795550000)
 concurrent mark-sweep generation total 699072K, used 439837K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38146K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4687K, capacity 4748K, committed 4864K, reserved 1048576K
Event: 561.558 GC heap after
Heap after GC invocations=6207 (full 4):
 par new generation   total 314560K, used 8721K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K,   0% used [0x0000000780000000, 0x0000000780000000, 0x0000000791110000)
  from space 34944K,  24% used [0x0000000793330000, 0x0000000793bb4740, 0x0000000795550000)
  to   space 34944K,   0% used [0x0000000791110000, 0x0000000791110000, 0x0000000793330000)
 concurrent mark-sweep generation total 699072K, used 439841K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38146K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4687K, capacity 4748K, committed 4864K, reserved 1048576K
}
Event: 611.988 GC heap before
{Heap before GC invocations=6207 (full 4):
 par new generation   total 314560K, used 288337K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K, 100% used [0x0000000780000000, 0x0000000791110000, 0x0000000791110000)
  from space 34944K,  24% used [0x0000000793330000, 0x0000000793bb4740, 0x0000000795550000)
  to   space 34944K,   0% used [0x0000000791110000, 0x0000000791110000, 0x0000000793330000)
 concurrent mark-sweep generation total 699072K, used 439841K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38162K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4688K, capacity 4748K, committed 4864K, reserved 1048576K
Event: 611.993 GC heap after
Heap after GC invocations=6208 (full 4):
 par new generation   total 314560K, used 6435K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K,   0% used [0x0000000780000000, 0x0000000780000000, 0x0000000791110000)
  from space 34944K,  18% used [0x0000000791110000, 0x0000000791758dc8, 0x0000000793330000)
  to   space 34944K,   0% used [0x0000000793330000, 0x0000000793330000, 0x0000000795550000)
 concurrent mark-sweep generation total 699072K, used 439845K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38162K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4688K, capacity 4748K, committed 4864K, reserved 1048576K
}
Event: 643.860 GC heap before
{Heap before GC invocations=6208 (full 4):
 par new generation   total 314560K, used 286051K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K, 100% used [0x0000000780000000, 0x0000000791110000, 0x0000000791110000)
  from space 34944K,  18% used [0x0000000791110000, 0x0000000791758dc8, 0x0000000793330000)
  to   space 34944K,   0% used [0x0000000793330000, 0x0000000793330000, 0x0000000795550000)
 concurrent mark-sweep generation total 699072K, used 439845K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38167K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4688K, capacity 4748K, committed 4864K, reserved 1048576K
Event: 643.864 GC heap after
Heap after GC invocations=6209 (full 4):
 par new generation   total 314560K, used 11249K [0x0000000780000000, 0x0000000795550000, 0x0000000795550000)
  eden space 279616K,   0% used [0x0000000780000000, 0x0000000780000000, 0x0000000791110000)
  from space 34944K,  32% used [0x0000000793330000, 0x0000000793e2c680, 0x0000000795550000)
  to   space 34944K,   0% used [0x0000000791110000, 0x0000000791110000, 0x0000000793330000)
 concurrent mark-sweep generation total 699072K, used 439853K [0x0000000795550000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 38167K, capacity 38468K, committed 38656K, reserved 1083392K
  class space    used 4688K, capacity 4748K, committed 4864K, reserved 1048576K
}

Deoptimization events (10 events):
Event: 280.604 Thread 0x00007fcf1606f000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00000001103cd174 method=org.elasticsearch.common.xcontent.XContentBuilder.field(Lorg/elasticsearch/common/xcontent/XContentBuilderString;Ljava/lang/String;)Lorg/elasticsearch/common/xcontent/XC
Event: 280.604 Thread 0x00007fcf1606f000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00000001103cd174 method=org.elasticsearch.common.xcontent.XContentBuilder.field(Lorg/elasticsearch/common/xcontent/XContentBuilderString;Ljava/lang/String;)Lorg/elasticsearch/common/xcontent/XC
Event: 280.604 Thread 0x00007fcf1606f000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00000001103cd174 method=org.elasticsearch.common.xcontent.XContentBuilder.field(Lorg/elasticsearch/common/xcontent/XContentBuilderString;Ljava/lang/String;)Lorg/elasticsearch/common/xcontent/XC
Event: 280.604 Thread 0x00007fcf1606f000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00000001103cd174 method=org.elasticsearch.common.xcontent.XContentBuilder.field(Lorg/elasticsearch/common/xcontent/XContentBuilderString;Ljava/lang/String;)Lorg/elasticsearch/common/xcontent/XC
Event: 343.081 Thread 0x00007fcf17948000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x000000011044c7ac method=org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer.warmNewReaders(Lorg/elasticsearch/index/shard/IndexShard;Lorg/elasticsearch/clus
Event: 343.081 Thread 0x00007fcf17948000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x000000011044c7ac method=org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer.warmNewReaders(Lorg/elasticsearch/index/shard/IndexShard;Lorg/elasticsearch/clus
Event: 343.081 Thread 0x00007fcf17948000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x000000011044c7ac method=org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer.warmNewReaders(Lorg/elasticsearch/index/shard/IndexShard;Lorg/elasticsearch/clus
Event: 343.081 Thread 0x00007fcf17948000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x000000011044c7ac method=org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer.warmNewReaders(Lorg/elasticsearch/index/shard/IndexShard;Lorg/elasticsearch/clus
Event: 416.207 Thread 0x00007fcf14462800 Uncommon trap: reason=unreached action=reinterpret pc=0x00000001100623a4 method=org.apache.lucene.util.fst.ByteSequenceOutputs.add(Lorg/apache/lucene/util/BytesRef;Lorg/apache/lucene/util/BytesRef;)Lorg/apache/lucene/util/BytesRef; @ 49
Event: 425.334 Thread 0x00007fcf1606f000 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00000001104f7830 method=org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse.getIndices()Ljava/util/Map; @ 128

Internal exceptions (10 events):
Event: 208.000 Thread 0x00007fcf14a1d800 Exception &lt;a 'java/security/PrivilegedActionException'&gt; (0x00000007815d5590) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jvm.cpp, line 1248]
Event: 208.000 Thread 0x00007fcf14a1d800 Exception &lt;a 'java/security/PrivilegedActionException'&gt; (0x00000007815dcd90) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jvm.cpp, line 1248]
Event: 246.542 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x000000078d6809b8) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 306.547 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x000000078459a320) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 366.551 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x000000078bfc0e30) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 426.558 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x00000007822b3fa8) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 486.562 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x00000007810b1a88) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 546.565 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x00000007885f9bb0) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 606.570 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x00000007900e5a60) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]
Event: 666.577 Thread 0x00007fcf151c1000 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x0000000786dac030) thrown at [/HUDSON/workspace/8-2-build-macosx-x86_64/jdk8u5/2488/hotspot/src/share/vm/prims/jni.cpp, line 742]

Events (10 events):
Event: 683.728 Executing VM operation: RevokeBias
Event: 683.728 Executing VM operation: RevokeBias done
Event: 683.730 Executing VM operation: RevokeBias
Event: 683.730 Executing VM operation: RevokeBias done
Event: 683.765 Executing VM operation: RevokeBias
Event: 683.765 Executing VM operation: RevokeBias done
Event: 683.765 Executing VM operation: RevokeBias
Event: 683.766 Executing VM operation: RevokeBias done
Event: 683.769 Executing VM operation: RevokeBias
Event: 683.769 Executing VM operation: RevokeBias done


Dynamic libraries:
0x000000000925d000  /System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa
0x000000000925d000  /System/Library/Frameworks/Security.framework/Versions/A/Security
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices
0x000000000925d000  /usr/lib/libz.1.dylib
0x000000000925d000  /usr/lib/libSystem.B.dylib
0x000000000925d000  /usr/lib/libobjc.A.dylib
0x000000000925d000  /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation
0x000000000925d000  /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation
0x000000000925d000  /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit
0x000000000925d000  /System/Library/Frameworks/CoreData.framework/Versions/A/CoreData
0x000000000925d000  /System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices
0x000000000925d000  /System/Library/PrivateFrameworks/UIFoundation.framework/Versions/A/UIFoundation
0x000000000925d000  /System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface
0x000000000925d000  /System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox
0x000000000925d000  /System/Library/Frameworks/AudioUnit.framework/Versions/A/AudioUnit
0x000000000925d000  /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/DataDetectorsCore
0x000000000925d000  /System/Library/PrivateFrameworks/DesktopServicesPriv.framework/Versions/A/DesktopServicesPriv
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox
0x000000000925d000  /System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition
0x000000000925d000  /usr/lib/libauto.dylib
0x000000000925d000  /usr/lib/libicucore.A.dylib
0x000000000925d000  /usr/lib/libxml2.2.dylib
0x000000000925d000  /System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI
0x000000000925d000  /System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio
0x000000000925d000  /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration
0x000000000925d000  /usr/lib/liblangid.dylib
0x000000000925d000  /System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport
0x000000000925d000  /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit
0x000000000925d000  /usr/lib/libDiagnosticMessagesClient.dylib
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices
0x000000000925d000  /System/Library/PrivateFrameworks/PerformanceAnalysis.framework/Versions/A/PerformanceAnalysis
0x000000000925d000  /System/Library/PrivateFrameworks/GenerationalStorage.framework/Versions/A/GenerationalStorage
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL
0x000000000925d000  /System/Library/PrivateFrameworks/Sharing.framework/Versions/A/Sharing
0x000000000925d000  /System/Library/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics
0x000000000925d000  /System/Library/Frameworks/CoreText.framework/Versions/A/CoreText
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/ImageIO
0x000000000925d000  /usr/lib/libextension.dylib
0x000000000925d000  /System/Library/PrivateFrameworks/Backup.framework/Versions/A/Backup
0x000000000925d000  /usr/lib/libarchive.2.dylib
0x000000000925d000  /System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork
0x000000000925d000  /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration
0x000000000925d000  /usr/lib/libCRFSuite.dylib
0x000000000925d000  /usr/lib/libc++.1.dylib
0x000000000925d000  /usr/lib/libc++abi.dylib
0x000000000925d000  /usr/lib/system/libcache.dylib
0x000000000925d000  /usr/lib/system/libcommonCrypto.dylib
0x000000000925d000  /usr/lib/system/libcompiler_rt.dylib
0x000000000925d000  /usr/lib/system/libcopyfile.dylib
0x000000000925d000  /usr/lib/system/libcorecrypto.dylib
0x000000000925d000  /usr/lib/system/libdispatch.dylib
0x000000000925d000  /usr/lib/system/libdyld.dylib
0x000000000925d000  /usr/lib/system/libkeymgr.dylib
0x000000000925d000  /usr/lib/system/liblaunch.dylib
0x000000000925d000  /usr/lib/system/libmacho.dylib
0x000000000925d000  /usr/lib/system/libquarantine.dylib
0x000000000925d000  /usr/lib/system/libremovefile.dylib
0x000000000925d000  /usr/lib/system/libsystem_asl.dylib
0x000000000925d000  /usr/lib/system/libsystem_blocks.dylib
0x000000000925d000  /usr/lib/system/libsystem_c.dylib
0x000000000925d000  /usr/lib/system/libsystem_configuration.dylib
0x000000000925d000  /usr/lib/system/libsystem_coreservices.dylib
0x000000000925d000  /usr/lib/system/libsystem_coretls.dylib
0x000000000925d000  /usr/lib/system/libsystem_dnssd.dylib
0x000000000925d000  /usr/lib/system/libsystem_info.dylib
0x000000000925d000  /usr/lib/system/libsystem_kernel.dylib
0x000000000925d000  /usr/lib/system/libsystem_m.dylib
0x000000000925d000  /usr/lib/system/libsystem_malloc.dylib
0x000000000925d000  /usr/lib/system/libsystem_network.dylib
0x000000000925d000  /usr/lib/system/libsystem_networkextension.dylib
0x000000000925d000  /usr/lib/system/libsystem_notify.dylib
0x000000000925d000  /usr/lib/system/libsystem_platform.dylib
0x000000000925d000  /usr/lib/system/libsystem_pthread.dylib
0x000000000925d000  /usr/lib/system/libsystem_sandbox.dylib
0x000000000925d000  /usr/lib/system/libsystem_secinit.dylib
0x000000000925d000  /usr/lib/system/libsystem_stats.dylib
0x000000000925d000  /usr/lib/system/libsystem_trace.dylib
0x000000000925d000  /usr/lib/system/libunc.dylib
0x000000000925d000  /usr/lib/system/libunwind.dylib
0x000000000925d000  /usr/lib/system/libxpc.dylib
0x000000000925d000  /usr/lib/libbz2.1.0.dylib
0x000000000925d000  /usr/lib/liblzma.5.dylib
0x000000000925d000  /usr/lib/libbsm.0.dylib
0x000000000925d000  /usr/lib/libsqlite3.dylib
0x000000000925d000  /usr/lib/system/libkxld.dylib
0x000000000925d000  /usr/lib/libxar.1.dylib
0x000000000925d000  /usr/lib/libpam.2.dylib
0x000000000925d000  /usr/lib/libOpenScriptingUtil.dylib
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices
0x000000000925d000  /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices
0x000000000925d000  /System/Library/Frameworks/NetFS.framework/Versions/A/NetFS
0x000000000925d000  /System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth
0x000000000925d000  /System/Library/PrivateFrameworks/login.framework/Versions/A/Frameworks/loginsupport.framework/Versions/A/loginsupport
0x000000000925d000  /System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC
0x000000000925d000  /usr/lib/libmecabra.dylib
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSync.framework/Versions/A/ColorSync
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
0x000000000925d000  /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontParser.dylib
0x000000000925d000  /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib
0x000000000925d000  /System/Library/PrivateFrameworks/AppleVPA.framework/Versions/A/AppleVPA
0x000000000925d000  /System/Library/PrivateFrameworks/AppleJPEG.framework/Versions/A/AppleJPEG
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJP2.dylib
0x000000000925d000  /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib
0x000000000925d000  /System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCVMSPluginSupport.dylib
0x000000000925d000  /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib
0x000000000925d000  /usr/lib/libcups.2.dylib
0x000000000925d000  /System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos
0x000000000925d000  /System/Library/Frameworks/GSS.framework/Versions/A/GSS
0x000000000925d000  /usr/lib/libresolv.9.dylib
0x000000000925d000  /usr/lib/libiconv.2.dylib
0x000000000925d000  /System/Library/PrivateFrameworks/Heimdal.framework/Versions/A/Heimdal
0x000000000925d000  /usr/lib/libheimdal-asn1.dylib
0x000000000925d000  /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory
0x000000000925d000  /System/Library/PrivateFrameworks/CommonAuth.framework/Versions/A/CommonAuth
0x000000000925d000  /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory
0x000000000925d000  /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation
0x000000000925d000  /System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling
0x000000000925d000  /usr/lib/libcmph.dylib
0x000000000925d000  /System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement
0x000000000925d000  /usr/lib/libxslt.1.dylib
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink
0x000000000925d000  /System/Library/Frameworks/QuartzCore.framework/Versions/A/Frameworks/CoreImage.framework/Versions/A/CoreImage
0x000000000925d000  /System/Library/PrivateFrameworks/CrashReporterSupport.framework/Versions/A/CrashReporterSupport
0x000000000925d000  /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL
0x000000000925d000  /System/Library/PrivateFrameworks/FaceCore.framework/Versions/A/FaceCore
0x000000000925d000  /System/Library/PrivateFrameworks/Ubiquity.framework/Versions/A/Ubiquity
0x000000000925d000  /System/Library/PrivateFrameworks/IconServices.framework/Versions/A/IconServices
0x000000000925d000  /System/Library/PrivateFrameworks/ChunkingLibrary.framework/Versions/A/ChunkingLibrary
0x000000000925d000  /System/Library/PrivateFrameworks/Apple80211.framework/Versions/A/Apple80211
0x000000000925d000  /System/Library/Frameworks/CoreWLAN.framework/Versions/A/CoreWLAN
0x000000000925d000  /System/Library/Frameworks/IOBluetooth.framework/Versions/A/IOBluetooth
0x000000000925d000  /System/Library/PrivateFrameworks/CoreWiFi.framework/Versions/A/CoreWiFi
0x000000000925d000  /System/Library/Frameworks/CoreBluetooth.framework/Versions/A/CoreBluetooth
0x000000000925d000  /System/Library/PrivateFrameworks/DebugSymbols.framework/Versions/A/DebugSymbols
0x000000000925d000  /System/Library/PrivateFrameworks/CoreSymbolication.framework/Versions/A/CoreSymbolication
0x000000000925d000  /System/Library/PrivateFrameworks/Symbolication.framework/Versions/A/Symbolication
0x000000000925d000  /System/Library/PrivateFrameworks/SpeechRecognitionCore.framework/Versions/A/SpeechRecognitionCore
0x000000010dc00000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/server/libjvm.dylib
0x000000000925d000  /usr/lib/libstdc++.6.dylib
0x000000010cb80000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/libverify.dylib
0x000000010cb8e000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/libjava.dylib
0x000000010cbcc000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/libzip.dylib
0x000000012375f000  /System/Library/Frameworks/JavaVM.framework/Frameworks/JavaRuntimeSupport.framework/JavaRuntimeSupport
0x0000000123777000  /System/Library/Frameworks/JavaVM.framework/Versions/A/Frameworks/JavaNativeFoundation.framework/Versions/A/JavaNativeFoundation
0x000000012378c000  /System/Library/Frameworks/JavaVM.framework/Versions/A/JavaVM
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Carbon
0x0000000123799000  /System/Library/PrivateFrameworks/JavaLaunching.framework/Versions/A/JavaLaunching
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print
0x000000000925d000  /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI
0x0000000125b0b000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/libnet.dylib
0x0000000125edc000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/libnio.dylib
0x0000000125f11000  /Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/jre/lib/libmanagement.dylib
0x0000000126583000  /Users/donkoink/Development/elasticsearch-1.5.2/lib/sigar/libsigar-universal64-macosx.dylib

VM Arguments:
jvm_args: -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/Users/donkoink/Development/elasticsearch-1.5.2 
java_command: org.elasticsearch.bootstrap.Elasticsearch
java_class_path (initial): :/Users/donkoink/Development/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/antlr-runtime-3.5.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/apache-log4j-extras-1.2.17.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/asm-4.1.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/asm-commons-4.1.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/elasticsearch-1.5.2.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/groovy-all-2.4.0.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/jna-4.1.0.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/jts-1.13.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/log4j-1.2.17.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-analyzers-common-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-core-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-expressions-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-grouping-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-highlighter-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-join-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-memory-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-misc-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-queries-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-queryparser-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-sandbox-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-spatial-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/lucene-suggest-4.10.4.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/spatial4j-0.4.1.jar:/Users/donkoink/Development/elasticsearch-1.5.2/lib/sigar/sigar-1.6.4.jar
Launcher Type: SUN_STANDARD

Environment Variables:
PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Users/donkoink/Library/Android/sdk:/Users/donkoink/Library/Android/sdk/tools/:/Users/donkoink/Library/Android/sdk/platform-tools/
SHELL=/bin/bash

Signal Handlers:
SIGSEGV: [libjvm.dylib+0x55f7df], sa_mask[0]=0xfffefeff, sa_flags=0x00000043
SIGBUS: [libjvm.dylib+0x55f7df], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGFPE: [libjvm.dylib+0x442cfa], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGPIPE: [libjvm.dylib+0x442cfa], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGXFSZ: [libjvm.dylib+0x442cfa], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGILL: [libjvm.dylib+0x442cfa], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGUSR1: SIG_DFL, sa_mask[0]=0x63807efb, sa_flags=0x00000000
SIGUSR2: [libjvm.dylib+0x442818], sa_mask[0]=0x00000000, sa_flags=0x00000042
SIGHUP: [libjvm.dylib+0x440ba5], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGINT: [libjvm.dylib+0x440ba5], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGTERM: [libjvm.dylib+0x440ba5], sa_mask[0]=0xfffefeff, sa_flags=0x00000042
SIGQUIT: [libjvm.dylib+0x440ba5], sa_mask[0]=0xfffefeff, sa_flags=0x00000042


---------------  S Y S T E M  ---------------

OS:Bsduname:Darwin 14.3.0 Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64 x86_64
rlimit: STACK 8192k, CORE infinity, NPROC 709, NOFILE 10240, AS infinity
load average:2.46 3.12 3.41

CPU:total 8 (4 cores per cpu, 2 threads per core) family 6 model 70 stepping 1, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, ht, tsc, tscinvbit

Memory: 4k page, physical 16777216k(47852k free)

/proc/meminfo:


vm_info: Java HotSpot(TM) 64-Bit Server VM (25.5-b02) for bsd-amd64 JRE (1.8.0_05-b13), built on Mar 18 2014 00:36:13 by "java_re" with gcc 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)

time: Wed Jun 24 10:23:56 2015
elapsed time: 683 seconds
```
</description><key id="90898445">11873</key><summary>Random crashes even when idle (on Mac)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">donkoink</reporter><labels /><created>2015-06-25T08:48:50Z</created><updated>2015-06-26T15:15:54Z</updated><resolved>2015-06-25T09:59:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-25T09:59:15Z" id="115195667">Hiya. It's libsigar causing this crash.  See #11808 for a duplicate report and what you can do about it.

We're removing libsigar in 2.0, see #11034
</comment><comment author="donkoink" created="2015-06-25T10:50:59Z" id="115208661">Hi clintongormley. Thanks for the quick reply. I added bootstrap.sigar: false in elasticsearch.yml but I'm still getting crashes with the same error details. Maybe a dumb question, but do I need to restart something apart from starting Elasticsearch after it crashes? Thanks
</comment><comment author="clintongormley" created="2015-06-25T10:52:35Z" id="115208893">@donkoink you don't mention the version you're on.  that option is only supported in 1.6.0.  For previous versions you should delete libs/sigar
</comment><comment author="donkoink" created="2015-06-25T12:28:17Z" id="115234336">Thanks @clintongormley and sorry my bad. I'm on version 1.5.2. Will follow your instructions and thanks again!
</comment><comment author="donkoink" created="2015-06-26T15:15:54Z" id="115724765">@clintongormley I've fixed my issue by upgrading to 1.6.0 and modifying elasticsearch.yml. This solved my problem. On version 1.5.2, it kept on crashing even after deleting libs/sigar. Hopes this helps someone else eventually.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Score based on date timestamp are not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11872</link><project id="" key="" /><description>Hi,

I tried to create a query with a score based on a date, but couldn't make it to work. The score I got is inconsistent.
Using the date alone gave me 3 identical scores where the date are actually different.
## Timstamp

```
# Delete the index.
curl -XDELETE http://localhost:9200/test_date_scoring

# Insert 3 documents with different dates.
curl -XPUT http://localhost:9200/test_date_scoring/foo/1 -d '{
    "title": "Foo 1",
    "date": "2015-06-25T10:15:00"
}'
curl -XPUT http://localhost:9200/test_date_scoring/foo/2 -d '{
    "title": "Foo 2",
    "date": "2015-06-25T10:15:01"
}'
curl -XPUT http://localhost:9200/test_date_scoring/foo/3 -d '{
    "title": "Foo 3",
    "date": "2015-06-25T10:15:02"
}'

# Query with score based on date. The 3 scores should be different.
# Got three 1435227260000.
curl -XPOST http://localhost:9200/test_date_scoring/foo/_search -d '{
    "query": {
        "function_score": {
            "functions": [
                {
                    "field_value_factor": {
                        "field": "date"
                    }
                }
            ],
            "boost_mode": "replace"
        }
    }
}'

# Result
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 3,
    "max_score": 1435227260000,
    "hits": [
      {
        "_index": "test_date_scoring",
        "_type": "foo",
        "_id": "1",
        "_score": 1435227260000,
        "_source": {
          "title": "Foo 1",
          "date": "2015-06-25T10:15:00"
        }
      },
      {
        "_index": "test_date_scoring",
        "_type": "foo",
        "_id": "2",
        "_score": 1435227260000,
        "_source": {
          "title": "Foo 2",
          "date": "2015-06-25T10:15:01"
        }
      },
      {
        "_index": "test_date_scoring",
        "_type": "foo",
        "_id": "3",
        "_score": 1435227260000,
        "_source": {
          "title": "Foo 3",
          "date": "2015-06-25T10:15:02"
        }
      }
    ]
  }
}
```

I made some more tests with factor and sum, and again results are not what I expected.
## Factor

```
# Query with score based on date and 0.001 factor. Score should be the same minus three 0.
# Got 1435227390 instead of 1435227260.
curl -XPOST http://localhost:9200/test_date_scoring/foo/_search -d '{
    "query": {
        "function_score": {
            "functions": [
                {
                    "field_value_factor": {
                        "field": "date",
                        "factor": 0.001
                    }
                }
            ],
            "boost_mode": "replace"
        }
    }
}'
```
## Sum

```
# Query with score based on date, then 1 is added. Score should be the same + 1.
# Got 1435227260000 instead of 1435227260001.
curl -XPOST http://localhost:9200/test_date_scoring/foo/_search -d '{
    "query": {
        "function_score": {
            "query": {
                "function_score": {
                    "functions": [
                        {
                            "field_value_factor": {
                                "field": "date"
                            }
                        }
                    ],
                    "boost_mode": "replace"
                }
            },
            "functions": [
                {
                    "weight": 1
                }
            ],
            "boost_mode": "sum"
        }
    }
}'
```

So my question is :
**Is this the correct behavior ? If it is, how can I base my score on a date ?**
</description><key id="90894470">11872</key><summary>Score based on date timestamp are not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">valentin-claras</reporter><labels /><created>2015-06-25T08:31:58Z</created><updated>2016-01-24T00:13:03Z</updated><resolved>2015-06-25T09:57:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-25T09:57:03Z" id="115195313">If you add the `?explain` parameter you will see that you are exceeding the max value for score, which is why they end up all being the same.  However, if you set the factor to (eg) `0.000001` then you get different scores.

A better way to score on date (or recency, really) is to use a decay function.
</comment><comment author="valentin-claras" created="2015-06-25T12:46:03Z" id="115239759">Thanks for your quick answer.

Still, I'm not sure I understand your argument because when using `?explain` (thanks btw, didn't know this one) I see that the maxBoost is way larger than the field value.

```
_explanation: {
    value: 1435227260000,
    description: "function score, product of:",
    details: [
        {
            value: 1435227260000
            description: "Math.min of",
            details: [
                {
                    value: 1435227260000,
                    description: "field value function: (doc['date'].value * factor=1.0)"
                },
                {
                    value: 3.4028235e+38,
                    description: "maxBoost"
                }
            ]
        },
        {
            value: 1,
            description: "queryBoost"
        }
    ]
}
```
</comment><comment author="clintongormley" created="2015-06-26T12:26:14Z" id="115660012">@valentin-claras The final `_score` is a float, which can only represent integers accurately up to to 2^25.  Timestamps are of the order of 2^40, so cannot be represented accurately, hence the rounding that you are seeing.  
</comment><comment author="valentin-claras" created="2015-06-26T12:36:44Z" id="115664027">Yes that's what I understood later.

I'm now using a "0.0000166666666667" factor, so I can use my date with an accuracy to the minute and it's working so far (until the day I will need an accuracy to the second).

Thanks for your answer.
</comment><comment author="sundarv85" created="2016-01-24T00:13:03Z" id="174236182">The value "0.0000166666666667" indeed is working. To understand it better, is it correct that the `_score` only store 8 digits (32-bits), there by multiplying with `0.00001` would give the score the top 8 digits of the timestamp and adding the `6ss` at the end is to throw in a random multiplier.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix RecoveryState timestamps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11871</link><project id="" key="" /><description>Use System.currentTimeMillis() instead of System.nanoTime() since it's for wall clock time.
Fixes #11870.
</description><key id="90890838">11871</key><summary>Fix RecoveryState timestamps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">masaruh</reporter><labels><label>:Stats</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-25T08:04:42Z</created><updated>2015-07-14T13:46:40Z</updated><resolved>2015-06-29T11:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-25T08:12:34Z" id="115157833">@masaruh talked a bout using System.currentTimeMillis for the time markers and nano time for the delta. 
</comment><comment author="mikemccand" created="2015-06-25T18:07:14Z" id="115347805">Sorry, I caused this with #11058!  I didn't realize we were also using the absolute msec values against a calendar (not just for elapsed msec).
</comment><comment author="masaruh" created="2015-06-29T02:04:11Z" id="116375720">So, backport would look like this? https://github.com/masaruh/elasticsearch/commit/4ecd530f5033cf35927e859f7011c1e0ffbd655b
</comment><comment author="bleskes" created="2015-06-29T08:20:34Z" id="116519747">Thx @masaruh . Looks great - I left one question about the test and a simple comment on the backport (which looks good).
</comment><comment author="masaruh" created="2015-06-29T09:31:57Z" id="116570472">Thanks @bleskes. Changed test to use &gt;= 0. Since it can be 0ms when it's done really quick.
</comment><comment author="bleskes" created="2015-06-29T09:38:31Z" id="116575067">are you sure a 0 is possible? see https://github.com/elastic/elasticsearch/pull/11871/files#diff-064f839a8d24936e06aeb470ef25c9faR149
</comment><comment author="masaruh" created="2015-06-29T09:48:25Z" id="116584727">Ah right. That one should have been greaterThan. But I believe another one https://github.com/elastic/elasticsearch/pull/11871/files#diff-064f839a8d24936e06aeb470ef25c9faR289 can be 0.
</comment><comment author="masaruh" created="2015-06-29T09:49:28Z" id="116585625">Pushed another fix.
</comment><comment author="bleskes" created="2015-06-29T09:53:31Z" id="116587900">awesome. LGTM
</comment><comment author="masaruh" created="2015-06-29T11:02:10Z" id="116608295">@bleskes thanks, also pushed to 1.x and 1.6.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>recovery state returns 1970s start/end time.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11870</link><project id="" key="" /><description>It occurs only version &gt;= 1.6.0.

`GET /_recovery?pretty&amp;human` returns something like:

```
   "index_name": {
      "shards": [
         {
            "id": 0,
            "type": "GATEWAY",
            "stage": "DONE",
            "primary": true,
            "start_time": "1970-01-01T12:05:18.603Z",
            "start_time_in_millis": 43518603,
            "stop_time": "1970-01-01T12:05:18.681Z",
            "stop_time_in_millis": 43518681,
            "total_time": "78ms",
            "total_time_in_millis": 78,
```

I believe it's because System.nanoTime() is used for wall clock time in RecoveryState like https://github.com/elastic/elasticsearch/blob/v1.6.0/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java#L405
</description><key id="90888527">11870</key><summary>recovery state returns 1970s start/end time.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/masaruh/following{/other_user}', u'events_url': u'https://api.github.com/users/masaruh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/masaruh/orgs', u'url': u'https://api.github.com/users/masaruh', u'gists_url': u'https://api.github.com/users/masaruh/gists{/gist_id}', u'html_url': u'https://github.com/masaruh', u'subscriptions_url': u'https://api.github.com/users/masaruh/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/572174?v=4', u'repos_url': u'https://api.github.com/users/masaruh/repos', u'received_events_url': u'https://api.github.com/users/masaruh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/masaruh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'masaruh', u'type': u'User', u'id': 572174, u'followers_url': u'https://api.github.com/users/masaruh/followers'}</assignee><reporter username="">masaruh</reporter><labels><label>:Stats</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-06-25T07:51:43Z</created><updated>2015-07-14T03:17:09Z</updated><resolved>2015-06-29T10:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java</file></files><comments><comment>Fix RecoveryState timestamps</comment></comments></commit></commits></item><item><title>Unable to maintain proper SAYT search results with multi field multi word data..</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11869</link><project id="" key="" /><description>Hi,
I would like to achieve proper SAYT(Search As You Type) search results like.
1) When we give a letter need to get the data using that letter at starting position (unable to achieve this)
2) The SAYT data should be in sorted order ( I covered this part).

My intention is to search data in multiple fields and those fields have multiple words. 

1) Starting I preferred QueryString , where in general query string will search on multiple fields and we can give priorities to those fields and we can use 'AND' /'OR' operations.

2) But failed at special characters like ( '/', ',', '.', '(', ')' ). In order to use these characters we have to escape these characters, but that is not yielding accurate results.

3) Where match queries can accept any type of characters, decided to use that and I used following analyzers in order to make match query to work on any situation.

Here are my settings:

"analysis": {
               "analyzer": {
                  "analyzer_startswith": {
                     "type": "custom",
                     "filter": "lowercase",
                     "tokenizer": "keyword"
                  },
                  "whitespace_analyzer": {
                     "type": "custom",
                     "filter": [
                        "lowercase",
                        "asciifolding"
                     ],
                     "tokenizer": "whitespace"
                  },
                  "wordAnalyzer": {
                     "type": "custom",
                     "filter": [
                        "lowercase",
                        "asciifolding",
                        "nGram_filter"
                     ],
                     "tokenizer": "whitespace"
                  }
               },
               "filter": {
                  "nGram_filter": {
                     "max_gram": "20",
                     "min_gram": "1",
                     "type": "nGram",
                     "token_chars": [
                        "letter",
                        "punctuation",
                        "symbol",
                        "digit"
                     ]
                  }
               }
            }

Here are my mappings : 

 "ymme": {
      "mappings": {
         "ymme_type": {
            "_all": {
               "auto_boost": true,
               "index_analyzer": "wordAnalyzer",
               "search_analyzer": "whitespace_analyzer"
            },
            "properties": {
               "Engine": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "EngineCode": {
                  "type": "string",
                  "include_in_all": false
               },
               "Make": {
                  "type": "string",
                  "boost": 3,
                  "index": "not_analyzed",
                  "norms": {
                     "enabled": true
                  }
               },
               "MakeCode": {
                  "type": "string",
                  "include_in_all": false
               },
               "Model": {
                  "type": "string",
                  "boost": 2,
                  "index": "not_analyzed",
                  "norms": {
                     "enabled": true
                  }
               },
               "ModelCode": {
                  "type": "string",
                  "include_in_all": false
               },
               "ShortYear": {
                  "type": "string",
                  "boost": 4,
                  "index": "not_analyzed",
                  "norms": {
                     "enabled": true
                  }
               },
               "Year": {
                  "type": "string",
                  "boost": 5,
                  "index": "not_analyzed",
                  "norms": {
                     "enabled": true
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed"
                     }
                  }
               },
               "YearCode": {
                  "type": "string",
                  "include_in_all": false
               }
            }
         }
      }
}

Now I used query as : 

GET ymme/ymme_type/_search
{
    "query": {
        "bool": {
            "must": [
               {
                   "match": {
                      "_all": 
                      {
                          "query": "2",
                          "operator": "and"
                      }
                   }
               }
            ]
        }
    }
}

Every thing is working fine......
But problem is with words starting with:
when I give my query as  "query": "2000 Audi",

1) Here '2000' belongs to year / 'Audi' belongs to Make, so when I am entering the letter 'A'  my search result is not showing the data starting with a, searching is done from middle of the word. I know the reason that because of my analyzers. 
2) If I use match_phrase_prefix without these analyzers an every thing am unable to get the data with query:"2000 a", and I have to give the data in indexing order like 

query:" "Year"  "Model"    "ShortYear"   "Engine"   "Make" ", in this order I have search if i miss the order am unable to retrive the data with match_phrase_prefix....

Can anyone give me a solution to my problem that need to get the data starting with and need to search in multiple fields multiple words by preserving special characters..
</description><key id="90873236">11869</key><summary>Unable to maintain proper SAYT search results with multi field multi word data..</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HemaAnusha</reporter><labels /><created>2015-06-25T06:48:31Z</created><updated>2015-06-25T06:52:36Z</updated><resolved>2015-06-25T06:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HemaAnusha" created="2015-06-25T06:50:49Z" id="115128812">The analyzers that I mentioned in my settings tried all those but unable to achieve my requirement..
</comment><comment author="dadoonet" created="2015-06-25T06:52:35Z" id="115129751">Please open this on discuss.elastic.co.
We can help there.

Closing. Not an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>correct mis-type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11868</link><project id="" key="" /><description>follow the context, it should means if document exists then fresh it.
</description><key id="90828997">11868</key><summary>correct mis-type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YoungHu</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-25T02:09:15Z</created><updated>2015-06-26T15:46:15Z</updated><resolved>2015-06-26T15:46:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-25T09:22:44Z" id="115181460">Hi @YoungHu 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="YoungHu" created="2015-06-26T12:58:34Z" id="115671640">Hi @clintongormley CLA is signed, could you help to check if can merge it now or I need resubmit the pull request?
</comment><comment author="clintongormley" created="2015-06-26T15:46:15Z" id="115736982">thanks @YoungHu - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11868 from YoungHu/patch-1</comment></comments></commit></commits></item><item><title>sort ignore_unmapped fails for String fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11867</link><project id="" key="" /><description>When sorting values using an alias, and one of the aliased indexes is missing the values, I added the `ignore_unmapped=true` option. This works well for Long fields, but does not work for String fields. 

It appears that it tries to insert Long.MAX_VALUE or Long.MIN_VALUE to force items to the bottom of the sort. But when it does this for String values, I get the error `ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef]`

The newer version of the Query DSL support the `unmapped_type` parameter which allows specifying that the field is a String or a Long which works correctly. However in my use case, these fields are dynamically mapped and the code does not have knowledge of what their mapped types are.

Steps to reproduce:

```
# create some documents in the first index with a string field
POST my_index1/my_type
{
  "FieldStr": "this is a string"
}
POST my_index1/my_type
{
  "FieldStr": "another string"
}

# create some documents in a second index with a different long field
POST my_index2/my_type
{
  "FieldLong": 234
}
POST my_index2/my_type
{
  "FieldLong": 56
}

# create an alias that points to both indexes
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "my_index1",
        "alias": "my_alias"
      }
    },
    {
      "add": {
        "index": "my_index2",
        "alias": "my_alias"
      }
    }
  ]
}

# sort by the string field
# Fails for my_index2 but returns results from my_index1
GET my_alias/my_type/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldStr": {
        "order": "desc"
      }
    }
  ]
}

# sort by the long field
# Fails for my_index1 but returns results from my_index2
GET my_alias/my_type/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldLong": {
        "order": "desc"
      }
    }
  ]
}

# sort by the string field
# put missing fields last and ignore unmapped
# FAILS completely
GET my_alias/my_type/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldStr": {
        "order": "desc",
        "missing": "_last",
        "ignore_unmapped": true
      }
    }
  ]
}

# sort by the long field
# put missing fields last and ignore unmapped
# Works perfectly
GET my_alias/my_type/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldLong": {
        "order": "desc",
        "missing": "_last",
        "ignore_unmapped": true
      }
    }
  ]
}

# sort by the string field
# use unmapped_type
# Works perfectly
GET my_alias/my_type/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldStr": {
        "order": "desc",
        "missing": "_last",
        "unmapped_type": "String"
      }
    }
  ]
}

# sort by the long field
# use unmapped_type
# Works perfectly
GET my_alias/my_type/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldLong": {
        "order": "desc",
        "missing": "_last",
        "unmapped_type": "Long"
      }
    }
  ]
}
```
</description><key id="90820023">11867</key><summary>sort ignore_unmapped fails for String fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MassTroy</reporter><labels><label>:Search</label><label>high hanging fruit</label></labels><created>2015-06-25T01:12:57Z</created><updated>2015-06-26T11:44:00Z</updated><resolved>2015-06-26T11:44:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="szroland" created="2015-06-25T22:25:25Z" id="115418328">It seems like this was a known issue with `ignore_unmapped`, which is why I believe it would have been deprecated in favor of `unmapped_type`, e.g. see #2255. The current behavior of `ignore_unmapped=true` is practically to set `unmapped_type` to `long`. You might as well stop using `ignore_unmapped` and use `"unmapped_type": "long"` yourself to understand what is going on.

Now, I believe there is an implicit assumption that if you sort across indices, the same field will be consistently mapped everywhere, or not mapped at all, this latter being handled by `unmapped_type`. If this is the case, you can actually discover what the field ended up being mapped to before doing the sort by checking `GET my_alias/my_type/_mapping`. Whatever the consistent mapping is, you can simply plug the same type into `unmapped_type` when you search and have a working solution with the current code. Maybe elastic could actually do this for you, which is likely the semantics you may be expecting from `ignore_unmapped`, but currently this doesn't happen.

However, if what you are writing is true, and you have no control over mapping and fields get auto-mapped in each index, you can run into a situation where you have conflicting mapping, and there will be no way to trivially sort in that case.

Let me demonstrate, assuming the setup you described already loaded, let's have `FieldStr` auto-mapped to `long` in `my_index2`:

```
POST my_index2/my_type
{
  "FieldStr": 234
}
```

Now, if you run your query, that worked before:

```
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FieldStr": {
        "order": "desc",
        "missing": "_last",
        "unmapped_type": "String"
      }
    }
  ]
}
```

You will run into the same issue of casting because of the different mapping of the same field in the indices:

```
{
  type: "class_cast_exception"
  reason: "java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef"
}
```

There is no way to sort by this field anymore. Note that, in your case, assuming you truly have no control in your app, you can use the `GET my_alias/my_type/_mapping` call to detect this situation as well (e.g. discover conflicting types in the various indices for the same field) and either:
1. disable sorting on this field altogether knowing it will fail
2. maybe use scripting to create a field on the fly with the right semantics (e.g. numeric or string), and sort by that artificial sort support field (https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_script_based_sorting) (less storage, less performance)
3. use multifields to store a string version of all fields which you can fall back to during sorting (https://www.elastic.co/guide/en/elasticsearch/guide/current/multi-fields.html) (more storage, higher performance I'd guess)

In general, Elastic would not be able to decide for you whether to sort numerically or alphabetically, even if it supported type-conversions during sorting. Also, you can get weird results in your example with analyzed text fields (default), this could also be addressed by option 3, e.g. set the raw version, which you use for sorting, not analyzed, and always use that for sorting, unless you have a consistent numeric mapping on the primary field.
</comment><comment author="clintongormley" created="2015-06-26T11:44:00Z" id="115653708">Well answered @szroland - I think you've captured all of the problems.

This looks like a won't fix, so I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms aggregation does not work with geo bounding box filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11866</link><project id="" key="" /><description>I am trying to perform a terms aggregation with the context scoped by a geo_bouding_box filter. The search always returns no buckets in the aggregation when a geo_bouding_box filter is applied to attempt to scope the query to documents in a bounding box.

I have a `properties` index that have `property` documents with children `listing` documents. Listings have a `broker` attribute. I need to find distinct `broker` attributes and the number of their occurrences for 
all properties in a bounding box.

The query I am performing is as follows:

``` json
{
    "query" : {
        "filtered": {
            "query": { "match_all": {} },
            "filter": {
                "geo_bounding_box" : {
                    "location" : {
                        "top_left" : {
                            "lat" : 40.73,
                            "lon" : -74.1
                        },
                        "bottom_right" : {
                            "lat" : 40.01,
                            "lon" : -71.12
                        }
                    }
                }
            }
        }
    },
    "aggs" : {
        "brokers" : {
            "terms" : {
                "field" : "listing.broker",
                "order" : { "_term" : "asc" }
            }
        }
    }
}
```

The result I get back is the following:

``` json
{
  "took": 6,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 2179,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "brokers": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": []
    }
  }
}
```

The mapping of the index is as follows:

``` json
{
  "properties": {
    "aliases": {},
    "mappings": {
      "property": {
        "properties": {
          "address": {
            "type": "string"
          },
          "city": {
            "type": "string"
          },
          "country": {
            "type": "string"
          },
          "location": {
            "type": "geo_point"
          },
          "id": {
            "type": "long"
          },
          "state": {
            "type": "string"
          },
          "street": {
            "type": "string"
          },
          "title": {
            "type": "string"
          },
          "zip_code": {
            "type": "string"
          }
        }
      },
      "listing": {
        "_parent": {
          "type": "property"
        },
        "_routing": {
          "required": true
        },
        "properties": {
          "broker": {
            "type": "string",
            "index": "not_analyzed"
          }
        }
      }
    }
  }
}
```

**Notes:**
- Replacing the `geo_bounding_box` with other filter types like a `term` filter works as expected
- I am expecting back buckets with the value of the broker attribute on a listing with the document count within the bounding box
</description><key id="90807990">11866</key><summary>Terms aggregation does not work with geo bounding box filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spencerapplegate</reporter><labels /><created>2015-06-24T23:46:34Z</created><updated>2015-06-25T12:48:54Z</updated><resolved>2015-06-25T09:03:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-25T09:03:00Z" id="115174469">Hi @spencerapplegate 

It sounds like you have no documents which match your geo filter.  I'd suggest choosing one document which you think SHOULD match and testing it against your query with the explain API: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-explain.html

Btw, a better place to ask questions like these is in the forum: http://discuss.elastic.co/.  This issues list is meant for bug reports and feature requests

thanks
</comment><comment author="spencerapplegate" created="2015-06-25T11:59:03Z" id="115222472">@clintongormley I have 2179 documents that match my geo filter, I'd be pleased to post the response with the hits here but it is obviously rather large. When I remove the `search_type=count` parameter, they all appear in the hits section of the response. This is absolutely unexpected behavior
</comment><comment author="clintongormley" created="2015-06-25T12:16:14Z" id="115229765">@spencerapplegate I missed that.  But your geo-point is in the `property` type and `broker` is in the `listing` type, so your matching docs (all of type `property`) don't have any values called `broker.listing`.  

You have a parent-child relationship between property and broker, so presumably the thing you're missing is a `children` aggregation.
</comment><comment author="spencerapplegate" created="2015-06-25T12:29:09Z" id="115234460">@clintongormley when I change the filtered query to something more basic, like a term filter, its works completely as expected. This is with aggregating on the child document. For instance, If I change the query to this:

``` json
{
    "query" : {
        "filtered": {
            "query": { "match_all": {} },
            "filter": {
                "term": {
                    "broker": "CBRE"
                }
            }
        }
    },
    "aggs" : {
        "brokers" : {
            "terms" : {
                "field" : "listing.broker",
                "order" : { "_term" : "asc" }
            }
        }
    }
}
```

It returns the correct bucket with its counts:

``` json
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1533,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "brokers": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "CBRE",
          "doc_count": 1533
        }
      ]
    }
  }
}
```

We have 1533 properties with listings with a broker of `CBRE`.

Its only when we try to use the geo_bounding_box filter that it will return `[]` for buckets. We have tried flipping top_right to top_left and vis-versa for the bottom bounds. We have also changed the bounding box to be all inclusive, but nothing will return in the buckets.
</comment><comment author="clintongormley" created="2015-06-25T12:46:37Z" id="115240006">@spencerapplegate that's because your term query is matching the `broker` field in the `listing` type, and that's the same document type that you're aggregating on.  If you change your term query to check a field that only exists in the `property` type then you'll see that you get no aggregations from the `listing.broker` field, same as for geo.

Please take this to the forum - this is a problem with usage, not Elasticsearch.
</comment><comment author="spencerapplegate" created="2015-06-25T12:48:54Z" id="115240990">@clintongormley thank you for the direction!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactoring of FuzzyQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11865</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="90789215">11865</key><summary>Refactoring of FuzzyQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-24T22:01:12Z</created><updated>2015-09-14T21:30:55Z</updated><resolved>2015-08-10T15:27:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-25T14:00:17Z" id="115267339">Did a round of review, agree with most of the changes. I left some comments, mostly concerning questions of where to set default Rewrite method when in filter context and some thoughts concerning leaving internal query value representation as `Object`. We might have to check if it is possible to restrict the Java-API to only accept Numeric, String and Date values (at least that's what seems allowed reading the docs). Then we might be able to just have one internal String representation.
</comment><comment author="alexksikes" created="2015-06-25T20:32:56Z" id="115389242">Thanks for the review. I went for storing `value` as a String internally. I'm not entirely sure this is the right decision though. We should probably only accept Numeric, String and Date values.
</comment><comment author="cbuescher" created="2015-06-26T13:50:40Z" id="115697251">@alexksikes I went through your last commit, left a few comments. Sorry I didn't realize they show up in such a strange way if I do the comments on the commit directly here in Github, will try to avoid this in the future. 
</comment><comment author="cbuescher" created="2015-06-29T10:21:24Z" id="116595065">Went through the changes another time, would like for @javanna to have another look and for an opinion about how much we should restrict the current constructor that allows (but won't work) for any `Object` argument
</comment><comment author="alexksikes" created="2015-06-29T17:25:18Z" id="116768177">Thanks for the review. I restricted the set of accepted values for now.
</comment><comment author="javanna" created="2015-07-02T08:45:19Z" id="117960886">I had a quick look at this and left a few comments. There is an inconsistency between parser and builder here in the existing code. The builder supports an object for the value mamber but the parser only parses a string. I am not 100% sure which of the two is wrong. Feels like fuzzy query should allow for an object similar to what the term query does, but then our MappedFieldType#fuzzyQuery method takes a string as argument, so object wouldn't really work out, that would lead us to support strings only then? Thoughts @jpountz ?
</comment><comment author="javanna" created="2015-07-02T08:46:42Z" id="117961390">Also @alexksikes would be nice if you can rebase, it is hard to review at this point after #11974 got in. Thanks!
</comment><comment author="jpountz" created="2015-07-02T09:47:30Z" id="117982301">@javanna I looked at how classes that extend MappedFieldType implement fuzzyQuery, and they seem to all need to perform some parsing of the string value. So maybe we could make it an object and update all fuzzyQuery implementations to parse the object to the appropriate type (Date, Long, ...) using exactly the same logic as the termQuery method?
</comment><comment author="javanna" created="2015-07-02T09:48:36Z" id="117983194">Agreed @jpountz thanks for looking. We will do that upstream rather than in the query-refactoring branch then. Makes sense @alexksikes ?
</comment><comment author="jpountz" created="2015-07-02T09:48:56Z" id="117983559">+1 to doing it in master
</comment><comment author="alexksikes" created="2015-07-03T12:08:28Z" id="118330609">Yes agreed. Thanks @jpountz.
</comment><comment author="alexksikes" created="2015-07-10T09:28:18Z" id="120317360">@javanna @cbuescher Thanks for the review. I rebased the PR and updated it. I had to add a fix which will be removed in order to avoid NPE on unmapped fields in test.
</comment><comment author="javanna" created="2015-07-10T10:25:05Z" id="120359777">I did another round of review
</comment><comment author="alexksikes" created="2015-07-10T15:10:29Z" id="120432291">@javanna Thanks for the review. I updated the PR accordingly.
</comment><comment author="javanna" created="2015-07-10T16:03:38Z" id="120445246">left a few more minor comments
</comment><comment author="alexksikes" created="2015-07-10T19:46:41Z" id="120505449">Thanks for the review. I updated the PR.
</comment><comment author="javanna" created="2015-07-11T06:42:37Z" id="120587559">left one comment around testing, LGTM otherwise
</comment><comment author="alexksikes" created="2015-07-12T21:40:47Z" id="120763735">@javanna I updated the PR. Parsing Fuzziness is tricky as it is type dependant. On String field, fuzziness is an edit distance which can be a String with "AUTO", or an int with value 0, 1, 2 or floats in [0-1). For other types, fuzziness translates into a range query and is kept as a String if it does not fall within the given values above.
</comment><comment author="alexksikes" created="2015-07-29T15:59:33Z" id="125998717">@javanna I rebased the PR and took into account #12229.
</comment><comment author="javanna" created="2015-08-04T07:52:29Z" id="127510496">left come comments
</comment><comment author="alexksikes" created="2015-08-10T10:00:37Z" id="129391648">@javanna I addressed the comments and rebased. Thanks for the review.
</comment><comment author="javanna" created="2015-08-10T11:26:41Z" id="129412533">I did another round and replied to your comments. @cbuescher wanna have a quick look too?
</comment><comment author="cbuescher" created="2015-08-10T13:36:13Z" id="129457475">@alexksikes did a very quick round and left only a few comments, mostly small stuff and questions for clarfication
</comment><comment author="alexksikes" created="2015-08-10T14:42:49Z" id="129478123">@javanna rebased and comments addressed. Thanks for the review as always.
</comment><comment author="javanna" created="2015-08-10T14:51:47Z" id="129480233">left a small comment
</comment><comment author="javanna" created="2015-08-10T15:19:43Z" id="129491759">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/test/ESTestCase.java</file></files><comments><comment>Refactoring of FuzzyQuery</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file></files><comments><comment>Fix FuzzyQuery to properly handle Object, number, dates or String.</comment></comments></commit></commits></item><item><title>Typo in RecoveryStats.java: currentAsTarget docstring says source, should be target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11864</link><project id="" key="" /><description>See also https://github.com/elastic/elasticsearch/pull/11765
</description><key id="90786999">11864</key><summary>Typo in RecoveryStats.java: currentAsTarget docstring says source, should be target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">discordianfish</reporter><labels /><created>2015-06-24T21:46:26Z</created><updated>2015-06-25T09:24:48Z</updated><resolved>2015-06-24T21:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="discordianfish" created="2015-06-24T21:47:53Z" id="115021353">Oh nice, never mind, looks like this was merged even without CLA.
</comment><comment author="s1monw" created="2015-06-25T09:24:48Z" id="115181808">@discordianfish I fixed it after you closed the PR. trivial enough. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mappings: Remove close() from Mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11863</link><project id="" key="" /><description>There was only a single actual "use" of close, for a threadlocal
in VersionFieldMapper. However, that threadlocal is completely
unnecessary, so this change removes the threadlocal and
close() altogether.
</description><key id="90784774">11863</key><summary>Mappings: Remove close() from Mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T21:32:32Z</created><updated>2015-06-25T08:58:49Z</updated><resolved>2015-06-24T21:33:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-24T21:32:57Z" id="115018579">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file></files><comments><comment>Merge pull request #11863 from rjernst/remove/mapper-close</comment></comments></commit></commits></item><item><title>Rename cluster state uuid to updateId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11862</link><project id="" key="" /><description>The cluster state diffs introduced a concept of cluster state uuid that is uniquely identifying a particular iteration (version) of the the cluster state. This uuid is ephemeral and changes with each cluster state update, which causes confusion with other 2 uuids in the system - metadata uuid and index metadata uuid which are persistent. This commit renames cluster state uuid to updateId to reflect the difference in use and lifecycle.

Closes #11831
</description><key id="90783804">11862</key><summary>Rename cluster state uuid to updateId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Cluster</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T21:27:09Z</created><updated>2015-06-28T01:29:13Z</updated><resolved>2015-06-28T00:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-25T08:45:03Z" id="115165439">Code change LGTM. I don't like the changeId name.. sorry . instanceID is best so far imho - still not ideal..
</comment><comment author="imotov" created="2015-06-26T14:33:45Z" id="115709739">@beiske how about `revision`, `edition`, `fingerprint`, `diffId`?
</comment><comment author="bleskes" created="2015-06-27T18:32:26Z" id="116114574">I assume this means me :) … sorry but I don’t like any of the above. from everything I heard so far I would go with either just `id` or `instanceId` plus a big fat java doc..

&gt; On 26 Jun 2015, at 16:34, Igor Motov notifications@github.com wrote:
&gt; 
&gt; @beiske how about revision, edition, fingerprint, diffId?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="imotov" created="2015-06-28T00:31:35Z" id="116172388">Sorry, I meant @bleskes, of course. Yeah, sorry, I don't think `id` and `instanceId` are appropriate names. `id` is just way too generic, which makes it confusing and not much better then confusing `uuid` and as I mentioned before `instance` is used all over the place in our documentation as synonym of a node. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `default_index` analyzer name in favour of just `default`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11861</link><project id="" key="" /><description>We use `analyzer` and `search_analyzer` in mappings (having removed `index_analyzer`).  We should do the same for the default analyzers set at the index level
</description><key id="90765868">11861</key><summary>Remove `default_index` analyzer name in favour of just `default`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>breaking</label><label>low hanging fruit</label><label>v2.0.0</label></labels><created>2015-06-24T19:43:06Z</created><updated>2015-10-12T21:24:21Z</updated><resolved>2015-10-12T21:24:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>core/src/test/java/org/elasticsearch/index/analysis/AnalysisServiceTests.java</file></files><comments><comment>Deprecate `index.analysis.analyzer.default_index` in favor of `index.analysis.analyzer.default`.</comment></comments></commit></commits></item><item><title>Mark translog as upgraded in the engine even if a legacy generation exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11860</link><project id="" key="" /><description>Today we mark a translog as upgraded by adding a marker to the engine commit.
Yet, this commit was only added if there was no translog present before ie. only
if we have a fresh engine which is missing the entire point. Yet, this commit
adds a backwards index tests that ensures we can open old indices more than once
ie. mark the index as upgraded.

Closes #11858
</description><key id="90758572">11860</key><summary>Mark translog as upgraded in the engine even if a legacy generation exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T18:54:35Z</created><updated>2015-06-25T14:03:56Z</updated><resolved>2015-06-25T12:24:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-24T19:13:13Z" id="114984170">@bleskes can you take a look
</comment><comment author="bleskes" created="2015-06-25T10:18:57Z" id="115200292">LGTM. left two minor comments. I wonder if we should also change the translog ugprade logic to only look at files without the .tlog extension. Maybe even better - throw and exception if it runs into a .tlog file. Something is really odd in this case?
</comment><comment author="s1monw" created="2015-06-25T12:17:18Z" id="115230437">@bleskes I added a new commit fixing your comments
</comment><comment author="bleskes" created="2015-06-25T12:23:16Z" id="115232647">LGTM again
</comment><comment author="s1monw" created="2015-06-25T14:03:56Z" id="115268828">@bleskes I opened #11875 regarding the parsing comment you had
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Internal: make sure ParseField is always used in combination with parse flags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11859</link><project id="" key="" /><description>Removed `ParseField#match` variant that accepts the field name only, without parse flags. Such a method is harmful as it defaults to empty parse flags, meaning that no deprecation exceptions will be thrown in strict mode, which defeats the purpose of using ParseField. Unfortunately such a method was used in a lot of places were the parse flags weren't accessible (outside of query parsing), and in a lot of other places just by mistake.

Parse flags have been introduced now as part of SearchContext and mappers where needed. There are a few places (e.g. java api requests) where it is not possible to retrieve them as they depend on settings, in that case we explicitly pass in EMPTY_FLAGS, but it has to be seen as an exception.
</description><key id="90733138">11859</key><summary>Internal: make sure ParseField is always used in combination with parse flags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T17:00:17Z</created><updated>2015-07-02T12:40:13Z</updated><resolved>2015-07-02T12:40:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-24T20:01:19Z" id="114994052">LGTM

Just thinking out loud, but for future usage of this API, I'm wondering how we could make it easier to use in order to discourage lazy users to pass empty flags. Could be something like SearchContext.match(ParseField) that would automatically pass in the right flags.
</comment><comment author="javanna" created="2015-06-25T08:09:51Z" id="115157398">I agree @jpountz we could make it even better , I will look into your suggestion
</comment><comment author="javanna" created="2015-06-25T14:06:24Z" id="115269439">@jpountz I pushed another commit that should make it harder to forget parse flags, naming might need some adjustments, but I think this looks much better, thanks for your suggestion!
</comment><comment author="jpountz" created="2015-06-26T11:42:19Z" id="115653415">One concern I have is that it introduces yet another API: in addition to SearchContext and ParseField, we now have ParseFieldMatcher. Should we just have this class hidden and just a SearchContext.match(ParseField) method?
</comment><comment author="javanna" created="2015-06-26T12:08:19Z" id="115656850">I see your point @jpountz , that said we have different places where we need the parseFlags and we would need the match method, it is not just about the `SearchContext`. We could have a common interface (e.g. ParseFieldMatcher itself) and have different implementors for it (SearchContext, QueryParseContext and anywhere we use ParseField) but that would still introduce a new api which is what doesn't make you happy. Anyways we do use ParseField to define fields but the match method there is now package private which means that you have to go through ParseFieldMatcher all the time. I still think it is a good improvement cause we make sure that we don't miss parse flags anywhere. @s1monw what do you think? Maybe you have even better ideas?
</comment><comment author="javanna" created="2015-06-30T08:26:05Z" id="117053061">ping @s1monw this is quite a big PR I would love to get to some agreement and merge it in soon otherwise it gets outdated pretty quickly. Opinions?
</comment><comment author="s1monw" created="2015-07-01T11:42:51Z" id="117625606">I like this but I also like @jpountz suggestion. Yet, this is a good improvement and we should not let it get old. I am +1 on moving forward here @javanna 
</comment><comment author="s1monw" created="2015-07-02T11:13:26Z" id="118002053">@javanna I think we should push as is. The Matcher looks ok to me for now?
</comment><comment author="javanna" created="2015-07-02T11:39:56Z" id="118006026">cool will push as is then, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java</file><file>core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchType.java</file><file>core/src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/common/ParseField.java</file><file>core/src/main/java/org/elasticsearch/common/ParseFieldMatcher.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>core/src/main/java/org/elasticsearch/script/AbstractScriptParser.java</file><file>core/src/main/java/org/elasticsearch/script/Script.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptParameterParser.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/Template.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/SuggestUtils.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestParser.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptParameterParserTest.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Internal: make sure ParseField is always used in combination with parse flags</comment></comments></commit></commits></item><item><title>Translog file already exists exception thrown on old indices on second restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11858</link><project id="" key="" /><description>Start Elasticsearch 1.6 and run:

```
DELETE *

PUT good/t/1
{"foo": "bar"}
```

Then shutdown and start ES from the master branch.  On the first start, the index recovers correctly:

```
[2015-06-24 18:38:02,888][INFO ][gateway                  ] [Cobra] recovered [1] indices into cluster_state
[2015-06-24 18:38:03,315][WARN ][cluster.metadata         ] [Cobra] [good] re-syncing mappings with cluster state for types [[t]]
```

Then shutdown and restart the node.  Recovery fails (continuously) with the following exception:

```
[2015-06-24 18:38:16,260][INFO ][gateway                  ] [Fight-Man] recovered [1] indices into cluster_state
[2015-06-24 18:38:16,810][WARN ][indices.cluster          ] [Fight-Man] [[good][0]] marking and sending shard failed due to [failed recovery]
[good][0] failed to recovery from gateway
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:258)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:60)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:133)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: [good][0] failed to create engine
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:135)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1287)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1282)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:829)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:810)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:249)
        ... 5 more
Caused by: java.nio.file.FileAlreadyExistsException: data/elasticsearch/nodes/0/indices/good/0/translog/translog-1435163866460.ckp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)
        at java.nio.channels.FileChannel.open(FileChannel.java:287)
        at java.nio.channels.FileChannel.open(FileChannel.java:334)
        at org.elasticsearch.index.translog.Checkpoint.write(Checkpoint.java:87)
        at org.elasticsearch.index.translog.Translog.upgradeLegacyTranslog(Translog.java:263)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:177)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:131)
        ... 11 more
[2015-06-24 18:38:16,814][WARN ][cluster.action.shard     ] [Fight-Man] [good][0] received shard failed for [good][0], node[NQ3bYnFUTfiyF-2RjSacbg], [P], s[INITIALIZING], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-06-24T16:38:16.219Z]], indexUUID [fx8Dwd1GRvivqRVezmK3sg], reason [shard failure [failed recovery][IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileAlreadyExistsException[data/elasticsearch/nodes/0/indices/good/0/translog/translog-1435163866460.ckp]; ]]
[2015-06-24 18:38:16,815][WARN ][indices.cluster          ] [Fight-Man] [[good][1]] marking and sending shard failed due to [failed recovery]
[good][1] failed to recovery from gateway
```
</description><key id="90729007">11858</key><summary>Translog file already exists exception thrown on old indices on second restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T16:40:34Z</created><updated>2015-06-25T12:24:54Z</updated><resolved>2015-06-25T12:24:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file></files><comments><comment>Mark translog as upgraded in the engine even if a legacy generation exists</comment></comments></commit></commits></item><item><title>Conflicting mappings causes runaway shard failures on upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11857</link><project id="" key="" /><description>Start Elasticsearch 1.6.0, and run the following commands:

```
DELETE *

PUT bad
{
  "mappings": {
    "x": {
      "properties": {
        "foo": {
          "type": "string"
        }
      }
    },
    "y": {
      "properties": {
        "foo": {
          "type": "date"
        }
      }
    }
  }
}

PUT bad/x/1
{"foo": "bar"}

PUT bad/y/1
{"foo": "2015-01-01"}

POST _flush
```

Then shutdown and start ES compiled from master.  The `bad` index fails to recover because of the conflicting mappings, and the failures just keep being repeated endlessly:

```
[2015-06-24 18:22:46,644][WARN ][indices.cluster          ] [Crossbones] [bad] failed to add mapping [y], source [{"y":{"properties":{"foo":{"t
ype":"date","format":"dateOptionalTime"}}}}]
java.lang.IllegalArgumentException: Mapper for [foo] conflicts with existing mapping in other types[mapper [foo] cannot be changed from type [s
tring] to [date]]
        at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
        at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:350)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:305)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:255)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:422)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:376)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:484)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThr
eadPoolExecutor.java:209)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolE
xecutor.java:179)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2015-06-24 18:22:46,701][WARN ][indices.cluster          ] [Crossbones] [[bad][0]] marking and sending shard failed due to [failed to update m
appings]
java.lang.IllegalArgumentException: Mapper for [foo] conflicts with existing mapping in other types[mapper [foo] cannot be changed from type [s
tring] to [date]]
        at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
        at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:350)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:305)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:255)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:422)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:376)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:484)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThr
eadPoolExecutor.java:209)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolE
xecutor.java:179)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2015-06-24 18:22:46,703][WARN ][cluster.action.shard     ] [Crossbones] [bad][0] received shard failed for [bad][0], node[_DucCUykRy6JHLfcchxVVg], [P], s[INITIALIZING], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-06-24T16:22:46.149Z]], indexUUID [RET0PNdWQQWhX_fglK4FIw], reason [shard failure [failed to update mappings][IllegalArgumentException[Mapper for [foo] conflicts with existing mapping in other types[mapper [foo] cannot be changed from type [string] to [date]]]]]
[2015-06-24 18:22:46,705][WARN ][indices.cluster          ] [Crossbones] [[bad][1]] marking and sending shard failed due to [failed to update mappings]
java.lang.IllegalArgumentException: Mapper for [foo] conflicts with existing mapping in other types[mapper [foo] cannot be changed from type [string] to [date]]
        at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
        at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:350)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:305)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:255)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:422)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:376)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:484)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2015-06-24 18:22:46,706][WARN ][cluster.action.shard     ] [Crossbones] [bad][0] received shard failed for [bad][0], node[_DucCUykRy6JHLfcchxVVg], [P], s[INITIALIZING], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-06-24T16:22:46.149Z]], indexUUID [RET0PNdWQQWhX_fglK4FIw], reason [master [Crossbones][_DucCUykRy6JHLfcchxVVg][Slim-2.local][inet[/127.0.0.1:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]
```
</description><key id="90726863">11857</key><summary>Conflicting mappings causes runaway shard failures on upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Recovery</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T16:32:25Z</created><updated>2015-08-05T00:33:10Z</updated><resolved>2015-08-05T00:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="szroland" created="2015-06-28T20:04:49Z" id="116321839">A simpler way to reproduce this is to try put this mapping to ES master, and it will fail.
The ability to have a field with the same name in different types with uncompatible mapping seems to have been deliberately removed in #11812 by @rjernst.
</comment><comment author="clintongormley" created="2015-06-29T12:27:37Z" id="116636396">@szroland the change is intentional.  This issue is about what happens when you upgrade to 2.0 with conflicting mappings.  We should fail once and stop, instead of trying over and over again.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/TransportUpgradeAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java</file></files><comments><comment>Check for incompatible mappings while upgrading old indices</comment></comments></commit></commits></item><item><title>Elasticsearch start, specify the path of logging.yml in elasticsearch.yml,could not resolve placeholder 'httpfs.log.dir'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11856</link><project id="" key="" /><description>![image](https://cloud.githubusercontent.com/assets/12627595/8334902/f90c4726-1acc-11e5-931f-b76abc56cb71.png)
</description><key id="90719191">11856</key><summary>Elasticsearch start, specify the path of logging.yml in elasticsearch.yml,could not resolve placeholder 'httpfs.log.dir'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Judyccb</reporter><labels /><created>2015-06-24T15:59:12Z</created><updated>2015-06-29T12:10:38Z</updated><resolved>2015-06-24T17:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-24T17:29:21Z" id="114948604">Hi @Judyccb 

Your config file contains a placeholder that you are supposed to replace with a concrete value when you start Elasticsearch.  Elasticsearch complains because you are not setting this. This is your own custom local setup, not to do with Elasticsearch itself.
</comment><comment author="Judyccb" created="2015-06-27T03:52:25Z" id="115953325">Hi,
Do you know which placeholder I need to replace? I just want to use the default value of the logging.yml, do not want to change anything. I do not find 'httpfs.log.dir' parameter in the logging.yml，which is as follow.
![image](https://cloud.githubusercontent.com/assets/12627595/8390376/ce8518fa-1cc2-11e5-9974-8f2bbe50ba80.png)
![image](https://cloud.githubusercontent.com/assets/12627595/8390378/e98ef8fa-1cc2-11e5-8160-08c1d6ddc7e4.png)
</comment><comment author="clintongormley" created="2015-06-29T12:10:38Z" id="116630149">@Judyccb please use text, not images.  The error message tells you what to look for: `httpfs`.  Find out where that string appears in your setup.  A better place to ask questions like these is in the forum: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>defining custom _all field without disabling default one does not fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11855</link><project id="" key="" /><description>After some mistake (#11851) I found that one can create a custom `_all` field without disabling the default one, and the default `_all` field is still used at search time.

Here is the scenario:

```
DELETE myindex

POST myindex
{
  "mappings": {
    "record": {
        "properties": {
          "_all": {
            "type": "string"
          }
        }
    }
  }
}

==&gt; no error even though default _all field is still enabled. 

PUT myindex/record/1
{
  "title": "my big universe"
}

GET myindex/_search
{
  "query": {
    "match": {
      "_all": "Universe"
    }
  }
}

==&gt; returns the document. Default _all field is still used.

DELETE myindex

POST myindex
{
  "mappings": {
    "record": {
      "_all": { "enabled": false },
        "properties": {
          "_all": {
            "type": "string"
          }
        }
    }
  }
}

PUT myindex/record/1
{
  "title": "my big universe"
}

GET myindex/_search
{
  "query": {
    "match": {
      "_all": "Universe"
    }
  }
}

==&gt; no result. OK

```

But strangely the search does not work even when I write in the `_all` field.

```
PUT myindex/record/1
{
  "_all": "Universe"
}

GET myindex/_search
{
  "query": {
    "match": {
      "_all": "Universe"
    }
  }
}

==&gt; no document even though the _all field has been disabled before. KO
```

I am using the elasticsearch 1.6 container from docker.io

This is a minor issue as most people won't name their custom fields `_all`. Throwing an error in the mean time could be useful for people misplacing `_all` in their mapping. Or maybe forbidding completely the definition of a field named `_all`.
</description><key id="90717044">11855</key><summary>defining custom _all field without disabling default one does not fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nharraud</reporter><labels /><created>2015-06-24T15:50:43Z</created><updated>2015-06-24T17:26:37Z</updated><resolved>2015-06-24T17:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-24T17:26:36Z" id="114947600">Closing in favour of #10456
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Detect matched locations of searches with geo_distance filter using "explanation" string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11854</link><project id="" key="" /><description>EDIT: the behavior is consistent, still, my question on how to solve my scenario still holds :D

I have been experiencing a tricky elasticsearch issue which looks like bug, although it could also mean I am missing something, and I would be happy to receive some feedback.

I have the following mapping (cropped some unnecessary stuff):

``` json
{
   "locationtest":{
      "mappings":{
         "searchdocument":{
            "dynamic":"strict",
            "_all":{
               "analyzer":"western_europe"
            },
            "properties":{
               "birthdate":{ },
               "birthplace":{
                  "properties":{
                     "coordinates":{
                        "type":"geo_point",
                        "fielddata":{
                           "format":"compressed",
                           "precision":"5m"
                        },
                        "geohash":true,
                        "geohash_prefix":true,
                        "geohash_precision":5
                     },
                     "value":{
                        "type":"string",
                        "norms":{
                           "enabled":false
                        },
                        "analyzer":"western_europe",
                        "fields":{
                           "raw":{
                              "type":"string",
                              "index":"not_analyzed"
                           }
                        },
                        "include_in_all":true,
                        "position_offset_gap":10
                     }
                  }
               },
               "name":{
                  "type":"string",
                  "norms":{
                     "enabled":false
                  },
                  "analyzer":"western_europe",
                  "fields":{
                     "raw":{
                        "type":"string",
                        "index":"not_analyzed"
                     }
                  },
                  "include_in_all":true
               }
            }
         }
      }
   }
}
```

In my application I need to to retrieve what parts of the queries gets matched, and the only way I found so far to do so is to retrieve the "explanation" string and then try to parse it (btw, is there a better way?).
So I perform this search:

``` json
{
  "size" : 40,
  "query" : {
    "filtered" : {
      "query" : {
        "bool" : {
          "must" : {
            "filtered" : {
              "query" : {
                "function_score" : {
                  "functions" : [ {
                    "gauss" : {
                      "birthplace.coordinates" : {
                        "origin" : [ 4.85667, 52.3067 ],
                        "scale" : "5km"
                      }
                    }
                  } ],
                  "boost" : 3.0
                }
              },
              "filter" : {
                "geo_distance" : {
                  "birthplace.coordinates" : [ 4.85667, 52.3067 ],
                  "distance" : "5.0km",
                  "distance_type" : "arc"
                }
              }
            }
          }
        }
      }
    }
  },
  "explain" : true,
  "fields" : [ "name", "birthplace.value", "birthplace.coordinates" ],
  "highlight" : {
    "highlight_query" : {
      "match_all" : {
        "boost" : 0.0
      }
    },
    "fields" : {
      "sections" : {
        "fragment_size" : 80,
        "number_of_fragments" : 4
      }
    }
  }
}
```

I basically filter around a point ("52.3667, 49", Amstelveen) and then perform a "function score" query because I still would like to a have ranking. Originally, I had another filter performing geo_hashing (and this is why I have "geohash" set to true in the mapping) as suggested, but it does not seem to matter.

The results are:

``` json
{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 3.0,
    "hits" : [ {
      "_shard" : 0,
      "_node" : "1Hc5dBcUSBi0SFLIz0nX3g",
      "_index" : "locationtest",
      "_type" : "searchdocument",
      "_id" : "6",
      "_score" : 3.0,
      "fields" : {
        "birthplace.coordinates" : [ "52.3067, 4.85667" ],
        "birthplace.value" : [ "Amstelveen" ],
        "name" : [ "Dwayne Catral" ]
      },
      "_explanation" : {
        "value" : 3.0,
        "description" : "function score, product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "ConstantScore(*:*), product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "boost"
          }, {
            "value" : 1.0,
            "description" : "queryNorm"
          } ]
        }, {
          "value" : 1.0,
          "description" : "Math.min of",
          "details" : [ {
            "value" : 1.0,
            "description" : "Function for field birthplace.coordinates:",
            "details" : [ {
              "value" : 1.0,
              "description" : "exp(-0.5*pow(MIN of: [Math.max(arcDistance([52.30670166015625, 4.856658935546875](=doc value),[52.3067, 4.85667](=origin)) - 0.0(=offset), 0)],2.0)/1.803368801111204E7)"
            } ]
          }, {
            "value" : 3.4028235E38,
            "description" : "maxBoost"
          } ]
        }, {
          "value" : 3.0,
          "description" : "queryBoost"
        } ]
      }
    }, {
      "_shard" : 0,
      "_node" : "1Hc5dBcUSBi0SFLIz0nX3g",
      "_index" : "locationtest",
      "_type" : "searchdocument",
      "_id" : "3",
      "_score" : 3.0,
      "fields" : {
        "birthplace.coordinates" : [ "52.3067, 4.85667" ],
        "birthplace.value" : [ "Eindhoven", "Amstelveen" ],
        "name" : [ "INGA Molloy" ]
      },
      "_explanation" : {
        "value" : 3.0,
        "description" : "function score, product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "ConstantScore(*:*), product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "boost"
          }, {
            "value" : 1.0,
            "description" : "queryNorm"
          } ]
        }, {
          "value" : 1.0,
          "description" : "Math.min of",
          "details" : [ {
            "value" : 1.0,
            "description" : "Function for field birthplace.coordinates:",
            "details" : [ {
              "value" : 1.0,
              "description" : "exp(-0.5*pow(MIN of: [Math.max(arcDistance([52.30670166015625, 4.856658935546875](=doc value),[52.3067, 4.85667](=origin)) - 0.0(=offset), 0)],2.0)/1.803368801111204E7)"
            } ]
          }, {
            "value" : 3.4028235E38,
            "description" : "maxBoost"
          } ]
        }, {
          "value" : 3.0,
          "description" : "queryBoost"
        } ]
      }
    }, {
      "_shard" : 0,
      "_node" : "1Hc5dBcUSBi0SFLIz0nX3g",
      "_index" : "locationtest",
      "_type" : "searchdocument",
      "_id" : "5",
      "_score" : 3.0,
      "fields" : {
        "birthplace.coordinates" : [ "52.3667, 4.9", "52.3067, 4.85667" ],
        "birthplace.value" : [ "Amsterdam", "Amstelveen" ],
        "name" : [ "GARGI AKINMULERO" ]
      },
      "_explanation" : {
        "value" : 3.0,
        "description" : "function score, product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "ConstantScore(*:*), product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "boost"
          }, {
            "value" : 1.0,
            "description" : "queryNorm"
          } ]
        }, {
          "value" : 1.0,
          "description" : "Math.min of",
          "details" : [ {
            "value" : 1.0,
            "description" : "Function for field birthplace.coordinates:",
            "details" : [ {
              "value" : 1.0,
              "description" : "exp(-0.5*pow(MIN of: [Math.max(arcDistance([52.30670166015625, 4.856658935546875](=doc value),[52.3067, 4.85667](=origin)) - 0.0(=offset), 0), Math.max(arcDistance([52.36669921875, 4.899993896484375](=doc value),[52.3067, 4.85667](=origin)) - 0.0(=offset), 0)],2.0)/1.803368801111204E7)"
            } ]
          }, {
            "value" : 3.4028235E38,
            "description" : "maxBoost"
          } ]
        }, {
          "value" : 3.0,
          "description" : "queryBoost"
        } ]
      }
    } ]
  }
}
```

What I notice is that the first document (Dwayne) only has "Amstelveen" as location, and the explanation says something about calculating arcDistance between my "origin" and the coordinates of the document, and that's ok. The second document (INGA) has "Eindhoven" and "Amstelveen" as locations, I see in the explained part that only one "arcDistance" shows up, again fine. The third result (GARGI) instead has "Amsterdam" and "Amstelveen" as locations. Here, both of them appear in the explanation as "arcDistance" calculations, but why didn't this happen with INGA too?

Moreover, if I try to search documents in a radius of 5km from Amsterdam:

``` json
{
  "size" : 40,
  "query" : {
    "filtered" : {
      "query" : {
        "bool" : {
          "must" : {
            "filtered" : {
              "query" : {
                "function_score" : {
                  "functions" : [ {
                    "gauss" : {
                      "birthplace.coordinates" : {
                        "origin" : [ 4.9, 52.3667 ],
                        "scale" : "5km"
                      }
                    }
                  } ],
                  "boost" : 3.0
                }
              },
              "filter" : {
                "geo_distance" : {
                  "birthplace.coordinates" : [ 4.9, 52.3667 ],
                  "distance" : "5.0km",
                  "distance_type" : "arc"
                }
              }
            }
          }
        }
      }
    }
  },
  "explain" : true,
  "fields" : [ "name", "birthplace.value", "birthplace.coordinates" ],
  "highlight" : {
    "highlight_query" : {
      "match_all" : {
        "boost" : 0.0
      }
    },
    "fields" : {
      "sections" : {
        "fragment_size" : 80,
        "number_of_fragments" : 4
      }
    }
  }
}
```

I get only GARKI

``` json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 3.0,
    "hits" : [ {
      "_shard" : 0,
      "_node" : "1Hc5dBcUSBi0SFLIz0nX3g",
      "_index" : "locationtest",
      "_type" : "searchdocument",
      "_id" : "5",
      "_score" : 3.0,
      "fields" : {
        "birthplace.coordinates" : [ "52.3667, 4.9", "52.3067, 4.85667" ],
        "birthplace.value" : [ "Amsterdam", "Amstelveen" ],
        "name" : [ "GARGI AKINMULERO" ]
      },
      "_explanation" : {
        "value" : 3.0,
        "description" : "function score, product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "ConstantScore(*:*), product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "boost"
          }, {
            "value" : 1.0,
            "description" : "queryNorm"
          } ]
        }, {
          "value" : 1.0,
          "description" : "Math.min of",
          "details" : [ {
            "value" : 1.0,
            "description" : "Function for field birthplace.coordinates:",
            "details" : [ {
              "value" : 1.0,
              "description" : "exp(-0.5*pow(MIN of: [Math.max(arcDistance([52.30670166015625, 4.856658935546875](=doc value),[52.3667, 4.9](=origin)) - 0.0(=offset), 0), Math.max(arcDistance([52.36669921875, 4.899993896484375](=doc value),[52.3667, 4.9](=origin)) - 0.0(=offset), 0)],2.0)/1.803368801111204E7)"
            } ]
          }, {
            "value" : 3.4028235E38,
            "description" : "maxBoost"
          } ]
        }, {
          "value" : 3.0,
          "description" : "queryBoost"
        } ]
      }
    } ]
  }
}
```

Which means that elasticsearch correctly filters out people with "Amstelveen" as location (according to geocoordinates, 7 km away), otherwise I would have got back the other 2 results.

So my question is, why this incoherence in the "explain" string? And also, is there another way to solve my scenario (find out which geo-coordinates were matched internally)?
</description><key id="90709422">11854</key><summary>Detect matched locations of searches with geo_distance filter using "explanation" string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">winterismute</reporter><labels /><created>2015-06-24T15:26:57Z</created><updated>2015-06-24T17:25:48Z</updated><resolved>2015-06-24T17:25:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="winterismute" created="2015-06-24T15:49:20Z" id="114921711">I am now noticing that only one of my results has 2 geo points, INGA has in fact only one in "birthplace.coordinates". So the behavior of elasticsearch is probably consistent, and due to the "function_score" query.
This being said, is there a way to achieve what I would like to have (checking what gets matched)?
</comment><comment author="clintongormley" created="2015-06-24T17:25:48Z" id="114947401">Hi @winterismute 

As you've figured out, the function_score examines all of the values in the field, not just the ones matching the filter.  This is to be expected.  To work around this you could look at using nested docs.

Regarding figuring out which clauses match, look at named queries: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html#_named_queries
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlight when query contains fields and text</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11853</link><project id="" key="" /><description>Hello,

I'm using elasticsearch 1.6.0.
Assume having the following document indexed in elasticsearch:

``` json
{
    "id" : 2,
    "user" : "jack",
    "message" : "I'm jack",
    "comment" : "my first tweet"
}
```

When query:

``` json
{
  "from" : 0,
  "size" : 5,
  "query" : {
    "query_string" : {
      "query" : "user:jack first",
      "use_dis_max" : true
    }
  },
  "highlight" : {
    "pre_tags" : [ "&lt;span class=\"mark\"&gt;" ],
    "post_tags" : [ "&lt;/span&gt;" ],
    "order" : "score",
    "encoder" : "html",
    "require_field_match" : false,
    "fields" : {    
      "*" : {}
    }
  }
}
```

I'm using "query string query".
As you can see, my query combines field match (user:jack) and text (first).
As a result, elasticsearch will highlight the word jack in both user field and message.
But I query for user:jack, mean I want to highlight only user name jack and not jack in other fields.
I can't set require_field_match to true, because than elasticsearch will not highlight 'first'.

What should I do?

Thanks in advance,
Effi
</description><key id="90705483">11853</key><summary>Highlight when query contains fields and text</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">effoeffi</reporter><labels /><created>2015-06-24T15:16:08Z</created><updated>2015-06-24T17:22:21Z</updated><resolved>2015-06-24T17:22:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-24T17:22:20Z" id="114946663">Hi @effoeffi 

Please ask questions like these in the forum instead of on the issues list: http://discuss.elastic.co/
Also see this very similar issue: https://github.com/elastic/elasticsearch/issues/11667

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>clarify allocation awareness docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11852</link><project id="" key="" /><description>as pointed out  by https://github.com/elastic/elasticsearch/issues/1352#issuecomment-114152281
</description><key id="90703292">11852</key><summary>clarify allocation awareness docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label></labels><created>2015-06-24T15:07:21Z</created><updated>2015-06-29T09:35:59Z</updated><resolved>2015-06-25T15:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-24T16:00:13Z" id="114925115">@bleskes note that the docs have changed in master: https://www.elastic.co/guide/en/elasticsearch/reference/master/allocation-awareness.html
</comment><comment author="clintongormley" created="2015-06-25T09:28:41Z" id="115183349">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_all field's analyzer is not used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11851</link><project id="" key="" /><description>I just tried setting an analyzer for the `_all` field but it does not work. The field is still analyzed with the standard analyzer.

Here is the scenario:

```
DELETE myindex

POST myindex
{
  "settings": {
    "analysis": {
        "filter": {
          "title-synonym": {
            "type" : "synonym",
            "synonyms" : [
                "universe, cosmos"
            ]
          }
        },
        "analyzer": {
          "all-field-analyzer": {
            "tokenizer" : "whitespace",
            "filter" : ["title-synonym"]
          }
        }
    }
  },
  "mappings": {
    "record": {
        "properties": {
          "_all": {
            "type": "string",
            "analyzer": "all-field-analyzer"
          },
          "title": {
            "type": "string",
            "analyzer": "all-field-analyzer"
          }
        }
    }
  }
}

PUT myindex/record/1
{
  "title": "my big universe"
}

GET myindex/_search
{
  "query": {
    "match": {
      "title": "cosmos"
    }
  }
}

=&gt; OK returns doc 1 thanks to synonyms.

GET myindex/_search
{
  "query": {
    "match": {
      "title": "Universe"
    }
  }
}

=&gt; OK returns no doc because of the whitespace tokenizer

GET myindex/_search
{
  "query": {
    "match": {
      "_all": "cosmos"
    }
  }
}

=&gt; KO returns no doc. The synonyms are not used.

GET myindex/_search
{
  "query": {
    "match": {
      "_all": "Universe"
    }
  }
}

=&gt; KO returns doc 1 even though it should not match because of the character case.
```

Maybe I am doing something wrong. The documentation says that an analyzer can be specified for the `_all` field: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html

```
The _all fields allows for store, term_vector and analyzer (with specific index_analyzer and search_analyzer) to be set.
```

I am using the elasticsearch 1.6 container from docker.io
</description><key id="90697626">11851</key><summary>_all field's analyzer is not used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nharraud</reporter><labels /><created>2015-06-24T14:44:46Z</created><updated>2015-06-24T17:12:48Z</updated><resolved>2015-06-24T14:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-24T14:53:18Z" id="114896941">`whitespace` tokenizer doesn't lowercase so I think you also want a `lowercase` filter before the synonym filter to fix that... Please use the mailing list instead for questions like this.
</comment><comment author="johtani" created="2015-06-24T14:57:35Z" id="114898127">Hi @nharraud ,

please ask questions like these on the mailing list instead.

https://discuss.elastic.co

And you should put `_all` section before `properties` and after `record` in mappings section. Your mapping put wrong place...
</comment><comment author="nharraud" created="2015-06-24T15:13:27Z" id="114902630">Thank you @johtani. I know about the mailing but I just didn't see my mistake and after trying for some time assumed it was a bug. My bad.
@s1monw  The idea was not to lowercase with `whitespace` tokenizer but to create a scenario reproducing the invalid "bug". But thank you.

However it seems to me that this shows another issue. Maybe creating a custom `_all` field without explicitly disabling the default `_all` field should return an error? Especially if the new field cannot be accessed.
Once I disable the default `_all` field with `"_all": { "enabled": false }`. I have a normal behavior, i.e. no result for the two last queries.
</comment><comment author="clintongormley" created="2015-06-24T17:12:48Z" id="114944631">Relates to https://github.com/elastic/elasticsearch/issues/10456
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow IBM J9 2.8+ in version check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11850</link><project id="" key="" /><description>Elasticsearch is not impacted by the lucene fieldcache miscompilation, etc, because it does not use UninvertingReader.

Additionally ES test suite passes.

I will leave my lucene beasting job running, but I think we should enable it. The guys at IBM are fast to respond to any issues so if we find something, we will deal with it.
</description><key id="90683774">11850</key><summary>Allow IBM J9 2.8+ in version check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T14:00:45Z</created><updated>2015-06-24T15:50:32Z</updated><resolved>2015-06-24T15:36:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-24T14:01:30Z" id="114877415">LGTM
</comment><comment author="mikemccand" created="2015-06-24T15:08:32Z" id="114901419">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java</file></files><comments><comment>Merge pull request #11850 from rmuir/enable_j9</comment></comments></commit></commits></item><item><title>Render strucutred exception in multi search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11849</link><project id="" key="" /><description>MultiMatch still only returns the exception message but should return the
actual exception and render it in a structured fashion...

@rashidkpc here you go :)
</description><key id="90683349">11849</key><summary>Render strucutred exception in multi search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T13:58:33Z</created><updated>2015-06-24T14:24:27Z</updated><resolved>2015-06-24T14:22:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2015-06-24T14:08:52Z" id="114879334">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify ShardRouting abstraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11848</link><project id="" key="" /><description>This commit folds ShardRouting, ImmutableShardRouting and MutableShardRouting
into ShardRouting. All mutators are package private anyway today so it's just
unnecessary abstraction.
ShardRoutings are now frozen once they are added to the IndexRoutingTable
to prevent modifications outside of the allocation code.

Take 2....
</description><key id="90641974">11848</key><summary>Simplify ShardRouting abstraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2015-06-24T10:54:15Z</created><updated>2015-06-24T12:44:05Z</updated><resolved>2015-06-24T12:44:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-24T10:54:51Z" id="114827181">@kimchy here you go
</comment><comment author="kimchy" created="2015-06-24T11:11:21Z" id="114830658">LGTM, thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consolidate shard level abstractions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11847</link><project id="" key="" /><description>This commit consolidates several abstractions on the shard level in
ordinary classes not managed by the shard level guice injector.

Several classes have been collapsed into IndexShard and IndexShardGatewayService
was cleaned up to be more lightweight and self-contained. It has also been moved into
the index.shard package and it's operation is renamed from recovery from "gateway" to recovery
from "store" or "shard_store".
</description><key id="90628317">11847</key><summary>Consolidate shard level abstractions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T09:52:01Z</created><updated>2015-07-05T18:19:39Z</updated><resolved>2015-06-24T13:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-24T10:23:00Z" id="114816364">You need to update `docs/reference/indices/recovery.asciidoc` as well which contains a reference to `GATEWAY` instead of `STORE`. Otherwise LGTM, it's exciting to get this stuff out of the hands of Guice.
</comment><comment author="s1monw" created="2015-06-24T10:49:20Z" id="114823612">pushed some changed - thanks @jpountz 
</comment><comment author="bleskes" created="2015-06-24T12:54:53Z" id="114859807">I did as sweep as well. Left minor comments. 
</comment><comment author="s1monw" created="2015-06-24T13:00:43Z" id="114860807">pushed a new commit @bleskes 
</comment><comment author="bleskes" created="2015-06-24T13:02:07Z" id="114861077">awesome. LGTM
</comment><comment author="jpountz" created="2015-06-24T13:15:13Z" id="114863882">LGTM too
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/bitset/ShardBitsetFilterCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/filter/ShardFilterCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/ShardQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/ShardFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/ShardFieldDataModule.java</file><file>core/src/main/java/org/elasticsearch/index/gateway/IgnoreGatewayRecoveryException.java</file><file>core/src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>core/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayException.java</file><file>core/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayRecoveryException.java</file><file>core/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>core/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewaySnapshotNotAllowedException.java</file><file>core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java</file><file>core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java</file><file>core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/CommitPoint.java</file><file>core/src/main/java/org/elasticsearch/index/shard/CommitPoints.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShardModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShardRecoveryException.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/SnapshotStatus.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecoveryService.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotAndRestoreService.java</file><file>core/src/main/java/org/elasticsearch/index/warmer/ShardIndexWarmerService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/CommitPointsTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardModuleTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file></files><comments><comment>Consolidate shard level abstractions</comment></comments></commit></commits></item><item><title>Remove XContentParser.map[Ordered]AndClose().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11846</link><project id="" key="" /><description>It is a bit weird to have an API to read a map then close the parser.
</description><key id="90619406">11846</key><summary>Remove XContentParser.map[Ordered]AndClose().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T09:16:15Z</created><updated>2015-06-24T15:26:51Z</updated><resolved>2015-06-24T09:33:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-24T09:28:05Z" id="114803465">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/completion/CompletionFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/test/rest/json/JsonPath.java</file><file>core/src/test/java/org/elasticsearch/test/rest/test/DoSectionParserTests.java</file><file>core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolTests.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file></files><comments><comment>Merge pull request #11846 from jpountz/fix/parser_map_and_close</comment></comments></commit></commits></item><item><title>Java api: remove duplicated Operator enums from different queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11845</link><project id="" key="" /><description>This removes all enums called "Operator" that have two entries: OR/AND except for oe.index.query.Operator 

Relates to #10217

This PR goes against the feature/query-refactoring feature branch.
</description><key id="90617017">11845</key><summary>Java api: remove duplicated Operator enums from different queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Java API</label><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-06-24T09:07:26Z</created><updated>2015-08-28T09:36:20Z</updated><resolved>2015-06-25T09:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-24T09:17:08Z" id="114798929">good change, I think we can replace even more switches in all of the parsers corresponding to the builders that were touched here and use `Operator.fromString` instead. also when converting to BooleanClause.Occur (either toQuery or parse) we can use the proper method from the `Operator` class rather than a duplicated switch.
</comment><comment author="javanna" created="2015-06-24T09:19:38Z" id="114800897">one more thing, this is a breaking change for the java api, can you add an entry to the `migrate_query_refactoring` file and mark as breaking please?
</comment><comment author="MaineC" created="2015-06-24T09:59:31Z" id="114811469">Thanks for your comments. Makes sense eliminating the code duplication in parsers too.

About the migrate_query_refactoring file - where do I find that?
</comment><comment author="MaineC" created="2015-06-24T19:29:25Z" id="114987091">To answer my last question (and keep the answer here for reference next time I'm searching for it): 

find . -name 'migrate*' found it in docs/reference/migration - note the quotes to stop the shell from expanding the pattern.
</comment><comment author="javanna" created="2015-06-25T08:24:07Z" id="115160683">LGTM
</comment><comment author="javanna" created="2015-07-01T13:24:30Z" id="117673085">I just marked this as breaking, as it is for the java api given that we removed the enums called "Operator" from MatchQueryBuilder, QueryStringQueryBuilder,
SimpleQueryStringBuilder, and CommonTermsQueryBuilder in favour of using the new enum
defined in org.elasticsearch.index.query.Operator.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/Operator.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/percolator/MultiPercolatorTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringTests.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file></files><comments><comment>Merge pull request #11845 from MaineC/feature/simple-query-string-operator</comment></comments></commit></commits></item><item><title>Make file based index templates visible via API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11844</link><project id="" key="" /><description>Currently, if you choose to set up an index template via placing a json file in the `path.conf/templates`. It will be applied when Elasticsearch starts, however it will not be visible in any interface making it very difficult to spot errors or update it easily.
</description><key id="90584275">11844</key><summary>Make file based index templates visible via API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">orweinberger</reporter><labels><label>:Index Templates</label><label>discuss</label></labels><created>2015-06-24T06:44:12Z</created><updated>2015-10-29T12:25:50Z</updated><resolved>2015-10-29T12:25:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T12:25:50Z" id="152163017">Hi @orweinberger 

As of 2.0, index templates can now only be specified via the API, not in config files.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace references to ImmutableSettings with Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11843</link><project id="" key="" /><description>ImmutableSettings was merged into Settings in commit 4873070.

Change-Id: I06bd0150381d131593920c2328c46beacf49661f
</description><key id="90574406">11843</key><summary>Replace references to ImmutableSettings with Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dpursehouse</reporter><labels><label>:Settings</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-24T05:55:47Z</created><updated>2015-06-24T10:00:31Z</updated><resolved>2015-06-24T06:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-24T06:33:21Z" id="114746638">Thanks @dpursehouse !
</comment><comment author="mikemccand" created="2015-06-24T10:00:31Z" id="114811623">Woops, thanks @dpursehouse!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11843 from dpursehouse/immutable-settings-update</comment></comments></commit></commits></item><item><title>Set tarLongFileMode to posix in maven-assembly-plugin config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11842</link><project id="" key="" /><description>Fixes #11824

Change-Id: I61bdee6c96cdfa8fbf26e3d654eaa78d6a303526
</description><key id="90540609">11842</key><summary>Set tarLongFileMode to posix in maven-assembly-plugin config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dpursehouse</reporter><labels><label>bug</label><label>build</label><label>review</label></labels><created>2015-06-24T02:04:15Z</created><updated>2015-11-11T05:49:21Z</updated><resolved>2015-09-03T14:29:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-24T03:47:01Z" id="114719414">w00t! Thank you !

Wondering if we should set that in parent pom instead ?

https://github.com/elastic/elasticsearch/blob/master/pom.xml#L482
</comment><comment author="dpursehouse" created="2015-06-24T04:06:56Z" id="114722290">I put it in the core's pom because that's where the existing configuration is, and the core build was the only one failing due to this anyway.
</comment><comment author="dpursehouse" created="2015-06-25T02:52:36Z" id="115082647">@dadoonet do you want me to move it to the parent pom, or is it OK to merge like this?
</comment><comment author="dadoonet" created="2015-06-25T07:03:32Z" id="115131859">I don't have a strong opinion here. I was just thinking that it might happen also on other modules at some point.

I'd also like to understand if it's a temporary fix (do we expect a new version of the assembly plugin?) so it worths a comment in the pom like "remove when version X will be released"...

I'll look at this hopefully soonish. And probably merge it as is...
</comment><comment author="dpursehouse" created="2015-07-02T12:40:45Z" id="118019555">Any progress on this?
</comment><comment author="dadoonet" created="2015-07-03T07:11:56Z" id="118257981">@dpursehouse I was looking at it this morning and get through the maven issue https://issues.apache.org/jira/browse/MASSEMBLY-479 and came to http://www.delorie.com/gnu/docs/tar/tar_114.html

&gt; So, a future tar will have a --posix flag forcing the usage of truly POSIX headers, and so, producing archives previous GNU tar will not be able to read. So, once pretest will announce that feature, it would be particularly useful that users test how exchangeable will be archives between GNU tar with --posix and other POSIX tar.

If I understand this correctly, it means that when filenames will have more than 100 characters, they just won't be readable by older `tar` versions. Is that how you read this as well?

So there is no real consequence of having that option set for artifacts we produce but those might not be readable on older systems?

I wonder if you have this issue because you are running `mvn` in a subdir like `/home/thisisaverylongdirname/thisiswhereiputgitrepos/andiwanttoputelasticsearchhere/elasticsearch`? :)

Where do you run maven from?
</comment><comment author="dpursehouse" created="2015-07-06T00:59:13Z" id="118683678">&gt; If I understand this correctly, it means that when filenames will have more than 100 characters, they just won't be readable by older tar versions. Is that how you read this as well?

Yes, that's how it seems to me.

&gt; Where do you run maven from?

`/home/corporate/username/git/elasticsearch`
</comment><comment author="dadoonet" created="2015-07-06T10:14:34Z" id="118797473">I think this should be a common option to all modules. Could you update your PR and move this setting to the parent pom?

@rmuir WDYT of this? Any side effect I can't see? I wonder why it's not set by default in maven assembly plugin... Probably because they prefer to have compatibility out of the box for older tar versions???
</comment><comment author="dpursehouse" created="2015-07-06T10:21:11Z" id="118800634">@dadoonet I've rebased it on the latest head of master, and moved the setting into the parent pom as requested.
</comment><comment author="dpursehouse" created="2015-07-14T07:48:46Z" id="121155884">@dadoonet any chance of getting this merged?
</comment><comment author="rmuir" created="2015-07-14T10:36:40Z" id="121196561">&gt; @rmuir WDYT of this? Any side effect I can't see? I wonder why it's not set by default in maven assembly plugin... Probably because they prefer to have compatibility out of the box for older tar versions???

sorry i havent had time to dig into this. Why is it really needed? Why isn't it the default.

All tests pass for me without it.
</comment><comment author="dpursehouse" created="2015-07-16T12:45:58Z" id="121946852">@rmuir 

&gt; Why is it really needed?

I need it because otherwise the mvn build fails for me.  There may be some underlying root cause, but I haven't been able to find it.
</comment><comment author="dpursehouse" created="2015-07-27T06:50:32Z" id="125105684">Ping.  I'm still unable to build without this patch.

&gt; All tests pass for me without it.

The test pass for me too (without the patch).  It's `mvn package` that fails on the `maven-assembly-plugin` part of the core build.
</comment><comment author="dpursehouse" created="2015-09-03T14:29:48Z" id="137466378">This is no longer a blocker for me.  Since 2.0.0-beta1 was released I can get it off Maven Central instead of building it myself.

It doesn't look like it's going to get merged, so I will just close this PR.
</comment><comment author="dpursehouse" created="2015-11-11T05:49:21Z" id="155673452">FWIW I was able to build on my Macbook Pro without this fix.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Latitude and longitude are returned when requesting geohash sub field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11841</link><project id="" key="" /><description>### The problem

I'm trying to access the raw geohash per the [documentation here](https://www.elastic.co/guide/en/elasticsearch/reference/1.6/mapping-geo-point-type.html), but it's not returning what I expect. Please forgive me if this is user error.
### Background

What I _really_ want to do is count of the unique geohashes at a given level of precision, but there's doesn't seem to be a way to do that easily so here I am.
### Basics

``` json
{
  "status" : 200,
  "name" : "Soulfire",
  "cluster_name" : "elasticsearch1",
  "version" : {
    "number" : "1.6.0",
    "build_hash" : "cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0",
    "build_timestamp" : "2015-06-09T13:36:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
### Mapping

``` json

{
  "koop_geohash" : {
    "mappings" : {
      "features" : {
        "properties" : {
          "centroid" : {
            "type" : "geo_point",
            "geohash" : true,
            "geohash_prefix" : true,
            "geohash_precision" : 8
          }
        }
      }
    }
  }
}
```
### Query

``` json
{
    "query":{
        "match_all": {}
    },
    "size": 1,
    "fields": ["centroid.geohash"]
}
```
### Response

``` json
{
  "took": 39,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 89,
    "max_score": 1,
    "hits": [
      {
        "_index": "koop_geohash",
        "_type": "features",
        "_id": "AU4jL53QJUVxb01NyiaD",
        "_score": 1,
        "fields": {
          "centroid.geohash": [
            -80.23283572892831,
            38.81329972121544
          ]
        }
      }
    ]
  }
}
```
</description><key id="90539247">11841</key><summary>Latitude and longitude are returned when requesting geohash sub field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dmfenton</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-06-24T01:54:53Z</created><updated>2015-06-24T14:12:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-24T14:05:10Z" id="114878199">Hi @dmfenton 

The geohash would need to be stored in order to be accessible, but there seems to be no way to set it to stored, eg with this mapping:

```
    "centroid": {
      "type": "geo_point",
      "geohash": true,
      "geohash_prefix": true,
      "geohash_precision": 8,
      "store": true
    }
```

this request:

```
GET _mapping/field/centroid*?include_defaults
```

returns

```
       "centroid": {
           "full_name": "centroid",
           "mapping": {
              "centroid": {
                 "type": "geo_point",
                 "boost": 1,
                 "index": "not_analyzed",
                 "store": true,
                 "doc_values": false,
                 "term_vector": "no",
                 "norms": {
                    "enabled": false
                 },
                 "index_options": "docs",
                 "analyzer": "_keyword",
                 "similarity": "default",
                 "fielddata": {},
                 "path": "full",
                 "lat_lon": false,
                 "geohash": true,
                 "geohash_prefix": true,
                 "geohash_precision": 8,
                 "validate": true,
                 "normalize": true
              }
           }
        }
```

@nknize would exposing the ability to set the geohash as stored make sense?
</comment><comment author="dmfenton" created="2015-06-24T14:11:57Z" id="114880802">Thanks @clintongormley. Just want to draw attention to my real use case.

As a user, I want to know how many unique geohashes will be returned for a given filtered aggregation, without transporting all the data over the wire.

Even better,

As a user, I want to request a geohash aggregation with a limit of x unique geohashes and have the response automatically determine the precision to send.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>work around</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11840</link><project id="" key="" /><description /><key id="90524367">11840</key><summary>work around</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels /><created>2015-06-24T00:16:36Z</created><updated>2015-06-24T00:19:55Z</updated><resolved>2015-06-24T00:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Aborting snapshot might not abort snapshot of shards in very early stages in the snapshot process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11839</link><project id="" key="" /><description>If the abort command is issued very early in shard snapshot lifecycle, it might not cancel this shard. This commit backports the change discovered and fixed as part of #11756.
</description><key id="90502445">11839</key><summary>Aborting snapshot might not abort snapshot of shards in very early stages in the snapshot process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-06-23T22:09:05Z</created><updated>2015-06-26T18:11:19Z</updated><resolved>2015-06-26T18:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-24T14:29:54Z" id="114887679">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed grammar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11838</link><project id="" key="" /><description>Added 'to' on line 79 to make consistent and fix grammar.
</description><key id="90489244">11838</key><summary>Fixed grammar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnpickett</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-23T20:58:52Z</created><updated>2015-07-08T16:21:28Z</updated><resolved>2015-07-08T16:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-24T13:21:11Z" id="114865580">Hi @johnpickett 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="johnpickett" created="2015-06-24T13:42:03Z" id="114871672">Signed.  Thanks.
</comment><comment author="jpountz" created="2015-07-08T16:21:28Z" id="119647377">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Workaround deadlock on Codec initialisation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11837</link><project id="" key="" /><description>Not need for Lucene 5.2.1 anymore, but Lucene 4.9.4 needs it, see LUCENE-6482 for more info)

Closes #11170
</description><key id="90466858">11837</key><summary>Workaround deadlock on Codec initialisation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>:Core</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-06-23T19:06:19Z</created><updated>2015-09-30T16:59:21Z</updated><resolved>2015-06-23T19:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-23T19:09:25Z" id="114613364">LGTM, thanks @uschindler I'll push this to 1.x.
</comment><comment author="mikemccand" created="2015-06-23T19:11:21Z" id="114613765">Note there's nothing to do for ES 2.0 here because we use Lucene 5.2.1 which already has the fix for https://issues.apache.org/jira/browse/LUCENE-6482
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix endless looping if starting fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11836</link><project id="" key="" /><description>log_end_msg does not break the loop or exit the script results in endlessly printing 'failed' in red to the screen in case of a failed start.
</description><key id="90463334">11836</key><summary>Fix endless looping if starting fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wolfgangkarall</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T18:52:37Z</created><updated>2015-07-01T10:09:45Z</updated><resolved>2015-06-24T13:42:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T19:43:46Z" id="114621152">Hi @wolfgangkarall 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="wolfgangkarall" created="2015-06-23T22:17:42Z" id="114657895">I can't say I enjoyed it, having to provide email addresses to Adobe is not my idea of fun in OSS. Trashmail addresses FTW.
</comment><comment author="clintongormley" created="2015-06-24T13:27:08Z" id="114867009">@tlrx please could you review
</comment><comment author="tlrx" created="2015-06-24T13:41:45Z" id="114871611">LGTM, thanks @wolfgangkarall 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11836 from wolfgangkarall/patch-1</comment></comments></commit></commits></item><item><title>Fix FieldDataTermsFilter.equals.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11835</link><project id="" key="" /><description>DoublesFieldDataFilter and LongsFieldDataFilter were performing casts against
the wrong class.

Close #11779
</description><key id="90450438">11835</key><summary>Fix FieldDataTermsFilter.equals.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T17:51:29Z</created><updated>2015-06-24T14:11:42Z</updated><resolved>2015-06-24T06:28:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-23T22:40:44Z" id="114663396">LGTM. One small test question.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Refactoring: Minor consistency cleanups for default values and validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11834</link><project id="" key="" /><description>Revisited the queries we already refactored, corrected and aligned some of the codebase based on the conventions that we decided to follow. Also including some cosmetic fixes (making members final where possible, avoiding this references outside of setters/getters).
In addition to that this PR changes:
- preventing NPEs in doXContent when rendering out nested queries that are `null`. We now render out empty object (`{ }`) which then gets parser to `null` to be consistent with queries than come only through the rest layer
- prevent adding nested `null` queries to collections of clauses like in BoolQueryBuilder
- add validate() method to all builders (even when empty)

This PR is against the query-refactoring branch.
</description><key id="90438068">11834</key><summary>Query Refactoring: Minor consistency cleanups for default values and validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-23T16:54:21Z</created><updated>2016-03-11T11:51:23Z</updated><resolved>2015-06-26T12:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-24T08:20:55Z" id="114779269">left a couple of comments
</comment><comment author="cbuescher" created="2015-06-24T09:29:03Z" id="114803666">@javanna I went through the builders again and checked for places where we might need to set default values in setters when argument is `null`. Most of the times argument is some primitive int/float/boolean, so `null` in not possible. When argument is a nested query builder, `null` is unfortunately an option we have to accept and check for later when generating the lucene query. In other cases value is really optional.
</comment><comment author="javanna" created="2015-06-24T09:46:58Z" id="114808838">thanks @cbuescher for looking into this, I left a small comment inline, also went over all the refactored queries and found something that needs to be updated in SimpleQueryStringBuilder#doXContent .
</comment><comment author="cbuescher" created="2015-06-25T17:24:46Z" id="115333217">@javanna With the last commit I added utility method that prevents potential NPEs when trying to render XContent for inner queries that have a `null`reference. I also added `Objects.notNull()` checks where we might potentially add a `null`query to one of the query builder collections (e.g. the clause-lists in BoolQB). 
I'm not sure if we already finally decided on this kind of fail-early-checks in setters/adders/constructors, but doing this in validate would require iterating through all clauses in a list. Also we avoid adding null-clauses on the parser side, so if a user of the builder tries this he should immediately get the exception I think. Let me know if you agree or I should change this back.
</comment><comment author="javanna" created="2015-06-26T06:55:19Z" id="115550860">This looks great @cbuescher . The natural next step is really replacing those null query builder with a no op query builder as we previously discussed. We can do that on a subsequent PR.

I left a couple of minor comments but LGTM besides those.

One more thing, I think you have to rebase and have a look at CommonTermsQuery and SImpleQueryStringQuery that got merged meanwhile and fix a few things there too.

&gt; I'm not sure if we already finally decided on this kind of fail-early-checks in setters/adders/constructors, but doing this in validate would require iterating through all clauses in a list.

I think it makes sense to do it this way, because it's a check especially for java api users, assuming that our own parsers will never set null there.
</comment><comment author="cbuescher" created="2015-06-26T11:06:48Z" id="115648624">@javanna did a rebase, addressed your two remarks concerning additional comment and null checks when adding q.b. to collections. I also added the `flags` field to SimpleQueryStringBuilder equals()/hashCode() unless there was a reason why this was ommited, maybe you can double check this makes sense. CommonTermsQ.B. looked good as far as current default checks/validation is concerned.
</comment><comment author="javanna" created="2015-06-26T11:42:50Z" id="115653469">I did another review and found a few places where I think we should add null checks, so that we make sure we never have null inner queries anywhere. again, it is just a check for java api users, cause we expect our parsers to skip null queries, which has a special meaning at the moment.
</comment><comment author="cbuescher" created="2015-06-26T11:57:19Z" id="115655507">Added two fixes, the constructor and setter changes can only happen if we change innerParserXY() to always return a QueryBuilder which I hope will happen with the PR we discussed. As long as that we need to allow it. We could add null-checks like when adding to the collections around all setter invocations in the fromXContent methods, but I think this would complicate things even more, given that we plan to change it.
</comment><comment author="javanna" created="2015-06-26T12:03:01Z" id="115656205">alright, thanks @cbuescher LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file></files><comments><comment>Merge pull request #11834 from cbuescher/feature/query-refactoring-minorcleanup</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file></files><comments><comment>Revisited the queries already refactored, corrected and aligned some of</comment><comment>the codebase based on the conventions that we decided to follow. Also including</comment><comment>some cosmetic fixes (making members final where possible, avoiding this</comment><comment>references outside of setters/getters).</comment></comments></commit></commits></item><item><title>Give the filter cache a smaller maximum number of cached filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11833</link><project id="" key="" /><description>Currently the filter cache is configured to have a maximum size in bytes of 10%
of the JVM memory, and a maximum number of cached filters (across all segments
of all shard on the same node) of 100000. I would like to change the latter to
a more reasonable value of 1000.

Given that we track the most 256 most recently used filters per index and only
cache those that have been seen 5 times or more, a single index cannot have more
than 50 hot filters, so a maximum number of cached filters of 1000 per node
should be more than necessary.
</description><key id="90424055">11833</key><summary>Give the filter cache a smaller maximum number of cached filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T15:47:14Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-06-24T06:23:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-23T21:04:10Z" id="114641419">+1
</comment><comment author="rjernst" created="2015-06-24T04:23:21Z" id="114723799">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file></files><comments><comment>Merge pull request #11833 from jpountz/fix/smaller_filter_cache</comment></comments></commit></commits></item><item><title>Add MetaData.uuid to ClusterState.toXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11832</link><project id="" key="" /><description>The MetaData.uuid is guaranteed to be unique across clusters and is handy (which may or may not have the same human readable cluster name).
</description><key id="90417782">11832</key><summary>Add MetaData.uuid to ClusterState.toXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T15:22:39Z</created><updated>2015-07-13T18:31:23Z</updated><resolved>2015-07-13T18:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-23T15:49:24Z" id="114554081">LGTM
</comment><comment author="bleskes" created="2015-06-30T12:16:49Z" id="117156111">I'm waiting for the dust to settle on #11831 as it may effect the field name
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file></files><comments><comment>Add MetaData.clusterUUID to ClusterState.toXContent</comment></comments></commit></commits></item><item><title>Rename cluster state uuid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11831</link><project id="" key="" /><description>The cluster state diffs introduced a concept of cluster state uuid that is uniquely identifying a particular iteration (version) of the the cluster state. This uuid is ephemeral and changes with each cluster state update, which causes confusion with other 2 uuids in the system - metadata uuid and index metadata uuid which are persistent. We need to rename cluster state uuid to reflect the difference in use and lifecycle. 
</description><key id="90415375">11831</key><summary>Rename cluster state uuid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Cluster</label><label>blocker</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T15:14:08Z</created><updated>2015-07-13T18:19:33Z</updated><resolved>2015-07-13T18:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-23T15:14:36Z" id="114539692">++
</comment><comment author="imotov" created="2015-06-28T01:52:02Z" id="116182446">If somebody has a good name for it to renamed to, please let me know. Rejected proposals so far:
- `id` - too generic, can be perceived as a persistent identification the same way as uuid today
- `instanceId` - the term instance is used as a synonym for a node everywhere in documentation
- `updateId` - rejected by the reviewer 
- `diffId` - rejected by the reviewer
- `revision` - rejected by the reviewer
- `edition` - rejected by the reviewer
- `fingerprint` - rejected by the reviewer
</comment><comment author="pickypg" created="2015-06-28T02:26:58Z" id="116183807">What about
- `clusterStateId`
- `clusterStateVersionId`
- `clusterStateDiffId`
- `stepId`
</comment><comment author="imotov" created="2015-06-28T13:22:10Z" id="116276778">Thanks! In my mind, `clusterStateId` implies persistence. Since we are trying to find a name for a field of a cluster state I don't think clusterState prefix is very useful here. I don't really see how `clusterStateDiffId` is better than `diffId`, for example. But to move things forward I would be ok with `versionId`, `clusterStateVersionId`, `clusterStateDiffId` or `stepId`. @bleskes?
</comment><comment author="bleskes" created="2015-06-29T08:25:35Z" id="116520759">My problem with all the diff/step related things is that the field is not related to the diff, but rather to the state. A diff has from and to .  The versionId related things are confusing w.r.t the existing version field.

I took a step back and I think this is may be difficult because we are trying to rename the wrong thing. Instead we should rename the uuid on the IndexMeta to indexUUID and the one on the MetaData to clusterUUID . I think things fall together nicely (https://github.com/elastic/elasticsearch/compare/master...bleskes:cs_uuid ) . @pickypg @imotov how would you feel about that?
</comment><comment author="imotov" created="2015-06-29T14:01:04Z" id="116674223">I think renaming uuid in IndexMetaData and MetaData wouldn't hurt. However, I don't think that step, updated and revision has anything to do with diffs. They simply emphasize transient nature of this id as related to iterations, updates, or revisions (whatever you want to call them) of the cluster state without overloading term "instance" that is already used in several places. 
</comment><comment author="clintongormley" created="2015-06-29T14:04:35Z" id="116678347">`generation` or `generation_id`?  
</comment><comment author="imotov" created="2015-06-29T14:14:58Z" id="116690736">That would work for me as well.
</comment><comment author="bleskes" created="2015-06-29T14:55:34Z" id="116717240">copying a comment from #11914 here, to have everything in one place:

&gt; Sorry I misunderstood your proposal. I thought you want to rename MetaData.uuid into MetaData.metaDataUUID. In my opinion renaming MetaData.uuid into MetaData.ClusterUUID (persistent) while keeping ClusterState.uuid (transient) is extremely confusing and is much worse than what we have today. I don't think we should get this in until we fix #11831.

OK. Naming is hard :) . I'm going give it one last shot before letting this go for a week or so. How about

-&gt; ClusterState.uuid -&gt; stateUUID (transient)
-&gt; MetaData.uuid -&gt; clusterUUID (persistent)
-&gt; IndexMetaData.uuid -&gt; indexUUID (persistent).

(brace, brace)
</comment><comment author="pickypg" created="2015-06-29T15:46:56Z" id="116738221">I like the "smurf" naming approach. Each field becomes much clearer and much simpler.

I might suggest making the transient ones more obvious though by calling them out as such: `transientStateUUID` (or if there will only be one, then maybe even `transientUUID`). Just to add more problems, "state" has a lot of potential associations (it's too generic), but as long as it's directly tied to the cluster state, then I'm okay with it.
</comment><comment author="clintongormley" created="2015-06-30T09:35:27Z" id="117087873">I'm good with the proposal in https://github.com/elastic/elasticsearch/issues/11831#issuecomment-116717240
</comment><comment author="imotov" created="2015-06-30T13:08:50Z" id="117173308">That works for me as well. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/VersionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Rename MetaData.uuid -&gt; MetaData.clusterUUID and IndexMetaData.uuid-&gt; IndexMetaData.indexUUID</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShardPath.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/VersionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Rename MetaData.uuid -&gt; MetaData.clusterUUID and IndexMetaData.uuid-&gt; IndexMetaData.indexUUID</comment></comments></commit></commits></item><item><title>[DOC] Multi-level nested query syntax, add more clarity in docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11830</link><project id="" key="" /><description>Nested query on one level is working great, but after the second level onwards (nesting) the query returns no results.

Here is my example:

```
{
    "mappings": {
        "customers": {
            "properties": {
                "lastUpdated": {
                    "type": "long"
                },
                "isDeleted": {
                    "type": "boolean"
                },
                "person": {
                    "type": "nested",
                    "properties": {
                        "car": {
                            "type": "nested",
                            "properties": {
                                "model": {
                                    "type": "string"
                                },
                                "make": {
                                    "type": "string"
                                }
                            }
                        },
                        "last_name": {
                            "type": "string"
                        },
                        "first_name": {
                            "type": "string"
                        }
                    }
                }
            }
        }
    }
}
```

My documents:

```
{
    "person": {
        "first_name": "Zach",
        "last_name": "Foobar",
        "car": [
            {
                "make": "Saturn",
                "model": "SL"
            },
            {
                "make": "Subaru",
                "model": "Imprezza"
            }
        ]
    },
    "isDeleted": false,
    "lastUpdated": 1433257051959
}

{
    "person": {
        "first_name": "Bob",
        "last_name": "Doe",
        "car": [
            {
                "make": "Saturn",
                "model": "Imprezza"
            },
            {
                "make": "Honda",
                "model": "Accord"
            }
        ]
    },
    "isDeleted": false,
    "lastUpdated": 1433257051959
}
```

Lastly, my query and result:

```
{
    "query": {
        "nested": {
            "path": "person",
            "query": {
                "match": {
                    "person.car.make": "Honda"
                }
            },
            "inner_hits": {}
        }
    }
}

{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": null,
    "hits": []
  }
}
```

---

EDIT: So, It seems that this query works, but not the one above. 

```
{
    "query": {
        "nested": {
            "path": "person",
            "query": {
                "match": {
                    "person.car.make": "Honda"
                }
            },
            "inner_hits": {}
        }
    }
}
```

I would suggest adding more clarity in the docs for multi-level nested queries about the path and query path. The [doc](https://www.elastic.co/guide/en/elasticsearch/guide/current/nested-query.html)  for nested query should be enhance for multi-level nesting.
</description><key id="90414493">11830</key><summary>[DOC] Multi-level nested query syntax, add more clarity in docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RanadeepPolavarapu</reporter><labels><label>:Nested Docs</label><label>adoptme</label><label>docs</label></labels><created>2015-06-23T15:11:00Z</created><updated>2017-06-08T17:01:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-06-23T16:13:36Z" id="114560042">Hey @RanadeepPolavarapu you need to use two `nested` queries to get this working correctly:

``` json
{
    "query": {
        "nested": {
            "path": "person",
            "query": {
                "nested" : {
                    "path" :  "person.car",
                    "query" :  {
                       "match": {
                           "person.car.make": "Honda"
                        }
                    }
                }
            },
            "inner_hits": {}
        }
    }
}
```
</comment><comment author="martijnvg" created="2015-06-23T16:27:13Z" id="114563203">ES should do a better job at failing your query here instead of just returning no results. Issue #9255 will implement `nested` query validation.
</comment><comment author="RanadeepPolavarapu" created="2015-06-23T17:07:27Z" id="114574181">Hi @martijnvg ,

Thank You, your reply was very helpful.

I see both queries (mine and the one you provided) work, but which one is preferred? Is one better to use over the other? Does one adhere to the ES standard better than the other? One does two nested queries and the other accesses the nested object using I guess nested dot notation (for lack of a better term)?

```
{
    "query": {
        "nested": {
            "path": "person",
            "query": {
                "nested" : {
                    "path" :  "person.car",
                    "query" :  {
                       "match": {
                           "person.car.make": "Honda"
                        }
                    }
                }
            },
            "inner_hits": {}
        }
    }
}
```

vs.  

```
{
    "query": {
        "nested": {
            "path": "person",
            "query": {
                "match": {
                    "person.car.make": "Honda"
                }
            },
            "inner_hits": {}
        }
    }
}
```
</comment><comment author="martijnvg" created="2015-06-24T05:31:27Z" id="114732144">@RanadeepPolavarapu You should use the first query. The second query shouldn't work, because it is incorrect. Not sure why it works on your side, but I tried it out and the second query doesn't work here.
</comment><comment author="syzspectroom" created="2015-12-10T12:03:45Z" id="163591490">is there any way to query nested fields using wildcard in path? 
my document:

```
{
   "source": "forbes",
   "external_id": "1234",
   "versions": {
      "en": {
         "title": "title en",
         "content": "content en"
      },
      "localized_en": {
         "title": "title localized en",
         "content": "content localized en"
      },
      "fr": {
         "title": "title fr",
         "content": "content fr"
      }
   }
}
```

and I need to do query like this:

```
{
   "query": {
      "nested": {
         "path": "versions",
         "query": {
            "multi_match": {
               "query": "content",
               "fields": [
                  "versions.*.content"
               ]
            }
         }
      }
   }
}
```

but didn't get any results
</comment><comment author="roxma" created="2016-07-11T02:17:10Z" id="231628241">If I have two nested fields, how do I perform a query on both of them?

```
{
    "mappings": {
        "customers": {
            "properties": {
                "lastUpdated": {
                    "type": "long"
                },
                "isDeleted": {
                    "type": "boolean"
                },
                "person": {
                    "type": "nested",
                    "properties": {
                        "car": {
                            "type": "nested",
                            "properties": {
                                "model": {
                                    "type": "string"
                                },
                                "make": {
                                    "type": "string"
                                }
                            }
                        },
                        "last_name": {
                            "type": "string"
                        },
                        "first_name": {
                            "type": "string"
                        }
                    }
                }
            }
            "person2": .......................
        }
    }
}
```

For example, How do I perform a query on person and person2 ?

**EDIT: I think [this](https://www.elastic.co/guide/en/elasticsearch/guide/current/nested-query.html) is clear enough for my use case, please ignore my comment**
</comment><comment author="abhishek5678" created="2017-04-27T11:54:58Z" id="297692555">Hi Guys,
In this data when i am trying with one level nested data then it's working fine but when i am adding one more nested data then it's not working if anybody will be knows about that then please reply me.I am sending my document data and mapping file also.

POST /test_word14/doc/1
{
"name": "Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism (D50-D89)",
"depth": 1,
"children": [{
"name": "Nutritional anemias (D50-D53)",
"depth": 2,
"children": [{
"code": "D50",
"name": "Iron deficiency anemia",
"depth": 3,
"children": [{
"code": "D50.0",
"name": "Iron deficiency anemia secondary to blood loss (chronic)",
"depth": 4
}, {
"code": "D50.1",
"name": "Sideropenic dysphagia",
"depth": 4
}, {
"code": "D50.8",
"name": "Other iron deficiency anemias",
"depth": 4
}, {
"code": "D50.9",
"name": "Iron deficiency anemia, unspecified",
"depth": 4
}]
}]
}]
}

(mapping file)
PUT /test_word16
{
"mappings": {
"doc": {
"properties": {
"name": { "type": "string" },
"depth": {"type":"long"},
"children": {
"type": "nested",
"properties": {
"name": { "type": "string" },
"depth": { "type": "long" },
"children": {
"type": "nested",
"properties": {
"code": { "type": "string" },
"name": { "type": "string" },
"depth": { "type": "long" },
"children": {
"type": "nested",
"properties": {
"code": { "type": "string" },
"name": { "type": "string" },
"depth": { "type": "long" },
}
}
}
}
}
}
}
}
}
}

please tell me what is the query i am applying here.i applied the query like this:
GET /icd10_codes/doc/_search
{
"_source": false,
"query": {
"nested": {
"path": "children",
"query": {
"nested": {
"path": "children.children",
"query": {
"nested": {
"path": "children.children.children",
"query": {
"bool": {
"must": [
{
"match": {
"children.children.name": "Iron"
}
}
]
}
},
"inner_hits": {
"_source": {
"excludes":["name"]
}
}
}
}
}
}
}
}</comment><comment author="siddharth1199" created="2017-06-08T12:44:45Z" id="307092003">Hi guys, i have a query with 2 bool conditions as follows:

'''GET /resume/candidates/_search
{
    "query": {
        "nested" : {
            "path" : "sections",
            "query" : {
                "bool" : {
                    "should" : [
                    { "match:{"sections.skills": "CSS"} },
                    { "match:{"sections.education": "photography"} }
                    ]
                }
            }
        }
    }
}
'''
When I run the 2 match queries independently, I get results pointing towards the same document. However, when i combine and run the, it does not return any result. Can anyone please help?

Mapping:

PUT /resume
{
"mappings" : {
  "candidates" : {
   "properties" : {
     "name": { "type" : "text" },
     "phone":{ "type" : "text" },
     "email":{ "type" : "text" },
     "sections": {
       "type": "nested",
       "properties": {
         "introduction":{"type":"text"},
         "summary":{"type":"text"},
         "awards":{"type":"text"},
         "achievements":{"type":"text"},
         "skills":{"type":"text","analyzer":"keyword"},
         "education":{"type":"text"},
         "work-experience": [],
         "projects":{"type":"text"},
         "extra-curricular":{"type":"text"}
  }}}}}


Data:

PUT /resume/candidates/2
{
  "name": "XXXXXXX",
  "phone": "XXXXXXXXXXXX",
  "email": "XXXXXXXXXXXXXXXXX",
  "sections": [
    {
      "introduction": "XXXXXXXXXXXXXXXi"
    },
    {
      "summary": ""
    },
    {
      "achievements": ["best photographer of the year: 2009"]
    },
    {
      "skills": [
        "APACHE",
        "CASSANDRA",
        "MYSQL",
        "CDN",
        "AMAZON CLOUD FRONT",
        "MS OFFICE",
        "HLS",
        "DASH",
        "CSS",
        "JSON",
        "RUBY"
      ]
    },
    {
      "education": [
        "XXXXXXXXXXXX Photography",
        "XXXXXXXXXXXXXXxx"
      ]
    },
    {
      "work-experience": [
        {
          "0": [
            {
              "company-name": "CREATIVE ARTISTS AGENCY, Beverly Hills, CA"
            },
            {
              "start-date": "2002"
            },
            {
              "end-date": "2004"
            },
            {
              "job-description": "XXXXXXXXXx"
            },
            {
              "designation": "Video Game Assistant"
            }
          ]
        },
        {
          
        },
        {
          "2": [
            {
              "company-name": "OC&amp;C STRATEGY, New York, NY"
            },
            {
              "start-date": "2008"
            },
            {
              "end-date": "2009"
            },
            {
              "job-description": "XXXXXXXXXXXXXXXX"
            },
            {
              "designation": "Associate Consultant"
            }
          ]
        },
        {
          "3": [
            {
              "company-name": "XXXXXXXXXXXXX"
            },
            {
              "start-date": "2009"
            },
            {
              "end-date": "2011"
            },
            {
              "job-description": "XXXXXXXXXXXXXXXXxxx"
            },
            {
              "designation": "designer"
            }
          ]
        }
      ]
    },
    {
      "extra-curricular": ""
    },
    {
      "awards": ""
    },
    {
      "projects": [
        ""
      ]
    }
  ]
}


</comment><comment author="lukas-gitl" created="2017-06-08T17:01:16Z" id="307165170">@siddharth1199 Not sure what your problem is. It looks like it should work. Formatting your json would help greatly with readability and encourage people to look at it more closely...</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify ShardRouting abstraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11829</link><project id="" key="" /><description>This commit folds ShardRouting, ImmutableShardRouting and MutableShardRouting
into ShardRouting. All mutators are package private anyway today so it's just
unnecessary abstraction.
</description><key id="90405771">11829</key><summary>Simplify ShardRouting abstraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>won't fix</label></labels><created>2015-06-23T14:33:37Z</created><updated>2015-06-23T15:09:22Z</updated><resolved>2015-06-23T15:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-23T15:09:21Z" id="114537989">we decided not do this afterall.....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Encapsualte common code in methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11828</link><project id="" key="" /><description>This commit groups duplicated code in methods to make the actual decision
easier to read. There is no change in functionality in this change.
</description><key id="90391569">11828</key><summary>Encapsualte common code in methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T13:36:18Z</created><updated>2015-06-23T13:49:30Z</updated><resolved>2015-06-23T13:49:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-23T13:47:03Z" id="114511158">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parallel start node in test cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11827</link><project id="" key="" /><description>We can start the nodes in parallel when setting up a cluster predefined with specific set of nodes
</description><key id="90377524">11827</key><summary>Parallel start node in test cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>review</label><label>test</label></labels><created>2015-06-23T12:43:31Z</created><updated>2016-03-10T11:03:59Z</updated><resolved>2016-03-10T11:03:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-23T14:40:27Z" id="114529955">LGTM
</comment><comment author="bleskes" created="2015-06-23T14:55:44Z" id="114533786">LGTM2
</comment><comment author="s1monw" created="2015-06-23T15:08:45Z" id="114537843">actually thinking about this I am not sure if this is a good idea... it will make things even harder to reproduces and reason about we maybe should remove all the node starting concurrency instead?
</comment><comment author="jpountz" created="2015-07-08T16:22:49Z" id="119648058">+1 on not starting concurrently if it may prevent failures from reproducing
</comment><comment author="jpountz" created="2015-07-17T11:33:16Z" id="122251841">Should we close?
</comment><comment author="clintongormley" created="2016-03-10T11:03:59Z" id="194792594">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log warn message if leftover shard is detected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11826</link><project id="" key="" /><description>@clintongormley can you take a look 
</description><key id="90370895">11826</key><summary>Log warn message if leftover shard is detected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T12:13:48Z</created><updated>2015-06-23T19:24:15Z</updated><resolved>2015-06-23T12:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-23T12:35:21Z" id="114478666">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds cumulative sum aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11825</link><project id="" key="" /><description>This adds a new pipeline aggregation, the cumulative sum aggregation. This is a parent aggregation which must be specified as a sub-aggregation to a histogram or date_histogram aggregation. It will add a new aggregation to each bucket containing the sum of a specified metrics over this and all previous buckets.
</description><key id="90363406">11825</key><summary>Adds cumulative sum aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T11:38:07Z</created><updated>2015-06-25T13:38:03Z</updated><resolved>2015-06-25T13:37:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-24T08:39:25Z" id="114789172">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Build fails with maven-assembly-plugin version 2.5.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11824</link><project id="" key="" /><description>Running the command `mvn install -pl core -DskipTests` on the master branch results in a failure:

```
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 39.880s
[INFO] Finished at: Tue Jun 23 19:50:18 JST 2015
[INFO] Final Memory: 52M/1462M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.5.5:single (default) on project elasticsearch: Execution default of goal org.apache.maven.plugins:maven-assembly-plugin:2.5.5:single failed: group id '2060452353' is too big ( &gt; 2097151 ). Use STAR or POSIX extensions to overcome this limit -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
```

If I remove the `version` of the maven-assembly-plugin in `pom.xml` like this:

```
$ git diff
diff --git a/pom.xml b/pom.xml
index 6083089..8df98e2 100644
--- a/pom.xml
+++ b/pom.xml
@@ -482,7 +482,6 @@
                 &lt;plugin&gt;
                     &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                     &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
-                    &lt;version&gt;2.5.5&lt;/version&gt;
                 &lt;/plugin&gt;
                 &lt;plugin&gt;
                     &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
```

then it uses version 2.2-beta-5 of the plugin, and the build succeeds.
</description><key id="90354741">11824</key><summary>Build fails with maven-assembly-plugin version 2.5.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dpursehouse</reporter><labels><label>build</label></labels><created>2015-06-23T10:59:07Z</created><updated>2015-12-07T23:32:59Z</updated><resolved>2015-11-28T13:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-23T11:03:41Z" id="114445490">I just ran it and it works just fine for me maybe it's something else or your version is corrupted?
</comment><comment author="dpursehouse" created="2015-06-23T11:15:31Z" id="114447293">I tried to remove that version from my ~/.m2/repository folder but got the same error after it downloaded again.  And the same error if I explicitly set the version to 2.5.4 or 2.5.3 (the other versions that I happened to have locally).

This is on Ubuntu 12.04 LTS if it matters.

As long as I can build by removing the explicit version in pom.xml it's fine; I was just hoping someone else had also had the same problem and could point me in the right direction to fix it...
</comment><comment author="dadoonet" created="2015-06-23T11:18:04Z" id="114447607">What is your maven version?
</comment><comment author="dpursehouse" created="2015-06-23T11:19:46Z" id="114447828">@dadoonet  version 3.0.4.  Full details below.

```
$ mvn --version
Apache Maven 3.0.4
Maven home: /usr/share/maven
Java version: 1.7.0_45, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle-1.7.0.45/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "3.2.0-82-generic", arch: "amd64", family: "unix"
```
</comment><comment author="dadoonet" created="2015-06-23T11:56:13Z" id="114466643">@dpursehouse Could you try to update to 3.1.1 and see if it fixes your issue?
</comment><comment author="dpursehouse" created="2015-06-24T00:16:53Z" id="114680738">@dadoonet unfortunately I'm getting the same error also with version 3.1.1.
</comment><comment author="xuzha" created="2015-06-24T00:24:23Z" id="114681610">This also happened to me. The workaround here is to add posix mode tar extensions (tarLongFileMode=posix).  Works for me. 

Maven home: /usr/local/apache-maven/apache-maven-3.3.3
Java version: 1.8.0_45, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "mac os x", version: "10.10.3", arch: "x86_64", family: "mac"
</comment><comment author="dpursehouse" created="2015-06-24T01:57:33Z" id="114697345">@xuzha that has fixed it.  Thanks!
</comment><comment author="xuzha" created="2015-06-24T04:30:32Z" id="114724405">I'm just curious why it doesn't happen to other people, is this related to maven-assembly-plugin upgrade?
</comment><comment author="lmenezes" created="2015-07-31T13:01:23Z" id="126685210">also happened to me, and adding tarLogFileMode also fixed. If this of any interest:

Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T18:29:23+01:00)
Maven home: /usr/local/Cellar/maven/3.2.5/libexec
Java version: 1.8.0_40, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "mac os x", version: "10.10.4", arch: "x86_64", family: "mac"
</comment><comment author="dadoonet" created="2015-09-15T07:16:08Z" id="140301636">This is really weird. I'm running on MacOS as well, with Maven 3.3.3 and never got this issue.
And I'm building elasticsearch from a long pathname: `/Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch`. I would expect getting the same issue as you guys get.

Anyone has an idea on how I could reproduce this? What are the exact commands you are running?
Does it break also when running `mvn clean install -DskipTests` from the elasticsearch root dir?
</comment><comment author="xuzha" created="2015-09-15T07:34:14Z" id="140306012">@dadoonet  I didn't put too much time investigating this, but I couldn't reproduce it now. Looks like this is not related to the long pathname. 
</comment><comment author="lmenezes" created="2015-09-15T07:37:50Z" id="140307352">still the same result:
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:41 min
[INFO] Finished at: 2015-09-15T09:33:57+02:00
[INFO] Final Memory: 53M/832M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.5.5:single (default) on project elasticsearch: Execution default of goal org.apache.maven.plugins:maven-assembly-plugin:2.5.5:single failed: group id '1757119603' is too big ( &gt; 2097151 ). Use STAR or POSIX extensions to overcome this limit -&gt; [Help 1]

Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T13:57:37+02:00)
Maven home: /usr/local/Cellar/maven/3.3.3/libexec
Java version: 1.8.0_40, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "mac os x", version: "10.10.5", arch: "x86_64", family: "mac"
</comment><comment author="dadoonet" created="2015-09-15T07:38:45Z" id="140307669">@lmenezes could you run the same with `-X` option and upload somewhere the full log?
</comment><comment author="lmenezes" created="2015-09-15T07:46:12Z" id="140310139">@dadoonet let me know if this works: https://www.dropbox.com/s/0o3gfqyxfmaotny/maven.log?dl=0
</comment><comment author="dpursehouse" created="2015-09-15T07:55:51Z" id="140312792">@dadoonet Note that I'm building on Ubuntu, not MacOS.
</comment><comment author="dadoonet" created="2015-09-15T10:11:02Z" id="140344406">Interesting. It sounds like the problem is coming from the `group id` which is set to inner files.

Basically it then fails if for a given tar entry, we have:

``` java
    private void failForBigNumbers(TarArchiveEntry entry) {
        this.failForBigNumber("entry size", entry.getSize(), 8589934591L);
        this.failForBigNumber("group id", (long)entry.getGroupId(), 2097151L);
        this.failForBigNumber("last modification time", entry.getModTime().getTime() / 1000L, 8589934591L);
        this.failForBigNumber("user id", (long)entry.getUserId(), 2097151L);
        this.failForBigNumber("mode", (long)entry.getMode(), 2097151L);
        this.failForBigNumber("major device number", (long)entry.getDevMajor(), 2097151L);
        this.failForBigNumber("minor device number", (long)entry.getDevMinor(), 2097151L);
    }
```

I'm trying to get more information now. 
</comment><comment author="khmarbaise" created="2015-11-20T07:28:14Z" id="158310383">You should configure the usage of [posix as mentioned in the FAQ.](https://maven.apache.org/plugins/maven-assembly-plugin/faq.html#tarFileModes)
</comment><comment author="dadoonet" created="2015-11-20T07:36:47Z" id="158311853">@khmarbaise sure. Wondering why it's not the default then? Is there any tradeoff?
</comment><comment author="dadoonet" created="2015-11-28T13:33:14Z" id="160296246">As we moved to gradle in master branch, I think we can live with this and default maven options for now. At least we have a workaround if some people get this issue.

So let's close it as "fixed in 3.0" :P 
</comment><comment author="larryfast" created="2015-12-07T23:32:59Z" id="162705134">FYI had this problem at our site and the source was userids and groupids. My user &amp; group ID is 831100298 (well, not exactly that).  So the error seems to come from file creation and userids.  And yes, the posix fix worked for me as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: NotQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11823</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217
Closes #11823 
</description><key id="90352826">11823</key><summary>Query refactoring: NotQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-23T10:49:23Z</created><updated>2015-06-23T11:48:04Z</updated><resolved>2015-06-23T11:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-23T10:53:24Z" id="114443504">left a small comment, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11823 from cbuescher/feature/query-refactoring-not</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file></files><comments><comment>Query refactoring: NotQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>High CPU and Memory usage by elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11822</link><project id="" key="" /><description>We are using java client to communicate with Elasticsearch (search requests only). Once started, CPU and Memory usage keeps going up and doesn't come down unless the process is restarted.

Threaddump shows around 120 blocked threads with below stack trace:

```
Thread 26356: (state = BLOCKED)
 - java.lang.Thread.sleep(long) @bci=0 (Compiled frame; information may be imprecise)
 - org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick() @bci=81, line=445 (Compiled frame)
 - org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run() @bci=43, line=364 (Compiled frame)
 - org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=108 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=724 (Interpreted frame)
```

We are using TranspostClient object and close it once the search is done. Following is the hot_threads output:

```
0.2% (795.3micros out of 500ms) cpu usage by thread 'elasticsearch[es1][scheduler][T#1]'
 10/10 snapshots sharing following 9 elements
   sun.misc.Unsafe.park(Native Method)
   java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
   java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
   java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
   java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   java.lang.Thread.run(Thread.java:745)

0.1% (647.2micros out of 500ms) cpu usage by thread 'elasticsearch[es1][management][T#4]'
 10/10 snapshots sharing following 9 elements
   sun.misc.Unsafe.park(Native Method)
   java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
   java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:731)
   java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
   java.util.concurrent.LinkedTransferQueue.poll(LinkedTransferQueue.java:1145)
   java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   java.lang.Thread.run(Thread.java:745)

0.1% (489.6micros out of 500ms) cpu usage by thread 'elasticsearch[es1][[transport_server_worker.default]][T#5]{New I/O worker #14}'
 10/10 snapshots sharing following 15 elements
   sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
   sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
   sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
   sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
   sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
   org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
   org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
   org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
   org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
   org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   java.lang.Thread.run(Thread.java:745)
```

Before closing TransportClient, we are also shutting down associated threadpool (shutdownNow()) to kill the threads, still there are too many blocked threads in thread dump.

Let me know if anything else is required.

Thanks
</description><key id="90351675">11822</key><summary>High CPU and Memory usage by elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darshanmehta10</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2015-06-23T10:41:40Z</created><updated>2016-01-13T17:04:57Z</updated><resolved>2016-01-13T17:04:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T19:15:21Z" id="114614485">Hi @darshanmehta10 

What version are you on, and what does your code look like?
</comment><comment author="darshanmehta10" created="2015-06-23T23:07:28Z" id="114668320">Hi,

We are using elasticsearch 1.4.2. We have a Java API which connects with remote elasticsearch server, executes search and closes Transport client. The transport client object is cached for some time and is closed when evicted from cache.
Following is the code to create transport client:

```
Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name",     elasticSearch.getClusterName()).build();
TransportClient tc = new TransportClient(settings);
tc.addTransportAddress(new InetSocketTransportAddress(elasticSearch.getHost(), elasticSearch.getPort()));
```

Following is the code to close Transport Client:

```
tc.threadPool().shutdownNow();
tc.close();
```

Below is the screen shot of heapdump taken 3 hours after application restart (CPU : 11%, Memory 18%):
![hd1](https://cloud.githubusercontent.com/assets/12479117/8319226/32fa1aea-1a04-11e5-852a-698a9febdc39.JPG)
It has 60k objects of HashedWheelBucket and thread dump showed 120 blocked threads of same.

Below is the screen shot of heapdump taken 6 hours after application restart (CPU 18%, Memory : 21%):
![hd2](https://cloud.githubusercontent.com/assets/12479117/8319264/aa439af4-1a04-11e5-8b71-d7312d567be6.JPG)
94k objects and 180 blocked threads now.

It seems closing Transportclient doesn't kill off threads even though we explicitly shut the threadpool down. This causes memory and cpu usage to reach 100% and then requires restart.
</comment><comment author="clintongormley" created="2015-06-24T13:29:06Z" id="114867619">I know that a number of improvements have been made to the transport client in recent versions. Not sure if this is the same thing or not.  /cc @spinscale ?

@darshanmehta10 Would probably be useful to have some examples of what you're doing between opening and closing the connection too.
</comment><comment author="darshanmehta10" created="2015-06-24T14:58:32Z" id="114898424">Hello,

Below is the method which uses transport client object to query:

```
public Object execute(SearchQuery query, TransportClient client) {
    Object result;
    ElasticsearchTemplate elasticSearchTemplate = new ElasticsearchTemplate(client);
    SearchResponse response = elasticSearchTemplate.query(query, new ResultsExtractor&lt;SearchResponse&gt;() {
        @Override
        public SearchResponse extract(SearchResponse response) {
            return response;
        }
    });

    result = processResponse(response);//Extracts data from response
    return result;
```

}

We are using spring implementation of EhCache (EhCacheCache) to store TransportClient object and have configured listener to close the client in overridden notifyElementRemoved, notifyElementExpired and notifyElementEvicted methods.

A day after the last restart (24 hours), the API is using 32% CPU and 22% memory.

Please let me know if anything else is required.
</comment><comment author="spinscale" created="2015-06-26T06:34:18Z" id="115542963">Are you sure that you are instantiating only a single TransportClient? Can you try the same by reusing some TransportClient object? I just want to rule out that you have started dozens of TransportClients.

You should also check with netstat how many open connections there are.

The reason why I have the feeling that instantiate thousands of TransportClients is that, usually each Client only has one HashedWheelTimer, but you have tons of those.
</comment><comment author="darshanmehta10" created="2015-06-29T10:02:10Z" id="116590280">We are not using a single TransportClient. We can't really use the same TransportClient as we get the elasticsearch cluster details at runtime. So, we do the following steps when a request to query elasticsearch is received to our api:
- Create TransportClient object
- Cache TransportClient object
- Execute search
- Send back results

We are using Spring EhCache with 30 mins timeout and close the TransportClient object when it is evacuated. However, we believe that closing TransportClient (and explicitly shutting down threadpool) doesn't kill off all the request threads and that's why there are tons of HashedWheelTimer objects. Netstat also shows plenty of connections which ideally should not be the case.

We found the following in close() method of TransportClient:
`injector.getInstance(ThreadPool.class).shutdown();`
This means threadpool won't accept the new thread but it will still execute the existing threads. May be existing threads are hung/blocked and the count of such threads keeps on increasing every time a new TransportClient is created?
</comment><comment author="spinscale" created="2015-06-30T07:44:04Z" id="117042199">You can try with Elasticsearch 1.5 and above. We changed the closing of threadpools in there, see [here](https://github.com/elastic/elasticsearch/blob/1.5/src/main/java/org/elasticsearch/client/transport/TransportClient.java#L283)
</comment><comment author="eskibars" created="2016-01-13T17:04:57Z" id="171365254">I'm going to close this since we haven't heard from this in over 6 months and it was potentially solved with #7868.  If there's still an issue, we can reopen.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrading 1.4.2 to 1.5.2 is not providing the data to 1.5.2 which was there in 1.4.2 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11821</link><project id="" key="" /><description>Hi,
Am upgrading elasticsearch 1.4.2 to 1.5.2 
For that I have done
1) copied the config file of 1.4.2 and pasted that in 1.5.2...

when am looking for the indexes in 1.5.2, I am unable to see the indexes which I have created in 1.4.2 .
So how to upgrade to higher version by keeping the indexes and data as it is which were there in old version
</description><key id="90341807">11821</key><summary>Upgrading 1.4.2 to 1.5.2 is not providing the data to 1.5.2 which was there in 1.4.2 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HemaAnusha</reporter><labels /><created>2015-06-23T09:58:51Z</created><updated>2015-06-23T10:19:02Z</updated><resolved>2015-06-23T10:19:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-23T10:19:01Z" id="114431920">You will not lose data like this with an upgrade.

I'd recommend you join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fragment_size option is ignored when highlighting a stopwords filtered field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11820</link><project id="" key="" /><description>Hi there,

I am trying to get highlights from a field that contains a lot of english text. I built an index that uses the `standard` analyzer with the english stopwords. The highlights I get when I perform a search can be very short or quite big, regardless of the `fragment_size` option.

Here is my test setup:
### Settings

**POST /test**

``` json
{
  "index": {
    "mapper": {
      "dynamic": "strict"
    },
    "analysis": {
      "analyzer": {
        "english": {
          "type": "standard",
          "stopwords": "_english_"
        }
      }
    }
  }
}
```
### Mapping

**POST /test/chapter/_mapping**

``` json
{
  "chapter": {
    "properties": {
      "content": {
        "type": "string",
        "analyzer": "english",
        "index_options": "offsets"
      }
    }
  }
}
```
### Test document (with a big content)

**POST /test/chapter**

``` json
{
    "title": "3. DETAILS OF THE INVESTMENT IN GOVERNMENT BONDS",
    "content": "3. DETAILS OF THE INVESTMENT IN GOVERNMENT BONDS Following are details divided by governments with respect to the total securities portfolio: The Bank's exposure to leveraged finance according to the economic sector: CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review SCHEDULE \"A\" - RATES OF INTEREST INCOME AND EXPENSES AND ANALYSIS OF CHANGES IN INTEREST INCOME AND EXPENSES - CONSOLIDATED() Part \"A\" - Average balances and interest rates - assets For the three months ended March 31 Average balance 2015 Interest income Rate of income Average balance 2014 Interest income Rate of income CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review SCHEDULE \"A\" - RATES OF INTEREST INCOME AND EXPENSES AND ANALYSIS OF CHANGES IN INTEREST INCOME AND EXPENSES - CONSOLIDATED() (CONTINUED) Part \"B\" - Average balances and interest rates - liabilities and equity For the three months ended March 31 Average balance Interest expenses Rate of expense Average balance Interest expenses Rate of expense 2015 2014 Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"A\" - RATES OF INTEREST INCOME AND EXPENSES AND ANALYSIS OF CHANGES IN INTEREST INCOME AND EXPENSES - CONSOLIDATED() (CONTINUED) Part \"C\" - Average balances and interest rates - additional information regarding interest bearing assets and liabilities attributed to operations in Israel For the three months ended March 31 Average balance 2015 Interest income (expense) Rate of income (expense) Average balance 2014 Interest income (expense) Rate of income (expense) For footnotes see next page. SCHEDULE \"A\" - RATES OF INTEREST INCOME AND EXPENSES AND ANALYSIS OF CHANGES IN INTEREST INCOME AND EXPENSES - CONSOLIDATED() (CONTINUED) Part \"D\" - Analysis of changes in interest income and expenses For the three months ended March 31 2015 Compared to 2014 Increase (decrease) due to change Quantity Price Net change In NIS millions CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review Interest bearing assets: Credit to the public: Footnotes: (1) The data is presented after the effect of hedge derivative instruments. (2) Based on monthly opening balances, except for the non-linked shekels segment in respect of which the average balances are based on daily data. (3) Before deduction of the average stated balance of allowances for credit losses. Including impaired debts that do not accrue interest income. (4) From the average balance of trading bonds and of available-for-sale bonds was deducted (added) the average balance of non-realized gains (losses) from adjustment to fair value of trading bonds as well as gains (losses) in respect of available-for-sale bonds included in shareholders' equity as part of accumulated other comprehensive income, in the item \"Adjustments in respect of available-for-sale securities according to fair value\" in the amount of NIS 10 million and NIS 497 million, respectively; 2014 - NIS 2 million and NIS 218 million respectively. (5) Including derivative instruments and other assets that do not carry interest and net of allowance for credit losses. (6) Including derivative instruments. (7) Net return - net interest income divided by total interest bearing assets. (8) The quantitative impact has been computed by multiplying the interest margin by the change in the average balance between the periods. The price impact has been calculated by multiplying the average balance for the corresponding period last year by the change in the interest margin between the periods. (9) Interest income on other assets and interest expenses on other liabilities include income tax interest income and expenses, respectively. (10) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 E (1). (11) The drop in the CPI in the first quarter of 2015, led to the recording of negative linkage increments on assets and liabilities. As a result thereof, the CPI linked segments presents net interest expenses on assets and net interest income on liabilities. (12) An amount lower than NIS 1 million. Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"B\" - EXPOSURE TO CHANGES IN INTEREST RATES - CONSOLIDATED On demand or within 1 month Over 1 month and up to 3 months As at March 31, 2015 Over 3 months and up to 1 year in NIS millions Over 1 year and up to 3 years Over 3 years and up to 5 years Notes: (1) Not including balances of derivative financial instruments and fair value of off-balance sheet financial instruments. (2) Weighted average by fair value of average effective duration. (3) Including shares listed under \"No fixed maturity\". (4) Including Israeli currency linked to foreign currency. CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review Over 5 years and up to 10 years Over 10 years and up to 20 years Over 20 years No fixed maturity date Total fair value Internal rate of return In % Total fair value As at March 31, 2014 Internal rate of return In % Total fair value Effective average duration In years As at December 31, 2014 Internal rate of return In % Effective average duration In years General notes: (a) Data by period in this table represent the present value of future cash flows for each financial instrument, discounted at such interest rate as to discount them to the fair value included in the financial instrument, in a manner consistent with assumptions used in calculation of the fair value of said financial instrument. For details regarding the assumptions used in calculating the fair value of financial instruments, see \"Management and measurement of market risks\" under \"Exposure to risks and risk management\". (b) The internal rate of return is the interest rate used to discount the expected cash flows from a financial instrument to its fair value, as included in Note 10 a. (c) The average effective duration of a group of financial instruments is an approximation of the change, in percentage, in fair value of said group of financial (d) Full data as the exposure to changes in interest rates in each segment according to the various balance sheet items, is available on request. As at March 31, 2015 Effective average duration In years in NIS millions instruments resulting from a small change (0.1% increase) in the internal rate of return of each of the financial instruments. Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"B\" - EXPOSURE TO CHANGES IN INTEREST RATES - CONSOLIDATED (CONTINUED) On demand or within 1 month Over 1 month and up to 3 months As at March 31, 2015 Over 3 months and up to 1 year in NIS millions Over 1 year and up to 3 years Over 3 years and up to 5 years Notes: (1) Not including balances of derivative financial instruments and fair value of off-balance sheet financial instruments. (2) Weighted average by fair value of average effective duration. (3) Including shares listed under \"No fixed maturity\". (4) Including Israeli currency linked to foreign currency. CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review As at March 31, 2015 Over 5 years and up to 10 years Over 10 years and up to 20 years Over 20 years No fixed maturity date Total fair value Internal rate of return In % Total fair value As at March 31, 2014 Internal rate of return In % Total fair value Effective average duration In years As at December 31, 2014 Internal rate of return In % Effective average duration In years General notes: (a) Data by period in this table represent the present value of future cash flows for each financial instrument, discounted at such interest rate as to discount them to the fair value included in the financial instrument, in a manner consistent with assumptions used in calculation of the fair value of said financial instrument. For details regarding the assumptions used in calculating the fair value of financial instruments, see \"Management and measurement of market risks\" under \"Exposure to risks and risk management\". (b) The internal rate of return is the interest rate used to discount the expected cash flows from a financial instrument to its fair value, as included in Note 10 a. (c) The average effective duration of a group of financial instruments is an approximation of the change, in percentage, in fair value of said group of financial (d) Full data as the exposure to changes in interest rates in each segment according to the various balance sheet items, is available on request. instruments resulting from a small change (0.1% increase) in the internal rate of return of each of the financial instruments. Effective average duration In years in NIS millions Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"C\" - EXPOSURE TO FOREIGN COUNTRIES - CONSOLIDATED() A. Information regarding the total exposure to foreign countries and to countries where the total exposure to each country amounts to over 1% of total consolidated assets or over 20% of the Bank's equity, the lower of the two Of which - Total exposure to LDC countries Notes: (1) Based on the final risk, net of the effect of guarantees, liquid collateral and credit derivatives. (2) Balance sheet and off-balance sheet credit risk, Problematic credit risk and impaired debts are presented before the impact of the allowance for credit losses and before the impact of collateral that are deductible for the purpose of a borrower or a group of borrowers liability. (3) Credit risk of off-balance sheet financial instruments as computed for the purpose of borrower indebtedness limitations. (4) Governments, official institutions and central banks. (5) Portugal, Ireland, Italy, Greece and Spain. B. Information regarding countries the amount of exposure in respect of each amounts to between 0.75% and 1% of total consolidated assets or between 15% and 20% the equity, whichever is lower. C. Information regarding balance sheet exposure to foreign countries having liquidity problems, for the period of three months ended March 31, 2015 As of March 31, 2015, the Bank had no such exposure. The Country March 31, 2015 Balance sheet exposure Across the border balance sheet exposure To governments To banks To others In NIS millions 1. Information regarding balance-sheet exposure to foreign countries As of March 31, 2015 the Bank had no such exposure. 2. Information regarding balance-sheet exposures that have undergone restructuring As of March 31, 2015 the Bank had no such exposure. CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review Balance sheet exposure to local resident customers of extensions of the banking corporation in a foreign country Balance sheet exposure March 31, 2015 Off-balance sheet exposure Across the border balance sheet exposure Balance sheet exposure before deduction of local liabilities Deduction in respect of local liabilities Net balance sheet exposure after deduction of local liabilities Total balance sheet exposure Balance sheet problematic credit risk Impaired debts Total offbalance sheet exposure Of which off-balance sheet problematic credit risk Due up to one year Due over one year The item \"Total LDC countries\" includes the total exposure to countries defined as less developed countries (LDC) in Proper Banking Management Directive No. 315 regarding \"Supplementary provision for doubtful debts\". Balance sheet exposure to a foreign country includes across the border balance sheet exposure and balance sheet exposure of overseas extensions of the banking corporation to local resident customers; across the border balance sheet exposure includes balance sheet exposure of the banking corporation offices in Israel to residents of a foreign country and the balance sheet exposure of the overseas extensions of the banking corporation to customers who are not residents of the country in which the extension is located. Balance sheet exposure of extensions of the banking corporations in a foreign country to local resident customers includes the balance sheet exposure of extensions of the banking corporation in that foreign country to residents of that country, net of the extensions liabilities (the deduction is performed up to the exposure amount). In NIS millions Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"C\" - EXPOSURE TO FOREIGN COUNTRIES - CONSOLIDATED() (CONTINUED) A. Information regarding the total exposure to foreign countries and to countries where the total exposure to each country amounts to over 1% of total consolidated assets or over 20% of the Bank's equity, the lower of the two. Of which - Total exposure to LDC countries Notes: (1) Based on the final risk, net of the effect of guarantees, liquid collateral and credit derivatives. (2) Balance sheet and off-balance sheet credit risk, commercial criticized exposure and impaired debts are presented before the impact of the allowance for credit losses and before the impact of collaterals that are deductible for the purpose of a borrower or a group of borrowers liability. (3) Credit risk of off-balance sheet financial instruments as computed for the purpose of borrower indebtedness limitations. (4) Governments, official institutions and central banks. (5) Portugal, Ireland, Italy, Greece and Spain. (6) Reclassified - classification between countries (7) Reclassified - classification of balance to \"local residents\", following classification in a subsidiary. Balance sheet exposure Across the border balance sheet exposure The Country To governments To banks To others In NIS millions The Country March 31, 2014 Balance sheet exposure Across the border balance sheet exposure To governments In NIS millions To banks To others CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review March 31, 2014 Balance sheet exposure Balance sheet exposure to local resident customers of extensions of a banking corporation in a foreign country Balance sheet exposure before deduction of local liabilities Deduction in respect of local liabilities Net balance sheet exposure after deduction of local liabilities Total balance sheet exposure Balance sheet commercial criticized exposure Impaired debts In NIS millions Off-balance sheet exposure Across the border balance sheet exposure Total offbalance sheet exposure Of which off-balance sheet commercial criticized exposure Due up to one year Due over one year 2 The item \"Total LDC countries\" includes the total exposure to countries defined as less developed countries (LDC) in Proper Banking Management Directive No. 315 regarding \"Supplementary provision for doubtful debts\". Balance sheet exposure to a foreign country includes across the border balance sheet exposure and balance sheet exposure of overseas extensions of the banking corporation to local resident customers; across the border balance sheet exposure includes balance sheet exposure of the banking corporation offices in Israel to residents of a foreign country and the balance sheet exposure of the overseas extensions of the banking corporation to customers who are not residents of the country in which the extension is located. Balance sheet exposure of extensions of the banking corporations in a foreign country to local resident customers includes the balance sheet exposure of extensions of the banking corporation in that foreign country to residents of that country, net of the extensions liabilities (the deduction is performed up to the exposure amount) Balance sheet exposure Off-balance sheet exposure Balance sheet exposure to local resident customers of extensions of the banking corporation in a foreign country Across the border balance sheet exposure Balance sheet exposure before deduction of local liabilities Of which off-balance sheet problematic credit risk Due up to one year Due over one year Deduction in respect of local liabilities Net balance sheet exposure after deduction of local liabilities Total balance sheet exposure Balance sheet problematic credit risk Impaired debts Total offbalance sheet exposure In NIS millions Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"C\" - EXPOSURE TO FOREIGN COUNTRIES - CONSOLIDATED() (CONTINUED) As of March 31, 2014, and December 31, 2014, the Bank had no such exposure. B. Information regarding countries the amount of exposure in respect of each amounts to between 0.75% and 1% of total consolidated assets or between 15% and 20% of shareholders' equity, whichever is lower. C. Information regarding exposure to foreign countries having liquidity problems for the period of three months ended March 31, 2014 and for the year ended December 31, 2014 1,139 604 14,822 13,784 11,257 3,302 16,541 1,919 3,247 2,351 8,841 5,825 2,318 85,950 21,817 150,264 42,497 3,142 20,613 34 158 6,767 516 9,007 372 853 6,577 1,119 201 11,552 8,210 2,273 3,431 106 42,860 2,324 45,290 9,287 1,998 56,575 230,594 Total() Credit Performance Rating 1,104 603 13,572 13,241 10,661 3,231 1,585 15,463 3,014 1,287 8,576 5,625 2,286 80,248 21,257 143,400 41,895 3,142 20,613 167,155 34 158 6,102 516 8,594 6,479 372 853 1,101 173 11,424 8,210 2,258 3,387 100 41,451 2,323 43,874 9,287 1,998 55,159 222,314 Problematic Of which: 24,232 4,750 24 22,687 1,443 1,604 1,289 102 99 4,546 730 172 794 516 8,916 58,381 20,270 20,796 99,447 1,205 147,405 385 149,570 101,037 34 9 1,138 604 14,517 (6)13,754 11,147 2,726 16,390 1,902 3,162 2,151 7,546 5,806 2,313 83,156 21,817 42,432 1,524 641 34 138 6,673 4,029 465 6,981 353 853 6,563 1,095 176 1,935 - 2,812 - 2,235 1,362 106 31,726 2,300 34,132 4,898 24 Total Debts Problematic in NIS millions 860 359 9,874 5,376 9,216 1,782 1,596 13,321 2,515 1,593 6,269 3,960 1,660 14 24 1 987 475 494 10 229 1,002 216 724 257 110 15 376 334 - - - - 147 227 333 - 97 - 18 27 96 - - 959 6 1 966 100 29 14 24 1 983 475 486 11 230 1,002 211 724 258 109 15 377 334 - - - - 147 227 333 - 97 - 7 27 96 - - 948 6 1 955 - - 29,006 39,054 188,624 130,043 4,544 5,254 5,254 1,095 6,349 955 6,194 5,240 5,240 4,529 14 - 376 172 430 9 535 221 165 551 123 74 2,678 8 - 87 - 2,765 - 2,765 - - 22 227 121 - - - 6 26 95 - - - - 497 - 497 - - 497 3,262 Impaired 4 Periodic Credit Loss Expenses (2) (1) (28) (15) (2) - 8 (2) 19 68 (8) 6 42 (1) - 39 (3) - - 39 (1) - 20 (11) (29) - 15 1 - 2 (2) - - (1) - (4) (5) (2) - (7) 32 Credit Losses Net Accounting Write-Offs Recognized during the Period - - 5 8 1 - 7 1 15 14 (19) (2) - 30 91 10 131 - - 131 - - - - (7) 1 - - - - (2) - - - - (8) (1) (9) - - (9) 122 Balance of Allowance for Credit Losses 18 - 280 127 138 4 321 13 56 134 86 64 8 1,249 172 381 2 1,802 - 1,804 1 - 61 3 74 75 1 7 9 25 29 - 23 13 321 - 14 335 1 - 336 2,140 Footnotes: (1) Balance Sheet and Off-Balance Sheet Credit Risk, including in respect of derivative instruments. Including: Debts, bonds, securities borrowed or purchased under resale agreements, assets in respect of derivative instruments, credit risk in respect of off-balance sheet financial instruments, as calculated for single borrower liability limitation, and guarantees and liabilities on account of clients in an amount of NIS 130,043, 35,703, 387, 5,159, 59,302 million, respectively. (2) Credit to the Public, Credit to Governments, deposits with banks and other debts, excluding investments in bonds and securities borrowed or purchased under resale agreements. (3) Credit risk in respect of off-balance sheet financial instruments, as calculated for single borrower liability limitation, excluding in respect of derivative instruments. (4) Including in respect of off-balance sheet credit instruments (stated in the balance sheet under \"Other liabilities\"). (5) Balance sheet and off-balance sheet credit risk, which is impaired, substandard or under special mention, including in respect of housing loans, in respect of which an allowance is made according to the extent of arrears, and housing loans in respect of which no allowance is made according to the extent of arrears, and are in arrears of 90 days or more. (6) Including acquisition groups in an amount of NIS 469 millions. (7) Including mortgage backed securities in the amount of NIS 5,164 million, issued by GNMA and in the amount of NIS 5,088 million, issued by FNMA and FHLMC. (8) Including mainly municipal bonds and bonds of states in the U.S. (9) Including credit facilities guaranteed by banks outside the Group in the amount of NIS 5,495 million. (10) Credit risk, the credit rating thereof at date of reporting matches the credit rating for the granting of new credit in accordance with the Bank's policy of the Bank. Total Credit Risk Debts and off-balance sheet Credit Risk (excluding Derivatives) March 31, 2015 - 1 2,899 2,777 1 1 - - 1 Israel Discount Bank Limited and its Subsidiaries SCHEDULE \"D\" - OVERALL CREDIT RISK IN RESPECT OF THE PUBLIC CLASSIFIED BY ECONOMIC SECTORS - CONSOLIDATED (CONTINUED) Excluding balances classified as assets and liabilities held for sale - see Note 18 Footnotes: (1) Balance Sheet and Off-Balance Sheet Credit Risk, including in respect of derivative instruments. Including: Debts, investments in bonds, securities borrowed or purchased under resale agreements, assets in respect of derivative instruments, and credit risk in respect of off-balance sheet financial instruments, as calculated for single borrower liability limitation, of NIS 124,489, 37,163, 624, 3,575, 54,301 million, respectively. (2) Credit to the Public, Credit to Governments, deposits with banks and other debts, excluding investments in bonds and securities borrowed or purchased under resale agreements. (3) Credit risk in respect of off-balance sheet financial instruments, as calculated for single borrower liability limitation, excluding in respect of derivative instruments. (4) Including in respect of off-balance sheet credit instruments (stated in the balance sheet under \"Other liabilities\"). (5) Balance sheet and off-balance sheet credit risk in respect of the public, which is impaired, substandard or under special mention, including in respect of housing loans, in respect of which a allowance is made according to the period in arrears, and housing loans in respect of which no allowance is made according to the period in arrears, and are in arrears of 90 days or more. (6) Includes problematic credit risk due to certain bonds issued by banking holding corporations, held by a subsidiary in an amount of NIS 138 million. (7) Including acquisition groups in an amount of NIS 741 millions. (8) Including mortgage backed securities in the amount of NIS 2,581 million, issued by GNMA and in the amount of NIS 5,736 million, issued by FNMA and FHLMC. (9) Including mainly municipal bonds and U.S. Government bonds. (10) Including credit facilities guaranteed by banks outside the Group in the amount of NIS 4,535 million. Lending Activity in Israel Agriculture Total Credit Risk Total Problematic March 31, 2014 Debts and off-balance sheet Credit Risk (excluding Derivatives) Credit Losses Total Of which: Debts Problematic Impaired Periodic Credit Loss Expenses Net Accounting Write-Offs Recognized during the Period Balance of Allowance for Credit Losses in NIS millions CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review Footnotes: (1) (2) (3) (4) (5) SCHEDULE \"D\" - OVERALL CREDIT RISK IN RESPECT OF THE PUBLIC CLASSIFIED BY ECONOMIC SECTORS - CONSOLIDATED (CONTINUED) Lending Activity in Israel Agriculture Mining &amp; Quarrying Industry Construction and Real Estate - Construction Construction and Real Estate - Real Estate Activity Electricity and Water Commerce Hotels, Hotel Services and Food Transportation and Storage Communication and Computer Services Financial Services Other Business Services Public and Community Services Total Commercial Private Individuals - Housing Loans Private Individuals - Other Total Public Banks in Israel Israeli Government Total Lending Activity in Israel Lending Activity Outside of Israel Agriculture Mining &amp; Quarrying Industry Construction and Real Estate - Construction Construction and Real Estate - Real Estate Activity Electricity and Water Commerce Hotels, Hotel Services and Food Transportation and Storage Communication and Computer Services Financial Services Of which: Federal agencies in the U.S. Other Business Services Public and Community Services Total Commercial Private Individuals - Housing Loans Private Individuals - Other Total Public Banks Outside of Israel Governments Outside of Israel Total Lending Activity Outside of Israel TOTAL Excluding balances classified as assets and liabilities held for sale - see Note 18 86,015 57,138 229,474 45,045 42,696 172,336 149,090 Total Credit Performance Rating Problematic 221,565 - 5,698 5,698 4,896 1,125 420 15,511 14,443 11,064 3,725 16,579 1,902 3,072 2,242 8,435 5,229 2,268 21,873 41,202 2,067 21,179 93 55 7,150 694 8,420 128 6,448 876 1,215 300 11,985 8,612 2,219 3,113 93 2,256 10,047 2,046 27 1 1,178 460 578 11 1,075 241 180 708 314 107 16 458 344 - - - - 91 264 318 - 27 - 15 89 129 - 13 - 946 6 1 953 122 30 1,084 421 14,249 13,908 10,468 3,682 15,436 1,547 2,858 1,529 8,114 5,032 2,236 80,564 21,281 40,443 142,288 2,067 21,179 165,534 93 55 7,059 429 7,988 128 6,424 876 1,200 209 11,821 8,612 2,204 3,112 41,598 87 2,253 43,938 10,047 2,046 56,031 1,105 6,803 4,879 3 Debts and off-balance sheet Credit Risk (excluding Derivatives) 946 6,626 27 1 1,177 459 570 11 1,075 240 174 707 313 107 17 458 344 - - - - 91 264 318 - 26 - 7 90 129 - 14 - 939 6 1 946 - - 5,680 Problematic Of which: Debts in NIS millions 884 319 9,884 5,608 9,310 2,092 13,381 1,594 2,382 1,490 5,666 3,474 1,610 57,694 20,308 20,350 98,352 604 5,680 4,878 23 29,202 115 4,422 644 6,508 93 9 4,142 1,123 415 15,185 (6)14,424 11,007 3,181 16,458 1,886 3,017 2,058 7,062 5,194 2,262 83,272 21,873 41,141 146,286 648 1,770 1,510 148,704 100,466 93 51 7,044 693 8,356 116 6,442 876 1,193 820 928 203 275 2,852 1,927 - 1,507 - 2,193 1,098 1,024 22,342 90 31,282 93 2,248 33,623 1,388 23,820 5,359 5,752 23 39,398 2,690 188,102 129,668 2,815 15 1 463 125 471 9 298 231 126 117 169 63 8 2,096 - 82 2,178 - - 2,178 - - - 264 152 - - - 7 87 128 - - - 638 - 1 639 - - 639 2,817 3 Total 3 Impaired Credit Losses Periodic Credit Loss Expenses (3) (6) 1 (56) 24 1 30 2 29 (20) (7) 5 (3) (3) 19 102 118 1 - 119 (2) (1) (11) 54 (12) (1) (106) (3) (3) 133 (5) - (15) 5 33 - 11 44 1 - 45 164 - Net accounting write-off for the year (1) (4) 97 (38) 30 1 12 3 10 (18) 1 11 - 104 11 49 164 - - 164 - - - 94 (31) - (110) (1) 29 123 16 - (3) - 117 - 7 124 - - 124 288 - Balance of allowance for credit loss 20 1 311 143 141 6 317 16 52 36 124 60 9 1,236 262 394 1,892 2 - 1,894 1 - 44 11 97 1 57 5 10 23 29 - 19 12 309 1 17 327 3 - 330 2,224 7 Balance Sheet and Off-Balance Sheet Credit Risk, including in respect of derivative instruments. Including: Debts, bonds, securities borrowed or purchased under resale agreements, assets in respect of derivative instruments, credit risk in respect of off-balance sheet financial instruments, as calculated for single borrower liability limitation, and guarantees and liabilities on account of clients in an amount of NIS 129,668, 35,661, 466, 4,596, 59,083 million, respectively. Credit to the Public, Credit to Governments, deposits with banks and other debts, excluding investments in bonds and securities borrowed or purchased under resale agreements. Credit risk in respect of off-balance sheet financial instruments, as calculated for single borrower liability limitation, excluding in respect of derivative instruments. Including in respect of off-balance sheet credit instruments (stated in the balance sheet under \"Other liabilities\"). Balance sheet and off-balance sheet credit risk, which is impaired, substandard or under special mention, including in respect of housing loans, in respect of which an allowance is made according to the extent of arrears, and housing loans in respect of which no allowance is made according to the extent of arrears, and are in arrears of 90 days or more. Including acquisition groups in an amount of NIS 480 millions. Including mortgage backed securities in the amount of NIS 3,976 million, issued by GNMA and in the amount of NIS 4,636 million, issued by FNMA and FHLMC. Including mainly municipal bonds and bonds of states in the U.S. Including credit facilities guaranteed by banks outside the Group in the amount of NIS 4,798 million. December 31, 2014 Total Credit Risk (6) (7) (8) (9) (10) Credit risk, the credit rating thereof at date of reporting matches the credit rating for the granting of new credit in accordance with the Bank's policy of the Bank. (11) Reclassified - improving classification in different sectors. Israel Discount Bank Limited and its Subsidiaries I, Lilach Asher-Topilsky, certify that: 1. I have reviewed the quarterly report of Israel Discount Bank Ltd. (hereinafter: \"the Bank\") as of March 31, 2015 (hereinafter: \"the Report\"). 2. Based on my knowledge, the Report does not contain any untrue statement of a material fact or omit to state a material fact necessary to make the statements made therein, in light of the circumstances under which such statements were made, not misleading with respect to the period covered by the Report. 3. Based on my knowledge, the interim financial statements, and other financial information included in the Report, fairly present in all material respects the financial condition, results of operations (including the comprehensive income), changes in equity and cash flows of the Bank as of, and for, the periods presented in this report. 4. Other officers of the Bank providing this certification and I are responsible for establishing and maintaining disclosure controls and procedures and to the internal control of the Bank over financial reporting (as defined in the public reporting instructions regarding \"Directors' Report\"), and have: Designed such disclosure controls and procedures, or caused such disclosure controls and procedures to be designed under our supervision, to ensure that material information relating to the Bank, including its consolidated subsidiaries, is made known to us by others within the Bank and those entities, particularly during the period of preparing this report; We established such internal control over financial reporting, or caused such internal control over financial reporting to be designed under our supervision, to provide reasonable assurance regarding the reliability of financial reporting and the preparation of financial statements for external purposes in accordance with accepted accounting principles and directives and guidelines of the Supervisor of Banks; Evaluated the effectiveness of the Bank's disclosure controls and procedures and presented in the Report our conclusions about the effectiveness of the disclosure controls and procedures, as of the end of the period covered by the Report based on such evaluation; Disclosed in the Report any change in the Bank's internal control over financial reporting that occurred during this quarter that has materially affected, or is reasonably likely to materially affect, the Bank's internal control over financial reporting; and 5. The other officers of the Bank providing this certification and I have disclosed, based on our most recent evaluation of internal control over financial reporting, to the Bank's Auditors, to the Board of Directors and to the Audit Committee of the Board of Directors: All significant deficiencies and material weaknesses in the design or operation of internal control over financial reporting which are reasonably likely to adversely affect the Bank's ability to record, process, summarize and report financial information; and Any fraud, whether or not material, that involves management or other employees who have a significant role in the Bank's internal control over financial reporting. (a) (b) (c) (d) (a) (b) Nothing in that stated above derogates my responsibility or the responsibility of any other person under any law. CERTIFICATION May 20, 2015 Ms. Lilach Asher-Topilsky President &amp; Chief Executive Officer CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Management Review CERTIFICATION I, Joseph Beressi, certify that: 1. I have reviewed the quarterly report of Israel Discount Bank Ltd. (hereinafter: \"the Bank\") as of March 31, 2015 (hereinafter: \"the Report\"). 2. Based on my knowledge, the Report does not contain any untrue statement of a material fact or omit to state a material fact necessary to make the statements made therein, in light of the circumstances under which such statements were made, not misleading with respect to the period covered by the Report. 3. Based on my knowledge, the interim financial statements, and other financial information included in the Report, fairly present in all material respects the financial condition, results of operations (including the comprehensive income), changes in equity and cash flows of the Bank as of, and for, the periods presented in this report. 4. Other officers of the Bank providing this certification and I are responsible for establishing and maintaining disclosure controls and procedures and to the internal control of the Bank over financial reporting (as defined in the public reporting instructions regarding \"Directors' Report\"), and have: Designed such disclosure controls and procedures, or caused such disclosure controls and procedures to be designed under our supervision, to ensure that material information relating to the Bank, including its consolidated subsidiaries, is made known to us by others within the Bank and those entities, particularly during the period of preparing this report; We established such internal control over financial reporting, or caused such internal control over financial reporting to be designed under our supervision, to provide reasonable assurance regarding the reliability of financial reporting and the preparation of financial statements for external purposes in accordance with accepted accounting principles and directives and guidelines of the Supervisor of Banks; Evaluated the effectiveness of the Bank's disclosure controls and procedures and presented in the Report our conclusions about the effectiveness of the disclosure controls and procedures, as of the end of the period covered by the Report based on such evaluation; Disclosed in the Report any change in the Bank's internal control over financial reporting that occurred during this quarter that has materially affected, or is reasonably likely to materially affect, the Bank's internal control over financial reporting; and 5. The other officers of the Bank providing this certification and I have disclosed, based on our most recent evaluation of internal control over financial reporting, to the Bank's Auditors, to the Board of Directors and to the Audit Committee of the Board of Directors: All significant deficiencies and material weaknesses in the design or operation of internal control over financial reporting which are reasonably likely to adversely affect the Bank's ability to record, process, summarize and report financial information; and Any fraud, whether or not material, that involves management or other employees who have a significant role in the Bank's internal control over financial reporting. (a) (b) (c) (d) (a) (b) Nothing in that stated above derogates my responsibility or the responsibility of any other person under any law. May 20, 2015 Joseph Beressi Senior Executive Vice President Chief Accountant REVIEW REPORT OF THE INDEPENDENT AUDITORS TO THE SHAREHOLDERS OF ISRAEL DISCOUNT BANK LTD. We have reviewed the accompanying financial information of Israel Discount Bank Ltd. and its subsidiaries (hereinafter: \"the Bank\") comprising of the condensed consolidated interim balance sheet as at March 31, 2015 and the related condensed consolidated interim statements of income, comprehensive income, changes in equity and cash flows for the three months period then ended. The Board of Directors and management are responsible for the preparation and presentation of the financial data for this interim period in accordance with Israeli GAAP regarding financial reporting for this interim period and in accordance with the guidelines and directives of the Supervisor of Banks. Our responsibility is to express a conclusion on the financial information for these interim periods based on our review. INTRODUCTION We have conducted our review in accordance with Standard on Review Engagements 1, \"Review of Interim Financial Information Performed by the Independent Auditor of the Entity\" of the Institute of Certified Public Accountants in Israel, and a review standard applied in the review of banking institutions according to the guidelines and directives of the Supervisor of Banks. A review of interim financial information consists of making inquiries, primarily of persons responsible for financial and accounting matters, and applying analytical and other review procedures. A review is substantially less in scope than an audit conducted in accordance with generally accepted auditing standards in Israel and consequently does not enable us to obtain assurance that we would become aware of all significant matters that might be identified in an audit. Accordingly, we do not express an audit opinion. SCOPE OF REVIEW Based on our review, nothing has come to our attention that causes us to believe that the accompanying financial information was not prepared, in all material respects, in accordance with Israeli GAAP regarding financial reporting for interim periods and in accordance with the instructions and directives of the Supervisor of Banks. CONCLUSION Without qualifying our above conclusion, we call attention to the Note 8 B items 4.9 and 5 concerning motion to approve certain lawsuits as class action suits and with regard to other claims against the Bank and investee companies and to that stated in Note 16 b (2) with respect to the notice given by the State Attorney Office, according to which the State Attorney is considering the filing of an indictment against ICC. According to the said Note, at this stage, the Managements of ICC and the Bank are unable to assess the results of the proceedings that would be instituted, if at all, and their consequences on ICC. EMPHASIS OF A MATTER Somekh Chaikin Certified Public Accountants (Isr. ) May 20, 2015 Somekh Chaikin, an Israeli partnership and a member firm of the KPMG network of independent member firms affiliated with KPMG International Cooperative (\"KPMG International\"), a Swiss entity Ziv Haft Certified Public Accountants (Isr. ) Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 CONDENSED CONSOLIDATED BALANCE SHEET Footnotes: (1) Of which NIS 176 million, NIS 177 million and NIS 170 million, as of March 31, 2015, March 31, 2014 and December 31, 2014, respectively, allowance for credit losses in respect of off-balance sheet credit instruments. (2) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 E (1). The notes to the condensed financial statements form an integral part thereof. Joseph Beressi Senior Executive Vice President, Chief Accountant Ms. Lilach Asher-Topilsky President &amp; Chief Executive Officer Dr. Yossi Bachar Chairman of the Board of Directors May 20, 2015 Unaudited Audited CONDENSED CONSOLIDATED STATEMENT OF INCOME Israel Discount Bank Limited and its Subsidiaries Consolidated Unaudited Notes For the three months ended March 31, 2015 Audited For the year ended December 31, 2014 2014 in NIS millions Footnotes: (1) For details regarding the provision for impairment in value of the investment in FIBI, see Note 6 D (3) to the financial statements of 2014 (p. 417). (2) For details as to the elimination of the Bank's share in the reserves of FIBI, previously recognized in other comprehensive income, see Note 6 D (4) to the financial statements of 2014 (p. 418). (3) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 (E) 1. The notes to the condensed financial statements form an integral part thereof. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 CONDENSED CONSOLIDATED STATEMENT OF COMPREHENSIVE INCOME() Footnotes: (1) See Note 14. (2) Reflects mostly adjustments in respect of actuarial assessments as of the end of the year of defined benefits pension plans and amortization of amounts recorded in the past in other comprehensive income. (3) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 E (1). The notes to the condensed financial statements are an integral part thereof. 2014 Unaudited For the three months ended March 31, 2015 Audited For the year ended December 31, 2014 in NIS millions 2013 (d) The Bank's updated accounting policy (c) Preparations by the Bank. The Bank is preparing for the implementation of the new policy. The following principal subjects are included in the said framework: 1. Definition of a process for the determination of the discount rate on the basis of Israeli CPI linked government bonds, in accordance with the average period to maturity of the estimated indebtedness, with the addition of an average spread of U.S. corporate bonds having a rating of AA and above. The process includes aspects of control of the appropriateness of choosing the bonds, validation of the resultant discount rate and examination of the reasonableness of changes in the discount rate. 2. Determination of a mechanism for defining the forecasted return on assets of the plan. The parameters used in determining the forecasted return are mostly the actual and past composition of the plan's assets, possible changes in the composition of the assets in accordance with the investment policy, as defined, past yields of the fund, the yield of the assets and its weight in the total portfolio, after deduction of operating expenses and commissions. The mechanism also includes examination of the need for the updating of the forecasted return during the reported period. 3. The change in the method of measurement of the Bank's liability in respect of benefits to retirees in relation to the population of active employees, and the transition to computing the liability on the date on which it becomes certain. This, instead of a computation assuming retirement rates prior to the age of 67 and a pro-rata charge in accordance therewith. 4. Definition of the work process for the treatment of actuarial profits and losses, differentiating in the framework thereof between: - The actuarial loss arising from the change in the discount rate as of January 1, 2013; Actuarial profits that would arise from changes in the discount rate subsequent to the date of initial implementation; - - Actuarial losses arising from changes in the discount rate and from other components subsequent to the date of initial - Actuarial profits/losses arising from the difference between the forecasted return on the plan's assets and the return actually The process includes reference to reasonableness of the resultant actuarial profit/loss, the accounting records, the mechanism for the amortization of the profits/losses and the determination of the amortization period. Post retirement benefits - defined deposits plans The Bank recognizes amounts relating to pension and severance plans and other post retirement plans on the basis of computations that include actuarial and other assumptions, including: discount rates, mortality rates, early retirement rates, forecasted long-term return rates on assets of the plan, remuneration increases and employee turnover; The Bank reviews its assumptions on a periodic basis and updates these assumptions where required. As a general rule, the actuarial estimates are made once a year, unless material changes occur in the actuarial assumptions in the interim period, which materially impact the actuarial liabilities or the assets of the plan; Changes in assumptions are in general recognized, subject to the instructions stated above, firstly in accumulated other comprehensive income, and are amortized to the statement of income in following periods; The liability is accumulated over the relevant period determined in accordance with the rules detailed in item 715 of the codification; The Bank implements the guidelines issued by the Supervisor of Banks with respect to internal control over the financial reporting process in the matter of employee rights, including with respect to examining the \"liability in substance\" of the Bank to grant its employees benefits comprising increased severance pay and/or early pension. A defined deposit plan is a plan according to which the Bank deposits fixed amounts with a third party, thereby avoiding any legal or inferred liability for additional payments. The Bank's commitment to deposit in the defined deposit plan, are recognized as an expense in the statement of income in the periods during which the employees have provided the relevant services. - - - - - - implementation; earned. - Post retirement benefits - pension, severance pay and other benefits - defined benefits plans - 1. ACCOUNTING POLICIES (CONTINUED) - Other long-term benefits to active employees: long-service (jubilee) awards Israel Discount Bank Limited and its Subsidiaries (e) The accounting policy applied in the past - The liability accrues over the period entitling to the benefit; For the purpose of computing the liability, the rates of discount and actuarial assumptions are taken into consideration; - - The whole cost component of the benefit for the period, including actuarial profits and losses, are recognized immediately in - The liability in respect of vacation pay is measured on a current basis, without the use of discount rates and actuarial assumptions; The Bank does not accrue a liability for sick-leave that may materialize during the employee's current service. - Retirement plan 2014. Within the Bank's strategic plan for the years 2015-2019, approved in August 2014, employees have been offered early retirement plan which, in view of its characteristics and circumstances, comprised a structural change. The costs that have been involved in its implementation were handled accordingly. In accordance with instructions of the Supervisor of Banks, the rate of discount used in the actuarial computations was 4%; Actuarial profits and losses were immediately recognized in the statement of income; In accordance with guidelines of the Supervisor of Banks regarding internal control over financial reporting in the matter of employee rights, the liability for severance pay had been presented either in the amount of the liability as computed on an actuarial basis, taking into account the additional cost that might arise in respect of these benefits, as stated, or in the amount of the liability computed as a multiplication of the monthly salary of the employee by the number of years of service, in accordance with Opinion No. 20 of the Institute of Certified Public Accountants in Israel, which ever was the higher amount; - Additional information regarding the accounting policy to be applied by the Bank in the matter of employee rights is presented in It is required to include in the report for the first quarter of 2015, extensive disclosure, in the format of the annual report, in addition to the disclosure required in a regular quarterly report, in respect of restated prior periods' data, mainly: - Income statement amounts in respect of the years 2013 and 2014; Outstanding balance sheet amounts and assumptions used as of December 31, 2013 and December 31, 2014. Notwithstanding the above, a bank is permitted not to include the required disclosure regarding the assets of a plan in accordance with the Reporting to the Public Directives as of December 31, 2013, and the required disclosure as to the \"movement in fair value of assets of the plan, which are measured on the basis of significant unobservable inputs (level 3)\" for the years 2013 and 2014. Furthermore, for the purpose of presentation of the comparative data for the years 2013 and 2014, a bank is permitted, for practical reasons, to use the actual rates of return in those years instead of determining forecasted rates of return. - the statement of income. Absence from work entitling compensation - vacation and sick leave - - - - Notes 1 and 16 to the financial statements as of December 31, 2014. (f) Disclosure requirements in interim financial statements in the year 2015 - Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 1. ACCOUNTING POLICIES (CONTINUED) (g) Effect of the initial Implementation of the new rules as of March 31, 2014 and December 31, 2014 Following are data regarding the effect of implementation of the new directive as of March 31, 2014 and December 31, 2014: December 31,2014 In accordance with the previous reporting directives (audited) Effect of implementation of the new rules In accordance with the new rules regarding employee rights In accordance with the previous reporting directives Unaudited March 31,2014 Effect of implementation of the new rules In accordance with the new rules regarding employee rights In accordance with the prior reporting instructions Unaudited For the three months ended March 31, 2014 The effect of implementation of the new rules in NIS millions In accordance with the new rules in the matter of employee rights 1. ACCOUNTING POLICIES (CONTINUED) Israel Discount Bank Limited and its Subsidiaries In accordance with the prior reporting instructions (Audited) For the year ended December 31, 2014 Effect of the implementation of the instruction regarding the measurement of interest income in NIS millions In accordance with the instruction regarding the measurement of interest income 2. Reporting according to U.S. generally accepted accounting principles in the matter of Distinguishing Liabilities from Equity. On October 6, 2014 the Supervisor of Banks published an instruction in the matter of reporting according to U.S. generally accepted accounting principles regarding distinguishing liabilities from equity. This, in continuation to the policy of the Supervisor of Banks adopting, in cases of material issues, the financial reporting layout applying to banks in the United States. According to the instruction, it is required to apply the U.S. generally accepted accounting principles in the matter of classification of financial instruments as equity or liabilities, including hybrid instruments. For this purpose, it would be required to apply, among other things the presentation, measurement and disclosure principles determined within the framework of the following topics in the codification: - - - In addition, in applying the differentiation between liabilities and capital, it is required to refer to the reporting to the public instructions as regards embedded instruments. Concurrently, the Supervisor of Banks published an FQA file in this matter, within the framework of which, it has been clarified that existing debt instruments having a conditional conversion component into shares (which under the Basel II instructions is included in Common equity tier I, and according to the transitional instructions agrees with the definition of a hybrid capital instrument, or which is included as a regulatory capital component under the Basel III Instructions) are to be classified as a liability measured according to amortized cost, without separating the embedded derivative. The Bank implements the said rules as from January 1, 2015. The implementation of the instruction did not have material effect. Statement of Income Other income Topic 480 regarding \"Distinguishing Liabilities From Equity\"; Topic 470-20 regarding \"Debt with Conversion and Other Options\"; and Topic 505-30 regarding \"Treasury stock\". Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 1. ACCOUNTING POLICIES (CONTINUED) F. NEW ACCOUNTING STANDARDS AND NEW DIRECTIVES OF THE SUPERVISOR OF BANKS IN THE PERIOD PRIOR TO THEIR - Full disclosure according to the new rules is required as from the financial report for the first quarter of 2016, excluding disclosure of the 1. Regulatory operating segments. An amendment to the reporting to the public instructions in the matter of regulatory operating segments was published on November 6, 2014. The circular is intended to allow a banking corporation to report operating segment data in accordance with a uniform and comparable format, as determined by the Supervisor of Banks. The main changes are: - Additional requirement for disclosure of \"regulatory operating segments\" was added, in accordance with the definition of the Supervisor of Banks. The format of disclosure regarding regulatory operating segments refers to the following segments: private banking, households, one-man and small businesses, medium businesses, large businesses, institutional bodies and financial management; New definitions were added clarifying which customers are to be included in each segment; A new requirement was added for a separate disclosure of the \"financial management\" segment. - - In addition, a FAQ file in the matter was distributed November 6, 2014. The circular determines that the disclosure in the matter of \"operating segments according to Management's approach\" shall be provided in accordance with Generally Accepted Accounting Principles at U.S. banks in the matter of operating segments (included in ASC 280), to the extent that a difference exists between Management's approach and operating segments according to guidelines of the Supervisor of Banks. The new rules apply as from the 2015 financial statements and thereafter, as follows: - The disclosure requirement in the 2015 statements shall apply to balance sheet data regarding supervisory operating segments, as defined in the new instructions. According to the new instructions, it is permitted not to provide disclosure of balance sheet comparative data for the supervisory operating segments, but to include comparative data in accordance with the Reporting to the Public Directives in effect prior to the letter taking effect. Furthermore, no disclosure is required for the financial management segment; financial management segment. The comparative data are to be adjusted retroactively. It is permitted to present in the financial statements for 2016 comparative data for one year only in respect of the Note regarding the supervisory operating segments. For the purpose of presentation of the comparative data it would be possible to rely on the classification of customers to supervisory operating segments as of January 1, 2016; Implementation in full of the guidelines of the circular is required as from the financial statements for the first quarter of 2017. - The Bank is of the opinion that the new instructions are not expected to have a material effect, save for the manner of presentation and disclosure. 2. Recognition of income from contracts with customers. A circular was published on January 14, 2015, in the matter of adoption of the update for accounting principles regarding income from contracts with customers. The circular updates the Reporting to the Public Directives in view of the publication of ASU 2014-09, which adopts in U.S. GAAP a new standard in the matter of income recognition. The Standard states that income shall be recognized by the implementation of a five stage model, which, among other things, include rules for the identification of the contract with the customer and for the determination of the transaction price, rules defining how the different components of the contract should be separated and the manner by which the total transaction price should be attributed to each separate and identified component. Furthermore, in accordance with the provisions of the Standard, income is to be recognized in respect of each identified component separately, and this in accordance with rules stated by the Standard with respect to the timing of recognition of the income - at a specific date or over a period of time. The amendments in the Directives will apply as from January 1, 2017. In accordance with the transitional instructions of the circular, upon initial implementation it would be possible to elect the retroactive application alternative by way of a restatement of the comparative data, or the alternative of retroactive application by way of recording the cumulative effect of the initial implementation of the Standard, while attributing the cumulative effect, to be recognized at date of the initial implementation, to the equity. IMPLEMENTATION 1. ACCOUNTING POLICIES (CONTINUED) Israel Discount Bank Limited and its Subsidiaries The new standard does not apply, among other things, to financial instruments and to contractual rights or liabilities under Chapter 310 of the Codification. The Bank has not yet examined the effect of the standard on its financial statements, and has not yet elected the alternative manner of implementation of the transitional instructions. 3. Guidelines in the matter of capitalization of in-house software development costs. The Bank implements International Accounting Standard No. 38 \"Intangible assets\" and the guidelines determined by \"SOP 98-1 - Accounting for the cost of Computer Software Developed or Obtained for Internal Use\". Due to the accounting complexity involved in the process of capitalizing in-house software development costs, and in view of the materiality of the amounts of software costs capitalized, the Supervisor of Banks has determined guidelines for the Bank in the matter of capitalization of software costs, as follows: - A minimum materiality level of between NIS 450 thousand and NIS 600 thousand, shall be determined for each software development project, in respect of which software development costs are capitalized. Any software development project, the total cost of which is lower than the determined materiality level, shall be recognized as an expense in the statement of income; The period of amortization of software development costs shall not exceed five years; - - Capitalization coefficients of lower that 1, shall be determined for hours worked, taking into consideration the potential for deviation in - The change in the accounting policy in accordance with the said guidelines shall be implemented starting with the interim financial statements as of June 30, 2015, by way of retroactive implementation, with a restatement of the comparative data. It is noted, that to the Bank's best knowledge, similar guidelines have been determined for other banking corporations in Israel. According to a preliminary assessment of the Bank, implementation of the requirements included in the draft, would reduce the equity by an amount of NIS 100 million after tax. It is noted that as from January 1, 2015, the Bank implements the contents of the draft with respect to current projects - as regards the materiality threshold with respect to capitalization coefficients and with respect to the level of employees whose costs are capitalized. computing the hours worked and the lack of economic efficiency; Limiting the level of employees, the costs of whom are to be capitalized. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 A. Composition (Continued) Unaudited March 31,2014 Book value Amortized cost Unrecognized gains from adjustment to fair value Unrecognized losses from adjustment to fair value In NIS millions Fair value (1) Held-to-maturity bonds Bonds and loans: Unaudited March 31,2014 Accumulated other comprehensive income Profits Losses Fair value Book value Amortized cost (in shares cost) In NIS millions (2) Available for sale securities 2. SECURITIES - CONSOLIDATED (CONTINUED) A. Composition (Continued) Israel Discount Bank Limited and its Subsidiaries Unaudited March 31,2014 Book value Amortized cost (in shares cost) Unrealized gains from adjustment to fair value Unrealized losses from adjustment to fair value In NIS millions Fair value Footnotes: (1) Fair value data based on market prices, does not necessarily reflect the price that may be obtained on the sale of securities in large volumes. (2) Including securities sold by overseas consolidated subsidiary under buy-back terms from held to maturity portfolio at a reduced cost of NIS832 million (approx. US$ 239 million) and from the available for sale portfolio with a market value of NIS 3,570 million (approx. US$ 1,024 million). (3) Included in \"Accumulated other comprehensive income\". (4) Including shares, the fair value of which is not readily available, stated at cost of NIS 755 million. (5) Recorded in the statement of income. (6) Including U.S. Government agencies and municipal bonds and bonds of states in the U.S.A, in amount of NIS 1,949 million (book value). (7) Including U.S. Government agencies, in amount of NIS 57 million (book value). (8) Excluding balances classified as assets and liabilities held for sale - see Note 18. *Loss amount lower then NIS 1 million. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. SECURITIES - CONSOLIDATED (CONTINUED) A. Composition (Continued) Book value Amortized cost Audited December 31, 2014 Unrecognized gains from adjustment to fair value In NIS millions Unrecognized losses from adjustment to fair value Fair value (1) Held-to-maturity bonds Bonds and loans: Audited December 31, 2014 Accumulated other comprehensive income Profits Losses Fair value Book value Amortized cost (in shares cost) In NIS millions 2. SECURITIES - CONSOLIDATED (CONTINUED) A. Composition (Continued) Israel Discount Bank Limited and its Subsidiaries Book value Amortized cost (in shares cost) Audited December 31, 2014 Unrealized gains from adjustment to fair value In NIS millions Unrealized losses from adjustment to fair value Fair value Footnotes:: (1) Fair value data based on market prices, does not necessarily reflect the price that may be obtained on the sale of securities in large volumes. (2) Including securities sold by overseas consolidated subsidiary under buy-back terms from held to maturity portfolio at a reduced cost of NIS 848 million (approx. US$ 218 million) and from the available for sale portfolio with a market value of NIS 3,810 million (approx. US$ 980 million). (3) Included in \"Accumulated other comprehensive income\". (4) Including shares, the fair value of which is not readily available, stated at cost of NIS 765 million. (5) Recorded in the statement of income. (6) Including U.S. Government agencies and municipal bonds and bonds of states in the U.S.A, in an amount of NIS 2,026 million (book value). (7) Including U.S. Government agencies, in an amount of NIS 67 million (book value). (8) Excluding balances classified as assets and liabilities held for sale - see Note 18. (9) Including investment in Tracking Funds in the amount of NIS 16 million. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. SECURITIES - CONSOLIDATED (CONTINUED) B. Amortized cost and unrealized losses, according to the length of the period and rate of impairment of held-to-maturity bonds which are in an unrealized loss position - consolidated Unaudited March 31,2014 Less than 12 months More than 12 months Unrecognized losses from adjustment to fair value Unrecognized losses from adjustment to fair value Audited December 31, 2014 Less than 12 months More than 12 months Unrecognized losses from adjustment to fair value Unrecognized losses from adjustment to fair value Unaudited March 31,2015 Less than 12 months Unrecognized losses from adjustment to fair value More than 12 months Unrecognized losses from adjustment to fair value 2. SECURITIES - CONSOLIDATED (CONTINUED) Israel Discount Bank Limited and its Subsidiaries C. Fair value and unrealized losses, according to the length of the period and rate of impairment of available-for-sale securities which are in an unrealized loss positionconsolidated Unaudited March 31,2014 Less than 12 months More than 12 months Unrealized losses Unrealized losses Unaudited March 31,2015 Less than 12 months More than 12 months Unrealized losses Unrealized losses Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. SECURITIES - CONSOLIDATED (CONTINUED) C. Fair value and unrealized losses, according to the length of the period and rate of impairment of available-for-sale securities which are in an unrealized loss positionconsolidated (Continued) D. The securities portfolio, as of March 31, 2015, includes investments in asset backed securities, primarily investment in mortgage - backed securities (MBS), which are held mainly by a consolidated subsidiary abroad. Details regarding the terms \"Mortgage-backed Securities - MBS\", \"Mortgage Pass - Through\" and \"Collateralized Mortgage Obligation - CMO\" were brought in Note 3 to the financial statements as of December 31, 2014. E. Most of the unrealized losses at March 31, 2015 are attributed to certain factors, including changes in market interest rate subsequent to acquisition, an increase in margins occurring in the credit market concerning similar types of securities, the impact of inactive markets and changes in the rating of securities. For debt securities, there are no securities past due or securities for which the Bank and/or it's relevant consolidated companies estimates that it is not probable that they will be able to collect all the amounts owed to them, pursuant to the investment contracts. Since the Bank and the relevant consolidated subsidiaries have the ability and intent to hold on to securities with unrealized losses until a market price recovery (which for debt securities, might not be until maturity), the Bank and the relevant consolidated subsidiaries do not view the impairment in value of these investments to be other than temporarily impaired at March 31, 2015. F. The securities portfolio of the Discount Group as at December 31, 2014, included a direct investment in bonds of the Federal Home Loan Bank (FHLB), Fannie Mae and Freddie Mac (hereinafter: \"the Federal Agencies\"), which were held by IDB New York, in an amount of US$25 million (NIS 97 million). The said bonds were redeemed in the course of the first quarter of 2015. G. Fair value presentation. The balances of securities as of March 31, 2015, March 31, 2014 and December 31, 2014, include securities amounting to *Loss amount lower then NIS 1 million. Audited December 31, 2014 Less than 12 months More than 12 months Unrealized losses Unrealized losses NIS 29,618 million, NIS 31,751 million and NIS 29,597 million, respectively, that are presented at fair value. 2. SECURITIES - CONSOLIDATED (CONTINUED) H. Additional details (consolidated) regarding mortgage and asset backed securities Israel Discount Bank Limited and its Subsidiaries Unaudited March 31,2015 Amortized cost Unrealized gains from adjustment to fair value Unrealized losses from adjustment to fair In NIS millions value Fair value *Loss amount lower then NIS 1 million. Footnote: (1) For available for sale securities-accumulated other comprehensive income. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. SECURITIES - CONSOLIDATED (CONTINUED) H. Additional details (consolidated) regarding mortgage and asset backed securities (continued) 1. Mortgage-backed securities (MBS): Available-for-sale securities Amortized cost Unaudited March 31,2014 Unrecognized gains from adjustment to fair value Unrecognized losses from adjustment to In NIS millions fair value Fair value *Loss amount lower then NIS 1 million. Footnote: (1) For available for sale securities-accumulated other comprehensive income. 2. SECURITIES - CONSOLIDATED (CONTINUED) H. Additional details (consolidated) regarding mortgage and asset backed securities (continued) Israel Discount Bank Limited and its Subsidiaries Audited December 31, 2014 Amortized cost Unrealized gains from adjustment to fair value Unrealized losses from adjustment to fair In NIS millions value Fair value *Loss amount lower then NIS 1 million. Footnote: (1) For available for sale securities-accumulated other comprehensive income. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. SECURITIES - CONSOLIDATED (CONTINUED) Additional details regarding mortgage and asset backed securities in unrealized loss position: I. Additional details (consolidated) regarding mortgage and asset backed securities Fair value Unaudited March 31, 2015 Less than 12 months 12 months and over Unrealized losses In NIS millions Fair value Unrealized losses *Loss amount lower then NIS 1 million. Israel Discount Bank Limited and its Subsidiaries 2. SECURITIES - CONSOLIDATED (CONTINUED) Additional details regarding mortgage and asset backed securities in unrealized loss position (continued): I. Additional details (Consolidated) regarding mortgage and asset backed securities (continued) Fair value Unaudited March 31, 2014 Less than 12 months 12 months and over Unrealized losses In NIS millions Fair value Unrealized losses *Loss amount lower then NIS 1 million. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. SECURITIES - CONSOLIDATED (CONTINUED) Additional details regarding mortgage and asset backed securities in unrealized loss position (continued): I. Additional details (Consolidated) regarding mortgage and asset backed securities (continued) Fair value Audited December 31, 2014 Less than 12 months 12 months and over Unrealized losses In NIS millions Fair value Unrealized losses *Loss amount lower then NIS 1 million. J. Information regarding impaired bonds - consolidated Recorded amount of non accruing interest income impaired bonds Unaudited 24 March 31, 2015 March 31, 2014 In NIS millions 22 Audited December 31, 2014 20 Israel Discount Bank Limited and its Subsidiaries 1. Change in the balance of the allowance for credit losses - Consolidated A. Debts and off-balance sheet credit instruments 2. Additional information regarding the mode of computing the allowance for credit losses in respect of debts and regarding the debts for which the allowance is computed - consolidated A. Debts and off-balance sheet credit instruments (continued) Includes the balance of allowance in excess of that required by the extent of arrears method, computed on a specific basis in amount of NIS 26 million, computed on a group basis in an amount of NIS 73 million. Including credit examined on a specific basis and found un-impaired and the allowance in respect of which was calculated on a group basis. Total Audited Credit to the public Private Commercial Individuals - Housing Loans Private Individuals - Other Loans Total Banks and Governments In NIS millions Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 B. Debts 1. Credit quality and arrears - consolidated 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) Lending Activity in Israel Public - Commercial Unaudited March 31, 2015 Problematic Non- problematic Unimpaired Impaired Total Unimpaired debts additional information In Arrears of 90 Days or More In Arrears of 30 to 89 Days In NIS millions For footnotes see page 153. Israel Discount Bank Limited and its Subsidiaries B. Debts (continued) 1. Credit quality and arrears - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) Lending Activity in Israel Public - Commercial Unaudited March 31, 2014 Problematic Non- problematic Unimpaired Impaired Total Unimpaired debts additional information In Arrears of 90 Days or More In Arrears of 30 to 89 Days In NIS millions For footnotes see next page. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 B. Debts (continued) 1. Credit quality and arrears - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) Footnotes: (1) Impaired, substandard or under special mention credit risk, including housing loans for which an allowance according to the extent of arrears exists and including housing loans in arrears for ninety days or over for which an allowance according to the extent of arrears does not exist. (2) As a general rule, interest income is not accrued in respect of impaired debts. For information regarding impaired debt restructured under problematic debt restructuring, see B.2. c. below. (3) Classified as unimpaired problematic debts. Accruing interest income. (4) Debts in arrears for between 30 and 89 days which accrue interest income, in amount of NIS 98 millions (March 31, 2014 - NIS 106 million, December 31, 2014 - NIS 119 million) are classified as unimpaired problematic debts. (5) Including housing loans in amount of NIS 10 million (March 31, 2014 - NIS 6 million, December 31, 2014 - NIS 10 million) with an allowance according to the extent of arrears, for which an arrangement was made for the repayment of overdue amounts, which included a change in the repayment schedule for the balance of the loan not yet due. Audited December 31, 2014 Problematic Non- problematic Unimpaired Impaired Total Unimpaired debts additional information In Arrears of 90 Days or More In Arrears of 30 to 89 Days In NIS millions Israel Discount Bank Limited and its Subsidiaries B. Debts (continued) 2. Additional information regarding impaired debts - consolidated A. Impaired debts and specific allowance 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) Balance of impaired debts in respect of which specific allowance exist Balance of specific allowance Balance of impaired debts for which specific allowance do not exist Total balance of Impaired Debts Contractual principal amount of impaired debts Unaudited March 31, 2015 In NIS millions Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. Additional information regarding impaired debts - consolidated (continued) A. Impaired debts and specific allowance (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Public - Commercial Balance of impaired debts in respect of which specific allowance exist Balance of specific allowance Balance of impaired debts for which specific allowance do not exist Total balance of Impaired Debts Contractual principal amount of impaired debts Unaudited March 31, 2014 In NIS millions Israel Discount Bank Limited and its Subsidiaries 2. Additional information regarding impaired debts - consolidated (continued) A. Impaired debts and specific allowance (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Debts under troubled debt restructurings Footnotes: (1) Recorded amount. (2) Specific allowance for credit losses. (3) The contractual balance of the principal amount includes accrued unpaid interest at date of the initial implementation of the instruction in respect of impaired debts, not yet written off or collected. (4) Reclassified due to changes in the data of a subsidiary company. Lending Activity in Israel Public - Commercial Audited December 31, 2014 Balance of impaired debts in respect of which specific allowance exist Balance of specific allowance Balance of impaired debts for which specific allowance do not exist of Impaired Debts Total balance Contractual principal amount of impaired debts In NIS millions Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 B. Debts (continued) B. Average balance and interest income 2. Additional information regarding impaired debts - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) Lending Activity in Israel Public - Commercial Recorded Interest Income Unaudited Three months ended March 31, 2015 Average balance of Impaired Debts In NIS millions Of which: recorded on cash basis For footnotes see next page. Israel Discount Bank Limited and its Subsidiaries 2. Additional information regarding impaired debts - consolidated (continued) B. Average balance and interest income (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Footnotes: (1) Average recorded amount of Impaired debts during the reported period. (2) Interest income recognized in the reported period, in respect of the average balance of impaired debts, during the time period in which these debts had been classified as impaired. (3) Total interest income that would have been recognized had such credit accrued interest according to its original terms is in amount of NIS 30 million and NIS 48 million for the three months ended March 31, 2015 and March 31, 2014, respectively. (4) Reclassified due to changes in the data of a subsidiary company. Recorded Interest Income Unaudited Three months ended March 31, 2014 Average balance of Impaired Debts In NIS millions Of which: recorded on cash basis Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Not accruing interest income Accruing debts ,in arrears for 90 days or more Unaudited March 31, 2015 Recorded amount Accruing debts , in Arrears for 30 to 89 Days In NIS millions Accruing debts not in arrears Total Footnotes: (1) Accruing interest income. (2) Included in impaired debts. Israel Discount Bank Limited and its Subsidiaries 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Public - Commercial Not accruing interest income Accruing debts ,in arrears for 90 days or more Unaudited March 31, 2014 Recorded amount Accruing debts , in Arrears for 30 to 89 Days In NIS millions Accruing debts not in arrears Total Footnotes: (1) Accruing interest income. (2) Included in impaired debts. (3) Reclassified due to changes in the data of a subsidiary company. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Not accruing interest income Accruing debts ,in arrears for 90 days or more Audited December 31, 2014 Recorded amount Accruing debts , in Arrears for 30 to 89 Days In NIS millions Accruing debts not in arrears Total Footnotes: (1) Accruing interest income. (2) Included in impaired debts. Israel Discount Bank Limited and its Subsidiaries 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Public - Commercial amount before restructuring Unaudited Three months ended March 31, 2015 Debt restructuring performed Recorded Number of contracts In NIS millions Recorded amount after restructuring Footnote: (1) An amount lower than NIS 1 million. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated (continued) 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Number of contracts Unaudited Three months ended March 31, 2014 Debt restructuring performed Recorded amount before restructuring In NIS millions Recorded amount after restructuring Footnote: (1) Reclassified due to changes in the data of a subsidiary company. Israel Discount Bank Limited and its Subsidiaries 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated (continued) Lending Activity in Israel Public - Commercial Construction and Real Estate - Construction Construction and Real Estate - Real Estate Activity Financial Services Commercial - Other Total Commercial Private Individuals - Other Total Public - Activity in Israel Banks in Israel Government of Israel Total Activity in Israel Lending Activity Outside of Israel Public - Commercial Construction and Real Estate Commercial - Other Total Commercial Private Individuals Total Public - Activity Outside of Israel Total Activity Outside of Israel Total For footnotes see next page. Foreign banks Foreign governments Unaudited Three months ended March 31, 2015 Failure of restructured debts Number of contracts Recorded amount In NIS millions - - 240 1 - - 3 4 236 - - 240 - - - - - - 240 2 1 - - - 1 1 - - 2 - - - - - - - - 2 Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) B. Debts (continued) Lending Activity in Israel Public - Commercial Construction and Real Estate - Construction Construction and Real Estate - Real Estate Activity Total Commercial Private Individuals - Other Total Public - Activity in Israel Banks in Israel Government of Israel Total Activity in Israel Lending Activity Outside of Israel Public - Commercial Commercial - Other Total Commercial Total Public - Activity Outside of Israel Foreign banks Foreign governments Total Activity Outside of Israel Total Financial Services Commercial - Other Unaudited Three months ended March 31, 2014 Failure of restructured debts Number of contracts In NIS millions 1 - - 25 26 484 510 - - 510 Recorded amount - - - 1 1 3 4 - - 4 - - - - - - 510 - - - - - - 4 Footnotes: (1) Debts, which in the reported year turned into debts in arrears for 30 days or over, which had been restructured under troubled debt restructurings during the period of twelve months prior to their having become debts in arrear. (2) An amount lower than NIS 1 million. 2. Additional information regarding impaired debts - consolidated (continued) C. Restructured troubled debts - consolidated (continued) Israel Discount Bank Limited and its Subsidiaries 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) Sensitivity to the domestic economic cycle in Israel. In addition, in view of material overseas investments by large Israeli corporations, the level of exposure to global crises increased; Sensitivity to private consumption; Exposure to foreign competition; In view of the high concentration of the ownership and control structure of corporations in the Israeli market - credit is typified by high concentration at the large borrower groups' level. Furthermore, the structure of the holding groups and their indebtedness at several levels within the holding corporations, increase the credit risk and the vulnerability of these corporations. Loans involving a high finance ratio carry risk in the event of impairment in the value of collateral below the balance of the loan. The Bank's underwriting policy limits the ratio of finance when granting a loan. Exposure to retail credit is affected by macro-economic factors. Intensification of competition in the banking system in recent years may lead to erosion in margins, decline in quality of borrowers with a resultant increase in credit risk. The credit policy does not allow at the present time the granting of credit to customers having a low internal credit rating, thus moderating such risks. - - - - (2) Credit to private individuals - housing loans - (3) Credit to private individuals - other - - (B) Indication of credit quality December 31, 2014 Total Commercial March 31, 2015 Private Individuals Housing Loans Other Loans Total Commercial Private Individuals Housing Loans Other Loans B. Debts (continued) 3. Additional disclosure regarding the quality of credit (A) Risk characteristics according to credit segments (1) Business credit Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) The period in arrears of debt is a central factor in determining the classification of the Bank's debts, and accordingly affects the allowance for credit losses and the accounting write-offs. A debt examined on a specific basis is classified as impaired when payments of principal or interest in respect thereof are in arrears of ninety days or over, except where the debt is well secured and is in the process of collection. A principal indication for the quality of the Bank's credit portfolio is the ratio of problematic debts at the Bank. During the first three months of 2015, the rate of performing credit to the public increased, stemming mainly from the commercial sector. This increase was accompanied by a decline in unimpaired problematic credit. B. Debts (continued) 3. Additional disclosure regarding the quality of credit (continued) 4. Additional information regarding housing loans - consolidated Balances for the period end, according to Loan-to-Value (LTV)() ratio, manner of repayment and type of interest: Total Balance of housing loans Of which: Bullet and Balloon debts In NIS millions Of which: variable interest Total Off- Balance Sheet Credit Risk Footnotes: (1) The ratio between the authorized credit line at the time the credit line was granted and the value of the asset, as confirmed by the Bank at the time the credit line was granted. The LTV ratio is another indication of the bank as to the assessment of the customer risk when the facility was granted. (2) Reclassified due to changes in the data of a subsidiary company. Unaudited Israel Discount Bank Limited and its Subsidiaries 3. CREDIT RISK, CREDIT TO THE PUBLIC AND ALLOWANCE FOR CREDIT LOSSES (CONTINUED) A. Type of deposits according to location of raising the deposit and type of depositor Loans acquired Loans sold Credit to the public Commercial Housing Other Credit to governments Total Credit to the public Commercial Housing Other Credit to governments Total For the three months ended March 31, 2015 - - 102 614 - - For the three months ended March 31, 2014 - - 79 - - - In NIS millions 3 181 105 795 8 - 87 - For details regarding net profits (losses) on the sale of loans, see Note 12 below. Minimum total capital adequacy ratio required by the Supervisor of Banks Footnotes: (1) The data in this item was computed in accordance with the rules mandatory in the U.S.A. (2) Beginning on January 1, 2015, IDB Bank became subject to new Basle III capital rules based on the final rules published by the Federal Reserve Board in July 2013. Capital ratios as of January 1, 2015 are as follows: 4.5% CET1 to risk-weighted assets; 6.0% Tier 1 capital to risk-weighted assets; and 8.0% Total capital to risk-weighted assets. (3) In view of the approach by the Supervisor of Banks, ICC is required to maintain a total capital ratio of not less than 15%, starting from December 31, 2010. (4) For details regarding the requirement for the raising of the Common equity tier 1 target, at a rate reflecting 1% of the outstanding balance of housing loans see item (b) above. 3. Ratio of capital risk assets 2015 March 31, Unaudited 2014 Audited December 31, 2014 Israel Discount Bank Limited and its Subsidiaries 5. CAPITAL ADEQUACY IN ACCORDANCE WITH INSTRUCTIONS OF THE SUPERVISOR OF BANKS (CONTINUED) 4. The effect of the transitional instructions on the ratio of common equity tier 1 Ratio of common equity tier 1 to risk assets before implementation of the effect of the provisional instructions in directive No.299. (1)8.9 0.5 9.4 2015 March 31, (1)7.4 1.7 9.1 In % 2014 December 31, 2014 (1)8.4 1.0 9.4 Effect of the provisional instructions Ratio of common equity tier 1 to risk assets after implementation of the effect of the provisional instructions in directive No.299. Footnotes: (1) Including the anticipated effect of the initial application of U.S. GAAP in the matter of employee rights, according to data expected as of January 1,2015. (2) Reclassified in order to include the effect of the transitional instructions regarding employee rights. (1) The liability of the Bank and its subsidiaries for severance pay to their employees, based on the customary one month's salary for each year of employment, is fully covered by deposits with severance pay funds, by insurance policies and pension funds and by a provision recorded in the Bank`s books. (2) Members of the Bank's Management are entitled to the customary severance payments, while several of whom are entitled also to an \"adjustment\" bonus of between 4 to 8 months' salary upon retirement, pursuant to individual agreements signed with them, and in respect of which adequate provisions have been included. The pension liability of foreign subsidiaries, based on actuarial computations, is covered by current deposits into a recognized foreign pension fund. (3) In certain consolidated banking subsidiaries, several officers are entitled to \"adjustment\" bonus\" equal to 6 to 9 months' salaries, and in respect of which adequate provisions have been included. (4) The Bank and its subsidiaries are not permitted to withdraw these deposits except for the purpose of making severance payments. (5) A number of the Bank's employees and those of its consolidated banking subsidiaries in Israel are entitled to long-service bonuses equal to a certain number of monthly salaries, and to a certain number of additional vacation days, upon completing 20, 30 and 40 years of employment in the Bank. In accordance with instructions of the Supervisor of Banks the provision in respect of this liability is computed on an actuarial basis and stated at its present value. The future payroll increase used to compute the amount of the liabilities for employee rights, in respect of the Bank's employees, is 1.8% per year (December 31, 2014: 2.5% per year). An agreement with the representatives of the employees was signed in 2007, regarding the \"Jubilee vacation\" days, according to which, among other things, the entitlement of new employees to \"Jubilee vacation\" was abolished. In 2011, the Bank signed with the representative committee of the employees a \"grades and stages\" agreement, according to which, among other things, new employees engaged or moved to the position of regular employees as from January 1, 2012, shall not be entitled to a \"jubilee award\". (6) Employees of the Bank and its consolidated subsidiaries in Israel are entitled to annual vacation as provided by labor agreements in force, and subject to the guidelines of the Annual Vacation Law - 1951. The liability for vacation pay is recognized over the period of employment in which the right to paid vacation accumulates. The liability is determined on the basis of the most recent salary in the reporting period with the addition of related payments. (7) Employees of the Bank and its subsidiaries are entitled to certain benefits after retirement. The said liability is computed on an actuarial basis and is recognized over the period of employment of the employee. In addition, some of the employees who accepted early retirement exchanged their retirement award with a pension for a determined period. This liability is presented at its discounted value. B. Details regarding the benefits March 31 Unaudited December 31 C. Defined benefit plan 1. Commitment and financing status 1.1 Change in commitment in respect of anticipated benefits Unaudited December 31 2014 Severance pay, retirement and pension December 31 2014 Post retirement retiree benefits 2013 2013 March 31 2015 March 31 2015 in NIS millions Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) C. Defined benefit plan (continued) 1. Commitment and financing status (continued) 1.2 Change in fair value of the plan's assets and financing status of the plan 1.4 Plans in which the commitment in respect of cumulative benefits and anticipated benefits exceeds the plan's assets * Included in the item \"other liabilities\" 1.3 Amounts recognized in accumulated other comprehensive income, before tax effect Unaudited December 31 2014 Severance pay, retirement and pension December 31 2014 Post retirement retiree benefits 2013 2013 March 31 2015 March 31 2015 in NIS millions * Stems from the change in the discount rate used in calculating the provisions in respect of employee rights. Unaudited December 31 2014 Severance pay, retirement and pension in NIS millions 2013 March 31 2015 Unaudited December 31 2014 Severance pay, retirement and pension in NIS millions 2013 March 31 2015 Israel Discount Bank Limited and its Subsidiaries 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) C. Defined benefit plan (continued) 2.1 Components of net benefit costs recognized in the statement of income 2. Expense for the period Unaudited For the year ended December 31, 2013 2014 Severance pay, retirement and pension payments 2013 For the year ended December 31, 2014 Post retirement retiree benefits in NIS millions Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) C. Defined benefit plan (continued) 2.1 Components of net benefit costs recognized in the statement of income (continued) 2. Expense for the period (continued) Severance pay, retirement and pension payments Cost of service Cost of interest Anticipated return on assets of the plan Amortization of unrecognized amounts: 2014 For the three months ended March 31, 2015 Unaudited in NIS millions 54 30 (27) 91 33 (48) Post retirement retiree benefits Cost of service Footnote: (1) Amount lower than NIS 1 million. Israel Discount Bank Limited and its Subsidiaries 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) 2. Expense for the period (continued) C. Defined benefit plan (continued) 2.2 Changes in assets of the plan and in the commitment for benefits recognized in other comprehensive income (loss), before tax effect Unaudited Net actuarial loss (income) for the period Amortization of actuarial income (loss) Amortization of credit (cost) in respect of prior service Amortization of net liability (asset) in respect of the transition Changes in foreign currency exchange rates Other 2.3 Estimate of amounts included in accumulated other comprehensive income expected to be amortized from accumulated other comprehensive income to the statement of income in 2015 as an expense (income), before tax effect Footnotes: (1) Amount lower than NIS 1 million. (2) See item 2.1 above. For the three months ended on March 31, 2015 For the year ended December 31, 2013 2014 Severance pay, retirement and pension payments For the three months ended on March 31, 2015 2013 For the year ended December 31, 2014 Post retirement retiree benefits in NIS millions Net actuarial loss (income) Net cost in respect of prior service Footnote: (1) Amount lower than NIS 1 million. Total amount expected to be amortized from other comprehensive income 2 Unaudited 2015 Severance pay, retirement and pension payments Post retirement retiree benefits in NIS millions (2) 1 (1) 2 - Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) 3. Assumptions C. Defined benefit plan (continued) 3.1 Assumptions on the basis of a weighted average used in determining the commitment in respect of the benefit and in measuring the net cost of the benefit Discount rate Anticipated long-term return on the plan's assets Remuneration growth rate Footnote: (1) In the financial statements as of June 30, 2014, the Bank has changed the estimate of the rate of real term increase in salary from a rate of 2.5% to a - - - - - - 5.19% 7.14% 2.16% - 2.75% 3.24% - 3.24% December 31 Severance pay, retirement and pension payments March 31 March 31 Post retirement retiree benefits December 31 December 31 Severance pay, retirement and pension payments March 31 Post retirement retiree benefits December 31 March 31 3.1.2 Principal assumptions used in measuring the net cost of benefit for the period Unaudited Severance pay, retirement and pension payments December 31 2014 1.70%-2.46% 2.26% - 2.54% 2.68%- 3.00% 2013 March 31 2015 Post retirement retiree benefits December 31 2014 1.42%-2.47% 2.03%-3.13% 1.93%-3.32% 2013 March 31 2015 rate of 1.8%. 3.2 Effect of a one percentage point change on the commitment for anticipated benefits, before the tax effect Unaudited Increase of one percentage point Decrease of one percentage point 3.1.1 Principal assumptions used in determining the commitment in respect of the benefit Unaudited Severance pay, retirement and pension payments Post retirement retiree benefits The said sensitivity test relates to the Bank, MDB and to ICC, which comprise over 90% of the liability in respect of an anticipated benefit. Israel Discount Bank Limited and its Subsidiaries 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) C. Defined benefit plan (continued) 4.2 Composition of the fair value of the plan's assets 4. The plan's assets level 1 Unaudited December 31, 2014 level 2 in NIS millions level 3 Total 4.2 Fair value of the plan's assets by type of asset and allotment target for the year 2015 Unaudited Allocation 5. Cash flow 5.1 Deposits Deposits *Estimate of deposits expected to be paid in 2015. Forecast* Unaudited Actual deposits For the three months ended March 31, 2014 Severance pay, retirement and pension payments in NIS millions 22 2015 2015 2013 For the year ended December 31, 2014 39 9 46 23 Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 6. EMPLOYEE BENEFITS - CONSOLIDATED (CONTINUED) C. Defined benefit plan (continued) 5.2 Benefits expected to be paid by the Bank in the future 5. Cash flow (continued) Year Unaudited Severance pay, retirement and pension payments In NIS millions Israel Discount Bank Limited and its Subsidiaries Footnotes: (1) Includes those linked to foreign currency. (2) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 E (1). Assets Israeli currency Non-linked Linked to the CPI In US$ December 31, 2014 Foreign currency In Euro in NIS millions In other currencies Non monetary items Total Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 B. Contingent liabilities and other special commitments (continued) The Bank responded to the motion to approve the claim as a class action suit. On May 15, 2008, the Court decided to stay the proceedings until a ruling is given in the appeal filed by the banks with respect to the action described in item 4.1 above. A decision was given on January 21, 2014, instructing the transfer of the hearing of the motion for approval of the claim as a class action in this case, to the District Court hearing the lawsuit described in item 4.1 above. The parties to the case conducted negotiations for reaching a compromise in the matter, which were unsuccessful. For details regarding the opposition by the Plaintiffs in this proceeding to the compromise arrangement discussed in item 4.1 above, see item 4.1 above. On March 15, 2015, the Plaintiff in these proceedings filed his response to questions raised by the Court in the proceedings described in item 4.1 above. The Attorney General of the Government submitted his position on March 24, 2015, according to which the hearing of the lawsuits should not be combined (in this item and item 4.1 above). Alternatively, in the event that the Court decides to combine the hearings, it is the opinion of the Attorney General that it would not be possible to reach a compromise if certain of the parties oppose it. The decision of the Court was given on March 25, 2015, as detailed in item 4.1 above. 4.3 Note 19 C 12.4 to the financial statements as of December 31, 2014, described an action together with a motion to approve the action as a class action suit, filed on April 27, 2009, against the Bank, Bank Leumi, Bank Hapoalim, Mizrahi-Tefahot Bank and the First International Bank. The Claimants argue that a binding arrangement had existed between the said banks with respect to commission rates charged by these banks and that the banks established a coordinated policy, which, as alleged by the claimants, was typified by prohibited cooperation and exchange of information. The class defined in the action constitutes all customers of the defendant banks, both private and business customers, in the period from 1990 to 2004. The total damage for all the defendant banks is assessed for the purpose of the action at approx. NIS 1 billion, with no allocation between them. On October 7, 2009, the District Court instructed that the hearing of this claim should be incorporated with the claim discussed in item 4.4 below. A motion for approval of a compromise arrangement between the parties was filed with the Court on November 16, 2014. The Court instructed the publication of an announcement regarding the filing of a motion for approval of a compromise arrangement and the submission of the motion to the Attorney General of the Government, to the Antitrust Commissioner and to the Courts Administrator. On November 27, 2014, a notice was published regarding the filing of a motion for approval of a compromise agreement. The response of the Attorney General to the Government was submitted on April 15, 2015, according to which the compromise arrangement should not be approved, and within the framework of which, among other things, the Attorney General expressed reservations as to the amount of compensation to the Plaintiffs and as to the lawyers' fees. He also suggested to the Court to consider the appointment of an examiner who would assess the value of the claim as against the value of an arrangement to the group and would assist the Court in evaluating the fairness of the proposed compromise. A preliminary hearing was held on April 16, 2015, within the framework of which it has been decided that the banks would submit an amended version of a compromise arrangement, and that the Attorney General for the Government would submit what possibilities are available to an examiner in order to assess the damage. The Attorney General for the Government submitted to the Court on May 3, 2015, his position, as stated above. The Banks shall submit their response as well as an amended version of a compromise arrangement until May 21, 2015. The payment to be made in accordance with the compromise arrangement, if and when approved, shall be made out of funds transferred in accordance with the agreed Order, see item 6 hereunder. 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) Israel Discount Bank Limited and its Subsidiaries 4.4 Note 19 C 12.5 to the financial statements as of December 31, 2014, described an action together with a motion for the approval of the action as a class action suit, filed on June 30, 2008, against the Bank, Bank Hapoalim and Bank Leumi, submitted to the Tel Aviv District Court. The core issue of the suit rests on the Plaintiffs' claim that, since the end of the 1990's and possibly even earlier, the three defendant banks created a cartel coordinating the prices of commissions charged to their customers. The Plaintiffs further claim that the banks have created an unlawful restrictive business practice regarding the rates of the various commissions charged to customers. As alleged by the Plaintiffs, as a result of the cartel, the price paid by the public is higher than the price that would have been paid had competition not been prevented by the cartel. The Plaintiff estimates the gap between commissions actually charged and the commissions that would have been charged had the banks not acted as they did, at 25%. Based on this assessment, the Plaintiffs claim an overall damage for all member of the group of NIS 3.5 billion. The Bank's share in the claimed amount is 22% (namely an amount of approx. NIS 770 million). According to the decision of the District Court of October 7, 2009, the claim will be heard together with the claim described in item 4.3 above. For details regarding a compromise arrangement submitted for approval of the Court, regarding the position of the Attorney General to the Government submitted to the Court and with respect to decisions made at the preliminary hearing held on April 16, 2015, see item 4.3 above. The payment to be made in accordance with the compromise arrangement, if and when approved, shall be made out of funds deposited in accordance with the agreed Order (see item 6 hereunder). 4.5 Note 19 C 12.7 to the financial statements as of December 31, 2014, described a lawsuit submitted on October 11, 2012, to the Tel Aviv District Court, against the Bank, FIBI, Leumi Bank and Mizrahi Bank together with a motion for approval of the suit as a class action suit. The matter of the lawsuit is the value date attributed by the banks to payments made by debtors directly to their account at the Debt Execution Office. As alleged by the Claimants, the practice of the banks is to determine the date on which payments have been received as the value date for these payments from the Debt Execution Office. In respect of the said time difference, the banks charge the debtors with interest in arrears. The Claimants argue that at this stage it is not possible to assess the amount of the claim, since in order to do so, specific examinations would have to be made at the banks. On March 10, 2014 the Bank`s response was filed. A preliminary hearing in the case was held on September 14, 2014. In view of the standpoint of the Respondents, according to which the issue of the delay in the transfer of the funds lies with the Debt Execution Office, the Court decided that the claims briefs would be delivered to the Attorney General of the Government and to the Supervisor of Banks, in order to obtain their position in this matter. The Attorney General for the Government submitted his position on February 2, 2015, according to which a winning bank may not charge the account of a customer with interest in respect of the period from date of deposit of a payment with the Execution Office and until it is transferred over to the bank. At a preliminary hearing held on April 19, 2015, the Court proposed to the banks to appoint a neutral public accountant, who would make a sample test of closed files of the Debt Execution Office, in order to determine whether any damage had been caused. The Court further proposed to the banks to consider submission of a third party notice against the Enforcement and Collection Authority. A further preliminary hearing was fixed for June 21, 2015. 4.6 Note 19 C 12.12 to the financial statements as of December 31, 2014, described a lawsuit filed on August 28, 2013, with the Tel Aviv District Court, against the Bank, Bank Hapoalim, Bank Leumi, Mizrahi-Tefahot Bank, the First International Bank and against the General Managers of the said banks, as well as a motion for the approval of the lawsuit as a class action suit. The Claimants allege that the respondent banks unlawfully charge a commission on the conversion and transfer of foreign currency with no proper disclosure to their customers. They claim that a customer who wishes to convert foreign currency is being charged an additional commission to that listed on the transaction price list, which, as alleged, is the difference between the rate at which the respondents buy foreign currency on the inter-bank market and the rate at which they sell the foreign currency to the customer. The Claimant stated the amount of the claim from all the Respondents and for all class members at NIS 10.5 billion. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) On January 26, 2014, the Court admitted the preliminary motions submitted in this case, including the motion by the Appellants for withdrawal of the suit against the general managers of the banks. An amended motion for the approval of the suit as a class action suit, was filed on February 4, 2014 and the amount of the claim was set at NIS 11.15 billion. On May 4, 2014, the Court decided that this motion will be heard together with the motion described in item 12.16 in the financial statements as of December 31, 2014. The Bank submitted on December 23, 2014, its response to the motion for approval of the claim as a class action suit. On August 10, 2014, a motion was filed for the consolidation of the hearing of this case with that of the case described in item 5.2 below. The response of the bank to the motion for the merger of the hearings was filed on January 8, 2015. A preliminary hearing has been set for March 8, 2015. At a preliminary hearing of the case, held on March 8, 2015, the Court ruled that the Appellants in this case and in the case described in item 5.2 below, shall submit within sixty days new motions for approval. Within sixty days following the submission of the new motions, the Respondents have to file a response. Following the filing of the claims briefs, the Court will rule in the matter of combining the hearing of the case with the proceedings described in item 5.2 below. A further preliminary hearing was fixed for October 25, 2015. 4.7 Note 19 C 12.14 to the financial statements as of December 31, 2014, described a lawsuit lodged on October 30, 2013, with the Jerusalem District Court together with a motion for its approval as a class action suit, against the Bank, Mercantile Discount Bank, Bank Hapoalim, Union Bank and FIBI. The Plaintiffs argue that the respondent banks charge their customers upon renewal of credit facilities, with a commission in respect of credit and collateral handling, despite the fact that the collateral in respect of the credit facility remains unchanged. The Plaintiffs assess the cumulative amount of the claim against all respondent banks for all class members at NIS 2 billion, and estimated the Bank's share at NIS 498 million and share of MDB at NIS 195 million. On March 5, 2015, a verdict was given according to which the motion for approval of the suit as a class action suit was dismissed. On May 13, 2015, the Bank received information as regards an appeal that had been submitted against the said Court ruling. At date of bringing the report to print, the appeal brief had not yet been delivered to the Bank or to MDB. 4.8 Note 19 C 13.2 to the financial statements as of December 31, 2014, described a lawsuit against Mercantile Discount Bank together with a motion for its approval as a class action suit which were filed with the Tel Aviv District Court on January 5, 2014. The Appellant claims that following the entry into effect of Proper Conduct of Banking Business Directive No. 325, MDB has unilaterally raised the interest rate on credit granted to its customers within the approved credit facility that had been agreed with the customers, and this after the customer had already borrowed funds from MDB within the framework of the credit facility allotted to him and on its basis. The group which the Appellant wishes to represent is defined as \"all customers of Mercantile Discount Bank Ltd., who have a credit facility renewable on a quarterly basis, and which, between the years 2007 to 2013, were charged with interest for utilizing the credit facility at a rate exceeding the rate agreed with them according to the last credit facility agreement signed by them with the bank\". The Appellant states the amount of the claim in respect of all group members at NIS 139 million. The response of MDB was submitted on July 24, 2014. The Appellant filed a response to the Bank's response, which may be seen as expanding the case matter. MDB has filed a motion for the dismissal of this response. On February 22, 2015, the Court decided that in view of the position of the Appellant, that there is no need for an amendment of the claims briefs, the contents of the claims briefs shall remain as-is. MDB is entitled to file a response to the counter response and all the arguments of the parties shall remain intact and shall be heard within the framework of the hearing of the motion for approval of the suit as a class action suit. The parties agreed to refer the case to mediation and therefore hearing of the evidence has been deferred to October 11, 2015. 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) Israel Discount Bank Limited and its Subsidiaries 4.9 Note 19 C 12.15 to the financial statements as of December 31, 2014, described a lawsuit filed with the Tel Aviv-Jaffa District Court on January 30, 2014, against the Bank and against ICC together with a motion for approval of the lawsuit as a class action suit. The Appellant claims that ICC charges on a monthly basis the accounts of holders of \"Active\" credit cards, in respect of charge amounts accumulated through use of the card, with a minimum amount only determined by ICC. The remainder of the said charge amounts turns into a loan carrying especially high interest rates. It is further alleged that upon the marketing of the plan, ICC refrained from emphasizing to the customers that cancellation of the credit requires an explicit request by the customer as well as from stating the cost of the credit granted. The Appellant claims that operating a revolving credit mechanism with respect to the customers and charging them with interest, has been made with no effective contractual basis and with the impairment of the customers' autonomy. The Appellant stated the amount of the claim in respect of all group members at NIS 2,225 million. A decision dismissing the claim against the Bank was given on August 19, 2014. The case against ICC continues. On October 21, 2014, an appeal was filed with the Supreme Court against the decision of the District Court to dismiss in limine the motion for approval against the Bank. The appeal has been fixed for oral complementary arguments on October 8, 2015, following the submission of summing-up briefs by the parties. ICC submitted its response to the motion on November 10, 2014. On December 21, 2014, the Appellant submitted his response to the comments of ICC. On October 29, 2014, ICC motioned for the stay of proceedings at the District Court until a ruling is given by the Supreme Court in an appeal filed by the Appellant. The preliminary hearing of the case was fixed for June 28, 2015. Within the framework of this hearing, the Court will also consider the request of ICC to stay the proceedings until a ruling is given in the appeal filed with the Supreme Court. 4.10 Note 19 C 13.3 to the financial statements as of December 31, 2014, described a lawsuit filed on March 4, 2014, against the Bank with the Central-Lod District Court, together with a motion for its approval as a class action suit. According to the Appellant, the Bank allows customers to deviate from their approved credit facility in contradiction of Proper Conduct of Banking Business Directive No. 325, thus causing them to pay high and the maximum interest rates in respect of the deviation from their approved credit facility. It is further claimed that the Bank charges the customers account with a commission in respect of notice as to the deviation and/or a warning letter regarding such deviation. The Appellant notes that he is unable to quote an exact amount in respect of the damage caused, but in his opinion this amounts to hundreds of millions of NIS. The Claimant has filed additional class actions on similar grounds and, in accordance with the Court's ruling from June 12, 2014, the additional lawsuits will be heard together with the claim described in this item. The Bank filed its response on October 5, 2014. In a preliminary hearing held on March 26, 2015, the case was deferred for an additional preliminary hearing on October 15, 2015, in presence of the Declarers on behalf of the parties, and within the framework of which, the factual disputes would be discussed by the parties. 4.11 Note 19 C 12.17 to the financial statements as of December 31, 2014, described a lawsuit filed on April 28, 2014, with the District Court Central Region against ICC and others, together with a motion for its approval as a class action suit. The above motion raises the allegation for two binding arrangements in the field of immediate debit cards (\"debit\") and pre paid cards (\"prepaid\"), which, as alleged by the Plaintiffs, constitute \"a systematic and continuous deceit\" of customers of the credit card companies. The Plaintiffs claim that the first binding arrangement is an arrangement for the charging of a cross commission in respect of transactions made through the use of debit or pre paid cards. As regards the second binding arrangement, the Plaintiffs claim that it involves the unlawful withholding of monies due to trading houses, in respect of transactions made by debit cards and pre-paid cards, for a period of twenty days, following the date of collection of the money by the credit card companies. The class of those directly affected, whom the Plaintiffs wish to represent, is defined as \"all trading houses in Israel which accept debit cards\". The class of those indirectly affected, whom the Plaintiffs wish to represent, is defined as \"anyone who purchased goods or services at trading houses that accept debit cards, including the Plaintiffs\". The Plaintiffs assess the amount of the claim against all defendants and in respect of all class members at NIS 1,736 million. On February 24, 2015, a motion for withdrawal from the claim was filed. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) A motion was filed with the Court on April 19, 2015, requesting the replacement of the parties applying for a withdrawal and their representatives by the Appellant and his representatives and to instruct the continuation of the proceedings by the Appellant and his representatives. The Court had instructed the submission of an amended withdrawal motion, which was filed on May 11, 2015. 5. Class action suits and requests to approve certain actions as class action suits as well as other actions are pending against the Bank and its consolidated subsidiaries, which, in the opinion of the Bank's Management, based on legal opinions and/or on the opinion of managements of its consolidated subsidiaries, which are based on the opinions of their legal counsels, respectively, as the case may be, it is not possible at this stage to evaluate their prospects of success, and therefore no provision have been included in respect therewith. 5.1 Note 19 C 13.1 to the financial statements as of December 31, 2014, described a petition for approval of an action as a class action suit filed with the Tel Aviv District Court On June 19, 2000 by two borrowers of DMB against DMB and against the Israel Phoenix Insurance Co. Ltd., where the properties of the borrowers are insured. The action is for the amount of NIS 105 million (on June 28, 2012, Discount Mortgage Bank was merged with and into the Bank). The borrowers claim, inter alia, that DMB has insured their properties for amounts which exceed their reinstatement value, and that the sum insured was increased in excess of the increase in the Consumer Price Index. On December 25, 2000, the Court decided that whereas the arguments in this case are similar to those argued in another class action suit, as described in item 12.1 to Note 19 C to the financial statements as of December 31, 2014, the hearing of the said case will be postponed until a decision is given in the other case. On December 5, 2011, the Court that hears the other motion, gave the compromise agreement the validity of a Court verdict between the parties. 5.2 Note 19 C 13.4 to the financial statements as of December 31, 2014, described a claim together with a motion for its approval as a class-action suit, filed on August 5, 2014, with the Tel Aviv-Jaffa District Court, against the Bank, against MDB and against other banks. The Appellant is alleging that the respondent banks charge foreign currency transfer and handling commissions contrary to the Banking Rules and contrary to the Antitrust Law. The banks, it is alleged, charge the aforesaid commissions on a graded scale, with the grade being determined according to the size of the amount transferred. The Appellant alleges that this is a binding agreement \"tacitly\" agreed to by the banks; in addition, it is alleged that some of the respondent banks do not disclose the amount of the commission and/or the way it is calculated. The group that the Appellant is seeking to represent is defined as \"persons or entities that have made use of bank services for the transfer of foreign currency and/or other dealings in foreign currency and the entire Israeli public, who have been directly and indirectly harmed by the infringements\". The Appellant has set the aggregate amount of the claim against all respondent banks at NIS 3 billion or, alternatively, at an amount of at least NIS 1.5 billion. Moreover, a motion has been filed to unite the hearing of this motion with the motions described in item 4.6 above. The response of the banks to the motion for the merger of the hearings was filed on January 8, 2015. The response of the banks to the motion for approval was submitted on February 25, 2015. At a preliminary hearing of the case, held on March 8, 2015, the Court ruled that the Appellants in this case and in the case described in item 4.6 above, shall submit within sixty days new motions for approval. Within sixty days following the submission of the new motions, the Respondents have to file a response. Following the filing of the claims briefs, the Court will rule in the matter of combining the hearing of the case with the proceedings described in item 4.6 above. An amended motion for approval of the suit as a class action suit was filed on April 27, 2015. A further preliminary hearing was fixed for October 25, 2015. 5.3 Note 19 C 13.5 to the financial statements as of December 31, 2014, described a lawsuit filed against the Bank on October 19, 2014, with the Central-Lod District Court, together with a motion for its approval as a class action suit. Israel Discount Bank Limited and its Subsidiaries 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) The Claimant argues that in violation of the law, the Bank charges its customers an excessive early repayment commission in respect of loans which are not housing loans. It is being argued that the Bank acts in contravention of Proper Conduct of Banking Business Directive No. 454. It was also argued that despite the fact that the Bank is permitted to charge customers with an early repayment commission in an amount reflecting only the damage caused to the Bank, it is the Bank's practice to charge commission fees reflecting considerable profit. The Claimant stated that it is unable to estimate the amount of the damage caused. An agreed motion for a procedural agreement was filed on February 18, 2015, according to which, the Appellant will file a motion for amendment of the motion for approval. The Bank's response to the original motion or to the amended motion shall be filed within 90 days from date of the ruling by the Court in the matter of the motion for amendment. 5.4 A lawsuit was filed against the Bank on May 4, 2015, with the Tel Aviv District Court together with a motion for its approval as a class action suit. The subject of the motion is the alleged charging of excess interest on housing loans. The Claimant argues that the Bank computes the monthly interest by dividing the agreed annual interest by twelve. According to the Claimant, this method of computation leads to the charging of higher interest than the interest that would have been charged had the Bank made a correct calculation. The interest charged by the Bank, as argued by the Claimant, reflects compound interest in contradiction to the law and to agreements signed with customers. The Claimant assesses the amount of the claim for all class members at NIS 80 million. 6. Note 19 C 14 to the financial statements as of December 31, 2013, described the decision of the Antitrust Commissioner regarding binding arrangements between banks, following an investigation conducted since 2004 by the Antitrust Authority. On April 26, 2009, the Antitrust Commissioner (\"the Commissioner\") issued a statement under Section 43(a)(1) of the Antitrust Act, 1988, according to which binding arrangements existed between Bank Hapoalim B.M., Bank Leumi Le-Israel B.M., Mizrahi-Tefahot Bank, the First International bank of Israel Ltd. and the Bank (hereinafter: \"the banks\"), in the matter of communication of information regarding commissions (\"the Commissioner's Statement\"). Under Section 43(e) of the Antitrust Law, the Commissioner's statement serves as prima facie evidence for its contents in any legal proceedings. In the wake of the publication of the Statement, the Bank and the other banks submitted appeals against the Commissioner's statement. On June 16, 2014 the Antitrust Tribunal approved the agreed order signed between the banks and the Commissioner (\"the agreed order\"), whereby it is determined that the banks would pay an amount of NIS 70 million, of which an amount of NIS 14 million to be paid by Discount Bank (\"the payment\"), and this without the banks admitting their liability under the provisions of the law or admitting a violation on their part of the provisions of the law. In view of the approval of the agreed order by the Antitrust Tribunal and to the deposit of the payment by the banks, the Commissioner's statement was cancelled and no enforcement measures would be taken against the banks in connection with the investigation that had led to the publication of the decision. It has been determined, within the framework of the agreed order, that the payment may be used for compromise arrangements that might be reached by the banks as regards class actions that are pending against them, and which are detailed in the agreed order. The balance of the payment, which would remain at the end of twenty-four months from date of approval of the agreed order, shall be assigned to the State's Treasury. 7. Agreement between the Swiss Authorities and the U.S. Department of Justice. On August 29, 2013, an agreement between the Swiss Authorities and the U.S. Department of Justice regarding the program for the settlement of disputes was published regarding deposits of U.S. citizens with Swiss banks (Program for Non-Prosecution Agreements or Non-Target Letters for Swiss Banks). The program differentiates between a number of categories. Category No. 1 includes banks being under investigation or proceedings with the U.S. Department of Justice. According to publications, this category includes fourteen banks and such banks are not permitted to participate in the program. It should be noted that IDB (Swiss) is not under investigation or other proceedings by the U.S. Department of Justice. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) Category No. 2 is designed for banks that assume the existence of a possibility of effecting violations as detailed in the program. According to the program, banks wishing to be included in this category, could have applied to the U.S. Department of Justice until December 31, 2013, for an agreement for avoiding criminal charges against the bank (Non-Prosecution Agreement), and this only after the Justice Department receives and studies a report of an independent examiner submitted by the applying bank, and subject to the consent of the applying bank to pay a fine in an amount derived from the amount of funds deposited with it by its U.S. customers during the period relevant to the program. Category No. 3 is designed for banks that declare and commit that they had not effected violations as detailed in the program. Banks that wished to be included in this category had to apply to the U.S. Department of Justice from July 1, 2014 to December 31, 2014, for conformation that they are not targeted for enforcement actions by it (Non-Target letter). As stated in the program, if it is found retroactively that the examination report does not support the original declaration, the case would be handled at the discretion of the U.S. Department of Justice. The said alternatives of the program require the delivery to the U.S. Department of Justice of information of various scopes, where in the case of Category No. 2 (non-prosecution agreement) detailed information regarding the said accounts will be required. Following an examination of the plan and relying, among other things, on outside legal advice rendered to IDB (Swiss) Bank, the Bank and IDB (Swiss) Bank decided not to join the plan. To the extent that IDB (Swiss) Bank would have elected to participate in the program under category No. 2, than the maximum fine computed in accordance with the approach detailed in the Swiss program with respect to all accounts of U.S. persons held by it, would have been reduced in relation to accounts that would have been recognized under the program as tax compliant, or as such which joined the voluntary disclosure program with the encouragement of IDB (Swiss) Bank, or as such that are out of scope of the program. According to the examination of IDB (Swiss), with the assistance of an external consultant, and considering the deductions detailed above, the worst case scenario does not amount to a material sum to the Bank. It is emphasized that in any event, the result of the said review is considered a crude assessment only, due to the fact that the formula in question is not a simple one but a formula requiring specific and complex discussions with the U.S. Justice Department, mostly due to the fact that different reliefs exist under the program, the effect of which is difficult to assess beforehand. The Bank informed IDB (Swiss) Bank that as long as it maintains the control thereof, it is the Bank`s intension to secure the financial ability of IDB (Swiss) Bank to comply with the regulatory requirements in Switzerland, as required for its business activity. Examination and investigation actions by the U.S. Authorities. It has been publicized recently, that Israeli banks find themselves in various stages of examination and investigation processes on the part of the U.S. authorities. It was published on December 22, 2014, that the Bank Leumi Group had reached an arrangement of the \"Deferred Prosecution Agreement\" type with the U.S. Department of Justice, and also reached an additional arrangement with the Financial Services Authority of the State of New York (hereinafter - \"the Leumi arrangement\"). According to the arrangement, Bank Leumi admitted conducting a series of operations, the aim of which, according to the publication, was assisting tax evasion by its U.S. customers. According to the arrangement, the U.S. Department of Justice agreed to defer the filing of an indictment against the Bank Leumi Group for a period of twenty-four months, during which Bank Leumi is required to abide by the commitments detailed in the agreement. Moreover, various sanctions have been imposed on the Bank Leumi Group, including the payment of a fine in the amount of US$400 million. 8. CONTINGENT LIABILITIES AND SPECIAL COMMITMENTS (CONTINUED) B. Contingent liabilities and other special commitments (continued) Israel Discount Bank Limited and its Subsidiaries The Bank Leumi arrangement has been presented and discussed at the Bank following its publication. This arrangement is based on specific facts dealing with many and continuous operations attributed to different companies in the Bank Leumi Group and as far as can be discerned from the publications, the agreement had been prepared and formed over a long period of time, with considerable investment of resources and of work time of consultants, the data itself remaining undisclosed. The agreement does not detail the formula for the fine, which determines the amounts that the Bank Leumi Group has agreed to pay, except with respect to the operations of Leumi in Switzerland. It would seem that in part, the amount of the fine had been based on agreements as to the amounts of tax evasion by customers, deriving from and in respect of activities attributed to the Bank Leumi Group. According to the published arrangement, the fine paid by Bank Leumi in respect of its operations in Switzerland, is derived from the formula detailed in the program with respect to category No.2. The Bank has no knowledge of investigative actions taken against the Bank or against any of its extensions by the U.S. authorities, as regards U.S. customers who had not complied with their obligations according to U.S. tax laws. Furthermore, as published, IDB (Swiss) Bank is not one of the corporations included in the category No. 1 of the Swiss program (namely, banks under investigation, which, therefore, may not participate in the Swiss program). The Bank is adopting a series of measures for the management of the risk involved in its operations with its U.S. customers. However, in view of the said enforcement actions and due to the uncertainty existing in this matter, it is not possible to assess to assess the risk involved in these operations. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 1. Par value of derivative instruments A. Volume of activity on a consolidated basis SPOT foreign currency swap contracts Footnotes: (1) Excluding credit derivatives and SPOT foreign currency swap contracts. (2) Derivatives comprising a part of the Bank's asset and liability management system, which were not designated for hedging relations. (3) An amount lower than NIS 1 million. A. Hedging derivatives Swaps Shekel/CPI Interest rate contracts Audited December 31, 2014 Other Foreign currency contracts Contracts on shares Commodities and other contracts in NIS millions Total Israel Discount Bank Limited and its Subsidiaries 2. Gross fair value of derivative instruments A. Volume of activity on a consolidated basis (continued) 9. DERIVATIVE INSTRUMENTS ACTIVITY - VOLUME, CREDIT RISK AND DUE DATES (CONTINUED) Shekel/CPI Interest rate contracts Unaudited March 31, 2015 Other Foreign currency contracts Contracts on shares Commodities and other contracts in NIS millions Total Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 2. Gross fair value of derivative instruments (continued) A. Volume of activity on a consolidated basis (continued) 9. DERIVATIVE INSTRUMENTS ACTIVITY - VOLUME, CREDIT RISK AND DUE DATES (CONTINUED) Shekel/CPI Interest rate contracts Unaudited March 31, 2014 Other Foreign currency contracts Contracts on shares Commodities and other contracts in NIS millions Total Of which: Balance sheet balance of liabilities in respect of derivative instruments not subject to net settlement arrangement or similar arrangements For footnotes see next page. Israel Discount Bank Limited and its Subsidiaries 2. Gross fair value of derivative instruments (continued) A. Volume of activity on a consolidated basis (continued) 9. DERIVATIVE INSTRUMENTS ACTIVITY - VOLUME, CREDIT RISK AND DUE DATES (CONTINUED) Footnotes: (1) Derivatives comprising a part of the Bank's asset and liability management system, which were not designated for hedging relations. (2) Of which: NIS 26 million (March 31, 2014: NIS 48 million; December 31, 2014: NIS 27 million) positive gross fair value of assets stemming from embedded derivative instruments. (3) Of which: NIS 34 million (March 31, 2014: NIS 14 million; December 31, 2014: NIS 35 million) negative gross fair value of liabilities stemming from embedded derivative instruments. (4) An amount lower than NIS 1 million. (5) Reclassified - following improvement of the data. Shekel/CPI Interest rate contracts Audited December 31, 2014 Other Foreign currency contracts Contracts on shares Commodities and other contracts in NIS millions Total Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 9. DERIVATIVE INSTRUMENTS ACTIVITY - VOLUME, CREDIT RISK AND DUE DATES (CONTINUED) B. Derivative Instruments credit risk based on the counterparty to the contract, on a consolidated basis Stock Exchange Governments and central banks Others Total Banks Dealers/ brokers In NIS millions For footnotes see next page. Israel Discount Bank Limited and its Subsidiaries 9. DERIVATIVE INSTRUMENTS ACTIVITY - VOLUME, CREDIT RISK AND DUE DATES (CONTINUED) B. Derivative Instruments credit risk based on the counterparty to the contract, on a consolidated basis (continued) Footnotes: (1) The difference, if positive, between the total amount in respect of derivative instruments (including derivative instruments with a negative fair value) included in the borrower's indebtedness, as computed for the purpose of limitation on the indebtedness of a borrower, before credit risk mitigation, and the balance sheet amount of assets in respect of derivative instruments of the borrower. (2) Of which: a balance sheet balance of standalone derivative instruments in the amount of NIS 5,159 million included in the item assets in respect of derivative instruments (March 31, 2014: NIS 3,576 million; December 31, 2014: NIS 4,596 million). (3) Of which: a balance sheet balance of standalone derivative instruments in the amount of NIS 5,249 million included in the item liabilities in respect of derivative instruments (March 31, 2014: NIS 4,124 million; December 31, 2014: NIS 4,475 million). Others Total Stock Exchange Banks Audited December 31, 2014 Dealers/ brokers In NIS millions Governments and central banks Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 9. DERIVATIVE INSTRUMENTS ACTIVITY - VOLUME, CREDIT RISK AND DUE DATES (CONTINUED) C. Due dates - Par value: consolidated period end balances 86,526 62,441 Audited December 31, 2014 54,817 22,841 226,625 Total Israel Discount Bank Limited and its Subsidiaries A. Composition - consolidated (continued) Footnotes: (1) Level 1 - fair value measurements using quoted prices in an active market. Level 2 - fair value measurements using other significant observable inputs. Level 3 - fair value measurements using significant unobservable inputs. (2) For further details of the stated balance sheet amount and the fair value of securities, see Note 2. (3) Of which: assets and liabilities in the amount of NIS 53,728 million and NIS 73,731 million, respectively, the stated balance sheet amounts of which are identical to their fair value (instruments stated in the balance sheet at their fair value). For additional information regarding instruments measured at fair value on a recurrent basis and on a non-recurrent basis, see Notes 10 B - 10 C. (4) See Note 18. (5) Reclassified - classification between levels. Financial assets Cash and deposits with banks Total Book value Level 1 Audited December 31, 2014 Level 2 in NIS millions Fair value Level 3 Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) Assets Available for sale securities Of the Israeli Government Of foreign governments Of Israeli financial institutions Of foreign financial institutions Mortgage-backed-securities or Assets -backed-securities Of others in Israel Of others abroad Shares Total available-for-sale securities Trading Securities Of the Israeli Government Of foreign governments Of Israeli financial institutions Of foreign financial institutions Mortgage-backed-securities or Assets -backed-securities Of others in Israel Of others abroad Shares Total trading securities Credit to the public in respect of securities loaned Assets in respect of derivative instruments Shekel/CPI Interest Rate Contracts Other Interest Rate Contracts Foreign Exchange Contracts Shares Contracts Commodity and other Contracts Total assets in respect of derivative instruments Other Assets in respect of the \"Maof\" market operations 2,778 1,891 - - 50 - - 12 20,719 13,366 1,104 290 - 528 - - 573 15,861 Quoted prices in an active market (level 1) 1,988 - 1 4,722 26 - 2,733 1 3,883 - - - Fair value measurements using - Other significant observable inputs (level 2) 924 225 40 2,001 7,402 203 117 - 10,912 2,440 239 3 - - 59 5 - 90 - 6 67 2 1 2 - 2,170 1,682 30 127 177 26 - 14,890 1,348 - - 127 51 - 178 Total assets 2,248 127 1 5,249 1,374 140 2,733 1,893 122 2,346 2,533 157 2,845 14,290 1,329 330 2,001 7,402 731 117 573 - - - - - - - - - 26,773 26,773 - 2,440 239 - - 3 5 - - 59 90 - - 2 7 - - - - - - - 1 5,159 - - - 26 12 - - 36,708 36,708 - - - - - - - 14,290 1,329 330 2,001 7,402 731 117 573 2,440 239 3 5 59 90 2 7 2,845 1,893 122 2,346 2,533 157 1 5,159 26 12 1,374 140 2,733 2,248 127 1 5,249 Significant unobservable inputs (level 3) Influence of deduction agreements In NIS millions - - - - - - - - - - - - - - - - - - - 122 176 801 - 1,099 - - - 1,099 - 140 - 209 - - 349 Total fair value Balance sheet balance 1. Items measured at fair value on a recurring basis B. Items measured at fair value - Consolidated Unaudited March 31, 2015 - - - - - - Liabilities Deposits from the public in respect of securities borrowed Liabilities in respect of derivative instruments Shekel/CPI Interest Rate Contracts Other Interest Rate Contracts Foreign Exchange Contracts Shares Contracts Commodity and other Contracts Total liabilities in respect of derivative instruments Other Commitments in respect of the \"Maof\" market operations Short sales of securities - - - Total liabilities 1,854 4,782 - 6,985 6,985 1. Items measured at fair value on a recurring basis (continued) B. Items measured at fair value - Consolidated (continued) 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) Israel Discount Bank Limited and its Subsidiaries Quoted prices in an active market (level 1) Fair value measurements using - Other significant observable inputs (level 2) Unaudited March 31, 2014 Significant unobservable inputs (level 3) Influence of deduction agreements In NIS millions Total fair value Balance sheet balance Assets Available for sale securities Of the Israeli Government Of foreign governments Of Israeli financial institutions Of foreign financial institutions Mortgage-backed-securities or Assets -backed-securities Of others in Israel 1. Items measured at fair value on a recurring basis (continued) B. Items measured at fair value - Consolidated (continued) 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 Quoted prices in an active market (level 1) Fair value measurements using - Other significant observable inputs (level 2) Audited December 31, 2014 Significant unobservable inputs (level 3) Influence of deduction agreements In NIS millions Total fair value Balance sheet balance Notes: (1) Reclassified - classification between levels. Israel Discount Bank Limited and its Subsidiaries 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) Other Impaired credit the collection of which is collateral dependent Level 1 - - Unaudited March 31, 2015 - 4 Level 2 Level 2 - - Level 3 In NIS millions 2,101 16 Unaudited March 31, 2014 Level 3 In NIS millions 2,052 11 Audited December 31, 2014 Total fair value Profit (Loss) for the three months ended March 31, 2014 2,101 16 (74) 2 2,052 15 (67) - Total fair value Profit (Loss) for the three months ended March 31, 2015 - - Level 2 Level 3 In NIS millions 1,478 13 (116) 2 Total fair value Profit (Loss) for the year ended December 31, 2014 1,478 13 Level 1 - - Level 1 - - Impaired credit the collection of which is collateral dependent Other Impaired credit the collection of which is collateral dependent Other 2. Items measured according to fair value not on a recurring basis B. Items measured at fair value - Consolidated (continued) Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) C. Changes in items measured at fair value on a recurring basis included in level 3 - Consolidated 1. For a period of three months ended March 31, 2015: Unaudited 2. For a period of three months ended March 31, 2014: Unaudited Footnotes: (1) Included in the statement of income in the item \"Non-interest financing income\" (2) An amount lower than NIS 1 million. Israel Discount Bank Limited and its Subsidiaries 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) C. Changes in items measured at fair value on a recurring basis included in level 3 - Consolidated (continued) 3. For the year ended December 31, 2014: Audited Immaterial transfers to or from level 3 were made in the first quarter of 2015, due to a clarification of the Supervisor of Banks, according to which, derivative instruments, the credit risk thereof is determined on the basis of unobservable inputs, shall be included in level 3. Footnotes: (1) Included in the statement of income in the item \"Non-interest financing income\". (2) An amount lower than NIS 1 million. D. Transfers between hierarchy levels of fair value Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) E. Additional details regarding significant unobservable inputs and valuation techniques used for the measurement of fair value of items classified to level 3 Fair value as at March, 2014 Valuation Techniques Unobservable inputs Range (Weighted Average) 1. Quantitative information regarding the measurement of fair value at level 3 Fair value as at March, 2015 Valuation Techniques Unaudited Unobservable inputs Range (Weighted Average) Israel Discount Bank Limited and its Subsidiaries 10. BALANCES AND FAIR VALUE ESTIMATES OF FINANCIAL INSTRUMENTS (CONTINUED) E. Additional details regarding significant unobservable inputs and valuation techniques used for the measurement of fair value of items classified to level 3 (continued) Significant unobservable inputs, which were used to measure the fair value of derivative financial instruments, are expectations of inflation up to one year, and adjustments regarding counterparty credit risk (CVA). As the inflation forecasts rise (fall) and the Bank commits to pay the index-linked amount, so the fair value falls (rises). As the inflation forecasts rise (fall) and the counterparty to the transaction is obligated to pay the Bank the index-linked amount, so the fair value rises (falls). The counterparty credit risk coefficient (CVA) expresses the probability of credit default of the counterparty to the transaction. A rise in the default probability reduces the fair value of the transaction, and vice versa. 2. Qualitative information regarding the measurement of fair value at level 3 1. Quantitative information regarding the measurement of fair value at level 3 (continued) Fair value as at December 31, 2014 Valuation Techniques Audited Unobservable inputs Range (Weighted Average) Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 11. INTEREST INCOME AND EXPENSES - CONSOLIDATED 2014 For the three months ended March 31, 2015 in NIS millions Financing income generated by mortgage backed securities (MBS) - in US $ millions Financing income generated by mortgage backed securities (MBS) - in NIS millions (2) Including the effective component of hedging relationships. (3) Details of the effect of hedge derivative instruments on subsection A. (4) An amount lower than NIS 1 million. Israel Discount Bank Limited and its Subsidiaries Unaudited Financial Households Small Businesses Corporate Banking Middle Market Banking Private Banking in NIS millions For the three months ended March 31, 2015 Non- Financial Companies Financial management Total Consolidated Footnotes: (1) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 E(1). (2) Restated, see B above. (3) Reclassified - Inclusion of the average balance of credit excluding interest in the average balance of assets. (4) Reclassified - improvement in the format of allocating the average balance of assets and the average balance of liabilities among the different operating segments and the financial management segment, considering the data of one of the subsidiary, see C (3). Interest income, net - From external sources For the three months ended March 31, 2014 Interest income, net - From external sources Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 13. BUSINESS SEGMENTS - CONSOLIDATED (CONTINUED) Households Small Businesses Corporate Banking Middle Market Banking Private Banking Non- Financial Companies Financial management Total Consolidated Footnotes: (1) Restated, in respect of the retroactive implementation of the guidelines of the Supervisor of Banks in the matter of employee rights, See Note 1 E(1). (2) Restated, see B above. Audited Financial Israel Discount Bank Limited and its Subsidiaries B. Changes in other comprehensive income (loss)() (continued) Before taxes March 31, 2015 Tax effect Unaudited For the three months ended After taxes Before taxes Audited For the year ended After taxes Before taxes Tax effect After taxes Before taxes Tax effect After taxes 2014 2013 March 31, 2014 Tax effect in NIS millions Israel Discount Bank Limited and its Subsidiaries (1) Including financial statements translation adjustments of a consolidated subsidiary - Discount Bancorp Inc., the functional currency of which is different from that of the Bank. (2) The pre-tax amount is reported in the statement of income in the item non-interest financing income. For further details see the note on non-interest financing income. (3) For details regarding the restatement, see Note 1 E (1). The pre-tax amount is reported in the statement of income in the item Salaries and related expenses. Footnotes: Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 15. THE INVESTMENT IN THE FIRST INTERNATIONAL BANK (\"FIBI\") B. An agreement with FIBI Holdings - 2010 On March 31, 2015 the Bank's holdings in FIBI were 9.28% in the equity and in the voting rights. The cost of the shares in the Bank's books amounted to NIS 527 million as of March 31, 2015. The market value of the Bank's holdings in FIBI totaled on March 31, 2015: NIS 509 million. The market value of this investment at May 17, 2015: NIS 492 million. The Bank and FIBI Holdings signed an agreement on March 28, 2010, which, among other things, limited the period in which Discount Bank shall have the right by which FIBI Holdings shall continue to support the appointment of one quarter of the directions of FIBI from among candidates recommended by Discount Bank. Details regarding the highlights of the agreement and regarding the approvals of the Supervisor of Banks and the Antitrust Commissioner with respect to the agreement, including the timetable determined by the Antitrust Commissioner for the reduction in the interest held by Discount Bank in FIBI, were brought in Note 6 D (1) to the financial statements as of December 31, 2014. C. Sale of shares in February 2015. On February 19, 2015, the Bank sold 7,054,625 shares of FIBI, comprising approx. 7% of the share capital of FIBI. The balance of the shares in the First International Bank held by the Bank comprises 9.28% of the share capital of FIBI. The sale was made in an offmarket transaction, at a price of NIS 0.4951 per share, reflecting a discount of 2.5% on the base price for the February 19 trading day. The total consideration amounted to NIS 349 million. In consequence of this sale and the decrease in the Bank's rate of holdings in the shares of FIBI to below 10%, the exceptional impact of the investment in these shares on capital adequacy has been removed, a fact that brought about an improvement in capital adequacy already in the first quarter of 2015. Completion of this move constitutes the Bank's attainment of a relevant milestone in the sale outline determined by the Antitrust Commissioner, prior to the final date that had been fixed for this sale. In respect of the above, a loss on impairment of a nature other than temporary (OTTI) in the amount of NIS 47 million, net, was recorded in the financial statements as of December 31, 2014. A. Data regarding the investment in FIBI Agreement with Bank Otsar Ha-Hayal. An agreement was signed on March 31, 2015, between ICC and Diners and Bank Otsar Ha-Hayal Ltd. (hereinafter: \"Otsar Ha-Hayal\"; \"the agreement\"). The agreement is for a period of five years since date of signature, and would be extended for further periods of five years each, unless one of the parties informed the other party, six months prior to the end of the agreement period, of his wish not to extend the agreement for an additional period. Within the framework of the agreement, ICC and Diners would issue credit cards to Otsar Ha-Hayal customers and would provide them with the services involved in the issue of the cards and inherent in the use thereof. The agreement establishes the rights of the parties as well as the operating arrangements and the provision of services by ICC and/or Diners in respect of the charge cards issued in terms of the agreement and the remaining terms relating thereto. B. (1) Events regarding the clearing of international electronic trade transactions and other matters. In the second half of 2009 and in the beginning of 2010, ICC faced allegations made by VISA Europe and the Global MasterCard Organization (hereinafter: \"the international organizations\") with respect to prima facie violations of the rules of these organizations pertaining to the clearing of international electronic trade, in transactions effected by a subsidiary of ICC, ICC International (which had in the meantime been merged with and into ICC). In this framework, fines have been imposed on ICC and its activity in this field of operations has been restricted for a period of several months. ICC has immediately implemented a reduction plan in order to comply with the requirements of the international organizations, in the framework of which it applied various measures, including changes in the company's management. A number of trading houses and clusterers had raised demands regarding the burden of monetary sanctions applying to them and the reduction in electronic trade clearing operations conducted with them, which as alleged by them, resulted in heavy damage. stated in the full pricelist in respect of a transfer by the RTGS system would apply to a transfer of amounts of over NIS 1 million; No commission may be charged in respect of a direct channel operation in respect of charge transaction using an immediate debit card; - - Commission regarding the handling of cash by a teller - in the case of providing service which includes a combination of two or more of the transactions included in the service, the Bank would be entitled to charge one commission only, the higher of the commissions in respect of these transactions; A commission may be charged for the issue of an ownership confirmation starting with the second confirmation in a calendar year; - - Housing loan ledger fees and collection fees in respect of non-housing loans granted immediately prior to the application of the new rules, will be made by debit cards and discounting services for trading houses. For this purpose, a business of a private individual or of a corporation during the first year since the beginning of its operations, or which the clearing turnover of transactions by debit cards made on its behalf by the banking corporation does not exceed NIS 3 million, shall be considered a small business. Examination of the clearing turnover shall be performed in accordance with the data existing in the hands of the banking corporation; - A definition of \"change\" has been added - the handing of a note or coins or a combination of the two in exchange for receiving a note or coins or a combination of the two in a value equal to the amount denominated therein, excluding the exchange of an old legal tender for a new legal tender; - An addendum to the definition of \"the start of obtaining the service\" as regards the clearing of debit cards - upon the signing by the customer of the - Excluding the commission in respect of clearing services for debit transactions from the rule, according to which, a commission which a banking corporation is entitled to charge for a service included in the pricelist for small businesses shall not exceed that determined in the pricelist for a large business. This instruction will take effect on July 1, 2015; - The duty has been set for the presentation of a summarized pricelist also to customers obtaining from the corporation clearing services for debit card - In Part 6, the part of credit cards in the full pricelist: the deferred payment commission shall be abolished; changes were made with respect to the commission in respect of foreign currency transactions and in respect of the withdrawal of foreign currency abroad. On March 23, 2015, an update was published in respect of the effective date of these changes. The changes will take effect on July 1, 2015 (instead of April 1, 2015); - Section 12 of the full list price includes a list of services regarding the clearing of debit card transactions field, in respect of which commissions are - - - Entering into effect of Sections 12 and 13 of the pricelist, presentation of the summarized pricelist to customers receiving clearing services, and the change in the definition of a \"small business\" as regards Sections 12 and 13 of the pricelist, shall take effect on July 1, 2015. Draft Amendment of the Banking Rules (Customer service) (Commissions), 2008. An updated draft was published on March 9, 2015, within the framework of which it is proposed as follows: - - to apply control to \"notices\" commission, the intention of the Supervisor of Banks being to determine a maximum price for such a commission, in - - to determine that \"card fees\" shall not be charges in respect of an immediate debit card issued to a customer who has already a credit card issued by abolished; clearing agreement. The change takes effect on February 1, 2015; transactions, and also an addendum to the rules regarding the format of the summarized pricelist has been added; chargeable in accordance with the price list; Section 13 of the pricelist has been added - \"discounting services for trading houses\"; A summary price list was added for customers receiving from the bank clearing services for debit card transactions; to abolish the commission charged to the depositor in respect of a dishonored check; accordance with the direct cost involved therein; to eliminate the possibility of charging a commission in respect of delivery of follow-up letters; that same banking corporation; - Changing the definition of \"credit card\" to \"debit card\"; Israel Discount Bank Limited and its Subsidiaries - Requiring the banking corporation to publish on its Internet website agreements of the certain types stated in the rules, and which are considered 17. REPORT OF THE TEAM FOR EXAMINING THE INCREASE IN COMPETITION IN THE BANKING INDUSTRY (CONTINUED) Amendments to Part 6 to the First Addendum (\"debit cards\") and to the pricelist of debit cards (the addition of the possibility for the presentation of the price of services in the foreign currency pricelist, cancellation of card fees for immediate debit cards issued to customers holding a credit card issued by the same corporation, amendments relating to commissions in respect of foreign currency transactions and the withdrawal of foreign currency abroad). Banking Order (Customer service) (Supervision over notices or warning service), 2015. The Order was published on May 10, 2015, and declares the \"notices or warning service\" as a service under supervision. The maximum commission amount that may be charged for such service would be NIS 5. The Order will take effect on July 1, 2015. Banking Rules (Customer service) (Proper disclosure and delivery of documents) (Amendment), 2014. The Amendment to the Rules was published on October 7, 2014. The principal elements of the Amendment are: - A banking corporation has to publish also on its Internet website, various data that under these rules have to be published on a notice board in the - Requiring a banking corporation to provide to anyone wishing to open an account for business purposes, an explanatory paper, in a separate paper, that includes, among other things, clarifications regarding the practical meaning of the classification of an account as a \"small business\" account with respect to the services price list; Requiring the banking corporation to provide to each customer wishing to join this lane, prior to his joining, information in writing regarding commission amounts charged to him during the quarter before the quarter preceding the date of submission of the joining application, in respect of services included in this lane, in the manner detailed in the Amendment; Authorization of The Supervisor of Banks to determine various instructions as regards the information to be provided to a customer, as above. - The Amendment entered into effect in full on November 7, 2014. Amendment of Banking Rules (Customer service) (Proper disclosure and submission of documents), 1992. The Amendment was published in the Official Gazette on December 30, 2014. The principal provisions of the Amendment are: - Granting the Commissioner the power to determine types of account and terms, which, if in existence, the signature of the customer on certain - Requiring the banking corporation to deliver to the customer or to present in the account of the customer on the website of the banking corporation a copy of an agreement or a document of the types detailed in the rules, which did not required the customers signature, proximately after obtaining his approval to their contents or alternatively present it in the account of the customer on the bank's website or their delivery by Email, subject to allowing him the possibility of printing and keeping the agreement/document, as stated. With respect to an agreement or document prepared in the presence of the customer, the possibility has been added of presenting it in the account of the customer on the bank's website or of delivery thereof by Email, subject to obtaining in writing the specific agreement of the customer to the delivery in the said manner. - Determining various changes relating to the notices which a banking corporation has to deliver to customers regarding changes in the terms of - Determining an additional exception to the instruction with respect to the non-delivery of notices under the rules to a customer residing abroad, and who has not provided an address in Israel for delivery of notices, and this in case where the customer has asked to receive notices via the Internet website of the banking corporation; - Determination that a banking corporation is required to inform the data that has to be delivered under the rules, in respect of a loan for a period The rules enter into effect of January 1, 2015, except for the sections dealing with types of accounts and terms under which certain agreements with the customer shall not require his signature and by presenting agreements or documents in the customer's account on the bank's website or delivering them by Email. - bank's branches; - types of agreements stated in the rules, would not be required; uniform contracts, as defined in the law; managing their accounts; exceeding one year, which is repayable in installments, once in each year and no later than February 28. Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 17. REPORT OF THE TEAM FOR EXAMINING THE INCREASE IN COMPETITION IN THE BANKING INDUSTRY (CONTINUED) The amendment regarding notices to certain customers pertaining to changes in the terms of management of their accounts will enter into effect on April 1, 2015. Proper Conduct of Banking Business Directive No. 414 in the matter of disclosure of services in securities costs. The instruction was published by the Supervisor of Banks on April 3, 2014. The principal items of the directive are: the duty to present to a customer who was charged with Israeli and/or foreign securities commission, within the framework of the semi-annual statement of commissions. Comparative data regarding commissions paid by customers holding deposits of similar value to that of the deposit held by the customer, this in the manner as detailed in the instruction; the duty to present on the Internet website of the Bank the said comparative data relating to the data for a period of six months; the duty to present to the customer, within the framework of the semiannual statement of commissions, detailed data relating to securities commission charged to him during a period of six months in the manner detailed in the directive. The directive will become effective on January 1, 2015. The Bank is implementing the requirements in accordance with the directive. Proper Conduct of Banking Business Directive No. 421 in the matter of reduction or addition to interest rates. The Directive was published by the Supervisor of Banks on September 9, 2013. Its main topics are: maintaining the reduction or addition to the basic interest rate granted to the customer upon granting credit, loan, credit facilities or upon depositing funds with the bank, also in the case of a change in interest rates or upon renewal of the deposit. The Directive will become effective on January 1, 2014. The Supervisor of Banks has deferred to July 1, 2014, the date on which the instruction is to take effect regarding deposits. Proper Conduct of Banking Business Directive No. 425 in the matter of \"Annual reports to customers of banking corporations\". The Supervisor of Banks published on November 19, 2014, a Proper Conduct of Banking Business Directive in the matter of \"annual reports to customers of banking corporations\", comprising the implementation of the Zaken Committee recommendations in the matter of \"bank identity card\". The directive is designed to regularize the annual reporting duty of the corporation for customers matching the definition of \"individual\" and \"small business\", as regards all assets and liabilities of the customer with the banking corporation, including his total income and expenses during the year regarding assets, liabilities and current operation in his account. The annual report is intended to assist customers in making educated consumer decisions, to improve the ability of customers to follow their activity in the account and to increase their ability to compare various banking products and services. The Supervisor of Banks stated that the directive requires indirect amendments and certain adjustments of the proper disclosure rules, which shall be made further on. It should be noted that the implementation of the various procedures as described above, will require the Bank to make wide range computerized preparations, training of staff and determination and absorption of work procedures, at a financial cost that cannot be assessed at this stage, and all this within a relatively short period of time. The Directive takes effect on February 28, 2016, with respect to the 2015 annual report. However, it is further stated in the directive that the data regarding the credit rating will not be presented until an explicit instruction is issued by the Supervisor of Banks, though the banks must be ready to present this data starting with the said date. Amendment of Proper Conduct of Banking Business Directive No. 439 in the matter of account charging authorization. On September 1, 2014, the Supervisor of Banks published the amendment to the Directive. The object of the amendment is to face the difficulties involved in the process of transferring a charge authorization relating to an existing account with one bank to a new account with another bank, a matter identified as a central barrier facing customers wishing to change banks. The principal items of the amendment are: a new chapter \"Submitting an application for establishment of an authorization to charge an account\" has been included, which regularizes the process of submitting an application for authorization to charge an account, directly by the customer or by the beneficiary (subject to obtaining a written consent of the customer). Israel Discount Bank Limited and its Subsidiaries 17. REPORT OF THE TEAM FOR EXAMINING THE INCREASE IN COMPETITION IN THE BANKING INDUSTRY (CONTINUED) Among other things, this chapter determines a mechanism for the transfer of a list of items from the customer or from the beneficiary to the bank, using each one of the communication means defined in the amendment; instructions have been determined regularizing the response to the customer and to the beneficiary within five business days, and stating that where the response is positive, the bank has to establish the authorization within the said time period; the chapter \"Application for a change of account charged by authorization\" has been updated and a new process has been determined, within the framework of which, for the transfer of authorization to charge an account from one bank to another, which includes several stages: submission of an application by the customer for the transfer of charges by authorization, examination of the authorization data and the establishment and transfer of information to the beneficiary. The effective date for the amendment has been fixed for October 1, 2015. Proper Conduct of Banking Business Directive No. 418 in the matter of the opening of accounts via the Internet. On July 15, 2014, the Supervisor of Banks published the Directive, constituting an additional layer in the adoption of the Zaken Committee recommendations. The Directive details the matters required from banking corporations, which are interested in allowing the opening online of bank accounts for customers, and determines limitations on operations in such accounts, designed to reduce the risks involved in conducting an online account. According to the Directive, an account may be opened on the basis of a copy of an identity card, a copy of an additional identification document, and an online signature on a declaration under the Prohibition of Money Laundering Order. In addition, identification of the customer shall be made by a visual meeting as part of performing a broader \"know your customer\" process. Accounts of this type would be limited in scope of their monetary operations. In addition, it would not be possible to appoint an \"authorized signatory\" for such an account, and checks issued to the owner of such an account shall be limited in endorsement. These limitations would be withdrawn only after a full face to face identification of the customer is made at the branch, in accordance with the provisions of the Prohibition of Money Laundering Order (Duties of identification, reporting and maintenance of records by banking corporations for the prevention of money laundering and terror financing), 2001. Implementation of the Directive has been postponed until the amendment to the proper disclosure rules of December 30, 2014 takes effect (see above), and to the publication of the circular of the Supervisor of Banks dated January 4, 2015, in the matter of types of accounts and terms under which an agreement with the customer shall not require his signature. Circular of the Supervisor of Banks in the matter of types of accounts and terms under which an agreement with the customer shall not require his signature. Further to the amendment of proper disclosure rules (see above), the circular was published on January 4, 2015, taking effect on date of publication, and stated that with respect to agreements as detailed below, the customer's signature would not be required, on condition that the customer would be able to confirm on the website of the banking corporation that he had been given the possibility of reviewing the agreements: Agreements regarding general business terms or the opening and managing of a current account, including an agreement which includes general terms for the provision of various banking services as detailed in the proper disclosure rules, to be opened online as prescribed in Proper Conduct of Banking Business Directive No. 418; an agreement for the deposit of funds for a specified period exceeding one year; agreement in the matter of telephonic instructions. Amendment of Proper Conduct of Banking Business Directive No. 432 in the matter of \"transfer of activity and closing down a customer's account\". The Amendment was published by the Supervisor of Banks on December 15, 2014, with the aim of simplifying the process of transferring the activity or the closing of an account of a customer. The essence of the Amendment is as follows: - The bank shall provide to a customer considering the feasibility of transferring to another bank or the closing of his account, a detailed annual report (\"banking identity card\") in a uniform format and easy to understand, updated to the month preceding the date of application, as well as a manual assisting the customer in understanding the actions involved in transferring the operations or in closing the account; - A customer may apply for the closing of his account or for the transfer of activity from his account, not only by visiting the bank branch but also by the bank's Internet website, by e-mail, by telephone and by any additional means of communication in accordance with the bank's decision; Financial Statements CONDENSED FINANCIAL STATEMENTS AS AT MARCH 31, 2015 - Emphasis has been given to the possibility given to the new bank to act on behalf of the customer in all actions required to transfer the customer's - The process of transferring operations shall be concluded within five business days since the date on which the customer submitted the application. 17. REPORT OF THE TEAM FOR EXAMINING THE INCREASE IN COMPETITION IN THE BANKING INDUSTRY (CONTINUED) The process of closing an account shall be concluded within five business days from the date on which the customer completed the actions which he had to execute in order to close the account. The process of transferring an Israeli securities portfolio shall be concluded within five business days from date of the instruction given by the customer, and the transfer of a foreign securities portfolio - within fourteen business days from date of the instruction. The Directive states that on date of closing the account, the account management agreement shall expire and it would be classified as a \"closed account\"; - A banking corporation shall not cancel benefits and rebates granted to the customer, only on grounds of his request for the transfer of activity or for - Guidelines has been determined in respect of cases where the customer applies for the transfer of current operations to another bank, though The Directive partly took effect on January 1, 2015, though most of the material amendments in the Directive will take effect on July 1, 2015. The amendments relating to the summarized and detailed information to be provided to the customer with respect to the operations in his account (\"banking identity card\") shall take effect on the date the Proper Conduct of Banking Business Directive No. 425 (\"annual statements to customers of banking corporations\") takes effect, namely, on February 28, 2016. Amendment of Proper Conduct of Banking Business Directive No. 435 in the matter of telephonic instructions. The amendment to the Directive was published on January 4, 2015, and took effect upon the publication thereof. In accordance with the amendment, it is possible to present to a customer on the Internet website an agreement for the giving of telephonic instructions, and the confirmation by the customer on the said website, that he had been given the possibility of reviewing the agreement and that this will serve instead of his signature on the agreement. Licensing and the establishment of banking associations in Israel. The Supervisor of Banks published on May 5, 2015, an outline for the establishment of banking associations in Israel. The outline details the threshold terms for the establishment of banking associations in Israel, and the stages required for their establishment. The outline comprises an infrastructure for increasing the number of players within and outside the banking industry and forms an additional step towards the adoption of the recommendation of the team examining the intensification of competition in the banking sector. Law Memorandum - Regulation of Off-Banking Loans Act (Amendment No. 3) (Institutional lenders, maximum interest and penalties), 2014. In February 2014, the Ministry of Justice published for public comment the Law Memorandum prepared in the wake of the recommendations of the Zaken Committee. In the framework of the Memorandum it is proposed to apply also to banks the limitation existing in the law in respect of off-banking entities. At this stage it is not yet possible to assess the effect of the Amendment upon the Bank. Details regarding Banking Rules (Customer service)(Commissions)(Amendment), 2012, and the Letter of the Supervisor of Banks regarding the re-pricing of commissions in respect of securities operations, which entered into effect in the beginning of 2013, were brought in the 2013 Annual Report. At this stage, prior to the completion of the required legislation and regulation amendment process, it is not possible to evaluate the impact of the various moves. The Bank estimates that the income of the Group will be adversely affected by an amount assessed at approx. NIS 100 million per year. the closing of an account; remaining in the account are rights or liabilities which are not yet due; operations from his old account to his account with the new bank and to close the account with the old bank. "
}
```
### ES Query

**POST /test/chapter/_search**

``` json
{
    "fields": "title",
    "query": {
        "match_phrase": {
            "content": {
                "query": "a credit card",
                "slop": 20
            }
        }
    },
    "highlight": {
        "fields": {
            "content": {
                "fragment_size": 10,
                "number_of_fragments": 1,
                "type": "plain"
            }
        }
    }
}
```

If you try this out, you'll see that the highlight returned is as big as the _source.content, it just doesn't care about the `"fragment_size": 10`.

Thank you for investigating this issue!
</description><key id="90340466">11820</key><summary>fragment_size option is ignored when highlighting a stopwords filtered field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">paulintrognon</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-06-23T09:55:08Z</created><updated>2016-11-24T19:31:43Z</updated><resolved>2016-11-24T19:31:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T19:13:31Z" id="114614160">To add to this:
- plain highlighter highlights the whole body
- postings highlighter returns a fragment but ignores the fragment size (documented to always return a sentence)
- fvh does the right thing
</comment><comment author="szroland" created="2015-06-24T00:27:03Z" id="114681903">I think there is an issue with how spans are calculated for higlighting in the plain highlighter when using the span fragmenter and multiple phrases, in this case it seems to lose track of spans and practically collects a huge fragment at the end, which, having multiple matches and as such higher score, will be the top "match" to highlight.

@paulintrognon, I guess while the root cause is fully established, you could try using `"fragmenter" : "simple"` explicitly, which will not try to be smart about keeping matches of various parts of your search together.

E.g. try this:

```
POST /test/chapter/_search
```

```
{
    "fields": "title",
    "query": {
        "match_phrase": {
            "content": {
                "query": "credit card",
                "slop": 0
            }
        }
    },
    "highlight": {
        "fields": {
            "content": {
                "fragment_size": 50,
                "number_of_fragments": 5,
                "type": "plain",
                "fragmenter" : "simple"

            }
        }
    }
}
```

I think this provides relatively useful matches respecting the fragment size.
</comment><comment author="clintongormley" created="2015-06-24T13:52:47Z" id="114874138">@markharwood you were looking at the plain highlighter recently. Could you investigate this one too please?
</comment><comment author="markharwood" created="2015-06-26T10:40:38Z" id="115638691">Will do
</comment><comment author="markharwood" created="2015-06-26T12:26:00Z" id="115659950">Looking deeper the SimpleSpanFragmenter is prioritizing accuracy of match-reporting over delivering the requested sizes of fragments.
It is not possible to both summarize and reflect all query logic accurately and this is an example case where the trade-off being made is questionable - in the pursuit of accurately reporting phrase matches this fragmenter temporarily overrides the fragment size limit to try and tie together reported sightings of phrase components which otherwise would straddle fragments introduced by summarization logic.

I can see from the code there is a deliberate policy of ignoring fragment sizes while connecting phrase elements in this query/doc so the override of your choice of fragment size is "working as designed" but arguably not doing a fantastic job.

For the record - the wikimedia foundation's "experimental-highlighter" plugin that they use on Wikipedia looks to do a decent job of summarising this text and is known to be significantly faster than the plain highlighter. See https://github.com/wikimedia/search-highlighter
</comment><comment author="paulintrognon" created="2015-09-25T13:39:47Z" id="143227110">OK, this is very interesting.

I have used  the simple fragmenter to get arround this issue, and it worked for me. Thank you all!
</comment><comment author="clintongormley" created="2016-11-24T19:31:42Z" id="262835588">This appears to be fixed in 5.0 or before</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tophits aggregation for nested field ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11819</link><project id="" key="" /><description>Hi,
Am using fields with nested type, in the mappings I kept for column1 and column2 as nested type fields..

I am using the following query to get the top hits:

GET partsfilter/partsfilter_type/_search
{
    "size": 10,
     "query" : {
        "match" : { "description" : "abc" }
    },
    "aggs" : {
        "column1" : {
            "nested" : {
                "path" : "column1"
            },
            "aggs" : {
               "col1_agg":
               {
                   "terms":
                   {
                       "field":"column1.@key",
                       "size":0
                   },
                   "aggs":
                   {
                       "top":
                       {
                           "top_hits":
                           {
                               "size":1
                           }
                       }
                   }
               }
            }
        }
    }
}

This is throwing  SearchPhaseExecutionException[Failed to execute phase [query]  exception. when am removing top_hits aggs it is working fine.
how to write a top_hits query to work for the above case..
</description><key id="90331001">11819</key><summary>Tophits aggregation for nested field ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HemaAnusha</reporter><labels><label>:Nested Docs</label><label>feedback_needed</label></labels><created>2015-06-23T09:08:44Z</created><updated>2015-06-25T06:17:50Z</updated><resolved>2015-06-25T06:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-06-23T12:44:49Z" id="114482199">@HemaAnusha What version of ES are you using and what exceptions are you in the response and log file?
</comment><comment author="HemaAnusha" created="2015-06-25T06:17:49Z" id="115120919">Hi Mart,

Thanks for your response, I done the issue, problem is with version. Upgraded ES version from 1.4.2 to 1.5.2, now am able to use nested aggregations.

Thank you..
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: simplify equals implementation and dedup code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11818</link><project id="" key="" /><description /><key id="90327291">11818</key><summary>Query refactoring: simplify equals implementation and dedup code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-23T08:48:06Z</created><updated>2015-06-23T12:09:35Z</updated><resolved>2015-06-23T12:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-23T09:15:24Z" id="114416234">I think it's great to remove the duplicated code in equals(), still wondering about the construction with the abstract method though. On the one hand this is good because it forces subclasses to implement it, on the other hand I have the feeling that when just looking at the subclass, it's a bit confusing to immediately realize this gets called by the super class. What about if the subclasses call `super.equals()` for the common code, that way the link is clearer from reading the code in the subclass? However I see that this won't force new implementations to care about equals() as much as the abstract solution. Any thoughts?  
</comment><comment author="javanna" created="2015-06-23T09:18:51Z" id="114417008">&gt; What about if the subclasses call super.equals() for the common code

as you said subclasses wouldn't be forced to override it, and `super.equals` feels incomplete cause it only checks the class of the object etc. it doesn't do any real comparison.
</comment><comment author="javanna" created="2015-06-23T09:36:13Z" id="114420618">pushed a new commit with the method rename ;)
</comment><comment author="cbuescher" created="2015-06-23T09:46:33Z" id="114423304">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file></files><comments><comment>Query refactoring: simplify equals implementation and dedup code</comment></comments></commit></commits></item><item><title>Query refactoring: OrQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11817</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217

PR goes against the query-refactoring branch
</description><key id="90320106">11817</key><summary>Query refactoring: OrQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-23T08:13:03Z</created><updated>2015-06-23T09:54:26Z</updated><resolved>2015-06-23T09:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-23T08:16:26Z" id="114401718">The changes include minor updates to AndQueryBuilder as well that I just discoverd. In fact, the two queries are so closely related that I was thinking about factoring out a common parent class, but since they are both deprecated and there are just two cases that could be grouped under that parent class I didn't do that just yet. Open to suggestions on this part though.
</comment><comment author="javanna" created="2015-06-23T08:31:24Z" id="114405894">LGTM besides the small comment I left. Regarding similarities with and query builder, I think I would keep everything as-is, otherwise we would complicate the class hierarchy even more which is probably not worth it for deprecated classes
</comment><comment author="javanna" created="2015-06-23T09:53:14Z" id="114425142">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11817 from cbuescher/feature/query-refactoring-or</comment></comments></commit></commits></item><item><title>Always return metadata in get/search APIs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11816</link><project id="" key="" /><description>This commit makes the get and search APIs always return `_parent`, `_routing`,
`_timestamp` and `_ttl` in addition to `_id` and `_type`. This way, consumers
always have all required information in order to reindex a document.

Closes #8068
</description><key id="90314598">11816</key><summary>Always return metadata in get/search APIs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-23T07:38:19Z</created><updated>2015-08-13T13:45:01Z</updated><resolved>2015-06-24T08:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-23T13:52:00Z" id="114513142">@colings86 I pushed a new commit
</comment><comment author="rjernst" created="2015-06-23T21:49:07Z" id="114651253">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/AllFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/CustomFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/JustSourceFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/JustUidFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/SingleFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/UidAndRoutingFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/UidAndSourceFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java</file><file>core/src/test/java/org/elasticsearch/explain/ExplainActionTests.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>Merge pull request #11816 from jpountz/enhancement/return_metadata</comment></comments></commit></commits></item><item><title>Java api: make BoostableQueryBuilder package private</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11815</link><project id="" key="" /><description /><key id="90312982">11815</key><summary>Java api: make BoostableQueryBuilder package private</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>non-issue</label></labels><created>2015-06-23T07:33:01Z</created><updated>2015-06-23T08:12:48Z</updated><resolved>2015-06-23T07:54:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-23T07:35:05Z" id="114391984">LGTM
</comment><comment author="javanna" created="2015-06-23T08:12:48Z" id="114400225">I backed out this change as I totally missed that FunctionScoreQueryBuilder is outside of the index.query package, we will see what to do about it in the query-refactoring branch. We may even go and remove the interface and share code through the QueryBuilder base class, we will see.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java</file></files><comments><comment>Merge pull request #11815 from javanna/enhancement/boostable_query_builder_pkg</comment></comments></commit></commits></item><item><title>Item Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11814</link><project id="" key="" /><description>## Introduction

This a proposal for a different implementation of More Like This (MLT) which is purely based on the term vectors API. The goal of MLT is to find documents that are more like or similar to a given input document (or set of user input documents). In order to do so MLT simply performs nearest neighbor search in the vector space model induced by the query. This consists of forming a disjunctive query of all the terms in the input document, to execute that query and to finally return the results. However, in order to speed things up, MLT first performs a greedy selection of the best terms, that is the terms that would most contribute to the score of each matching document. In this respect, MLT assumes that the default [Lucene similarity](https://lucene.apache.org/core/5_2_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html) measure is being used, and therefore selects the terms with highest tf-idf. Furthermore, in order to limit the expensive call on the document frequency of each term, MLT prunes the selection space by only considering terms that have a certain minimum term frequency or length.

Item Query better decouples the process performed by MLT in three different steps. First, a characteristic vector of the input document is found using the Term Vectors API with the terms filtering (#9561). Second, the query is formed from these terms. Third, the same query is then sent to and executed on every shard. This approach has a couple of advantages that we now list below.
## Advantages
- This approach is a cleaner and a more faithful implementation to nearest neighbor search with feature selection. It is simpler to implement and easier to understand. For example, we can easily explain the query without even having executed it. We could also always return the query formed together with the results returned at no additional cost.
- The same query is executed on every shard, which has some nice performance implication with the file system cache. Furthermore, this approach lowers the load on the cluster as terms filtering is only performed on the single shard holding the requested document.
- Together with the ongoing work on query refactoring (#10217), we would no longer need to serialize all candidate terms but only the selected ones used to form the query. Implementation wise we would simply re-use serialization of the terms query.
- Allows for more complex terms filtering with scripting, and therefore for different feature selection procedures. This would allow us to perform feature selection on a different feature metric space, not necessarily limited to the Lucene default similarity measure. See the example below on Bayesian Sets.
## Disadvantages
- We could now have a speed bottleneck if the shard holding the document and performing filtering is busy or slow.
- We are no longer based off Lucene MLT code.
- We are loosing on the terms that increase recall as the selection is only performed on the shard holding the input document. MLT does perform the term selection on every shard. However, with enough data document frequencies should be well distributed. With small indices it is also always possible to allow for the dfs option of the Terms Vectors API.
## Example of Usage: Bayesian Sets

[Bayesian Sets](http://papers.nips.cc/paper/2817-bayesian-sets.pdf) (BSets) is an item based search algorithm that has been shown to return good results for multimedia documents, such as [images](https://stat.duke.edu/~kheller/810_Heller_K.pdf), but not [only](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS07_SilvaHG.pdf). There are two important ideas behind item based search. First, the query is made of items (not only textual documents), and the results are items. Second, the similarity measure is solely implicitly defined by the choice of features, requiring very little tweaking of the underlying matching algorithm. The features of an item could be thought as defining some summary statistics characterizing this item. It usually takes the form of a vector of characteristics. For example, an image could be turned into a feature vector by using the histogram of its most common colors.

The formula employed by BSets to compute the score of each item with respect to the input item(s) is given as follow:

**s** = c + **X** · **q**

where **s** is the score vector for an item _i_, c is a pre-computed constant, **X** is an item-feature matrix and **q** is a computed query vector based off the input item(s).

This formula is in fact very similar to the vector space model but under the new similarity measure given by **q**. In essence, each feature _j_ is weighted according to some q__j_. Therefore, we can readily approximate BSets by changing the similarity measure to **q**, and by using an Item Query where the best features (represented as unique terms) are selected according to the q__j_. It is true that we do not have all the q__j_ as this would be too expensive, but we do now have the ones which would most contribute to the final score of each matching document, under this new similarity measure.
## Implementation Notes

Item Query would probably be implemented as an experimental feature or as a plugin. We pretty much have all the elements required for Item Query, but here is a list of things to do and additional features:
- add scripting for term vectors filtering.
- add an option to dfs only the best n + k terms, and return the best n terms.
- implement like_text as an artificial document with cross fields.
- benchmark traditional MLT vs. Item Query.
- provide an example of using BSets on an image dataset such as MIRFLICKR.
</description><key id="90240606">11814</key><summary>Item Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>feature</label><label>Meta</label></labels><created>2015-06-22T23:21:04Z</created><updated>2016-01-18T20:42:50Z</updated><resolved>2016-01-18T20:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T20:42:49Z" id="172648403">Nothing further here - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch /_nodes api vs OS for max_file_descriptors?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11813</link><project id="" key="" /><description>When I check the /_nodes/process API:

```
"process":{"refresh_interval_in_millis":1000,"id":14588,"max_file_descriptors":65535,"mlockall":false}}
```

But when I check the OS (Ubuntu 14.04) for max file descriptors, it shows 1024:

```
# ulimit -n
1024
```

I'm inclined to believe the OS over elasticsearch, and when I check the number in use, its &lt;1024:

```
/proc/12799/fd# ls -l | wc -l
504
```

But why would elasticsearch misreport this?

```
# curl localhost:9200
{
  "status" : 200,
  "version" : {
    "number" : "1.5.2",
    "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    "build_timestamp" : "2015-04-27T09:21:06Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="90221889">11813</key><summary>Elasticsearch /_nodes api vs OS for max_file_descriptors?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sinneduy</reporter><labels><label>feedback_needed</label></labels><created>2015-06-22T21:50:26Z</created><updated>2015-06-24T13:20:10Z</updated><resolved>2015-06-23T19:47:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sinneduy" created="2015-06-23T05:53:04Z" id="114367008">I just updated the file descriptor limit in the OS, but I find it very strange that you could see contradictory values like that.  Could someone clarify in documentation how the file descriptor total is determined by ES?
</comment><comment author="clintongormley" created="2015-06-23T18:31:46Z" id="114601211">Presumably because the Ubuntu script sets it for you? https://github.com/elastic/elasticsearch/blob/1.6/src/packaging/common/packaging.properties#L18
</comment><comment author="sinneduy" created="2015-06-23T19:27:44Z" id="114616775">I'm on 1.5.2 though, and the code you're referencing is 1.6?

Also, shouldn't a note be stated that this gets automatically set for ubuntu here: https://www.elastic.co/guide/en/elasticsearch/guide/current/_file_descriptors_and_mmap.html
</comment><comment author="clintongormley" created="2015-06-23T19:47:17Z" id="114621861">@sinneduy also set in 1.5.2: https://github.com/elastic/elasticsearch/blob/1.5/src/deb/systemd/elasticsearch.service#L19

&gt; Also, shouldn't a note be stated that this gets automatically set for ubuntu here:

This info is already provided here: https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html
</comment><comment author="sinneduy" created="2015-06-23T19:54:45Z" id="114623331">@clintongormley Do you think that it might be worth linking the page I referenced to yours?
</comment><comment author="clintongormley" created="2015-06-24T13:20:10Z" id="114865101">@sinneduy I don't think so. The Definitive Guide has a different purpose from the Reference. I think that users who choose to use a package should read the reference docs for that package.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restrict fields with the same name in different types to have the same core settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11812</link><project id="" key="" /><description>We currently are very lax about allowing data types to conflict for the
same field name, across document types. This change makes the underlying
map in MapperService a 1-1 map of field name to field type, and throws
exception when new types are not compatible.

To still allow changing a type, with parameters that are allowed to be
changed, but for a field that exists in multiple types, a new parameter
to index creation and put mapping API is added: update_all_types.
This defaults to false, and the exception messages suggest using
this parameter when trying to modify a setting that is allowed to be
modified but is being limited by this restriction.

There are also a couple changes which try to base fields from new types
for dynamic mappings, and root mappers, on existing settings. For
dynamic mappings this is important if the dynamic defaults have been
changed. For root mappings, this is mostly just for backcompat when
pre 2.0 root mappers could have their field type changed.

fixes #8871
</description><key id="90212704">11812</key><summary>Restrict fields with the same name in different types to have the same core settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T21:07:36Z</created><updated>2015-06-24T15:07:21Z</updated><resolved>2015-06-24T15:07:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-23T09:52:17Z" id="114424705">The change looks great. I also played with the change, trying to add conflicting mappings at index creation time, through dynamic mappings and explicit mappings updates and things seem to work fine overall. I found some bugs wrt dynamic mappings that I fixed in your branch (hope you don't mind) and left comments for some other bugs.
</comment><comment author="s1monw" created="2015-06-23T10:51:51Z" id="114442983">I left a bunch of comments - I'd like to talk the concurrency aspects through with you in person here and I also wonder if we can get rid of `MapperAnalyzer.java` now?
</comment><comment author="rjernst" created="2015-06-23T16:02:00Z" id="114557077">@s1monw @jpountz I pushed a new commit. The one outstanding thing is the better error message for conflicting data types.

As for `MapperAnalyzer.java`, I would love to get rid of this, but afaict right now it is still the one thing that restricts a document to have access only to the fields defined within a type?
</comment><comment author="rjernst" created="2015-06-23T16:03:37Z" id="114557795">Actually, now I'm not sure about `MapperAnalyzer.java`. The `DocumentParser` looks up fields there when parsing, so by the time the analyzer needs to lookup the field, we should be able to just lookup the fieldtype for the index. I will try removing it.
</comment><comment author="rjernst" created="2015-06-23T19:36:49Z" id="114619151">@jpountz @s1monw I removed MapperAnalyzer and added a better message when field types aren't the same.
</comment><comment author="jpountz" created="2015-06-24T08:11:59Z" id="114776600">LGTM!
</comment><comment author="s1monw" created="2015-06-24T15:05:55Z" id="114900753">&gt;  I removed MapperAnalyzer and added a better message when field types aren't the same.

awesome!!!! - please open a followup for the craziness around concurrent maps etc.

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingClusterStateUpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/FieldNameAnalyzer.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentFieldMappers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMappersLookup.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldTypeLookup.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldTypeReference.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperAnalyzer.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperUtils.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MergeResult.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>core/src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file><file>core/src/test/java/org/elasticsearch/exists/SimpleExistsTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldMappersLookupTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldTypeTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ParentFieldLoadingBwcTest.java</file><file>core/src/test/java/org/elasticsearch/search/child/ParentFieldLoadingTest.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file></files><comments><comment>Merge pull request #11812 from rjernst/pr/8871</comment></comments></commit></commits></item><item><title>Better Java API for creating mappings dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11811</link><project id="" key="" /><description>It seems there is no nice way to programmatically define mappings with the Java API.

One has to generate a json string, or a map of maps, or comma-separated string format.

This is true for CreateIndexRequestBuilder.setSettings(), CreateIndexRequestBuilder.addMapping(), PutMappingRequestBuilder.setSource().

Can we have a builder API for creating mappings?
One that allows us to add types (and specific support for setting default), add fields, specify types, with options?
</description><key id="90183087">11811</key><summary>Better Java API for creating mappings dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2015-06-22T19:04:30Z</created><updated>2015-11-10T11:41:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickminutello" created="2015-11-10T11:41:17Z" id="155397167">Pretty please?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java api: add missing support for boost to GeoShapeQueryBuilder and TermsQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11810</link><project id="" key="" /><description>Both parsers support boost and parse it properly but when we create the query through java api we couldn't set the boost.

Relates to #11744
</description><key id="90148926">11810</key><summary>Java api: add missing support for boost to GeoShapeQueryBuilder and TermsQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T16:40:17Z</created><updated>2015-06-23T07:34:17Z</updated><resolved>2015-06-23T07:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-23T07:04:01Z" id="114385162">LGTM

This made me learn about BoostableQueryBuilder, could we open a follow-up issue to make this class pkg-private?
</comment><comment author="javanna" created="2015-06-23T07:05:32Z" id="114385433">if we make `BoostableQueryBuilder` package private, custom queries cannot implement it unless they are in the same package? Not sure that's good for plugins.
</comment><comment author="jpountz" created="2015-06-23T07:16:14Z" id="114387805">My understanding is that this interface is just a way to avoid re-defining the boost method (including javadocs) on every builder since core has tens of queries. Is it more than that? If no, then plugins could just make their queries implement `setBoost` without inheriting this interface, or even duplicate this interface?
</comment><comment author="javanna" created="2015-06-23T07:20:56Z" id="114388619">yes they could do that, but why would they? What's the advantage of making the interface package private in core?
</comment><comment author="jpountz" created="2015-06-23T07:28:32Z" id="114389920">In my opinion, it has several benefits:
- if you generate javadocs, the setBoost() method would appear as defined on each query builder as opposed to being inherited from this BoostableQueryBuilder class
- BoostableQueryBuilder would not be part of the API: imagine someone starts using "instanceof BoostableQueryBuilder" to know if a query supports boosting, then modifying/removing this interface would be considered a break, although it was only added for convenience in the beginning
- limited coupling between the core and plugins: we could change this BoostableQueryBuilder class in whatever way we want without having any impact on plugins

I think these benefit are worth having a few duplicated lines of code in plugins.
</comment><comment author="javanna" created="2015-06-23T07:34:17Z" id="114391405">thanks for the explanation @jpountz , I opened #11815 for this. Also, btw we may be removing this interface at some point too in the query-refactoring branch, as part of the consolidation of queryname and boost, we will see. This change gives us the freedom to do it indeed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file></files><comments><comment>Java api: add missing boost support to TermsQueryBuilder</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file></files><comments><comment>Java api: add missing boost support to GeoShapeQueryBuilder</comment></comments></commit></commits></item><item><title>Retrieving `_ttl` rebases the expiration date against `now()` instead of the value of `_timestamp`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11809</link><project id="" key="" /><description>When you index a document with a `_ttl`, what we store/index internally is the expiration date, ie. the sum of `_timestamp` and `_ttl`. However when we retrieve the value of the `_ttl`, we return the difference between the expiration date and `now()`.

While this is useful to see in how long documents will be expired, it makes it impossible to reindex a document with the exact same expiration date. In my opinion, we should return the original value of the `_ttl` instead, since it is relative to the value of `_timestamp` anyway.
</description><key id="90142079">11809</key><summary>Retrieving `_ttl` rebases the expiration date against `now()` instead of the value of `_timestamp`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label></labels><created>2015-06-22T16:08:08Z</created><updated>2016-05-12T12:56:04Z</updated><resolved>2016-05-12T12:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-22T16:37:47Z" id="114173563">It would be nice to have it fixed in 2.0 to make sure that all 2.0+ indices can be safely reindexed.
</comment><comment author="s1monw" created="2015-06-22T17:59:13Z" id="114199908">++
</comment><comment author="nithyanv" created="2015-06-25T05:39:05Z" id="115112959">+1 
</comment><comment author="nozari" created="2015-10-12T12:47:52Z" id="147387835">+1
</comment><comment author="clintongormley" created="2016-05-12T12:55:59Z" id="218748514">Closing in favour of #18280
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES server  crashed in Windows XP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11808</link><project id="" key="" /><description>Full error log as follows:

```
SYSTEM INFO:
---------------  S Y S T E M  ---------------

OS: Windows 5.1 Build 2600 (5.1.2600.6532)

CPU:total 2 (2 cores per cpu, 1 threads per core) family 6 model 23 stepping 10, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, tsc

Memory: 4k page, physical 3105304k(1569612k free), swap 5041156k(3364072k free)

vm_info: Java HotSpot(TM) Client VM (25.45-b02) for windows-x86 JRE (1.8.0_45-b15), built on Apr 30 2015 12:31:00 by "java_re" with MS VC++ 10.0 (VS2010)

time: Mon Jun 22 22:26:26 2015
elapsed time: 12543 seconds (0d 3h 29m 3s)

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x7c93240b, pid=3840, tid=2708
#
# JRE version: Java(TM) SE Runtime Environment (8.0_45-b15) (build 1.8.0_45-b15)
# Java VM: Java HotSpot(TM) Client VM (25.45-b02 mixed mode windows-x86 )
# Problematic frame:
# C  [ntdll.dll+0x1240b]
#
# Failed to write core dump. Minidumps are not enabled by default on client versions of Windows
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x46425000):  JavaThread "elasticsearch[Artemis][management][T#1]" daemon [_thread_in_native, id=2708, stack(0x48290000,0x482e0000)]

siginfo: ExceptionCode=0xc0000005, writing address 0x00000000

Registers:
EAX=0x4a2f0048, EBX=0x000027f8, ECX=0x00000000, EDX=0x00450050
ESP=0x482df1a0, EBP=0x482df1b8, ESI=0x4a2f0040, EDI=0x46af0000
EIP=0x7c93240b, EFLAGS=0x00010207

Top of Stack: (sp=0x482df1a0)
0x482df1a0:   4a2f0040 4a2f0040 4a2f0000 46af0000
0x482df1b0:   4a2f0000 02000002 482df1e8 7c946970
0x482df1c0:   000027f8 102f0040 000027f8 46af0000
0x482df1d0:   00000000 00000000 00000000 00000008
0x482df1e0:   00000200 00000013 482df22c 7c9392a3
0x482df1f0:   46af0000 00000000 46af0102 00000000
0x482df200:   4a2f0000 4a304000 000001ec 00002401
0x482df210:   00000000 46af0000 46af0102 00014000 

Instructions: (pc=0x7c93240b)
0x7c9323eb:   c3 50 57 e8 59 02 00 00 8d 8f 78 01 00 00 3b c8
0x7c9323fb:   75 62 8b d0 8b 4a 04 8d 46 08 89 10 89 48 04 56
0x7c93240b:   89 01 57 89 42 04 e8 a6 e2 ff ff 89 5d 08 0f b7
0x7c93241b:   db 29 5d 10 f6 46 05 10 74 15 80 7e 07 40 0f 83 


Register to memory mapping:

EAX=0x4a2f0048 is an unknown value
EBX=0x000027f8 is an unknown value
ECX=0x00000000 is an unknown value
EDX=0x00450050 is an unknown value
ESP=0x482df1a0 is pointing into the stack for thread: 0x46425000
EBP=0x482df1b8 is pointing into the stack for thread: 0x46425000
ESI=0x4a2f0040 is an unknown value
EDI=0x46af0000 is an unknown value


Stack: [0x48290000,0x482e0000],  sp=0x482df1a0,  free space=316k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [ntdll.dll+0x1240b]
C  [ntdll.dll+0x26970]
C  [ntdll.dll+0x192a3]
C  [ntdll.dll+0x1248d]
C  [sigar-x86-winnt.dll+0x23c90]
C  [sigar-x86-winnt.dll+0x23b87]
C  [sigar-x86-winnt.dll+0x4754]
C  [sigar-x86-winnt.dll+0x45ea]
C  [sigar-x86-winnt.dll+0x5ddc]
C  [sigar-x86-winnt.dll+0x659a]
C  [sigar-x86-winnt.dll+0x16278]
j  org.hyperic.sigar.ProcFd.gather(Lorg/hyperic/sigar/Sigar;J)V+0
j  org.hyperic.sigar.ProcFd.fetch(Lorg/hyperic/sigar/Sigar;J)Lorg/hyperic/sigar/ProcFd;+11
j  org.hyperic.sigar.Sigar.getProcFd(J)Lorg/hyperic/sigar/ProcFd;+2
j  org.elasticsearch.monitor.process.SigarProcessProbe.processStats()Lorg/elasticsearch/monitor/process/ProcessStats;+47
j  org.elasticsearch.monitor.process.ProcessService$ProcessStatsCache.refresh()Lorg/elasticsearch/monitor/process/ProcessStats;+7
j  org.elasticsearch.monitor.process.ProcessService$ProcessStatsCache.refresh()Ljava/lang/Object;+1
j  org.elasticsearch.common.util.SingleObjectCache.getOrRefresh()Ljava/lang/Object;+28
j  org.elasticsearch.monitor.process.ProcessService.stats()Lorg/elasticsearch/monitor/process/ProcessStats;+4
j  org.elasticsearch.node.service.NodeService.stats(Lorg/elasticsearch/action/admin/indices/stats/CommonStatsFlags;ZZZZZZZZZ)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats;+65
j  org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(Lorg/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction$NodeStatsRequest;)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats;+49
j  org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(Lorg/elasticsearch/action/support/nodes/NodeOperationRequest;)Lorg/elasticsearch/action/support/nodes/NodeOperationResponse;+5
j  org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run()V+45
J 6951% C1 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00c43e78 [0x00c43c70+0x208]
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [jvm.dll+0x155cb5]
V  [jvm.dll+0x21b12e]
V  [jvm.dll+0x155d4e]
V  [jvm.dll+0x155ed6]
V  [jvm.dll+0x155f47]
V  [jvm.dll+0xfb0cf]
V  [jvm.dll+0x17901c]
V  [jvm.dll+0x17910a]
V  [jvm.dll+0x1bd6e6]
C  [msvcr100.dll+0x5c556]
C  [msvcr100.dll+0x5c600]
C  [kernel32.dll+0xb729]
C  0x00000000

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  org.hyperic.sigar.ProcFd.gather(Lorg/hyperic/sigar/Sigar;J)V+0
j  org.hyperic.sigar.ProcFd.fetch(Lorg/hyperic/sigar/Sigar;J)Lorg/hyperic/sigar/ProcFd;+11
j  org.hyperic.sigar.Sigar.getProcFd(J)Lorg/hyperic/sigar/ProcFd;+2
j  org.elasticsearch.monitor.process.SigarProcessProbe.processStats()Lorg/elasticsearch/monitor/process/ProcessStats;+47
j  org.elasticsearch.monitor.process.ProcessService$ProcessStatsCache.refresh()Lorg/elasticsearch/monitor/process/ProcessStats;+7
j  org.elasticsearch.monitor.process.ProcessService$ProcessStatsCache.refresh()Ljava/lang/Object;+1
j  org.elasticsearch.common.util.SingleObjectCache.getOrRefresh()Ljava/lang/Object;+28
j  org.elasticsearch.monitor.process.ProcessService.stats()Lorg/elasticsearch/monitor/process/ProcessStats;+4
j  org.elasticsearch.node.service.NodeService.stats(Lorg/elasticsearch/action/admin/indices/stats/CommonStatsFlags;ZZZZZZZZZ)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats;+65
j  org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(Lorg/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction$NodeStatsRequest;)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats;+49
j  org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(Lorg/elasticsearch/action/support/nodes/NodeOperationRequest;)Lorg/elasticsearch/action/support/nodes/NodeOperationResponse;+5
j  org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run()V+45
J 6951% C1 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00c43e78 [0x00c43c70+0x208]
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x464f7c00 JavaThread "elasticsearch[Artemis][listener][T#1]" daemon [_thread_blocked, id=3412, stack(0x48470000,0x484c0000)]
  0x4524fc00 JavaThread "elasticsearch[Artemis][generic][T#5]" daemon [_thread_blocked, id=3532, stack(0x4a0f0000,0x4a140000)]
  0x466c0400 JavaThread "elasticsearch[Artemis][search][T#4]" daemon [_thread_blocked, id=336, stack(0x490a0000,0x490f0000)]
  0x466bf400 JavaThread "elasticsearch[Artemis][search][T#3]" daemon [_thread_blocked, id=3444, stack(0x49050000,0x490a0000)]
  0x466be400 JavaThread "elasticsearch[Artemis][search][T#2]" daemon [_thread_blocked, id=708, stack(0x49000000,0x49050000)]
  0x466bd800 JavaThread "elasticsearch[Artemis][search][T#1]" daemon [_thread_blocked, id=4012, stack(0x48fb0000,0x49000000)]
  0x4666e400 JavaThread "elasticsearch[Artemis][flush][T#1]" daemon [_thread_blocked, id=2268, stack(0x48dd0000,0x48e20000)]
  0x46628400 JavaThread "elasticsearch[Artemis][merge][T#1]" daemon [_thread_blocked, id=2344, stack(0x48d80000,0x48dd0000)]
  0x46575000 JavaThread "elasticsearch[Artemis][refresh][T#1]" daemon [_thread_blocked, id=2716, stack(0x48d30000,0x48d80000)]
  0x4652e400 JavaThread "elasticsearch[Artemis][index][T#2]" daemon [_thread_blocked, id=2464, stack(0x48a00000,0x48a50000)]
  0x464d9800 JavaThread "elasticsearch[Artemis][index][T#1]" daemon [_thread_blocked, id=1720, stack(0x47e90000,0x47ee0000)]
  0x458b6000 JavaThread "elasticsearch[Artemis][management][T#5]" daemon [_thread_blocked, id=3748, stack(0x48be0000,0x48c30000)]
  0x458b5400 JavaThread "elasticsearch[Artemis][management][T#4]" daemon [_thread_blocked, id=2936, stack(0x48b90000,0x48be0000)]
  0x46621800 JavaThread "elasticsearch[Artemis][management][T#3]" daemon [_thread_blocked, id=1960, stack(0x48b40000,0x48b90000)]
  0x4655b800 JavaThread "elasticsearch[Artemis][management][T#2]" daemon [_thread_blocked, id=1060, stack(0x48af0000,0x48b40000)]
  0x4666ac00 JavaThread "elasticsearch[Artemis][warmer][T#1]" daemon [_thread_blocked, id=3280, stack(0x48aa0000,0x48af0000)]
  0x46486400 JavaThread "elasticsearch[Artemis][fetch_shard_started][T#2]" daemon [_thread_blocked, id=2832, stack(0x48510000,0x48560000)]
  0x0088a800 JavaThread "DestroyJavaVM" [_thread_blocked, id=216, stack(0x00970000,0x009c0000)]
  0x46484000 JavaThread "elasticsearch[keepAlive/1.6.0]" [_thread_blocked, id=3216, stack(0x484c0000,0x48510000)]
  0x4647bc00 JavaThread "elasticsearch[Artemis][http_server_boss][T#1]{New I/O server boss #15}" daemon [_thread_in_native, id=2812, stack(0x48420000,0x48470000)]
  0x46467400 JavaThread "elasticsearch[Artemis][http_server_worker][T#4]{New I/O worker #14}" daemon [_thread_in_native, id=2264, stack(0x483d0000,0x48420000)]
  0x46455c00 JavaThread "elasticsearch[Artemis][http_server_worker][T#3]{New I/O worker #13}" daemon [_thread_in_native, id=3720, stack(0x48380000,0x483d0000)]
  0x46444400 JavaThread "elasticsearch[Artemis][http_server_worker][T#2]{New I/O worker #12}" daemon [_thread_in_native, id=2552, stack(0x48330000,0x48380000)]
  0x46432400 JavaThread "elasticsearch[Artemis][http_server_worker][T#1]{New I/O worker #11}" daemon [_thread_in_native, id=3020, stack(0x482e0000,0x48330000)]
=&gt;0x46425000 JavaThread "elasticsearch[Artemis][management][T#1]" daemon [_thread_in_native, id=2708, stack(0x48290000,0x482e0000)]
  0x46423800 JavaThread "elasticsearch[Artemis][riverClusterService#updateTask][T#1]" daemon [_thread_blocked, id=3872, stack(0x48240000,0x48290000)]
  0x463fb800 JavaThread "elasticsearch[Artemis][transport_client_timer][T#1]{Hashed wheel timer #1}" daemon [_thread_blocked, id=3704, stack(0x47e40000,0x47e90000)]
  0x463ec800 JavaThread "elasticsearch[Artemis][clusterService#updateTask][T#1]" daemon [_thread_blocked, id=3472, stack(0x47d00000,0x47d50000)]
  0x463eb800 JavaThread "elasticsearch[Artemis][discovery#multicast#receiver][T#1]" daemon [_thread_in_native, id=2368, stack(0x47cb0000,0x47d00000)]
  0x46363400 JavaThread "elasticsearch[Artemis][[http_server_boss.default]][T#1]{New I/O server boss #10}" daemon [_thread_in_native, id=2272, stack(0x47c60000,0x47cb0000)]
  0x4635f400 JavaThread "elasticsearch[Artemis][[http_server_worker.default]][T#4]{New I/O worker #9}" daemon [_thread_in_native, id=3400, stack(0x47c10000,0x47c60000)]
  0x463be800 JavaThread "elasticsearch[Artemis][[http_server_worker.default]][T#3]{New I/O worker #8}" daemon [_thread_in_native, id=1372, stack(0x47bc0000,0x47c10000)]
  0x463ac400 JavaThread "elasticsearch[Artemis][[http_server_worker.default]][T#2]{New I/O worker #7}" daemon [_thread_in_native, id=492, stack(0x47b70000,0x47bc0000)]
  0x4639a400 JavaThread "elasticsearch[Artemis][[http_server_worker.default]][T#1]{New I/O worker #6}" daemon [_thread_in_native, id=2468, stack(0x47b20000,0x47b70000)]
  0x4635a400 JavaThread "elasticsearch[Artemis][transport_client_boss][T#1]{New I/O boss #5}" daemon [_thread_in_native, id=1808, stack(0x47ad0000,0x47b20000)]
  0x46387800 JavaThread "elasticsearch[Artemis][transport_client_worker][T#4]{New I/O worker #4}" daemon [_thread_in_native, id=2036, stack(0x47980000,0x479d0000)]
  0x46341400 JavaThread "elasticsearch[Artemis][transport_client_worker][T#3]{New I/O worker #3}" daemon [_thread_in_native, id=1332, stack(0x47930000,0x47980000)]
  0x4633f800 JavaThread "elasticsearch[Artemis][transport_client_worker][T#2]{New I/O worker #2}" daemon [_thread_in_native, id=2720, stack(0x478e0000,0x47930000)]
  0x4633c800 JavaThread "elasticsearch[Artemis][transport_client_worker][T#1]{New I/O worker #1}" daemon [_thread_in_native, id=3012, stack(0x47890000,0x478e0000)]
  0x457cb800 JavaThread "elasticsearch[Artemis][[ttl_expire]]" daemon [_thread_blocked, id=2624, stack(0x47840000,0x47890000)]
  0x457c9c00 JavaThread "elasticsearch[Artemis][master_mapping_updater]" [_thread_blocked, id=4068, stack(0x477f0000,0x47840000)]
  0x45221400 JavaThread "elasticsearch[Artemis][scheduler][T#1]" daemon [_thread_blocked, id=804, stack(0x476a0000,0x476f0000)]
  0x45864c00 JavaThread "elasticsearch[Artemis][[timer]]" daemon [_thread_blocked, id=3124, stack(0x45ba0000,0x45bf0000)]
  0x451ce800 JavaThread "Service Thread" daemon [_thread_blocked, id=2616, stack(0x45540000,0x45590000)]
  0x451be000 JavaThread "C1 CompilerThread0" daemon [_thread_blocked, id=1840, stack(0x454f0000,0x45540000)]
  0x451bc800 JavaThread "Attach Listener" daemon [_thread_blocked, id=880, stack(0x454a0000,0x454f0000)]
  0x451bb400 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=3644, stack(0x45450000,0x454a0000)]
  0x451b6c00 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=1048, stack(0x45400000,0x45450000)]
  0x4518c000 JavaThread "Finalizer" daemon [_thread_blocked, id=2008, stack(0x453b0000,0x45400000)]
  0x45187000 JavaThread "Reference Handler" daemon [_thread_blocked, id=1684, stack(0x45360000,0x453b0000)]

Other Threads:
  0x45183c00 VMThread [stack: 0x45310000,0x45360000] [id=1800]
  0x451d0800 WatcherThread [stack: 0x45590000,0x455e0000] [id=2056]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap:
 par new generation   total 118016K, used 43670K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K,  39% used [0x02e00000, 0x0560a218, 0x09480000)
  from space 13056K,  20% used [0x0a140000, 0x0a3db660, 0x0ae00000)
  to   space 13056K,   0% used [0x09480000, 0x09480000, 0x0a140000)
 concurrent mark-sweep generation total 131072K, used 61982K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24193K, capacity 24366K, committed 24448K, reserved 24960K

Card table byte_map: [0x42e00000,0x43010000] byte_map_base: 0x42de9000

Marking Bits: (CMSBitMap*) 0x02aac040
 Bits: [0x43010000, 0x44c10000)

Mod Union Table: (CMSBitMap*) 0x02aac0a0
 Bits: [0x44c20000, 0x44c58000)

Polling page: 0x009c0000

CodeCache: size=32768Kb used=3082Kb max_used=3263Kb free=29685Kb
 bounds [0x009f0000, 0x00d40000, 0x029f0000]
 total_blobs=1627 nmethods=1352 adapters=205
 compilation: enabled

Compilation events (10 events):
Event: 12543.478 Thread 0x451be000 58707             org.elasticsearch.common.jackson.core.io.JsonStringEncoder::quoteAsUTF8 (502 bytes)
Event: 12543.481 Thread 0x451be000 nmethod 58707 0x00ca9188 code [0x00ca93f0, 0x00ca9e50]
Event: 12543.481 Thread 0x451be000 58708             org.elasticsearch.common.netty.buffer.ChannelBuffers::wrappedBuffer (211 bytes)
Event: 12543.482 Thread 0x451be000 nmethod 58708 0x00afe848 code [0x00afea40, 0x00afef70]
Event: 12543.492 Thread 0x451be000 58709             java.lang.ThreadLocal::getMap (5 bytes)
Event: 12543.492 Thread 0x451be000 nmethod 58709 0x00c49088 code [0x00c49180, 0x00c49200]
Event: 12543.503 Thread 0x451be000 58710             org.elasticsearch.common.jackson.core.json.UTF8JsonGenerator::writeFieldName (198 bytes)
Event: 12543.504 Thread 0x451be000 nmethod 58710 0x00aadf88 code [0x00aae130, 0x00aae554]
Event: 12543.505 Thread 0x451be000 58711             org.elasticsearch.common.jackson.core.util.JsonGeneratorDelegate::writeFieldName (9 bytes)
Event: 12543.505 Thread 0x451be000 nmethod 58711 0x00cc6e08 code [0x00cc6f10, 0x00cc6fac]

GC Heap History (10 events):
Event: 12494.342 GC heap before
{Heap before GC invocations=1578 (full 3):
 par new generation   total 118016K, used 106736K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K, 100% used [0x02e00000, 0x09480000, 0x09480000)
  from space 13056K,  13% used [0x09480000, 0x0963c2d0, 0x0a140000)
  to   space 13056K,   0% used [0x0a140000, 0x0a140000, 0x0ae00000)
 concurrent mark-sweep generation total 131072K, used 61725K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
Event: 12494.347 GC heap after
Heap after GC invocations=1579 (full 3):
 par new generation   total 118016K, used 2677K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K,   0% used [0x02e00000, 0x02e00000, 0x09480000)
  from space 13056K,  20% used [0x0a140000, 0x0a3dd5e0, 0x0ae00000)
  to   space 13056K,   0% used [0x09480000, 0x09480000, 0x0a140000)
 concurrent mark-sweep generation total 131072K, used 61776K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
}
Event: 12521.204 GC heap before
{Heap before GC invocations=1579 (full 3):
 par new generation   total 118016K, used 107637K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K, 100% used [0x02e00000, 0x09480000, 0x09480000)
  from space 13056K,  20% used [0x0a140000, 0x0a3dd5e0, 0x0ae00000)
  to   space 13056K,   0% used [0x09480000, 0x09480000, 0x0a140000)
 concurrent mark-sweep generation total 131072K, used 61776K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
Event: 12521.209 GC heap after
Heap after GC invocations=1580 (full 3):
 par new generation   total 118016K, used 1707K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K,   0% used [0x02e00000, 0x02e00000, 0x09480000)
  from space 13056K,  13% used [0x09480000, 0x0962acc0, 0x0a140000)
  to   space 13056K,   0% used [0x0a140000, 0x0a140000, 0x0ae00000)
 concurrent mark-sweep generation total 131072K, used 61835K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
}
Event: 12521.744 GC heap before
{Heap before GC invocations=1580 (full 3):
 par new generation   total 118016K, used 106667K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K, 100% used [0x02e00000, 0x09480000, 0x09480000)
  from space 13056K,  13% used [0x09480000, 0x0962acc0, 0x0a140000)
  to   space 13056K,   0% used [0x0a140000, 0x0a140000, 0x0ae00000)
 concurrent mark-sweep generation total 131072K, used 61835K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
Event: 12521.748 GC heap after
Heap after GC invocations=1581 (full 3):
 par new generation   total 118016K, used 1513K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K,   0% used [0x02e00000, 0x02e00000, 0x09480000)
  from space 13056K,  11% used [0x0a140000, 0x0a2ba440, 0x0ae00000)
  to   space 13056K,   0% used [0x09480000, 0x09480000, 0x0a140000)
 concurrent mark-sweep generation total 131072K, used 61907K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
}
Event: 12522.307 GC heap before
{Heap before GC invocations=1581 (full 3):
 par new generation   total 118016K, used 106473K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K, 100% used [0x02e00000, 0x09480000, 0x09480000)
  from space 13056K,  11% used [0x0a140000, 0x0a2ba440, 0x0ae00000)
  to   space 13056K,   0% used [0x09480000, 0x09480000, 0x0a140000)
 concurrent mark-sweep generation total 131072K, used 61907K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
Event: 12522.312 GC heap after
Heap after GC invocations=1582 (full 3):
 par new generation   total 118016K, used 1974K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K,   0% used [0x02e00000, 0x02e00000, 0x09480000)
  from space 13056K,  15% used [0x09480000, 0x0966daa0, 0x0a140000)
  to   space 13056K,   0% used [0x0a140000, 0x0a140000, 0x0ae00000)
 concurrent mark-sweep generation total 131072K, used 61979K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
}
Event: 12522.832 GC heap before
{Heap before GC invocations=1582 (full 3):
 par new generation   total 118016K, used 106934K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K, 100% used [0x02e00000, 0x09480000, 0x09480000)
  from space 13056K,  15% used [0x09480000, 0x0966daa0, 0x0a140000)
  to   space 13056K,   0% used [0x0a140000, 0x0a140000, 0x0ae00000)
 concurrent mark-sweep generation total 131072K, used 61979K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
Event: 12522.837 GC heap after
Heap after GC invocations=1583 (full 3):
 par new generation   total 118016K, used 2669K [0x02e00000, 0x0ae00000, 0x0ae00000)
  eden space 104960K,   0% used [0x02e00000, 0x02e00000, 0x09480000)
  from space 13056K,  20% used [0x0a140000, 0x0a3db660, 0x0ae00000)
  to   space 13056K,   0% used [0x09480000, 0x09480000, 0x0a140000)
 concurrent mark-sweep generation total 131072K, used 61982K [0x0ae00000, 0x12e00000, 0x42e00000)
 Metaspace       used 24046K, capacity 24206K, committed 24320K, reserved 24960K
}

Deoptimization events (0 events):
No events

Internal exceptions (10 events):
Event: 12048.731 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x06df6400) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12108.732 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x07e52610) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12168.732 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x05318080) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12228.732 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x05991be0) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12288.733 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x06396a30) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12348.733 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x052634a8) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12408.733 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x08ef8d98) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12468.734 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x046e8710) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12528.734 Thread 0x463eb800 Exception &lt;a 'java/net/SocketTimeoutException': Receive timed out&gt; (0x051bcf20) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 735]
Event: 12543.404 Thread 0x4655b800 Exception &lt;a 'org/hyperic/sigar/SigarNotImplementedException'&gt; (0x0bc3a918) thrown at [C:\re\workspace\8-2-build-windows-i586-cygwin\jdk8u45\3627\hotspot\src\share\vm\prims\jni.cpp, line 709]

Events (10 events):
Event: 12543.511 loading class org/elasticsearch/transport/TransportStats$Fields
Event: 12543.511 loading class org/elasticsearch/transport/TransportStats$Fields done
Event: 12543.512 loading class org/elasticsearch/http/HttpStats$Fields
Event: 12543.512 loading class org/elasticsearch/http/HttpStats$Fields done
Event: 12543.513 loading class org/elasticsearch/indices/breaker/AllCircuitBreakerStats$Fields
Event: 12543.513 loading class org/elasticsearch/indices/breaker/AllCircuitBreakerStats$Fields done
Event: 12543.514 loading class org/elasticsearch/indices/breaker/CircuitBreakerStats$Fields
Event: 12543.514 loading class org/elasticsearch/indices/breaker/CircuitBreakerStats$Fields done
Event: 12543.515 Executing VM operation: RevokeBias
Event: 12543.516 Executing VM operation: RevokeBias done


Dynamic libraries:
0x00400000 - 0x00433000     C:\Program Files\Java\jdk1.8.0_45\bin\java.exe
0x7c920000 - 0x7c9b6000     C:\WINDOWS\system32\ntdll.dll
0x7c800000 - 0x7c91e000     C:\WINDOWS\system32\kernel32.dll
0x77da0000 - 0x77e49000     C:\WINDOWS\system32\ADVAPI32.dll
0x77e50000 - 0x77ee3000     C:\WINDOWS\system32\RPCRT4.dll
0x77fc0000 - 0x77fd1000     C:\WINDOWS\system32\Secur32.dll
0x77d10000 - 0x77da0000     C:\WINDOWS\system32\USER32.dll
0x77ef0000 - 0x77f39000     C:\WINDOWS\system32\GDI32.dll
0x77180000 - 0x77283000     C:\WINDOWS\WinSxS\x86_Microsoft.Windows.Common-Controls_6595b64144ccf1df_6.0.2600.6028_x-ww_61e65202\COMCTL32.dll
0x77be0000 - 0x77c38000     C:\WINDOWS\system32\msvcrt.dll
0x77f40000 - 0x77fb6000     C:\WINDOWS\system32\SHLWAPI.dll
0x76300000 - 0x7631d000     C:\WINDOWS\system32\IMM32.DLL
0x62c20000 - 0x62c29000     C:\WINDOWS\system32\LPK.DLL
0x73fa0000 - 0x7400b000     C:\WINDOWS\system32\USP10.dll
0x78aa0000 - 0x78b5f000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\msvcr100.dll
0x6d260000 - 0x6d620000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\client\jvm.dll
0x71a40000 - 0x71a4b000     C:\WINDOWS\system32\WSOCK32.dll
0x71a20000 - 0x71a37000     C:\WINDOWS\system32\WS2_32.dll
0x71a10000 - 0x71a18000     C:\WINDOWS\system32\WS2HELP.dll
0x76b10000 - 0x76b3a000     C:\WINDOWS\system32\WINMM.dll
0x77bd0000 - 0x77bd8000     C:\WINDOWS\system32\VERSION.dll
0x76bc0000 - 0x76bcb000     C:\WINDOWS\system32\PSAPI.DLL
0x6f6f0000 - 0x6f6fc000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\verify.dll
0x6daf0000 - 0x6db11000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\java.dll
0x6f780000 - 0x6f793000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\zip.dll
0x7d590000 - 0x7dd84000     C:\WINDOWS\system32\SHELL32.dll
0x759d0000 - 0x75a7f000     C:\WINDOWS\system32\USERENV.dll
0x6ee90000 - 0x6eea5000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\net.dll
0x76d30000 - 0x76d48000     C:\WINDOWS\system32\IPHLPAPI.DLL
0x76d10000 - 0x76d28000     C:\WINDOWS\system32\MPRAPI.dll
0x77c90000 - 0x77cc2000     C:\WINDOWS\system32\ACTIVEDS.dll
0x76de0000 - 0x76e05000     C:\WINDOWS\system32\adsldpc.dll
0x5fdd0000 - 0x5fe25000     C:\WINDOWS\system32\NETAPI32.dll
0x76f30000 - 0x76f5c000     C:\WINDOWS\system32\WLDAP32.dll
0x76af0000 - 0x76b01000     C:\WINDOWS\system32\ATL.DLL
0x76990000 - 0x76ace000     C:\WINDOWS\system32\ole32.dll
0x770f0000 - 0x7717b000     C:\WINDOWS\system32\OLEAUT32.dll
0x76e50000 - 0x76e5e000     C:\WINDOWS\system32\rtutils.dll
0x71b70000 - 0x71b83000     C:\WINDOWS\system32\SAMLIB.dll
0x76060000 - 0x761b6000     C:\WINDOWS\system32\SETUPAPI.dll
0x68000000 - 0x68036000     C:\WINDOWS\system32\rsaenh.dll
0x6eeb0000 - 0x6eebf000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\nio.dll
0x6edf0000 - 0x6edfa000     C:\Program Files\Java\jdk1.8.0_45\jre\bin\management.dll
0x719c0000 - 0x719fe000     C:\WINDOWS\System32\mswsock.dll
0x76ef0000 - 0x76f17000     C:\WINDOWS\system32\DNSAPI.dll
0x76f80000 - 0x76f88000     C:\WINDOWS\System32\winrnr.dll
0x76f90000 - 0x76f96000     C:\WINDOWS\system32\rasadhlp.dll
0x45b40000 - 0x45b7d000     C:\Documents and Settings\Administrator\Local Settings\Temp\jna-146731693\jna5942829506579235253.dll
0x46900000 - 0x4694b000     D:\elasticsearch-1.6.0\lib\sigar\sigar-x86-winnt.dll
0x724c0000 - 0x72518000     C:\WINDOWS\system32\pdh.dll
0x76320000 - 0x76367000     C:\WINDOWS\system32\comdlg32.dll
0x765e0000 - 0x76675000     C:\WINDOWS\system32\CRYPT32.dll
0x76db0000 - 0x76dc2000     C:\WINDOWS\system32\MSASN1.dll
0x73540000 - 0x7357d000     C:\WINDOWS\system32\ODBC32.dll
0x4b4f0000 - 0x4b4f6000     C:\WINDOWS\system32\odbcbcp.dll
0x46a90000 - 0x46aa7000     C:\WINDOWS\system32\odbcint.dll
0x5e8f0000 - 0x5e8f9000     C:\WINDOWS\system32\perfos.dll
0x5e8e0000 - 0x5e8ed000     C:\WINDOWS\system32\perfproc.dll
0x71a90000 - 0x71aa2000     C:\WINDOWS\system32\mpr.dll
0x5e920000 - 0x5e929000     C:\WINDOWS\system32\perfdisk.dll
0x60fd0000 - 0x61025000     C:\WINDOWS\system32\hnetcfg.dll
0x71a00000 - 0x71a08000     C:\WINDOWS\System32\wshtcpip.dll
0x68d60000 - 0x68e01000     C:\WINDOWS\system32\dbghelp.dll

VM Arguments:
jvm_args: -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des-foreground=yes -Des.path.home=D:\elasticsearch-1.6.0 
java_command: org.elasticsearch.bootstrap.Elasticsearch
java_class_path (initial): ;D:\elasticsearch-1.6.0/lib/elasticsearch-1.6.0.jar;D:\elasticsearch-1.6.0/lib/antlr-runtime-3.5.jar;D:\elasticsearch-1.6.0/lib/apache-log4j-extras-1.2.17.jar;D:\elasticsearch-1.6.0/lib/asm-4.1.jar;D:\elasticsearch-1.6.0/lib/asm-commons-4.1.jar;D:\elasticsearch-1.6.0/lib/elasticsearch-1.6.0.jar;D:\elasticsearch-1.6.0/lib/groovy-all-2.4.0.jar;D:\elasticsearch-1.6.0/lib/jna-4.1.0.jar;D:\elasticsearch-1.6.0/lib/jts-1.13.jar;D:\elasticsearch-1.6.0/lib/log4j-1.2.17.jar;D:\elasticsearch-1.6.0/lib/lucene-analyzers-common-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-core-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-expressions-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-grouping-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-highlighter-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-join-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-memory-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-misc-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-queries-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-queryparser-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-sandbox-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-spatial-4.10.4.jar;D:\elasticsearch-1.6.0/lib/lucene-suggest-4.10.4.jar;D:\elasticsearch-1.6.0/lib/spatial4j-0.4.1.jar;D:\elasticsearch-1.6.0/lib/sigar/sigar-1.6.4.jar
Launcher Type: SUN_STANDARD

Environment Variables:
JAVA_HOME=C:\Program Files\Java\jdk1.8.0_45
PATH=C:\Documents and Settings\All Users\Application Data\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\Python27;C:\Python27\Scripts;C:\Program Files\OpenVPN\bin;C:\Program Files\scala\bin
USERNAME=Administrator
OS=Windows_NT
PROCESSOR_IDENTIFIER=x86 Family 6 Model 23 Stepping 10, GenuineIntel

```
</description><key id="90127067">11808</key><summary>ES server  crashed in Windows XP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels /><created>2015-06-22T15:09:11Z</created><updated>2015-06-22T18:29:48Z</updated><resolved>2015-06-22T18:29:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T18:29:46Z" id="114209240">seems like you are running into a sigar issue. We are looking into removing the dependency on sigar in 2.0 but for you since you are running 1.6 you can try starting elasticsearch with `-Des.bootstrap.sigar=false` (or put `bootstrap.sigar: false` into your `elasticsearch.yml`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add OS name to _nodes and _cluster/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11807</link><project id="" key="" /><description>we currently don't expose this.

This adds the following to the OS section of `_nodes`:

```
"os": {
     "name": "Mac OS X",
     ...
 }
```

 and the following to the OS section of `_cluster/stats`:

```
 "os": {
    ...
    "names": [
       {
          "name": "Mac OS X",
           "count": 1
       }
     ],
     ...
 },
```
</description><key id="90120814">11807</key><summary>Add OS name to _nodes and _cluster/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T14:44:00Z</created><updated>2015-06-22T18:38:31Z</updated><resolved>2015-06-22T18:38:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T14:46:30Z" id="114137869">left some comments
</comment><comment author="bleskes" created="2015-06-22T18:32:55Z" id="114209911">@s1monw pushed another commit
</comment><comment author="s1monw" created="2015-06-22T18:34:20Z" id="114210198">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>core/src/main/java/org/elasticsearch/monitor/os/OsInfo.java</file><file>core/src/main/java/org/elasticsearch/monitor/os/OsService.java</file></files><comments><comment>Add OS name to _nodes and _cluster/nodes</comment></comments></commit></commits></item><item><title>Add option to the search api to deal with unmapped fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11806</link><project id="" key="" /><description>The way queries and aggregations deal with with unmapped fields is inconsistent. For example the `nested` query fails if the defined `path` doesn't point to an object field, but the `nested` aggregator doesn't fail and just returns an empty bucket. How the search api should deal with unmapped fields is application specific, so how ES should deal with unmapped fields should become controllable from the search api.

The idea is to add `unmapped_fields` top level field to the search api (and maybe also be settable via a query string parameter). All features that interact with the mapping (queries, aggregations) should take this setting into account and act accordingly.

The `unmapped_fields` field should support the following setting:
- `error`, returns a parse error if a field can't be resolved by the mapping. I think this should be the default.
- `ignore`, ignores the query or aggregation in question and returns no results.
- `warn`, like `ignore`, but also prints a warning in the log if a field can't be resolved.

I think initially we should support the `unmapped_fields` option for the following queries/aggregations:
- `nested` query.
- `has_child` query.
- `has_parent` query.
- `nested` aggregation.
- `reverse_nested` aggregation.
- `children` aggregation.
- Maybe also the features around geo point and geo shape?

See PR #9521 for the initial discussion.
</description><key id="90114279">11806</key><summary>Add option to the search api to deal with unmapped fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label></labels><created>2015-06-22T14:25:50Z</created><updated>2015-07-03T10:29:56Z</updated><resolved>2015-07-03T10:29:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-22T18:21:38Z" id="114207124">+1

But why have the parameter start with an underscore? I think it should just be `unmapped_fields`?
</comment><comment author="martijnvg" created="2015-06-22T18:24:05Z" id="114207703">&gt; But why have the parameter start with an underscore?

I copied this from the discussion in the pr... but yes, it should be without underscore.
</comment><comment author="clintongormley" created="2015-06-23T18:25:28Z" id="114599236">Three questions:
- Should this be top-level or per query/agg?  (more control if the latter)
- Should the behaviour be configurable to include/exclude results (eg consider this a match if it doesn't have the field)
- Should the default be strict or lenient?
</comment><comment author="martijnvg" created="2015-06-24T08:29:40Z" id="114786299">&gt; Should this be top-level or per query/agg? (more control if the latter)

Initially the idea is to expose it as a top level option, we already have many options on aggs and queries. But if we really need to we can the option on the each query and agg.

&gt; Should the behaviour be configurable to include/exclude results (eg consider this a match if it doesn't have the field)

Good question. I think think this should be configurable via an extra setting or extra `unmapped_fields` option next to `ignore`.

&gt; Should the default be strict or lenient?

I prefer to be explicit about these kind of misconfigurations, so strict.
</comment><comment author="clintongormley" created="2015-06-24T15:22:55Z" id="114908286">&gt; Initially the idea is to expose it as a top level option, we already have many options on aggs and queries. But if we really need to we can the option on the each query and agg.

I'm not so sure. Practically, you probably want to target a specific field, rather than all fields everywhere, eg "if no geo-point is specified, then include or exclude those docs".

&gt; I prefer to be explicit about these kind of misconfigurations, so strict.

In other words, this would break:

```
PUT foo/user/1
{
  "name": "joe bloggs"
}

PUT bar/user/1
{
  "full_name": "joe bloggs"
}

GET _search?name=joe
```

I'm leaning towards defaulting to leniency, which is what we do for most fields today.
</comment><comment author="clintongormley" created="2015-06-24T15:25:44Z" id="114909559">This is what we do with sorting today: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_ignoring_unmapped_fields
</comment><comment author="rjernst" created="2015-06-24T16:42:48Z" id="114937811">&gt; In other words, this would break:
&gt; 
&gt; ```
&gt; PUT foo/user/1
&gt; {
&gt;   "name": "joe bloggs"
&gt; }
&gt; PUT bar/user/1
&gt; {
&gt;   "full_name": "joe bloggs"
&gt; }
&gt; GET _search?name=joe
&gt; ```

That should not fail. It will find the `name` field type at the index level. It will simply not match any documents in the `bar` type.
</comment><comment author="rjernst" created="2015-06-24T16:43:19Z" id="114937913">Bah, disregard my last comment, I mistook foo/bar for types... :/
</comment><comment author="clintongormley" created="2015-06-24T16:44:06Z" id="114938258">@rjernst `foo` and `bar` are diffrent indices.  The equivalent of this will fail today for (eg) geo queries
</comment><comment author="rjernst" created="2015-06-24T16:45:27Z" id="114938546">But regarding that, this seems like something a user should be made aware of? If they are trying to do a search across multiple indexes, in which the fields they are using don't exist, they should have to actively decide what to do, not just get some default behavior that they don't know exists? Being strict will ensure they are aware, while being lenient will continue to lead to users asking why they aren't matching documents when they eg mispelled a field name.
</comment><comment author="clintongormley" created="2015-06-24T16:48:07Z" id="114939111">@rjernst We already have the `explain` API which will tell you why a document doesn't match.  I argue that being strict in this case makes every day use more onerous than it needs to be, while we already have tools to debug less frequent issues like the one you mention.
</comment><comment author="rjernst" created="2015-06-24T16:51:29Z" id="114939778">I don't think you should need a tool to realize you mispelled a field name. It is no different to me than mispelling a parameter.
</comment><comment author="martijnvg" created="2015-06-25T05:34:34Z" id="115110876">&gt; I'm leaning towards defaulting to leniency, which is what we do for most fields today.

@clintongormley I didn't really think about regular fields, but about fields types like `nested` and `geo_shape`. In those cases I would like to be informed if I picked an incorrect field, since those fields allow to do somewhat more advanced search features instead of simple matching.

I'm unsure about simple term / match queries etc. and what to do if the field doesn't exist in the mapping. Today the parser just create a kind of a raw term query, instead of returning all or no results or fail. I think this behaviour is historic as mapping updates used to be async (in 1.x and before), so a field may not exist in the mapping yet will the document did successfully indexed.
</comment><comment author="kmeiresonne" created="2015-07-02T08:07:11Z" id="117951880">&gt; Should this be top-level or per query/agg? (more control if the latter)

I believe a top-level control should be possible. We're working extensively with dynamic templates, so in these cases when you create the index and mapping, and have no documents yet, all our built-in nested queries currently fail since the nested paths haven't been created in the mapping yet.

&gt; Should the behaviour be configurable to include/exclude results (eg consider this a match if it doesn't have the field)

That would be nice to have, although I see little use cases to include a document in the result if it doesn't have the field.

&gt; Should the default be strict or lenient?

In my (short) experience with elasticsearch, it has the tendency to be lenient against these type of things, so that would be my preference.
</comment><comment author="clintongormley" created="2015-07-03T10:29:55Z" id="118309718">Closing in favour of #12016
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify Plugin Manager for official plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11805</link><project id="" key="" /><description>Plugin Manager can now use another simplified form when a user wants to install an official plugin hosted at elasticsearch download service.

The form we use is:

``` sh
bin/plugin install pluginname
```

As plugins share now the same version as elasticsearch, we can automatically guess what is the exact current version of the plugin manager script.

Also, download service will now use `/org.elasticsearch.plugins/pluginName/pluginName-version.zip` URL path to download a plugin.

If the older form is provided (`user/plugin/version` or `user/plugin`), we will still use:
- elasticsearch download service at `/user/plugin/plugin-version.zip`
- maven central with groupIp=user, artifactId=plugin and version=version
- github with user=user, repoName=plugin and tag=version
- github with user=user, repoName=plugin and branch=master if no version is set

Note that community plugin providers can use other download services by using `--url` option.

If you try to use the new form with a non core elasticsearch plugin, the plugin manager will reject
it and will give you all known core plugins.

```
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
    -i, --install [plugin name]       : Downloads and installs listed plugins [*]
    -t, --timeout [duration]          : Timeout setting: 30s, 1m, 1h... (infinite by default)
    -r, --remove  [plugin name]       : Removes listed plugins
    -l, --list                        : List installed plugins
    -v, --verbose                     : Prints verbose messages
    -s, --silent                      : Run in silent mode
    -h, --help                        : Prints this help message

 [*] Plugin name could be:
     elasticsearch-plugin-name    for Elasticsearch 2.0 Core plugin (download from download.elastic.co)
     elasticsearch/plugin/version for elasticsearch commercial plugins (download from download.elastic.co)
     groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
     username/repository          for site plugins (download from github master)

Elasticsearch Core plugins:
 - elasticsearch-analysis-icu
 - elasticsearch-analysis-kuromoji
 - elasticsearch-analysis-phonetic
 - elasticsearch-analysis-smartcn
 - elasticsearch-analysis-stempel
 - elasticsearch-cloud-aws
 - elasticsearch-cloud-azure
 - elasticsearch-cloud-gce
 - elasticsearch-delete-by-query
 - elasticsearch-lang-javascript
 - elasticsearch-lang-python
```
</description><key id="90104579">11805</key><summary>Simplify Plugin Manager for official plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T13:55:36Z</created><updated>2015-07-08T08:13:45Z</updated><resolved>2015-07-07T16:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-24T08:40:39Z" id="114789772">@tlrx Could you review it please?
</comment><comment author="spinscale" created="2015-06-30T12:09:48Z" id="117154524">one last thing: do we need to retain bwc compatibility or at least help the user in case an older plugin tries to be installed.

Checking the official [kuromoji docs](https://github.com/elastic/elasticsearch-analysis-kuromoji) this is the way to install the plugin

``` bash
bin/plugin install elasticsearch/elasticsearch-analysis-kuromoji/2.6.0
```

what happens if I do this with master? Will it work? Will it break? Should be catch this in the PluginManager and write it to `bin/plugin install kuromoji` or just output a big warning, that the user tries to install something old? Do we need documentation for this, that official plugins are now installed differently?

I havent formed an opinion here yet (except that we have to document it, not sure, if we should try to be smart or just issue a warning), but we should get a consensus
</comment><comment author="clintongormley" created="2015-06-30T12:13:18Z" id="117155550">&gt; Checking the official kuromoji docs this is the way to install the plugin

We're moving the plugin documentation for 2.0, but we will need to preserve the old instructions for the 1.x branches.  
</comment><comment author="spinscale" created="2015-06-30T12:37:43Z" id="117161878">@clintongormley sure thing. I meant, people might have put that in some automated installation scripts. And it is not given that they just go to the website they are used to go to, to extract the installation information and copy paste this into a terminal

I just played around with the current impl. Installing kuromoji without any version specified results in a cryptic error message on 1.6

```
bin/plugin install elasticsearch/elasticsearch-analysis-kuromoji
-&gt; Installing elasticsearch/elasticsearch-analysis-kuromoji...
Trying https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/archive/master.zip...
Downloading ..............................DONE
Installed elasticsearch/elasticsearch-analysis-kuromoji into /tmp/elasticsearch-1.6.0/plugins/analysis-kuromoji
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
    -i, --install [plugin name]       : Downloads and installs listed plugins [*]
    -t, --timeout [duration]          : Timeout setting: 30s, 1m, 1h... (infinite by default)
    -r, --remove  [plugin name]       : Removes listed plugins
    -l, --list                        : List installed plugins
    -v, --verbose                     : Prints verbose messages
    -s, --silent                      : Run in silent mode
    -h, --help                        : Prints this help message

 [*] Plugin name could be:
     elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)
     groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
     username/repository          for site plugins (download from github master)

Message:
   Error while installing plugin, reason: IllegalArgumentException: Plugin installation assumed to be site plugin, but contains source code, aborting installation.
```

My proposal is to spit out an error message for all the new official plugins 2.0 if someone tries to install kuromoji and all the others the old way, telling the user this is now an official supported plugin to be installed via `bin/plugin install kuromoji` in case the user typed `bin/plugin install elasticsearch/elasticsearch-analysis-kuromoji[/$VERSION]`
</comment><comment author="clintongormley" created="2015-06-30T12:43:03Z" id="117163207">++
</comment><comment author="dadoonet" created="2015-06-30T13:15:06Z" id="117176920">@spinscale Pushed new changes.

Was thinking of detecting when user ask for `bin/plugin install elasticsearch/foo/bar`. We can't really reject such a request because of commercial plugins which don't use (yet?) `org.elasticsearch.plugins`.

Or do you want us to have a static builtin list of plugins which moved to elasticsearch repo?
</comment><comment author="spinscale" created="2015-06-30T13:37:26Z" id="117187249">Hm, the best thing would be, if we could find out during build/packaging time which plugins we ship and only act on those. This would allow us to remove/add plugins arbitrarily per release. Do you think that is feasible?
</comment><comment author="dadoonet" created="2015-06-30T13:48:23Z" id="117190088">I don't think it is and I'm not sure we should. It's like having a circular dependency between plugin and core IMO.

The "easiest" workaround would be to publish our commercial plugins (and change the groupId to make all that consistent) to `org.elasticsearch.commercial`... or something.

I would not try to fix that for now so we don't block this PR but this is something we can think about for GA. WDYT?
</comment><comment author="spinscale" created="2015-06-30T15:45:12Z" id="117233975">@clintongormley what do you think? If we hardcode such a check, we might have problems, moving such a plugin out of the core again. Or we just make sure the plugin list is in sync. And then we can leave it hardcoded. As I think this is a huge improvement for the not-knowing user, I am for the hardcoded solution for now, which means we need to pay attention when we add or remove plugins to the elasticsearch repo.
</comment><comment author="clintongormley" created="2015-07-01T08:09:09Z" id="117521309">Besides Shield etc, there are also things like https://github.com/elastic/elasticsearch-migration.

I think hard coding is a simple solution. It's not like the list changes often, and we can add a test to https://github.com/elastic/elasticsearch/blob/master/dev-tools/smoke_test_plugins.py to ensure that the list is up to date.
</comment><comment author="dadoonet" created="2015-07-06T13:23:18Z" id="118852881">@spinscale I added a new commit. Let me know
</comment><comment author="spinscale" created="2015-07-06T14:26:40Z" id="118872066">should we also add a paragraph for the migration docs for this?
</comment><comment author="dadoonet" created="2015-07-06T15:53:49Z" id="118907065">@spinscale I added 2 more commits
</comment><comment author="spinscale" created="2015-07-06T17:30:53Z" id="118932983">we also need to list the plugins that are considered official somewhere I guess
</comment><comment author="dadoonet" created="2015-07-06T20:12:02Z" id="118981394">@spinscale I added the plugin list in the migration guide. All the documentation will be updated with this PR #12040.
</comment><comment author="spinscale" created="2015-07-07T09:26:40Z" id="119135798">I think we are pretty much there, I will do some further command line testing and report back.

One last thing: If someone uses the `bin/plguin install elasticsearch-analysis-kuromoji` pattern and the plugin cannot be found, do you think it makes sense to just list the available plugins in the output to help the user?
</comment><comment author="spinscale" created="2015-07-07T10:16:51Z" id="119158165">the log4j config seems to have a problem when using the zip package

```
bin/plugin install elasticsearch/license/latest
-&gt; Installing elasticsearch/license/latest...
Trying http://download.elastic.co/elasticsearch/license/license-latest.zip...
Downloading ............................................DONE
log4j:WARN No appenders could be found for logger (org.elasticsearch.bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Installed elasticsearch/license/latest into /tmp/elasticsearch-2.0.0-SNAPSHOT/plugins/license
```
</comment><comment author="spinscale" created="2015-07-07T10:21:41Z" id="119161400">**Update**: This is a problem in master, not in this branch, I will take a look
</comment><comment author="dadoonet" created="2015-07-07T10:40:57Z" id="119164881">@spinscale Yeah. See also #12064
I just hit the same issues in plugins.
</comment><comment author="dadoonet" created="2015-07-07T10:54:27Z" id="119168166">@spinscale Added a new commit based on your comments. I just print now the list of Core plugins.
I could also replace this list by `bin/plugin install XXX` for each element... Not sure what is the best option.
</comment><comment author="spinscale" created="2015-07-07T11:54:39Z" id="119180484">I would only output the official plugin list, if the user has tried to install a plugin that does not exist (personal taste, freel free to ignore)... apart from that LGTM
</comment><comment author="spinscale" created="2015-07-07T16:16:41Z" id="119257578">all right, rant tests with `-Dtests.slow` and `-Dtests.network`.. LGTM
</comment><comment author="dadoonet" created="2015-07-07T16:19:01Z" id="119258385">Thank you Alex! 
</comment><comment author="dadoonet" created="2015-07-08T08:13:45Z" id="119487669">@spinscale I was reading again this PR and I think I wrote something wrong.
I was considering `org.elasticsearch.plugins` as the `groupId` but it should be `org.elasticsearch.plugin` (for example: https://github.com/elastic/elasticsearch/blob/master/plugins/analysis-icu/pom.xml#L8) 

I will come with an update.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file></files><comments><comment>remove elasticsearch- from name of official plugins</comment></comments></commit></commits></item><item><title>Remove reroute with no reassign</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11804</link><project id="" key="" /><description>Its not used in our codebase anymore, so no need for it
</description><key id="90097035">11804</key><summary>Remove reroute with no reassign</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T13:24:41Z</created><updated>2015-06-23T18:10:36Z</updated><resolved>2015-06-22T13:54:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-22T13:36:08Z" id="114105019">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java</file></files><comments><comment>Remove reroute with no reassign</comment><comment>Its not used in our codebase anymore, so no need for it</comment><comment>closes #11804</comment></comments></commit></commits></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11803</link><project id="" key="" /><description>Someone raised an issue complaining that the call to create the account document doesn't work.

https://discuss.elastic.co/t/account-json-file-is-not-loading-in-cluster/24083

The accounts JSON is the syntax for a single request not a bulk request.
</description><key id="90084532">11803</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dantuffery</reporter><labels /><created>2015-06-22T12:26:20Z</created><updated>2015-06-22T12:45:28Z</updated><resolved>2015-06-22T12:45:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dantuffery" created="2015-06-22T12:45:27Z" id="114088829">Closing - This is not an issue as the request does work with the downloaded JSON.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11802</link><project id="" key="" /><description>Someone raised an issue complaining that the call to create the account document doesn't work.

https://discuss.elastic.co/t/account-json-file-is-not-loading-in-cluster/24083

The accounts JSON is the syntax for a single request not a bulk request.
</description><key id="90083901">11802</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2015-06-22T12:22:53Z</created><updated>2015-06-22T12:23:26Z</updated><resolved>2015-06-22T12:23:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>fix json syntax in filters-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11801</link><project id="" key="" /><description>Seems like somebody had missed the double quotes.
</description><key id="90083669">11801</key><summary>fix json syntax in filters-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels><label>docs</label></labels><created>2015-06-22T12:21:35Z</created><updated>2015-06-24T07:31:16Z</updated><resolved>2015-06-23T18:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="golubev" created="2015-06-22T12:21:54Z" id="114083643">My company - LUN UA LLC - signed the CCLA and had listed me as a contributor. And I've signed the ICCLA under that one above.
</comment><comment author="clintongormley" created="2015-06-23T18:01:22Z" id="114591612">thanks @golubev - merged
</comment><comment author="golubev" created="2015-06-24T07:31:15Z" id="114762886">Thanks @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11801 from golubev/patch-6</comment></comments></commit></commits></item><item><title>Fix indentation in deb init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11800</link><project id="" key="" /><description /><key id="90081838">11800</key><summary>Fix indentation in deb init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arcz</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T12:11:27Z</created><updated>2015-07-09T13:07:51Z</updated><resolved>2015-07-09T13:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T16:27:21Z" id="119650493">Thanks for your PR, would you mind signing the contributor agreement? https://www.elastic.co/contributor-agreement
</comment><comment author="arcz" created="2015-07-09T10:29:18Z" id="119902361">Hey, no problem, signed.
</comment><comment author="jpountz" created="2015-07-09T13:07:51Z" id="119955596">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11800 from arcz/master</comment></comments></commit></commits></item><item><title>Load percolator queries before shard is marked POST_RECOVERY</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11799</link><project id="" key="" /><description>If we mark the shard as being in POST_RECOVERY before the percolator
is fully set up we might expose it to the user as fully searchable before
all queries are loaded. This can lead to wrong results especially in tests
when a shard is concurrently marked as STARTED.

This commit also removes unneeded abstractions on IndexShard where readoperations
should be allowed when the purpose is a write.

Closes #10722
</description><key id="90072004">11799</key><summary>Load percolator queries before shard is marked POST_RECOVERY</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T11:14:08Z</created><updated>2015-06-23T18:12:25Z</updated><resolved>2015-06-22T14:33:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T11:14:44Z" id="114070476">her is a failure related to this http://build-us-00.elastic.co/job/es_core_master_centos/5330/
</comment><comment author="bleskes" created="2015-06-22T12:22:35Z" id="114083742">Looks good - left one comment.
</comment><comment author="martijnvg" created="2015-06-22T12:46:57Z" id="114089080">LGTM
</comment><comment author="bleskes" created="2015-06-22T14:22:22Z" id="114126776">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make dev-tools and rest-api-spec projects depend on elasticsearch-parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11798</link><project id="" key="" /><description>At the moment this have to depend on the sonatype parent pom because they are referenced in Elasticsearch-parent and so having elasticsearch-parent as their parent project creates a circular dependency.

Further comments about his can be found here: https://github.com/elastic/elasticsearch/pull/11752#issuecomment-113557238
</description><key id="90066976">11798</key><summary>Make dev-tools and rest-api-spec projects depend on elasticsearch-parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">colings86</reporter><labels><label>build</label></labels><created>2015-06-22T10:47:28Z</created><updated>2015-08-18T17:07:42Z</updated><resolved>2015-08-12T09:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-11T17:54:24Z" id="129992131">@colings86 I'm unsure if we still have to fix that. WDYT?
</comment><comment author="colings86" created="2015-08-12T08:26:41Z" id="130214956">@dadoonet both the projects still declare the sonatype parent pom as their parent but I have no opinion on whether that is actually an issue. Feel free to close if you think its ok as is
</comment><comment author="dadoonet" created="2015-08-12T09:11:55Z" id="130228546">Thanks @colings86. Closing as your "eclipse" initial issue has been fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fold `plugin` script into `elasticsearch` script with -plugin option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11797</link><project id="" key="" /><description>Today, we highjack a generic script name called `plugin` to install plugins with ES. Other products might have this script name and it would be nice if we are better citizens of $PATH here. We should allow to install plugins using the `elasticsearch` script, and deprecate the `plugin` script.
</description><key id="90061605">11797</key><summary>Fold `plugin` script into `elasticsearch` script with -plugin option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Plugins</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label></labels><created>2015-06-22T10:18:25Z</created><updated>2016-04-11T19:11:53Z</updated><resolved>2016-01-18T20:41:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimmyjones2" created="2015-06-22T13:13:37Z" id="114098517">Same as #11651?
</comment><comment author="proegssilb" created="2015-06-23T21:47:08Z" id="114650652">Fixes the same problem as #11651, but different proposal. This approach would still make packaging on Linux a lot easier, unless a distro decides to replace the elasticsearch script (I don't work on distros, I don't know how likely it is).
</comment><comment author="clintongormley" created="2016-01-18T20:41:54Z" id="172648213">We've decided against doing this as bin/elasticsearch should be run as the elasticsearch user, and bin/plugin as root
</comment><comment author="jimmyjones2" created="2016-01-20T20:29:13Z" id="173350174">This is fine, but a bit unfortunate that downstream packagers will always have to use a different name to upstream. How about elasticsearch-plugin?
</comment><comment author="Hubbitus" created="2016-01-24T10:32:48Z" id="174277472">In Fedora we already use name `elasticsearch-plugin`. But in any case as it should be compiled from source without network build procedure and start script significant differs than shipped upstream. Meantime it will be helpful if such distinguished name will appear in docs.
</comment><comment author="clintongormley" created="2016-01-26T10:35:32Z" id="174949066">@Hubbitus this is happening in https://github.com/elastic/elasticsearch/pull/16022
</comment><comment author="Hubbitus" created="2016-04-11T19:11:53Z" id="208507350">@clintongormley thank you.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: make QueryBuilder an interface again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11796</link><project id="" key="" /><description>It is handy to have a base interface, not just an abstract class, for all of our query builders. This gives us  more flexibility especialy with complex class hierarchies. For instance SpanTermQueryBuilder extends BaseTermQueryBuilder, but also needs to be marked as a SpanQueryBuilder. The latter is a marker interface that should extend QueryBuilder which is not possible unless QueryBuilder actually is an interface.

Also removed the ambiguity between `queryId` and `getName`, left only the latter.
</description><key id="90059950">11796</key><summary>Query refactoring: make QueryBuilder an interface again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-22T10:08:24Z</created><updated>2015-06-22T13:20:41Z</updated><resolved>2015-06-22T13:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-22T10:29:19Z" id="114060167">@javanna makes, sense to me, only left one comment, an idea I had when I extended SpanQueryBuilder interface in https://github.com/elastic/elasticsearch/pull/11717
</comment><comment author="javanna" created="2015-06-22T12:30:11Z" id="114085754">I pushed new commits that address review feedback, also cleaned up a bit the QueryBuilder interface. Can you have another look @cbuescher ? Also thinking if it makes sense to backport (part of) this to master to minimize merge conflicts. WDYT?
</comment><comment author="cbuescher" created="2015-06-22T13:05:51Z" id="114095251">LGTM, tried this on #11717 and it works great, makes inheritance structure much more understandable. Also won't hurt to backport the change from abstract class to interface to master.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Query refactoring: make QueryBuilder an interface again and rename current abstract class to AbstractQueryBuilder</comment></comments></commit></commits></item><item><title>Use abstract runnable in scheduled ping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11795</link><project id="" key="" /><description>this caused a test failure over the weekend

http://build-us-00.elastic.co/job/es_core_master_small/2232/testReport/junit/junit.framework/TestSuite/org_elasticsearch_transport_netty_NettyScheduledPingTests/
</description><key id="90033277">11795</key><summary>Use abstract runnable in scheduled ping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T07:40:55Z</created><updated>2015-06-23T17:56:24Z</updated><resolved>2015-06-22T08:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-06-22T08:19:53Z" id="114036907">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Merge pull request #11795 from s1monw/abstract_runnable_ping</comment></comments></commit></commits></item><item><title>Lockdown `_timestamp` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11794</link><project id="" key="" /><description>This is a follow up to #8143 and #6730 for _timestamp. It removes
support for `path`, as well as any field type settings, and
enables docvalues for _timestamp, for 2.0.  Users who need to
adjust these settings can use a date field.
</description><key id="90031460">11794</key><summary>Lockdown `_timestamp` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T07:30:12Z</created><updated>2015-06-25T11:01:04Z</updated><resolved>2015-06-22T17:21:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-22T09:28:19Z" id="114049856">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/document/BulkTests.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampTests.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLTests.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Merge pull request #11794 from rjernst/fix/lockdown-timestamp</comment></comments></commit></commits></item><item><title>Fail nicely if `nested` query with `inner_hits` is used in a percolator query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11793</link><project id="" key="" /><description>Previously using `inner_hits` on the `nested` query in the percolate api result in a NPE. With this change a more descriptive error is returned.

PR for #11672
</description><key id="90026479">11793</key><summary>Fail nicely if `nested` query with `inner_hits` is used in a percolator query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-22T06:42:46Z</created><updated>2015-06-23T19:28:30Z</updated><resolved>2015-06-23T13:05:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-22T09:31:47Z" id="114050368">LGTM

Can you put a nice description of the change here since pull requests are what ends up in the change log?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Oracle doc link to getting started page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11792</link><project id="" key="" /><description>Since there is a recommended version of JDK, it would be helpful to provide a link to the Oracle documentation. Since there are many versions of Java, those that are new or infrequent users of Java would find the link helpful. Thanks!
</description><key id="89953521">11792</key><summary>Add Oracle doc link to getting started page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">willingc</reporter><labels><label>docs</label></labels><created>2015-06-21T19:57:48Z</created><updated>2015-06-23T17:51:26Z</updated><resolved>2015-06-23T17:51:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T17:51:13Z" id="114589381">thanks @willingc - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add Oracle doc link to getting started page</comment></comments></commit></commits></item><item><title>Row-centric output for _cat/fielddata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11791</link><project id="" key="" /><description>After reviewing the code I believe the cleanest approach was to use the `_cat/fielddata` call to output node-level totals only and use the `_cat/fielddata/{fields}` call to have field-level details. The latter still outputs node-level totals, but if you only specify a field for which there is no fielddata, no row will be shown for the node. The first call could also be useful if you had many fields and were only interested in totals. You can use `*` to request all fields.

This clearly separates the row-level detail from the aggregate view. 

I updated the docs to reflect this behavior.

Closes #10249
</description><key id="89952999">11791</key><summary>Row-centric output for _cat/fielddata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">szroland</reporter><labels><label>:CAT API</label><label>breaking</label><label>enhancement</label></labels><created>2015-06-21T19:50:54Z</created><updated>2015-12-29T23:49:00Z</updated><resolved>2015-12-29T23:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-07-20T10:43:27Z" id="122845917">@jasontedor could you review this please?
</comment><comment author="drewr" created="2015-09-01T13:50:53Z" id="136729160">Left a small comment, but looks good. Let's fix that and then wait for @jasontedor. Thanks for taking this @szroland!
</comment><comment author="jasontedor" created="2015-12-14T18:09:30Z" id="164513569">@szroland Have you been able to address @drewr's comment? And would you be able to rebase this on master? You'll have some conflicts in the fielddata REST tests.
</comment><comment author="jasontedor" created="2015-12-29T23:48:57Z" id="167901210">Closing due to no feedback.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a null-check for XContentBuilder#field for BigDecimals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11790</link><project id="" key="" /><description>Add a null-check for XContentBuilder#field for BigDecimals

Closes #11699
</description><key id="89947920">11790</key><summary>Add a null-check for XContentBuilder#field for BigDecimals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbarthelmes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-21T19:10:24Z</created><updated>2015-06-25T09:32:24Z</updated><resolved>2015-06-25T07:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file></files><comments><comment>Merge pull request #11790 from jbarthelmes/master</comment></comments></commit></commits></item><item><title>Sync issue causing data loss when node comes back online</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11789</link><project id="" key="" /><description>Running Elastic Search 1.6 on OSX, from home brew, I have the following situation:
- Two node cluster.
- Both nodes can be master if the other goes down, all are allowed to store data.
- Node 1 goes down (say, I shut down the server). I connect to Node 2 and continue writing data.
- At some later point, Node 1 comes back up.
- Node 2 discards all its data in favor of the documents that were on Node 1.

It would seem like a bug, do let me know if this is expected to happen for some reason.
</description><key id="89784031">11789</key><summary>Sync issue causing data loss when node comes back online</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ricardojmendez</reporter><labels /><created>2015-06-20T15:39:47Z</created><updated>2015-06-29T10:09:17Z</updated><resolved>2015-06-24T14:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-21T02:51:02Z" id="113856722">This is not what should happen.
Are you seeing indices/documents go missing?
</comment><comment author="ricardojmendez" created="2015-06-21T06:48:09Z" id="113869770">Yes. All documents that were inserted on Node 2 while Node 1 was down disappear.
</comment><comment author="s1monw" created="2015-06-22T18:33:01Z" id="114209928">can you give me some more information ie. configuration of your index, are you using any special features, are you running on the same physical machine etc. etc. Can you reproduce this problem and if so can you switch on debug logging so we can see what's going on?
</comment><comment author="bleskes" created="2015-06-22T18:35:41Z" id="114210638">@ricardojmendez when node_1 came back online, did you see in the logs that it accepted node2 as a master? I'm looking for something like:

```
[2015-06-22 20:35:14,433][INFO ][cluster.service          ] [Infant Terrible] detected_master [Hulk 2099][dCuk05JITNCtrKKURPGYsw][Boazs-MBP.local][inet[/192.168.1.178:9300]], added {[Hulk 2099][dCuk05JITNCtrKKURPGYsw][Boazs-MBP.local][inet[/192.168.1.178:9300]],}, reason: zen-disco-receive(from master [[Hulk 2099][dCuk05JITNCtrKKURPGYsw][Boazs-MBP.local][inet[/192.168.1.178:9300]]])
```
</comment><comment author="ricardojmendez" created="2015-06-22T18:48:45Z" id="114217309">@s1monw: Not using any special features I'm aware of - it was a vanilla install. Both are running on different machines. I've seen the issue twice, once when testing what would happen, and the second one when I had just moved to work on a laptop for some time, then came back with it to the same network where the original desktop was.   I had meant to first back up the database in case it happened again, to have a repro case, but forgot.

I haven't tested since, will see about finding a time window.

@bleskes: On the logs for that day I see it on Node2. I believe Node1 was up before I brought Node2 into the network (I may have restarted it after booting Node1, as iirc I was running it from the terminal).
</comment><comment author="bleskes" created="2015-06-22T19:30:31Z" id="114229673">thx. I'm confused though - you said "Node 1 goes down" and "At some later point, Node 1 comes back up." . I was referring to the logs at the moment node 1 comes back. I suspect it doesn't see node2 and decides it's alone and re-uses it's local copy as a last resort (albeit it being stale).
</comment><comment author="ricardojmendez" created="2015-06-22T19:48:41Z" id="114236197">@bleskes: I was jetlagged when they rejoined the network, so looking back right now I can't be 100% sure that Node2 on the network when I booted Node 1 (I had turned wireless off on the plane) which is why I thought to clarify it.

When Node 1 comes back, it does not have any log of finding Node2 as a master. It does have a record of Node2 joining, which matches that of Node2 finding Node1 as a master.

Regarding your scenario:  I'd understand why Node 1 would keep a stale copy and _not sync_, but given that Node2 had data in it, would you expect that Node2 would then dump all new records in favor of Node1's old state?

When Node2 syncs with Node1, do they do it by exchanging transaction logs (or their equivalents) or by just having whomever gets designated as master propagate its current state?
</comment><comment author="bleskes" created="2015-06-24T14:44:16Z" id="114893923">&gt; I'd understand why Node 1 would keep a stale copy and not sync, but given that Node2 had data in it, would you expect that Node2 would then dump all new records in favor of Node1's old state?

It's important to realize that once node1 was elected as master and designated it's own copy as primary, that shard copy will accept (versioned) writes. Those writes may conflict with what ever operations that may have found their way to node2's copy while node1 was down. There are a couple of ways to solve this conflict,  almost all of them complex. Things like CRDTs (compromising on functionality to only what CRDTs can do), try to do some kind of last write wins and suffering all the pains of time shifting, offload the pain to the user (repair on read) and so fourth. In ES we chose to keep things simple and rely on the mastership of the master node and it's decision to choose a primary shard. We never try to resolve operations from multiple copies but rather make sure all copies revert themselves to the primary shard (this is similar to algorithms to like RAFT).

&gt; When Node2 syncs with Node1, do they do it by exchanging transaction logs (or their equivalents) or by just having whomever gets designated as master propagate its current state?

No it - make sure it will have a true copy of node1's primary.

Last I have to say that 2 is a tricky number in distributed systems as there is no majority. A two node cluster is good for not loosing data due to hardware failures but you have to make sure the nodes will not be able to operate independently (set `min_master_nodes` to 2). Without this you run the risk of having split brains and thus data loss and other bad things...

I'm going to close this now as we seemed to have found the source of the problem. Do feel free to reopen if you feel differently.
</comment><comment author="ricardojmendez" created="2015-06-24T17:07:02Z" id="114943523">Thanks. To be absolutely sure we're talking about the same case @bleskes (since there's mention of record versioning), on a two node scenario, if:

1) Node1 and Node2 lose contact with each other.
2) During that time, 
2.1) Node1 doesn't modify, insert or delete any records.
2.2) Node2 doesn't modify or delete any existing records, only inserts new ones.
3) When they rejoin, Node1 gets elected as master.

Then even though there are _no conflicts or changes_ on the shared records, and only new records inserted on Node2, then Node2 will discard all its newly created records, given they don't exist on the master.

Is that correct?
</comment><comment author="bleskes" created="2015-06-29T10:09:17Z" id="116592363">that's correct. At the moment we don't try to do a doc by doc analysis and try to figure what is safe to keep and what should be rejected.

Note that that point 3 is weird in normal procedure. If Node2 is was on and a master, node1 will join it rather than elect it self (in which case everything goes OK). The issue is that node1 was elected as master first and node2 has joined it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndexWithShadowReplicasTests.testIndexOnSharedFSRecoversToAnyNode failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11788</link><project id="" key="" /><description>This test fails on master if there is any load. It appears related to #11769. 
</description><key id="89720216">11788</key><summary>IndexWithShadowReplicasTests.testIndexOnSharedFSRecoversToAnyNode failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-20T05:27:20Z</created><updated>2015-07-07T21:00:35Z</updated><resolved>2015-07-07T21:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T17:31:13Z" id="114581420">@areek please take a look
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/recovery/SharedFSRecoverySourceHandler.java</file><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Currently when an engine is failed, it is marked as corrupted regardless of</comment><comment>the failure type. This change marks the engine as corrupted only when the failure</comment><comment>is caused by an actual index corrruption. When an engine is failed for other</comment><comment>reasons, the engine is only closed without removing the shard state.</comment></comments></commit></commits></item><item><title>Add Allocation Index Priority</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11787</link><project id="" key="" /><description>The idea is allow a simple, dynamically modifiable priority for indices. Something akin to:

```
$ curl -XPUT host:9200/my-index/_settings -d '{
  "index.priority" : 1
}'
```

By default, all priorities would default with `index.priority : 0`, thus being of equal priority unless modified. From there, the comparison could go to index creation date/time as the next priority level, then finally by name for the final order.

&gt; `index.priority` &gt; `index.creation_date` &gt; reverse sorted index name

This allows pretty much every use case to specify recovery order for best-case recovery order for their use case. Time-based solutions would pretty much work out of the box because the creation date would prioritize recovery of _today's_ index first, thus giving the best chance to return to fully replicated indexing as soon as possible. Similarly, falling back to the reverse sorted index name gives arbitrary--yet predictable--sorting for simultaneously-created indices (however unlikely) that have no time basis (`index-z`, `index-y`, ...), but YMD-formatted date/time strings will properly sort (`logstash-2015.05.03` will always come before `logstash-2015.05.02` or earlier dates). Finally, the arbitrary priority allows users to take control of the process for cases where there is no time basis, such as when a "main" index is created, but then minor indices are created from time-to-time. Even better, this would be as simple as setting it once with a high value, then not setting or selectively setting the minor indices.
</description><key id="89711723">11787</key><summary>Add Allocation Index Priority</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Recovery</label><label>feature</label><label>release highlight</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-20T03:50:42Z</created><updated>2015-07-07T08:25:59Z</updated><resolved>2015-07-06T20:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-20T03:55:20Z" id="113704323">This is a duplicate of #10069, but I had been thinking about it a bit over the course of the past few weeks and I wanted to write it down. We can close this one in favor of that one (or vice versa), but I wanted to get it written down.
</comment><comment author="avleen" created="2015-06-23T19:01:30Z" id="114610027">I like the defaults proposed here :)
</comment><comment author="s1monw" created="2015-07-07T05:44:20Z" id="119079739">FYI  pushed the #11975 to 1.x and master
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/PriorityComparator.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/test/java/org/elasticsearch/gateway/PriorityComparatorTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Add basic recovery prioritization to GatewayAllocator</comment></comments></commit></commits></item><item><title>`series_arithmetic` aggregation throws `aggregation_execution_exception`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11786</link><project id="" key="" /><description>Hi,

I am trying to use `series_arithmetic` aggregator ( I have cloned master ) to calculate a ratio from 2 buckets but I get `aggregation_execution_exception`. 

This is the mapping:

```
{
"listings": {
    "mappings": {
        "listing": {
            "_size": {
                "store": true
            },
            "_timestamp": {
                "store": false
            },
            "properties": {
                "agencyName": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "agencyPhoneNumber": {
                    "type": "string"
                },
                "agentName": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "agentPhoneNumber": {
                    "type": "string"
                },
                "amenities": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "area": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "building": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "city": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "coordinates": {
                    "type": "geo_point"
                },
                "furnished": {
                    "type": "string"
                },
                "lastUpdate": {
                    "type": "date",
                    "format": "dateOptionalTime"
                },
                "listingCategory": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "noOfBathrooms": {
                    "type": "long"
                },
                "noOfBedrooms": {
                    "type": "long"
                },
                "price": {
                    "type": "long"
                },
                "rentalTerm": {
                    "type": "string"
                },
                "size": {
                    "type": "double"
                },
                "source": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "subarea": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "title": {
                    "type": "string"
                },
                "type": {
                    "type": "string",
                    "index": "not_analyzed"
                }
            }
        }
    }
}}
```

And this is the query I'm running: 

```
{
"size":"0",
"query":{
"filtered":{
"filter":{
"and":[
{"term":{"area":"X1"}}, {"term":{"listingCategory":"C1"}}, {"range":{"price":{"lte":"1000000"}}}
]
}
}
},
"aggs":{
"Sale":{"filter":{"term":{"type":"sale"}}, "aggs":{"price":{"percentiles":{"field":"price", "percents":[75]}}}}, 
"Rent":{"filter":{"term":{"type":"rent"}}, "aggs":{"price":{"percentiles":{"field":"price", "percents":[75]}}}},
"ROI":{"series_arithmetic":{"buckets_path":{"var1":"Sale&gt;price", "var2":"Rent&gt;price"}, "script":"var1/var2"}}
}}
```

And this is the URL I'm requesting to, running elasticsearch locally

```
http://localhost:9200/listings/listing/_search
```

And this is the error I'm getting:

```
{
"error": {
    "root_cause": [
        {
            "type": "aggregation_execution_exception",
            "reason": "Invalid pipeline aggregation named [ROI] of type [series_arithmetic]. Only sibling pipeline aggregations are allowed at the top level"
        }
    ],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [
        {
            "shard": 0,
            "index": "listings",
            "node": "haHUkxYXQ_eKE1kAuNUhMw",
            "reason": {
                "type": "aggregation_execution_exception",
                "reason": "Invalid pipeline aggregation named [ROI] of type [series_arithmetic]. Only sibling pipeline aggregations are allowed at the top level"
            }
        }
    ]
},
"status": 500
```

Thank you.
</description><key id="89693336">11786</key><summary>`series_arithmetic` aggregation throws `aggregation_execution_exception`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HammadH</reporter><labels /><created>2015-06-20T00:41:50Z</created><updated>2015-06-22T09:41:05Z</updated><resolved>2015-06-22T09:41:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-22T09:41:04Z" id="114051757">The `series_arithmetic` aggregation is required to be used as a sub-aggregation to a multi-bucket aggregation (such as the `terms` aggregation or the `histogram` aggregation. Here you are trying to use it as a top-level aggregation, which is not supported.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Check licenses of jars extracted from the ZIP file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11785</link><project id="" key="" /><description>This changes the license checker to extract jars from the ZIP file instead of relying on the `target_lib/` directory.  If there is a tar.gz file, it checks to ensure that the JARs are identical to those in the ZIP file.

Also added LICENSE and NOTICE files for each plugin and for their dependencies.

Note: the slf4j jar was in the core/target/lib directory, but it is not included in the ZIP or tar.gz file.  Is this a bug?

Closes #11766
</description><key id="89665308">11785</key><summary>Check licenses of jars extracted from the ZIP file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-19T21:32:03Z</created><updated>2015-08-13T14:28:39Z</updated><resolved>2015-06-23T10:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T11:33:51Z" id="114073979">LGTM
</comment><comment author="clintongormley" created="2015-06-23T10:56:18Z" id="114444234">@kimchy when we build elasticsearch, slf4j is included in the target/libs directory, but not included in the .zip/.tar.gz packages.  Is this correct? Or should we be shipping it too?
</comment><comment author="kimchy" created="2015-06-23T10:57:15Z" id="114444564">@clintongormley no need to ship slf4j as part of any of our distributions
</comment><comment author="clintongormley" created="2015-06-23T10:57:49Z" id="114444665">thanks, pushing
</comment><comment author="s1monw" created="2015-06-23T11:04:28Z" id="114445611">@kimchy should we mark the dependency as provided then?
</comment><comment author="kimchy" created="2015-06-23T11:06:07Z" id="114445833">@s1monw yea, I think it makes sense
</comment><comment author="dadoonet" created="2015-06-23T11:20:24Z" id="114447921">Optional or Provided ?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11785 from clintongormley/license_check</comment></comments></commit></commits></item><item><title>PAMI Disconnect‏</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11784</link><project id="" key="" /><description>Hi,
       I am using PAMI to get Asterisk Manager Events. please see my code below. I wanted to get $event-&gt;getDialStatus() = 'ANSWER' of Dial  Event and as i get result disconnect the Asterisk Manager and read the remaining code. Thanks.

&lt;?php 
use PAMI\Client\Impl\ClientImpl as PamiClient;

require_once 'PAMI/Autoloader/Autoloader.php';
PAMI\Autoloader\Autoloader::register();

$pamiClientOptions = array(
    'host' =&gt; '127.0.0.1',
    'scheme' =&gt; 'tcp://',
    'port' =&gt; 5038,
    'username' =&gt; 'ncs',
    'secret' =&gt; 'ncs',
    'connect_timeout' =&gt; 10000,
    'read_timeout' =&gt; 10000
);

$pamiClient = new PamiClient($pamiClientOptions);

// Open the connection
$pamiClient-&gt;open();

use PAMI\Message\Event\EventMessage;
use PAMI\Message\Event\DialEvent;

$pamiClient-&gt;registerEventListener(function (EventMessage $event) {
    print_r($event);
},
        function ($event) {

```
    $re = $event instanceof DialEvent &amp;&amp; $event-&gt;getDialStatus() = 'ANSWER';
```

if ($re){
echo "Found Event\n";
}
  }
);

$running = true;

while($running) {
    $pamiClient-&gt;process();
    usleep(1000);
}

// Close the connection
# $pamiClient-&gt;close();

echo "Code Ended&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Read This Line\n";
?&gt;       
</description><key id="89661320">11784</key><summary>PAMI Disconnect‏</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qasimkhans</reporter><labels /><created>2015-06-19T21:07:18Z</created><updated>2015-06-23T17:01:38Z</updated><resolved>2015-06-23T17:01:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T17:01:38Z" id="114572963">Hi @qasimkhans 

please ask questions like these on the mailing list instead (and I advise explaining what the problem is, rather than just pasting your code and expecting them to figure it out).

https://discuss.elastic.co
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move merge simulation of fieldtype settings to fieldtype method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11783</link><project id="" key="" /><description>For #8871, we need to be able to check field types are compatible,
without comparing FieldMappers.  This change moves the simulation
checks (which generate merge conflicts) for any properties of
MappedFieldTypes into a new method, validateCompatible.

This also simplifies the merge code which merges settings
between the old and new fieldtypes. Previously, each subclass
of FieldMapper would have to set its own fieldtype settings.
However, now that we have .clone(), which perfectly copies
all properties (with subclasses accounted for), we can now
do a simple clone when merging.

Finally, this fixes a subtle bug in merging, in which if
merging has conflicts, and we were not simulating, we would
still update the field type, even though it was not compatible!

NOTE: there is one test failure I am trying to track down with
timestamp merging. Otherwise, all tests pass.
</description><key id="89657346">11783</key><summary>Move merge simulation of fieldtype settings to fieldtype method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-19T20:48:10Z</created><updated>2015-06-23T17:03:13Z</updated><resolved>2015-06-19T22:45:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-19T21:52:40Z" id="113654352">Looks good, let's figure out this test failure. :)
</comment><comment author="rjernst" created="2015-06-19T22:44:37Z" id="113663577">I found the bug; I accidentally removed the copy of customFieldDataSettings on merge. Good to know a single timestamp test is all that tests this...

I also added a TODO for the indexOptions issue, since I just copied the existing checks as they are (but I will address this in a future PR).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file></files><comments><comment>Merge pull request #11783 from rjernst/refactor/field-type-merge</comment></comments></commit></commits></item><item><title>Update filter-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11782</link><project id="" key="" /><description>Fixed inconsistency is filter aggregation documentation.
Request specifies a filter on `red_products`, but the response mentions `in_stock_products`.
</description><key id="89649201">11782</key><summary>Update filter-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">weeyum</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-19T20:02:12Z</created><updated>2015-07-01T08:18:07Z</updated><resolved>2015-07-01T08:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T17:00:12Z" id="114572671">Hi @weeyum 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="weeyum" created="2015-06-30T17:27:33Z" id="117270183">@clintongormley signed.
</comment><comment author="clintongormley" created="2015-07-01T08:17:11Z" id="117528875">thanks @weeyum - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update filter-aggregation.asciidoc</comment></comments></commit></commits></item><item><title>Provide warning in log files when shards cannot be allocated due to version differences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11781</link><project id="" key="" /><description>This is related to https://github.com/elastic/elasticsearch/issues/9471 previously filed.

This is to simulate a condition during rolling restart upgrade.  When performing rolling restart upgrade and there are nodes in the cluster running an older version (eg. 1.4, node1 below) and a newer version (eg. 1.5, node_name below).  If new indices are created repeatedly during this time, some of the replica shards can become unassigned (expected).  

![image](https://cloud.githubusercontent.com/assets/7216393/8259654/faedf854-1672-11e5-9949-57dc8dc4b70a.png)

And a reroute command (if you attempt to allocate the replicas manually: https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html) will return the following intuitive explanation:

``` json
            {
               "decider": "node_version",
               "decision": "NO",
               "explanation": "target node version [1.4.2] is older than source node version [1.5.0]"
            },
```

This is great, except that the administrator has no way of telling from the logs unless they go and try to run the reroute command with explain directly because we don't appear to be logging anything into the log files indicating the reason (not even at cluster.service: TRACE level).

This is a request to provide this information right at the INFO or WARN level so that the admins will know that there is not enough (newer verison, eg. 1.5.0) to allocate those replicas at that point in time.
</description><key id="89622606">11781</key><summary>Provide warning in log files when shards cannot be allocated due to version differences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>discuss</label><label>enhancement</label></labels><created>2015-06-19T17:54:35Z</created><updated>2015-12-04T14:42:02Z</updated><resolved>2015-12-04T14:42:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-23T12:26:24Z" id="114474680">I can see how this is useful but it's hard to distinguish between the cases when to log it and when not. I tried to think about a good way to log this by default but I can't come up with a good solution unfortunately
</comment><comment author="clintongormley" created="2015-06-23T19:23:30Z" id="114615966">Could this exception be shown in the `store_exception` added in https://github.com/elastic/elasticsearch/pull/11545?
</comment><comment author="bleskes" created="2015-06-29T10:21:12Z" id="116595018">@clintongormley that exception is indicative that the lucene index in the store can not be opened. Doesn't seem to be the right place to indicate that a shard is unassigned because there is no node that accommodate it. We need to find a more generic way to do report this and I think Pius was asking for a way that can be analyzed in retrospect.
</comment><comment author="msimos" created="2015-10-21T21:45:15Z" id="150034105">+1 this message should be always logged when deciding to assign replicas.
</comment><comment author="clintongormley" created="2015-12-04T14:42:02Z" id="161982905">Closing in favour of https://github.com/elastic/elasticsearch/issues/14593
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CommonTermsQuery fix for ignored coordination factor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11780</link><project id="" key="" /><description>CommonTermsQueryParser does not check for disable_coords, only for
disable_coord. Yet the builder only outputs disable_coords, leading to
disabling the coordination factor to be ignored in the Java API.

Closes #11730
</description><key id="89584414">11780</key><summary>CommonTermsQuery fix for ignored coordination factor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-19T14:58:01Z</created><updated>2015-06-23T16:57:34Z</updated><resolved>2015-06-19T15:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-19T15:00:08Z" id="113539663">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>CommonTermsQuery fix for ignored coordination factor</comment></comments></commit></commits></item><item><title>Incorrect casting in FielddataTermsFilter equality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11779</link><project id="" key="" /><description>Hi Adrien,

it looks like there is a small bug in the `equals` method implementation that has been introduced by #10728. Each subclass tries to cast to `BytesFieldDataFilter` the object passed as parameter to the equals method, instead of trying to cast to their own class. For example, the `LongsFieldDataFilter#equals` method should contain

```
return terms.equals(((LongsFieldDataFilter) obj).terms);
```

and not

```
return terms.equals(((BytesFieldDataFilter) obj).terms);
```
</description><key id="89581883">11779</key><summary>Incorrect casting in FielddataTermsFilter equality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rendel</reporter><labels /><created>2015-06-19T14:47:02Z</created><updated>2015-06-24T07:34:57Z</updated><resolved>2015-06-24T07:34:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-23T17:52:13Z" id="114589598">Thanks @rendel  for the detailed bug report! I opened #11835
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: moving_avg model parser should accept any numeric</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11778</link><project id="" key="" /><description>Fixes model validation/parsing so that any numeric is accepted, not just explicit doubles.

Also changes the models to throw ParseExceptions instead of SearchParseExceptions, so that
the validation can be unit-tested.

Fixes #11487
</description><key id="89580319">11778</key><summary>Aggregations: moving_avg model parser should accept any numeric</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-19T14:39:51Z</created><updated>2015-06-23T16:56:27Z</updated><resolved>2015-06-19T14:55:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-19T14:51:49Z" id="113536756">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Merge pull request #11778 from polyfractal/bugfix/11487</comment></comments></commit></commits></item><item><title>Fast vector highlighting boundaries does not work properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11777</link><project id="" key="" /><description>Hello,

  I want to highlight the entire URL if a word from URL has a match. For that, I use fast vector highlighting.
  When I try to use [fast vector highlighting](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#fast-vector-highlighter), a problem occurs when I set [boundary_chars](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#boundary-characters).

  My _boundry_chars_ attribute is set to " " (whitespace), but it seems it does not work. "." character seems to be a boundary too.

Example:

```
STEP 1: Create a new index: PUT conversation_new
{"settings":{"index":{"analysis":{"filter":{"subword":{"type":"word_delimiter","preserve_original":"true"}},"analyzer":{"search_fulltext_analyzer":{"filter":["subword","lowercase"],"tokenizer":"whitespace"},"index_fulltext_analyzer":{"filter":["subword","lowercase"],"tokenizer":"whitespace"}}}}}}

STEP 2: Add the following mapping: POST conversation_new/message/_mapping
{"message":{"properties":{"body":{"type":"string","index":"no","fields":{"contains":{"type":"string","index_analyzer":"index_fulltext_analyzer","search_analyzer":"search_fulltext_analyzer","term_vector":"with_positions_offsets"}}}}}}

STEP 3: Add a document: POST conversation_new/message/
{"body":"Iulian Test First  9.test.9.23.user from lorem_ipsum_test PowerShot  http://192.168.9.236.4grid.eu/person/at/9.23.user/"}

STEP 4: Execute highlight query: POST conversation_new/message/_search
{"query":{"filtered":{"query":[{"bool":{"should":[{"match":{"body.contains":"9.23.user"}}]}}]}},"fields":["_parent","_source"],"size":20,"highlight":{"fields":{"body.contains":{"number_of_fragments":0,"type":"fvh","boundary_chars":" ","boundary_max_scan":100}}}}
```

The response from STEP 4 will be:

```
"hits": {
  "total": 1,
  "max_score": 0.07527358,
  "hits": [
     {
        "_index": "conversation_new",
        "_type": "message",
        "_id": "AU4LqXYP_8lKZ4oIJkkZ",
        "_score": 0.07527358,
        "_source": {
           "body": "Iulian Test First  9.test.9.23.user from lorem_ipsum_test PowerShot  http://192.168.9.236.4grid.eu/person/at/9.23.user/"
        },
        "highlight": {
           "body.contains": [
              "Iulian Test First  &lt;em&gt;9&lt;/em&gt;.test.&lt;em&gt;9&lt;/em&gt;.&lt;em&gt;23&lt;/em&gt;.&lt;em&gt;user&lt;/em&gt; from lorem_ipsum_test PowerShot  http://192.168.&lt;em&gt;9&lt;/em&gt;.236.4grid.eu/person/at/&lt;em&gt;9&lt;/em&gt;.&lt;em&gt;23&lt;/em&gt;.&lt;em&gt;user&lt;/em&gt;/"
           ]
        }
     }
  ]
```

   }

I expect something like:

```
"body.contains": [
              "Iulian Test First  &lt;em&gt;9test.9.23.user&lt;/em&gt; from lorem_ipsum_test PowerShot  &lt;em&gt;http://192.168.9.236.4grid.eu/person/at/9.23.user&lt;/em&gt;/"
```

 ]

Actually, I want to highlight the entire URL if a word from URL has a match.

Is this a bug?

Thanks a lot,
    Iulian
</description><key id="89559136">11777</key><summary>Fast vector highlighting boundaries does not work properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iulianlaz</reporter><labels><label>:Highlighting</label><label>discuss</label></labels><created>2015-06-19T12:58:05Z</created><updated>2015-06-23T16:55:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove scheduled routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11776</link><project id="" key="" /><description>Today, we have scheduled reroute that kicks every 10 seconds and checks if a
reroute is needed. We use it when adding nodes, since we don't reroute right
away once its added, and give it a time window to add additional nodes.

We do have recover after nodes setting and such in order to wait for enough
nodes to be added, and also, it really depends at what part of the 10s window
you end up, sometimes, it might not be effective at all. In general, its historic
from the times before we had recover after nodes and such.

This change removes the 10s scheduling, simplifies RoutingService, and adds
explicit reroute when a node is added to the system. It also adds unit tests
to RoutingService.
</description><key id="89554889">11776</key><summary>Remove scheduled routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-19T12:36:29Z</created><updated>2015-06-23T19:38:31Z</updated><resolved>2015-06-23T16:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-19T16:14:29Z" id="113561777">I think it's awesome how easy this turned up to be. I looked at all the places we submit a cluster state update task and didn't see any place we miss a reroute (and would rely on the 10s) apart from the node join we already knew about. 

Made some small suggestions.
</comment><comment author="kimchy" created="2015-06-19T21:50:39Z" id="113653438">@bleskes I pushed a first review round, I don't like the suggestion on the atomic reference one, I don't think it adds a lot of value, on the other hand, I have an idea on what would, but its a bigger change, will see if I can work on it and if it stays small
</comment><comment author="bleskes" created="2015-06-22T09:25:38Z" id="114049210">LGTM. I'm +1 on 1.7 - it makes things easier to trace, reason about and debug.
</comment><comment author="s1monw" created="2015-06-23T15:13:32Z" id="114539387">I am ok with this too... this entire circular dep stuff is just really bad but lets move on
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Remove scheduled routing</comment><comment>Today, we have scheduled reroute that kicks every 10 seconds and checks if a</comment><comment>reroute is needed. We use it when adding nodes, since we don't reroute right</comment><comment>away once its added, and give it a time window to add additional nodes.</comment></comments></commit></commits></item><item><title>Rescore script: aggregation from a main query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11775</link><project id="" key="" /><description>I wonder if there's is a way to solve my issue.
1. I have an int field visits_number, and I would like to use it as an additional factor in a score calculation
2. I have an outer query, and a rescore query. 
3. In a rescore query I have a groovy script, where I'd like to have something like avg(visits_number) available - an average visits_number calculated for an outer query results.
</description><key id="89551307">11775</key><summary>Rescore script: aggregation from a main query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexkuk</reporter><labels /><created>2015-06-19T12:19:37Z</created><updated>2015-06-23T16:46:30Z</updated><resolved>2015-06-23T16:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T16:40:05Z" id="114566311">Hi @alexkuk 

I'm assuming you mean that you'd calculate the avg visits_number across all docs (or across all matching docs) and then use it in the score calculation?  If so, then no, there's no automatic way of doing this.  Essentially you'd require two requests: one to calculate the avg then another to apply that value to the search hits.

This is what Elasticsearch would have to do as well.  You can already do this today with two search requests. There's no real advantage to Elasticsearch doing it for you in this case.

thanks
</comment><comment author="alexkuk" created="2015-06-23T16:46:30Z" id="114567751">Thx for the answer. 
Sure. I see your point and I already implemented it. But I thought that when ES does rescoring, it already has the data returned by the main query. So I thought probably it's possible to access this data from rescore scripts.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GroovyScriptExecutionException: NumberFormatException[null]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11774</link><project id="" key="" /><description>Hello.
I have a mapping like this.

```
{  
    "user":{  
       "properties":{  
          "mCount":{  
             "type":"integer",
             "store":true,
             "doc_values":true
          }
       }
    },
    "media":{  
       "_parent":{  
          "type":"user"
       },
       "_routing":{  
          "required":true
       },
       "properties":{  
          "location":{  
             "type":"geo_point",
             "doc_values":true,
             "lat_lon":true,
             "geohash":true,
             "geohash_prefix":true
          }
       }
    }
 }
```

I run this query 

```
{
    "size" : 10,
    "query" : {
      "has_child" : {
        "query" : {
          "filtered" : {
            "filter" : {
              "geo_bbox" : {
                "location" : {
                  "top_left" : [ -74.1, 41.73 ],
                  "bottom_right" : [ -73.99, 40.717 ]
                }
              }
            }
          }
        },
        "child_type" : "media",
        "score_type" : "sum"
      }
    },
    "fields" : "mCount",
    "sort" : [ {
      "_script" : {
        "script" : "_score  / doc['mCount'].value",
        "type" : "number",
        "reverse" : true
      }
    } ],
    "track_scores" : true
  }
```

and got 

```
query[filtered(ChildrenQuery[min(0) max(2147483647)of media/user](filtered(ConstantScore(GeoBoundingBoxFilter(location, [41.73, -74.1], [40.717, -73.99])))-&gt;cache(_type:media)))-&gt;cache(_type:user)],from[0],size[10],sort[&lt;custom:"_script": org.elasticsearch.search.sort.ScriptSortParser$2@7144a535&gt;!]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:289)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:300)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
    Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NumberFormatException[null]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:278)
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.runAsDouble(GroovyScriptEngineService.java:294)
        at org.elasticsearch.search.sort.ScriptSortParser$2$1.get(ScriptSortParser.java:177)
        at org.elasticsearch.index.fielddata.fieldcomparator.DoubleValuesComparatorSource$1$1.get(DoubleValuesComparatorSource.java:87)
        at org.apache.lucene.search.FieldComparator$DoubleComparator.copy(FieldComparator.java:367)
        at org.apache.lucene.search.TopFieldCollector$OutOfOrderOneComparatorScoringMaxScoreCollector.collect(TopFieldCollector.java:363)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:150)
        ... 8 more
```

when i replace script in scriptsort to `"script" : "_score  * (1 / doc['mCount'].value")"` Im also get `NumberFormatException[null]` , but when i replace script to `"script" : "_score  * (1.0 / doc['mCount'].value")"` it is working fine. And i get 

```
{
    "took" : 2,
    "timed_out" : false,
    "_shards" : {
      "total" : 5,
      "successful" : 5,
      "failed" : 0
    },
    "hits" : {
      "total" : 14,
      "max_score" : 317.0,
      "hits" : [ {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "1433",
        "_score" : 317.0,
        "fields" : {
          "mCount" : [ 2562 ]
        },
        "sort" : [ 0.12373145979703357 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "1615",
        "_score" : 299.0,
        "fields" : {
          "mCount" : [ 6283 ]
        },
        "sort" : [ 0.04758873149769219 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "49",
        "_score" : 56.0,
        "fields" : {
          "mCount" : [ 1327 ]
        },
        "sort" : [ 0.042200452147701586 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "3264",
        "_score" : 64.0,
        "fields" : {
          "mCount" : [ 3210 ]
        },
        "sort" : [ 0.019937694704049845 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "2808",
        "_score" : 15.0,
        "fields" : {
          "mCount" : [ 911 ]
        },
        "sort" : [ 0.01646542261251372 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "1004",
        "_score" : 11.0,
        "fields" : {
          "mCount" : [ 760 ]
        },
        "sort" : [ 0.014473684210526316 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "3096",
        "_score" : 9.0,
        "fields" : {
          "mCount" : [ 638 ]
        },
        "sort" : [ 0.014106583072100312 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "6",
        "_score" : 7.0,
        "fields" : {
          "mCount" : [ 995 ]
        },
        "sort" : [ 0.007035175879396985 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "33",
        "_score" : 10.0,
        "fields" : {
          "mCount" : [ 1460 ]
        },
        "sort" : [ 0.00684931506849315 ]
      }, {
        "_index" : "qwe",
        "_type" : "user",
        "_id" : "4695",
        "_score" : 5.0,
        "fields" : {
          "mCount" : [ 934 ]
        },
        "sort" : [ 0.005353319057815846 ]
      } ]
    }
  }
```

Why it is so? Why i can not use just `"script" : "_score  / doc['mCount'].value"` ?
</description><key id="89549151">11774</key><summary>GroovyScriptExecutionException: NumberFormatException[null]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gkozyryatskyy</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-06-19T12:11:10Z</created><updated>2016-01-18T20:41:16Z</updated><resolved>2016-01-18T20:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="askaliuk" created="2015-06-19T19:29:38Z" id="113614684">Having the same problem. Debugging now. Let me know if you make any progress.
My assumption: `doc['mCount'].value` is something, which we don't expect.
</comment><comment author="gkozyryatskyy" created="2015-06-23T15:27:17Z" id="114545666">@askaliuk This exception throws just with division operator.
For exemple `"script" : "_score * doc['mCount'].value"` working fine.
Like i sad `"script" : "_score / doc['mCount'].value"` and `"script" : "_score * (1 / doc['mCount'].value")"` do not working, But "script" : "_score \* (1.0 / doc['mCount'].value")" working fine.
Thats why i think the problem is cast from int to double, or from string to double.
May be elastic can't represent _score as double for division. Like it can't divide `1 / doc['mCount'].value` but `1.0 / doc['mCount'].value` works fine.
</comment><comment author="clintongormley" created="2015-06-23T19:35:24Z" id="114618660">@jpountz could we cast integers in script scoring to floats to avoid this issue?
</comment><comment author="askaliuk" created="2015-06-23T19:39:28Z" id="114620021">@gkozyryatskyy 

This code:

```
termInfo=_index['text'].get('something', _POSITIONS);
for(pos in termInfo){
     return 1 / (pos.position.toInteger() + 1) * doc['somefield'].value;
};
```

Works fine for me by some reason.
</comment><comment author="clintongormley" created="2016-01-18T20:41:16Z" id="172648092">I think we've just got to go with using explicit floats.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update stats.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11773</link><project id="" key="" /><description>More detail about the difference between `primary` and `total` for index stats.
</description><key id="89513066">11773</key><summary>Update stats.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">robin13</reporter><labels><label>docs</label></labels><created>2015-06-19T09:11:32Z</created><updated>2015-06-19T14:49:23Z</updated><resolved>2015-06-19T14:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11773 from elastic/robin13-patch-1</comment></comments></commit></commits></item><item><title>this web application instance has been stopped already. Could not load [org.slf4j.Marker]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11772</link><project id="" key="" /><description>六月 19, 2015 4:26:02 下午 org.apache.catalina.loader.WebappClassLoaderBase checkStateForResourceLoading
信息: Illegal access: this web application instance has been stopped already. Could not load [org.slf4j.Marker]. The following stack trace is thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access.
java.lang.IllegalStateException: Illegal access: this web application instance has been stopped already. Could not load [org.slf4j.Marker]. The following stack trace is thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access.
    at org.apache.catalina.loader.WebappClassLoaderBase.checkStateForResourceLoading(WebappClassLoaderBase.java:1327)
    at org.apache.catalina.loader.WebappClassLoaderBase.checkStateForClassLoading(WebappClassLoaderBase.java:1313)
    at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1196)
    at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1157)
    at ch.qos.logback.classic.spi.TurboFilterList.getTurboFilterChainDecision(TurboFilterList.java:44)
    at ch.qos.logback.classic.LoggerContext.getTurboFilterChainDecision_0_3OrMore(LoggerContext.java:252)
    at ch.qos.logback.classic.Logger.callTurboFilters(Logger.java:772)
    at ch.qos.logback.classic.Logger.isTraceEnabled(Logger.java:644)
    at ch.qos.logback.classic.Logger.isTraceEnabled(Logger.java:640)
    at org.elasticsearch.common.logging.slf4j.Slf4jESLogger.isTraceEnabled(Slf4jESLogger.java:63)
    at org.elasticsearch.transport.TransportService$Adapter.traceEnabled(TransportService.java:360)
    at org.elasticsearch.transport.TransportService$Adapter.onRequestSent(TransportService.java:354)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:684)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:276)
    at org.elasticsearch.client.transport.TransportClientNodesService$SniffNodesSampler$1.run(TransportClientNodesService.java:447)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

六月 19, 2015 4:26:47 下午 org.apache.catalina.loader.WebappClassLoaderBase checkStateForResourceLoading
信息: Illegal access: this web application instance has been stopped already. Could not load [org.slf4j.Marker]. The following stack trace is thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access.
java.lang.IllegalStateException: Illegal access: this web application instance has been stopped already. Could not load [org.slf4j.Marker]. The following stack trace is thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access.
    at org.apache.catalina.loader.WebappClassLoaderBase.checkStateForResourceLoading(WebappClassLoaderBase.java:1327)
    at org.apache.catalina.loader.WebappClassLoaderBase.checkStateForClassLoading(WebappClassLoaderBase.java:1313)
    at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1196)
    at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1157)
    at ch.qos.logback.classic.turbo.ReconfigureOnChangeFilter.decide(ReconfigureOnChangeFilter.java:101)
    at ch.qos.logback.classic.spi.TurboFilterList.getTurboFilterChainDecision(TurboFilterList.java:51)
    at ch.qos.logback.classic.LoggerContext.getTurboFilterChainDecision_0_3OrMore(LoggerContext.java:252)
    at ch.qos.logback.classic.Logger.callTurboFilters(Logger.java:772)
    at ch.qos.logback.classic.Logger.isTraceEnabled(Logger.java:644)
    at ch.qos.logback.classic.Logger.isTraceEnabled(Logger.java:640)
    at org.elasticsearch.common.logging.slf4j.Slf4jESLogger.isTraceEnabled(Slf4jESLogger.java:63)
    at org.elasticsearch.transport.TransportService$Adapter.traceEnabled(TransportService.java:360)
    at org.elasticsearch.transport.TransportService$Adapter.onResponseReceived(TransportService.java:401)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

help，thanks！！！
</description><key id="89505053">11772</key><summary>this web application instance has been stopped already. Could not load [org.slf4j.Marker]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qianshangding</reporter><labels /><created>2015-06-19T08:29:12Z</created><updated>2015-06-19T08:43:47Z</updated><resolved>2015-06-19T08:43:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>XContentBuilder.writeValue causes StackOverflowError given Path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11771</link><project id="" key="" /><description>If `XContentBuilder` receives a `Path` object to write, then it results in a `StackOverflowError` because it runs into the `Iterable` check, which `Path` implements (`Path implements Iterable&lt;Path&gt;`).
</description><key id="89472931">11771</key><summary>XContentBuilder.writeValue causes StackOverflowError given Path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>adoptme</label><label>bug</label></labels><created>2015-06-19T05:33:32Z</created><updated>2015-07-10T08:57:09Z</updated><resolved>2015-07-10T08:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="szroland" created="2015-06-25T23:31:52Z" id="115430100">This is correct, since one segment of the path is still a path, which is its own first segment, etc. Weird API, that.

Still, @pickypg, I'm curious where you stumbled upon this, e.g. if you could post the stack trace (until it starts calling itself recursively). Because I don't see much value serializing a Path object like this, it would not be useful as an array of path elements even if there was not this issue of endless recursion. Calling toString() on it and storing it as a string could be more useful.

Not sure XContentBuilder needs explicit support of Path, or rather whatever is calling should decide how they want to serialize paths, especially if the value is expected to be read back as a Path eventually.
</comment><comment author="pickypg" created="2015-06-26T20:00:16Z" id="115858847">Hi @szroland, I agree that it's a slightly odd usage, but I came across it while integration testing against Elasticearch 1.6. As the Groovy client author, I had to update a bunch of my tests to include support for the new `path.repo` setting (amongst other things). This involved changing the path for integration tests for snapshots to use a "valid" temporary directory.

``` groovy
// Create the repository
PutRepositoryResponse putResponse = clusterAdminClient.putRepository {
    name repoName
    type "fs"
    settings {
        location = randomRepoPath()
    }
}
```

where `randomRepoPath()` returns a `Path`. Without going into too much detail, this indirectly/effectively calls `builder.field("location", randomRepoPath())`, which is [making use of `XContentBuilder field(String name, Object value)` and eventually `writeValue(Object value)`](https://github.com/elastic/elasticsearch/blob/v1.6.0/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java#L771).

This can be easily worked around by calling `randomRepoPath().toString()` or, presumably even better, `randomRepoPath().toAbsolutePath().toString()`, but there's nothing implying a problem until you call it.

&gt; Not sure XContentBuilder needs explicit support of Path, or rather whatever is calling should decide how they want to serialize paths, especially if the value is expected to be read back as a Path eventually.

We do have a bunch of `Path`-based settings (particularly on master in ES 2.0+), which I think will expose this "problem" more over time, particularly when something _else_ is doing the serialization for you.
</comment><comment author="szroland" created="2015-06-27T23:38:39Z" id="116165935">I added a pull request with a possible implementation to have something concrete to review / discuss.
</comment><comment author="jpountz" created="2015-07-10T08:57:08Z" id="120305202">Fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java</file></files><comments><comment>Treat path object as a simple value objects instead of Iterable in XContentBuilder, using toString() to create String representation.</comment></comments></commit></commits></item><item><title>Hide more fieldType access and cleanup null_value merging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11770</link><project id="" key="" /><description>There were some missed uses of AbstractFieldMapper.fieldType in #11764.
This change also moves null_value merging into AbstractFieldMapper.
</description><key id="89418082">11770</key><summary>Hide more fieldType access and cleanup null_value merging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T23:01:24Z</created><updated>2015-06-23T16:57:07Z</updated><resolved>2015-06-19T15:15:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-19T08:50:24Z" id="113436801">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file></files><comments><comment>Merge pull request #11770 from rjernst/fix/null_value_merge</comment></comments></commit></commits></item><item><title>Mark store as corrupted instead of deleting state file on engine failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11769</link><project id="" key="" /><description>Currently, we delete the shard _state file on engine failure.
This behaviour does not persist the engine failure reason for later inspection.

This change marks the shard store as corrupted instead of deleting
the _state file to ensure the store index can not be opened after and
the engine failure is persisted.
</description><key id="89407787">11769</key><summary>Mark store as corrupted instead of deleting state file on engine failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T21:57:25Z</created><updated>2015-06-23T17:06:52Z</updated><resolved>2015-06-19T19:56:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-19T08:02:01Z" id="113421678">looks good. Left a comment about the implementation. 
</comment><comment author="areek" created="2015-06-19T16:41:49Z" id="113569932">Updated PR addressing the comment
</comment><comment author="bleskes" created="2015-06-19T18:28:19Z" id="113598440">Looks great. Left one minor comment. No need for another review. Feel free to push whenever.
</comment><comment author="areek" created="2015-06-19T19:56:56Z" id="113626491">Thanks @bleskes for the review! pushed to master (https://github.com/elastic/elasticsearch/commit/a8c2886b3fe525727e7af906c8a43bf9fa83b9b8).
</comment><comment author="kimchy" created="2015-06-19T20:04:26Z" id="113628283">is this something we might want to consider back porting to 1.x?
</comment><comment author="areek" created="2015-06-19T20:41:28Z" id="113636604">We could back port this to 1.x as well
</comment><comment author="kimchy" created="2015-06-19T20:42:00Z" id="113637156">@areek I am leaning towards it, but will let @s1monw weight in
</comment><comment author="s1monw" created="2015-06-19T21:06:58Z" id="113645326">it's simple and might allow us to get better insight why stuff is failing so +1 to port to 1.x
</comment><comment author="areek" created="2015-06-19T23:11:21Z" id="113668901">back ported to 1.x (https://github.com/elastic/elasticsearch/commit/67892315c31b17d7ae14d6961397e1b1ca623568)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasTests.java</file></files><comments><comment>quite noisy test failure, related to #11769</comment></comments></commit></commits></item><item><title>Make sure messages are fully read even in case of EOS markers.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11768</link><project id="" key="" /><description>When using compression over the network, you might sometimes see warnings that
the stream was not fully read. This is because DeflaterOutputStream adds an
end-of-stream marker. When deserializing, we need to poll for one byte using
InputStream.read() to make sure to decode this EOS marker.

For the record, it does not strike all the time today because we perform
buffering when decompressing to avoid performing too many JNI calls, but it
is easy to make this warning happen all the time by decreasing the size of
the buffer we use.

Close #11748
</description><key id="89384892">11768</key><summary>Make sure messages are fully read even in case of EOS markers.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T20:05:42Z</created><updated>2015-06-30T11:36:09Z</updated><resolved>2015-06-30T07:00:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-18T20:37:54Z" id="113282485">LGTM
</comment><comment author="s1monw" created="2015-06-22T11:54:27Z" id="114077340">LGTM too @kimchy can you take one more look here pls
</comment><comment author="kimchy" created="2015-06-22T12:35:40Z" id="114086863">LGTM
</comment><comment author="clintongormley" created="2015-06-29T18:47:13Z" id="116794124">@jpountz can we get this in?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file></files><comments><comment>Merge pull request #11768 from jpountz/fix/stream_eos</comment></comments></commit></commits></item><item><title>Set randomized node/index settings in the right place</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11767</link><project id="" key="" /><description>Don't set node settings in the index template, and try and set less index settings in the node settings
</description><key id="89383845">11767</key><summary>Set randomized node/index settings in the right place</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label></labels><created>2015-06-18T20:00:37Z</created><updated>2015-06-23T11:11:52Z</updated><resolved>2015-06-23T11:11:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-18T20:30:53Z" id="113281031">LGTM, left a couple minor comments.
</comment><comment author="kimchy" created="2015-06-22T13:36:22Z" id="114105146">pushed first review, @s1monw would love for you to look at it, specifically my second comment to @rjernst
</comment><comment author="s1monw" created="2015-06-22T14:32:17Z" id="114130770">I left some comments change LGTM though
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchBackwardsCompatIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Set randomized node/index settings in the right place</comment><comment>Don't set node settings in the index template, and try and set less index settings in the node settings</comment><comment>closes #11767</comment></comments></commit></commits></item><item><title>license checker should inspect packages </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11766</link><project id="" key="" /><description>the license checker today looks at `/target/lib/*.jar` I personally would feel better if we would unzip / untar the packages and check that we have license and sha1 hashes for all the jars in there. This would also work for plugins since they don't have a `/target/lib` folder which is es core specific. Note, plugins put their deps on the top level not in `lib`
</description><key id="89382632">11766</key><summary>license checker should inspect packages </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T19:54:56Z</created><updated>2015-06-23T10:59:03Z</updated><resolved>2015-06-23T10:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typo in code comment (source -&gt; target)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11765</link><project id="" key="" /><description /><key id="89376456">11765</key><summary>Fix typo in code comment (source -&gt; target)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">discordianfish</reporter><labels /><created>2015-06-18T19:26:34Z</created><updated>2015-06-18T19:39:58Z</updated><resolved>2015-06-18T19:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="discordianfish" created="2015-06-18T19:26:58Z" id="113265358">Not going to sign CLA for that. ;)
</comment><comment author="s1monw" created="2015-06-18T19:39:58Z" id="113268419">bummer - thanks anyway....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace fieldType access in mappers with getter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11764</link><project id="" key="" /><description>A small refactoring to allow changing how the field type is stored later.
</description><key id="89368501">11764</key><summary>Replace fieldType access in mappers with getter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T18:50:20Z</created><updated>2015-06-18T20:10:32Z</updated><resolved>2015-06-18T20:01:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-18T19:45:57Z" id="113269622">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file></files><comments><comment>Mappings: Hide more fieldType access and cleanup null_value merging</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file></files><comments><comment>Merge pull request #11764 from rjernst/fix/field-type-ref</comment></comments></commit></commits></item><item><title>Improve logging of repository verification exceptions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11763</link><project id="" key="" /><description>Some repository verification exception are currently only returned to the users but not logged on the nodes where the exception occurred, which makes troubleshooting difficult.

 Closes #11760
</description><key id="89366436">11763</key><summary>Improve logging of repository verification exceptions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T18:40:04Z</created><updated>2015-06-23T17:28:37Z</updated><resolved>2015-06-20T03:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-18T20:22:21Z" id="113278876">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add @Repeat to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11762</link><project id="" key="" /><description>@Repeat should not be committed just like @Seed.
Use -Pdev to run annotated methods.
</description><key id="89364952">11762</key><summary>Add @Repeat to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T18:32:46Z</created><updated>2015-06-18T18:34:41Z</updated><resolved>2015-06-18T18:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-18T18:33:41Z" id="113252040">++, LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OutOfMemoryError during jackson serialization when bulk indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11761</link><project id="" key="" /><description>ES 1.6, -Xmx2024 (512BM usually sufficient), java client with elastic4s
I cannot bulk index 800 documents of constant size where single document looks like this : 

```
foo -&gt; "c0004"
bar -&gt; "10764182"
baz -&gt; "92fde3ff-a659-408c-8b5c-1e7465447234"
xyz -&gt; "123456789098765432345678987654324567"
```

CPU immediately at 100% until it fails on OOME on that single bulk. It is less than 350 characters which is not even 1MB of raw text... when I decrease 800 to 400 documents it is done in a few milliseconds. There must be some sort of bug in ES json serialization...

```
java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.jackson.core.util.BufferRecycler.calloc(BufferRecycler.java:156)
    at org.elasticsearch.common.jackson.core.util.BufferRecycler.allocCharBuffer(BufferRecycler.java:124)
    at org.elasticsearch.common.jackson.core.util.BufferRecycler.allocCharBuffer(BufferRecycler.java:114)
    at org.elasticsearch.common.jackson.core.io.IOContext.allocConcatBuffer(IOContext.java:186)
    at org.elasticsearch.common.jackson.core.json.UTF8JsonGenerator.&lt;init&gt;(UTF8JsonGenerator.java:127)
    at org.elasticsearch.common.jackson.core.JsonFactory._createUTF8Generator(JsonFactory.java:1284)
    at org.elasticsearch.common.jackson.core.JsonFactory.createGenerator(JsonFactory.java:1016)
    at org.elasticsearch.common.xcontent.json.JsonXContent.createGenerator(JsonXContent.java:74)
    at org.elasticsearch.common.xcontent.json.JsonXContent.createGenerator(JsonXContent.java:80)
    at org.elasticsearch.common.xcontent.XContentBuilder.&lt;init&gt;(XContentBuilder.java:109)
    at org.elasticsearch.common.xcontent.XContentBuilder.&lt;init&gt;(XContentBuilder.java:99)
    at org.elasticsearch.common.xcontent.XContentBuilder.builder(XContentBuilder.java:77)
    at org.elasticsearch.common.xcontent.json.JsonXContent.contentBuilder(JsonXContent.java:40)
    at org.elasticsearch.common.xcontent.XContentFactory.contentBuilder(XContentFactory.java:122)
    at org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder(XContentFactory.java:49)
    at com.sksamuel.elastic4s.IndexDefinition._fieldsAsXContent(IndexDsl.scala:50)
    at com.sksamuel.elastic4s.IndexDefinition.build(IndexDsl.scala:44)
    at com.sksamuel.elastic4s.BulkDefinition$$anonfun$1.apply(bulk.scala:82)
    at com.sksamuel.elastic4s.BulkDefinition$$anonfun$1.apply(bulk.scala:81)
    at scala.collection.immutable.List.foreach(List.scala:381)
    at com.sksamuel.elastic4s.BulkDefinition.&lt;init&gt;(bulk.scala:81)
    at com.sksamuel.elastic4s.BulkDsl$class.bulk(bulk.scala:18)
    at com.sksamuel.elastic4s.ElasticDsl$.bulk(ElasticDsl.scala:456)
```
</description><key id="89363456">11761</key><summary>OutOfMemoryError during jackson serialization when bulk indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">l15k4</reporter><labels /><created>2015-06-18T18:26:19Z</created><updated>2015-06-23T16:33:01Z</updated><resolved>2015-06-18T20:07:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="l15k4" created="2015-06-18T18:42:42Z" id="113253981">Ups, my apologies. It looks I underestimated scroll page size. 
I have 100 indices on 4 nodes using 5 shards and I'm scanning  all the indices. I thought that request could hit max 5 shards, so when I set page size 200, it would be 1000 max. But it looks like the bulks have like 10 000 documents which is 10MB of text.
</comment><comment author="clintongormley" created="2015-06-18T20:07:22Z" id="113275168">thanks for letting us know @l15k4 
</comment><comment author="l15k4" created="2015-06-19T08:58:22Z" id="113438090">@clintongormley what is the max result document size with pageSize=100 when scrolling 100 indices on this? : 

```
   "number_of_nodes": 4,
   "number_of_data_nodes": 4,
   "active_primary_shards": 530,
   "active_shards": 1060
```

I find it really strange that pageSize 200 leads to 10 000 results
</comment><comment author="clintongormley" created="2015-06-23T16:33:01Z" id="114564819">@l15k4 you get back a max of  `size * number_of_shards`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Repository verification exceptions should be logged</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11760</link><project id="" key="" /><description>Currently repository verification exceptions are returned to the user, but not logged on the node where the exception occurred. This behavior makes debugging of some repository registration issues difficult. See elastic/elasticsearch-cloud-aws#217 for example.
</description><key id="89354382">11760</key><summary>Repository verification exceptions should be logged</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2015-06-18T17:47:12Z</created><updated>2015-06-20T03:19:40Z</updated><resolved>2015-06-20T03:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java</file></files><comments><comment>Improve logging of repository verification exceptions.</comment></comments></commit></commits></item><item><title>Reset registeredNextDelaySetting on reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11759</link><project id="" key="" /><description>Need to reset the registered setting in order to make sure the nex round will capture the right delay interval
</description><key id="89350466">11759</key><summary>Reset registeredNextDelaySetting on reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T17:30:37Z</created><updated>2015-06-18T19:58:42Z</updated><resolved>2015-06-18T18:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-18T18:08:21Z" id="113242505">can there be race in between updating and checking this setting? I guess not though just double checking...
</comment><comment author="kimchy" created="2015-06-18T18:11:17Z" id="113243100">yea, I was thinking about that :), I don't think so, it follows "happens before" like semantics, so its conservative and makes sure to reset it before reroute happens (which will trigger the next callback afterwards if there are still delayed unassigned)
</comment><comment author="s1monw" created="2015-06-18T18:13:10Z" id="113243489">fair enough... I will likely look at this logic again, it's too complicated ATM IMO and need to be simplified eventually IMO but lets move forward here
</comment><comment author="kimchy" created="2015-06-18T18:18:11Z" id="113245506">cool, @s1monw I pushed another small change, to make sure we randomize this setting, and that we name it properly in our constant, looks good?
</comment><comment author="s1monw" created="2015-06-18T18:18:53Z" id="113245903">LGTM
</comment><comment author="kimchy" created="2015-06-18T18:19:38Z" id="113246359">thanks!, I am also working on simplifying RoutingService and writing unit tests for it, just keep finding stuff along the way...
</comment><comment author="bleskes" created="2015-06-18T18:26:43Z" id="113250210">Late to the party but LGTM. Good catch
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Reset registeredNextDelaySetting on reroute</comment><comment>Need to reset the registered setting in order to make sure the nex round will capture the right delay interval</comment></comments></commit></commits></item><item><title>Add Iterators.emptyIterator to forbidden apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11758</link><project id="" key="" /><description>As a follow up to #11741, this forbids Iterators.emptyIterator in
favor the of builtin Collections.emptyIterator.
</description><key id="89347290">11758</key><summary>Add Iterators.emptyIterator to forbidden apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T17:14:59Z</created><updated>2015-06-18T17:17:04Z</updated><resolved>2015-06-18T17:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-18T17:15:37Z" id="113224772">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11758 from rjernst/fix/empty-iterators</comment></comments></commit></commits></item><item><title>Fixes incorrect syntax in restore documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11757</link><project id="" key="" /><description>`index_settings.index.number_of_replicas` was not the right path, elasticsearch complained saying `index_settings` is not a valid key, should be `settings.number_of_replicas`. Updated the code example to fit
</description><key id="89340247">11757</key><summary>Fixes incorrect syntax in restore documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">beneggett</reporter><labels /><created>2015-06-18T16:43:59Z</created><updated>2015-06-22T22:51:36Z</updated><resolved>2015-06-22T22:51:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="beneggett" created="2015-06-18T16:46:08Z" id="113214291">Just signed the CLA
</comment><comment author="imotov" created="2015-06-18T19:49:21Z" id="113270242">@beneggett which version of elasticsearch are you using?
</comment><comment author="beneggett" created="2015-06-18T19:52:27Z" id="113270885">@imotov: I'm using 1.5.2
</comment><comment author="imotov" created="2015-06-18T20:10:16Z" id="113275771">That's strange. I just tried the original example from documentation and it works fine with 1.5.2. There was an issue (#10133) with the `"index."` prefix in the settings but, but it was fixed in 1.5.2. Are you sure all your nodes are upgraded to 1.5.2? If this is the case could you provide the original command that was causing the error and the complete error (if something is getting logged into the log file, please attach this too). 

By the way, switching to `settings` from `index_settings` will simply cause restore to ignore the settings that you specified - they are not going to be changed on restore.
</comment><comment author="beneggett" created="2015-06-19T18:50:05Z" id="113604672">@imotov: Hmm, it's a possibility that we had a version mismatch. Two of the nodes in my cluster were on 1.4.4. I've since upgraded them. Was this a change in 1.5.2 then?  I'll try again with the original syntax on 1.5.2
</comment><comment author="imotov" created="2015-06-20T02:43:03Z" id="113699732">In 1.4.4 this feature didn't exist (it was added only in 1.5.0). That's probably why you were getting an exception about `index_settings` being invalid key. In 1.5.2 we made it possible to specify settings without the `index.` prefix. In other words we started to support `index_settings.number_of_replicas` along side with `index_settings.index.number_of_replicas` (the later format from 1.5.0, when this feature was added). 
</comment><comment author="beneggett" created="2015-06-22T22:51:35Z" id="114292610">@imotov, thank you for the clarification. I will close this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Extract all shard-level snapshot operation into dedicated SnapshotShardsService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11756</link><project id="" key="" /><description>Currently the `SnapshotsService` is concerned with both maintaining the global snapshot lifecycle on the master node as well as responsible for keeping track of individual shards on the data nodes. This refactoring separates two areas of concerns by moving all shard-level operations into a separate `SnapshotShardsService`.
</description><key id="89332850">11756</key><summary>Extract all shard-level snapshot operation into dedicated SnapshotShardsService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T16:10:01Z</created><updated>2015-06-23T22:11:17Z</updated><resolved>2015-06-23T22:04:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T20:44:38Z" id="114256868">left some style comments LGTM in general
</comment><comment author="imotov" created="2015-06-23T01:30:37Z" id="114319025">@s1monw I pushed the changes, but I am not 100% sure I what you meant in some of your comments. Could you take another look?
</comment><comment author="s1monw" created="2015-06-23T08:28:56Z" id="114405519">left a tiny comment about naming LGTM - feel free to push
</comment><comment author="imotov" created="2015-06-23T22:11:17Z" id="114656511">@s1monw, yes, that looks like a bug, I would like to backport the fix to 1.x and 1.6 branches. Could you review #11839.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/repositories/RepositoriesModule.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file></files><comments><comment>Extract all shard-level snapshot operation into dedicated SnapshotShardsService</comment></comments></commit></commits></item><item><title>[test] ByteSizeUnit.BYTES should be explicitly set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11755</link><project id="" key="" /><description>With a recent change in core, we don't support anymore non explicit byte size units when setting values.
This commit fix that in azure repository. Otherwise, tests can't be executed.
</description><key id="89323017">11755</key><summary>[test] ByteSizeUnit.BYTES should be explicitly set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T15:29:45Z</created><updated>2015-07-08T14:27:05Z</updated><resolved>2015-06-18T16:25:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-18T16:21:06Z" id="113207495">LGTM, thanks @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreITest.java</file><file>plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTest.java</file></files><comments><comment>Merge pull request #11755 from dadoonet/azure/fix-tests-bytesize</comment></comments></commit></commits></item><item><title>Added Class Imports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11754</link><project id="" key="" /><description>I was unable to get my BulkProcessor script to work without importing the "ByteSizeUnit" and "ByteSizeValue" classes.  Perhaps I overlooked something in the example and do not understand its code.
</description><key id="89317521">11754</key><summary>Added Class Imports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">oyiadom</reporter><labels><label>:Java API</label><label>docs</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T15:05:12Z</created><updated>2015-06-19T07:23:56Z</updated><resolved>2015-06-19T07:23:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-18T15:42:21Z" id="113196857">Thanks! 

I agree but I think it would be better to those `imports` in that section: https://github.com/oyiadom/elasticsearch/blob/patch-2/docs/java-api/bulk.asciidoc#using-bulk-processor

Could modify your PR accordingly?
</comment><comment author="oyiadom" created="2015-06-18T15:59:56Z" id="113201602">You're welcome @dadoonet!  Thanks for catching my mistake.  I attempted to move the imports to the correct section in the docs.  Hopefully, everything is showing up properly on your end.  I am still learning how to use GitHub.
</comment><comment author="dadoonet" created="2015-06-19T07:23:50Z" id="113405970">I pulled your PR in with 6437478d65ff525d540568e344569e4dc9d813bc 6d702f67553a149b2b0a99658119f9a7faa3d07f and cdb9712b9d984b508ad6af1d07563fd14f3fa454

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `readonly` option for repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11753</link><project id="" key="" /><description>Based on discussion here: https://github.com/elastic/elasticsearch-cloud-azure/pull/93#issuecomment-113053203

We could support in our repositories whatever the provider is (`fs`, `hdfs`, `s3`, `url`, `azure`...) a `readonly` property which will:
- don't call `verify` or change the way `verify` works today
- make sure that a write to a readonly repo is not possible
</description><key id="89312414">11753</key><summary>Add `readonly` option for repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label></labels><created>2015-06-18T14:44:18Z</created><updated>2015-08-28T00:10:56Z</updated><resolved>2015-08-28T00:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/repositories/Repository.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java</file></files><comments><comment>Add `readonly` option for repositories</comment></comments></commit></commits></item><item><title>Build: Make rest-spec-api a project so eclipse build works</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11752</link><project id="" key="" /><description>The change makes rest-spec-api a project in the same way as we build dev-tools. it packages the tests and api in a bundle using the maven-remote-resources-plugin and uses the same plugin in the plugins and core pom to unpack the rest-api-spec into the target directory and references the rest tests there in the test resources.

The main stimulus for this change is that for those using Eclipse the current build does not work. After running `mvn eclipse:eclipse` the Eclipse IDE errors because the rest-api-spec is outside of the project scope, meaning that every time the command is run (required whenever any dependencies change), the class path of all the projects has to be manually fixed.
</description><key id="89305866">11752</key><summary>Build: Make rest-spec-api a project so eclipse build works</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T14:18:01Z</created><updated>2015-06-22T11:06:22Z</updated><resolved>2015-06-22T10:45:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-18T14:18:56Z" id="113170745">I realise this may not be the most ideal fix, but I would like to get this in unless we have a better plan we can implement now. We can always change the way we do this later if necessary.
</comment><comment author="dadoonet" created="2015-06-18T14:35:37Z" id="113174629">The change LGTM. I would just check what is exactly the consequence of moving all tests in another dir for all clients we have today.
Also, I would use something like `/tests/rest` as a new dir name instead of keeping `rest-api-spec`.
</comment><comment author="dadoonet" created="2015-06-19T16:02:08Z" id="113557238">@colings86 I think I was able to fix it within this branch: https://github.com/dadoonet/elasticsearch/tree/maven/rest-spec

The patch itself is: https://github.com/dadoonet/elasticsearch/commit/21245cd96f0607d7254bbc0898ceb48fce5fa0c1

Let me know if it works for you. I just checked that everything is now compiling as expected.
</comment><comment author="colings86" created="2015-06-22T09:53:59Z" id="114054494">@dadoonet I tried applying your fix (see latest commit) but I get the following error when I build:

```
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Elasticsearch Parent POM ........................... FAILURE [  2.942 s]
[INFO] Elasticsearch Build Resources ...................... SKIPPED
[INFO] Elasticsearch Rest API Spec ........................ SKIPPED
[INFO] Elasticsearch Core ................................. SKIPPED
[INFO] Elasticsearch Plugin POM ........................... SKIPPED
[INFO] Elasticsearch Japanese (kuromoji) Analysis plugin .. SKIPPED
[INFO] Elasticsearch Smart Chinese Analysis plugin ........ SKIPPED
[INFO] Elasticsearch Stempel (Polish) Analysis plugin ..... SKIPPED
[INFO] Elasticsearch Phonetic Analysis plugin ............. SKIPPED
[INFO] Elasticsearch ICU Analysis plugin .................. SKIPPED
[INFO] Elasticsearch Google Compute Engine cloud plugin ... SKIPPED
[INFO] Elasticsearch Azure cloud plugin ................... SKIPPED
[INFO] Elasticsearch AWS cloud plugin ..................... SKIPPED
[INFO] Elasticsearch Python language plugin ............... SKIPPED
[INFO] Elasticsearch JavaScript language plugin ........... SKIPPED
[INFO] Elasticsearch Delete By Query plugin ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.328 s
[INFO] Finished at: 2015-06-22T10:52:22+01:00
[INFO] Final Memory: 25M/963M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (generate-shared-specs) on project elasticsearch-parent: Resources archive cannot be found. Could not find artifact org.elasticsearch:rest-api-spec:jar:2.0.0-SNAPSHOT in oss-snapshots (https://oss.sonatype.org/content/repositories/snapshots/)
[ERROR] 
[ERROR] Try downloading the file manually from the project website.
[ERROR] 
[ERROR] Then, install it using the command:
[ERROR] mvn install:install-file -DgroupId=org.elasticsearch -DartifactId=rest-api-spec -Dversion=2.0.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file
[ERROR] 
[ERROR] Alternatively, if you host your own repository you can deploy the file there:
[ERROR] mvn deploy:deploy-file -DgroupId=org.elasticsearch -DartifactId=rest-api-spec -Dversion=2.0.0-SNAPSHOT -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]
[ERROR] 
[ERROR] 
[ERROR] org.elasticsearch:rest-api-spec:jar:2.0.0-SNAPSHOT
[ERROR] 
[ERROR] from the specified remote repositories:
[ERROR] elasticsearch-releases (http://maven.elasticsearch.org/releases, releases=true, snapshots=false),
[ERROR] oss-snapshots (https://oss.sonatype.org/content/repositories/snapshots/, releases=true, snapshots=true),
[ERROR] sonatype-nexus-snapshots (https://oss.sonatype.org/content/repositories/snapshots, releases=false, snapshots=true),
[ERROR] central (http://repo.maven.apache.org/maven2, releases=true, snapshots=false)
[ERROR] -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
```
</comment><comment author="colings86" created="2015-06-22T09:54:45Z" id="114054615">maven command was:

```
mvn -f pom.xml clean test -Pdev -Dtests.jvms=8 -Des.logger.level=DEBUG
```
</comment><comment author="colings86" created="2015-06-22T10:11:00Z" id="114057608">@dadoonet would you mind if I pushed this with dev-tools and rest-spec-api referencing sonatype parent pom for now and fix later? I want to get this in quick as the development experience in Eclipse is very awkward at the moment
</comment><comment author="dadoonet" created="2015-06-22T10:18:51Z" id="114058717">Works for me.
Could you open another issue and assigned it to me?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow security rule for advanced SSL configutation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11751</link><project id="" key="" /><description>if plugins need to install SSL factories etc. we have to allow
to `setFactory` in the security policy.
</description><key id="89303440">11751</key><summary>Allow security rule for advanced SSL configutation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T14:08:16Z</created><updated>2015-06-18T18:15:30Z</updated><resolved>2015-06-18T14:51:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-18T14:36:55Z" id="113175459">looks good
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Querystring treats nonexistent wildcard fields as existing in AND expression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11750</link><project id="" key="" /><description>Hi, ran into a bug while working with the query_string query. Here's a sample of my data:

``` javascript
  {
    "name": "document1",
    "tags": {
      "source1": { "tag1A": "A", "tag1B": "B" },
      "source2": { "tag2A": "A", "tag2B": "B" }
    }
  },
  {
    "name": "document2",
    "tags": {
      "source1": { "tag1A": "A", "tag1B": "B" },
      "source2": { "tag2C": "C", "tag2D": "D" }
    }
  }
```

Here's the query I'm running:

``` javascript
GET test_index/test_type/_search
  {
    "query": {
      "filtered": {
        "filter": {
          "query": {
            "query_string": {
              "query": "name:document1 AND tags.\\*.tag1E:A"
            }
          }
        }
      }
    }
  }
```

For some reason, this query returns the `document1` document, even though it has no field that matches `tags.\\*.tag1E`.

The equivalent query that uses the "and" filter to combine the predicates works correctly and returns no documents:

``` javascript
GET test_index/test_type/_search
  {
    "query": {
      "filtered": {
        "filter": {
          "and": {
            "filters": [
              {
                "query": {
                  "query_string": {
                    "query": "name:document1"
                  }
                }
              },
              {
                "query": {
                  "query_string": {
                    "query": "tags.\\*.tag1E:A"
                  }
                }
              }
            ]
          }
        }
      }
    }
  }
```

Am I doing anything wrong here?
</description><key id="89299240">11750</key><summary>Querystring treats nonexistent wildcard fields as existing in AND expression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iravid</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-06-18T13:49:36Z</created><updated>2015-06-18T21:38:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T19:39:27Z" id="113268318">A wildcarded field path which maps to no fields gets excluded.  eg:

```
GET _validate/query?explain
{
  "query": {
    "query_string": {
      "query": "name:document1 AND tags.\\*.tag1E:A"
    }
  }
}
```

returns:

```
  {
     "index": "t",
     "valid": true,
     "explanation": "name:document1"
  }
```
</comment><comment author="iravid" created="2015-06-18T21:38:10Z" id="113296935">Hey Clinton, thanks for pointing that out. Does clear up things. I'd say
that a field path that maps to no fields should be evaluated as false in
the Boolean expression.
On Thu, Jun 18, 2015 at 22:40 Clinton Gormley notifications@github.com
wrote:

&gt; A wildcarded field path which maps to no fields gets excluded. eg:
&gt; 
&gt; GET _validate/query?explain
&gt; {
&gt;   "query": {
&gt;     "query_string": {
&gt;       "query": "name:document1 AND tags.\*.tag1E:A"
&gt;     }
&gt;   }
&gt; }
&gt; 
&gt; returns:
&gt; 
&gt;   {
&gt;      "index": "t",
&gt;      "valid": true,
&gt;      "explanation": "name:document1"
&gt;   }
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11750#issuecomment-113268318
&gt; .
&gt; 
&gt; ## 
&gt; 
&gt; Itamar
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: nested filter aggregation with nested filter returning no results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11749</link><project id="" key="" /><description>Hello,

I am not able to combine normal (e.g., `term filter`) with `nested filter` in `filter aggregation` which is itself nested in `nested aggregation`.

Mapping: `tasks` with nested `events`, `events` with nested `parameters`.

&lt;pre&gt;
curl -XPUT 'localhost:9200/nestingtest/' -d '
{
  "mappings": {
    "tasks": {
      properties: {
        task_id: {
          type: "long",
          doc_values: true
        },
        events: {
          type: "nested",
          properties: {
            id: {
              type: "long",
              doc_values: true
            },
            parameters: {
              type: "nested",
              properties: {
                name: {
                  type: "string",
                  index: "not_analyzed",
                  doc_values: true
                },
                value: {
                  type: "string",
                  index: "not_analyzed",
                  doc_values: true
                }
              }
            }
          }
        }
      }
    }
  }
}
'
&lt;/pre&gt;

Data:

&lt;pre&gt;
curl -XPOST 'localhost:9200/nestingtest/tasks/1' -d '
{
  task_id: 1,
  events: [
    {
      id: 1,
      parameters: {
        name: "attribution",
        value: "campaignX"
      }
    }
  ]
}
'

curl -XPOST 'localhost:9200/nestingtest/tasks/2' -d '
{
  task_id: 2,
  events: [
    {
      id: 21,
      parameters: [
        {
          name: "attribution",
          value: "campaignY"
        }
      ]
    },
    {
      id: 22,
      parameters: {
        name: "attribution",
        value: "campaignY"
      }
    }
  ]
}
'
&lt;/pre&gt;


Query that does not work as expected:

&lt;pre&gt;
curl -XPOST 'localhost:9200/nestingtest/tasks/_search' -d '
{
  size: 0,
  aggs: {
    "to-events": {
      nested: {
        path: "events"
      },
      aggs: {
        filtered: {
          filter: {
            nested: {
              path: "events.parameters",
              filter: {
                term: {
                  "events.parameters.value": "campaignX"
                }
              }
            }
          }
        }
      }
    }
  }
}
'
&lt;/pre&gt;


Response:

&lt;pre&gt;
"aggregations": {
  "to-events": {
    "doc_count": 3,
    "filtered": {
      "doc_count": 0 //this is wrong, should be 1
    }
  }
}
&lt;/pre&gt;


If I change the query to use the `nested aggregation`, the results are as expected:

&lt;pre&gt;
curl -XPOST 'localhost:9200' -d '
{
  size: 0,
  aggs: {
    "to-events": {
      nested: {
        path: "events"
      },
      aggs: {
        "to-parameters": {
          nested: {
            path: "events.parameters"
          },
          aggs: {
            filtered: {
              filter: {
                term: {
                  "events.parameters.value": "campaignX"
                }
              }
            }
          }
        }
      }
    }
  }
}'
&lt;/pre&gt;

Response:

&lt;pre&gt;
"aggregations": {
      "to-events": {
         "doc_count": 3,
         "to-parameters": {
            "doc_count": 3,
            "filtered": {
               "doc_count": 1
            }
         }
      }
   }
&lt;/pre&gt;


Using `nested aggregation` is not an ideal solution for me, as I would like to combine that `nested filter` with other filters acting at the level of events, get one number of matched documents and then dive into sub-aggs for those documents.
# 

&lt;pre&gt;
"version" : {
    "number" : "1.6.0",
    "build_hash" : "cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0",
    "build_timestamp" : "2015-06-09T13:36:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  }
&lt;/pre&gt;

</description><key id="89294406">11749</key><summary>Aggregations: nested filter aggregation with nested filter returning no results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">crutch</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-06-18T13:31:13Z</created><updated>2017-03-25T20:38:12Z</updated><resolved>2016-07-26T07:21:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T19:33:04Z" id="113266972">@martijnvg please take a look
</comment><comment author="martijnvg" created="2015-06-21T20:23:05Z" id="113951709">@crutch The issue is that the `nested` aggregator doesn't tell what `path` has been set to the `nested` filter. This causes the `nested` to translate the matching docs incorrectly to the `nested` aggregator and this results in no results. So this is indeed a bug and should get fixed.
</comment><comment author="martijnvg" created="2015-06-22T08:00:34Z" id="114034328">@crutch About the `nested` aggregator solution, I think that should work for you too? You can just place another filter aggregator under the first `nested` aggregator:

``` json
{
  "size": 0,
  "aggs": {
    "to-events": {
      "nested": {
        "path": "events"
      },
      "aggs": {
        "id_filter": {
          "filter": {
            "term": {
              "events.id": 22
            }
          },
          "aggs": {
            "to-parameters": {
              "nested": {
                "path": "events.parameters"
              },
              "aggs": {
                "filtered": {
                  "filter": {
                    "term": {
                      "events.parameters.value": "campaignY"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="crutch" created="2015-06-22T12:42:31Z" id="114088294">@martijnvg thank you for investigating this.

Using a `filter` aggregator under the `nested` one is my current solution. I am able to query for correct numbers, but struggle to get e.g., correct `terms` aggs. 

Imagine that my filter is a compound one, matching some fields of `event` and some fields of nested `parameters`. And I would like to have a `terms` aggregation based on a field of `event` for all documents matching this compound filter. I can split the filter into two parts: event-level &amp; nested-level, which is used under `nested` aggregation...but where to put the `terms` aggregation then?

If my `terms` aggregation is used outside of the `nested` aggregation, its results are not filtered by my nested filter. And I cannot put this `terms` aggregation inside the `nested` one, as it does not have access to its parent's fields.
</comment><comment author="crutch" created="2015-07-30T10:29:32Z" id="126267601">@martijnvg I hit this issue once again. I need a `not` filter on top of a `nested filter`, in an aggregation. Do you have any idea how to achieve this using `not filter` and `nested aggregation` instead of `nested filter`?

Using the `not filter` inside the `nested aggregation` will match on sibling nested documents...
</comment><comment author="tomasfalt" created="2015-08-13T12:51:54Z" id="130658417">Any news about this? I have similar problems and a solution to this would help me a lot.
</comment><comment author="OlegIlyenko" created="2015-11-25T10:52:05Z" id="159570005">We also faced this issue during the migration from facets to aggregation framework. In our case we have a `bool` filter with `must_not` condition which contains a nested query. As described here and in https://github.com/elastic/elasticsearch/issues/12410, it is possible to represent simple conjunction filters with nested aggregations, but this is pretty much it. I don't see any way how one can represent a disjunctions or negations in this way (would appreciate if somebody could share how to do it, if it's possible). 

It makes it very hard for us to do the migration to an aggregations framework. Our main use-case for facets/aggregations is a faceted navigation (which means that we are using aggregation filter buckets in conjunction with the `post_filter`). I would really appreciate if somebody could revisit this issue or at least share an information whether this issue is planned to be solved (and if yet then. of course, it would be very helpful to know when since we need to plan our migration as well).
</comment><comment author="peter-gerhard" created="2015-11-26T15:22:35Z" id="159937135">:+1: 
</comment><comment author="sumitjainn" created="2015-11-30T19:11:03Z" id="160728527">This is really a blocker for me, and I think there is no possible workaround. 
I have 2 levels of nesting, product &gt; [offer] &gt; [invprice]. I want to calculate price range facet for the products, only considering offers which have popularity of 5 and  have inventory for all provided dates. This is the query I am trying to run:

```
{
  "query": {
    "match": {
      "productcode": "p1"
    }
  },
  "aggs": {
    "product_offers": {
      "nested": {
        "path": "offers"
      },
      "aggs": {
        "offers": {
          "filter": {
                "bool": {
                  "must": [
                    {
                      "term": {
                        "popularity": 5
                      }
                    },
                    {
                      "nested": {
                        "path": "offers.invprice",
                        "query": {
                          "terms": {
                            "offers.invprice.date": [1444501800000]
                          }
                        }
                      }
                    },
                    {
                      "nested": {
                        "path": "offers.invprice",
                        "query": {
                          "terms": {
                            "offers.invprice.date": [1447093800000]
                          }
                        }
                      }
                    }
                  ]
                }
            },
           "aggs": {
            "price_ranges": {
              "nested": {
                "path": "offers.invprice"
              }, 
              "aggs": {
                "ranges": {
                  "range": {
                    "field": "offers.invprice.price",
                    "ranges": [
                      {
                        "from": 50,
                        "to": 300
                      },
                      {
                        "from": 300,
                        "to": 700
                      },
                      {
                        "from": 700,
                        "to": 1000
                      }
                    ]
                  }
                }
              }
            }
          } 
          }
        }
    }
  }
}
```
</comment><comment author="ddombrowskii" created="2015-12-08T21:53:52Z" id="163030067">Since it is label as a bug, is there any ETA on a fix?

This is kind of an important feature for me
</comment><comment author="sebbulon" created="2015-12-11T13:24:20Z" id="163935486">Hi there,
are you consider to work on this in the next, say, 2-3 months? Otherwise we have to consider implemeting another solution to this problem.

thanks
</comment><comment author="clintongormley" created="2015-12-14T17:00:28Z" id="164494686">@ddombrowskii @sebbulon I suggest working on another solution in the meantime.
</comment><comment author="ddombrowskii" created="2016-03-28T18:09:02Z" id="202512653">@clintongormley Can we expecting a fix in Elastic Stack 5.0 ?
</comment><comment author="edudar" created="2017-03-25T20:38:04Z" id="289238006">It's very unfortunate that this fix is available in 5.x version only as migrating from 2.x to 5.x is not something I can do at the moment...</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregatorFactory.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/NestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java</file></files><comments><comment>aggs: Changed how `nested` and `reverse_nested` aggs know about their nested depth level.</comment></comments></commit></commits></item><item><title>Message not fully read warnings in logs on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11748</link><project id="" key="" /><description>when I run `TermsAggregationSearchBenchmark` I get warnings like this: 

```
[2015-06-18 06:30:28,641][WARN ][org.elasticsearch.transport.netty] [node0] Message not fully read (request) for requestId [99], action [indices:data/write/bulk[s]], readerIndex [114711] vs expected [114717]; resetting
```

if I disable compression for bulk they go away so I suspect it's something related to message compression that does some kind of padding or so?

if disable bulk compression it goes away `"action.bulk.compress", false` the funny part is that the `readerIndex [114711]` is always the same when I get the message while `expected [114717];` varies +- 5

the message is in `org.elasticsearch.transport.netty.MessageChannelHandler#118`
</description><key id="89291889">11748</key><summary>Message not fully read warnings in logs on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T13:19:56Z</created><updated>2015-06-30T07:01:01Z</updated><resolved>2015-06-30T07:00:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-18T13:34:35Z" id="113157772">I tried disabling bulk compression but i still see it. Maybe i dont disable it correctly though.
</comment><comment author="s1monw" created="2015-06-18T13:37:40Z" id="113159388">I did this to be sure:

``` DIFF
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkAction.java
index 42d0c22..3554516 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkAction.java
@@ -49,7 +49,7 @@ public class BulkAction extends Action&lt;BulkRequest, BulkResponse, BulkRequestBui
     public TransportRequestOptions transportOptions(Settings settings) {
         return TransportRequestOptions.options()
                 .withType(TransportRequestOptions.Type.BULK)
-                .withCompress(settings.getAsBoolean("action.bulk.compress", true)
+                .withCompress(settings.getAsBoolean("action.bulk.compress", false)
                 );
     }
 }
```
</comment><comment author="rmuir" created="2015-06-18T13:38:09Z" id="113159579">I'll try your patch and see.
</comment><comment author="rmuir" created="2015-06-18T13:39:58Z" id="113160424">also i am confused, if this is a serious thing, why does it simply get logged and not fail tests and so on?
</comment><comment author="rmuir" created="2015-06-18T13:46:15Z" id="113162000">Yeah, disabling the bulk compress works here too. I had fat finger'ed the option.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file></files><comments><comment>Make sure messages are fully read even in case of EOS markers.</comment></comments></commit></commits></item><item><title>init.d script does not stop currently running (old) process after upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11747</link><project id="" key="" /><description>Using repository install on Ubuntu 14.04 LTS

Sequence to reproduce.
- Install 1.5.2
- Upgrade to 1.6.0
- Run `service elasticsearch stop`

init.d script returns "[ OK ]" (indicates that process has been stopped), but process list shows process is still running.  If you manually kill the process, the init.d script then successfully start/stops 
</description><key id="89276412">11747</key><summary>init.d script does not stop currently running (old) process after upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-06-18T12:10:28Z</created><updated>2015-08-09T08:49:45Z</updated><resolved>2015-08-09T08:49:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-08-09T08:49:45Z" id="129141071">Closing in favour of #12649
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update translog.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11746</link><project id="" key="" /><description>`index.gateway.local.sync` is not actually dynamically updateable and the note refers to all the settings.
</description><key id="89268066">11746</key><summary>Update translog.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">astefan</reporter><labels><label>docs</label></labels><created>2015-06-18T11:30:33Z</created><updated>2015-06-23T11:36:37Z</updated><resolved>2015-06-19T14:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-18T13:19:42Z" id="113153439">LGTM. Side note: this settings was replace with `index.translog.sync_interval` in 2.0 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix number of deleted/missing documents in Delete-By-Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11745</link><project id="" key="" /><description>The deleted counter is incremented even if the document is missing. Also, this commit ensures that the scroll id is cleared even if no documents are found by the scan request.
</description><key id="89263657">11745</key><summary>Fix number of deleted/missing documents in Delete-By-Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T11:10:49Z</created><updated>2015-07-08T13:17:51Z</updated><resolved>2015-06-18T12:00:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-18T11:59:05Z" id="113129866">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>consolidate query _name and boost support in query DSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11744</link><project id="" key="" /><description>As part of the query refactoring we are trying to make support for `boost` and `_name` in our queries more generic and consistent (#10776) as it is currently copy pasted in all query parsers and builders, sometimes forgotten, and error prone. While working on that, I realized that `_name` and `boost` are supported slightly differently in the json depending on the type query. The main difference, besides bugs, is that some queries support both within the top level query object:

```
{
  "multi_match" : {
    "query" : "test",
    "fields" : ["field1", "field2"],
    "_name" : "query_name",
    "boost" : 10
  }
}
```

while others support them within the inner object named like the field that gets queried:

```
{
  "term" : {
    "field_name" : {
      "value" : "test",
      "_name" : "query_name",
      "boost" : 10
    }
  }
}
```

or even the following:

```
{
  "term" : {
    "field_name" : "value" : "test",
    "_name" : "query_name",
    "boost" : 10
  }
}
```

The following is a summary of all of the queries and their behaviour:

| Query | _name top level | _name inner object | boost top level | boost inner object |
| --- | :-: | :-: | :-: | :-: |
| and | X |  |  |  |
| bool | X |  | X |  |
| boosting |  |  | X |  |
| common_terms |  | X |  | X |
| constant_score |  |  | X |  |
| dis_max | X |  | X |  |
| exists | X |  |  |  |
| field_masking_span | X |  | X |  |
| filtered | X |  | X |  |
| function_score |  |  | X |  |
| fuzzy |  | X |  | X |
| geo_bbox ** | X |  |  |  |
| geo_distance ** | X |  |  |  |
| geo_distance_range ** | X |  |  |  |
| geohash_cell |  |  |  |  |
| geo_polygon ** | X |  |  |  |
| geo_shape ** | X |  | X |  |
| has_child | X |  | X |  |
| has_parent | X |  | X |  |
| ids | X |  | X |  |
| indices | X |  |  |  |
| limit |  |  |  |  |
| match_all |  |  | X |  |
| match |  | X |  | X |
| missing | X |  | X |  |
| more_like_this | X |  | X |  |
| multi_match | X |  | X |  |
| nested | X |  | X |  |
| not | X |  |  |  |
| or | X |  |  |  |
| prefix ** | X | X |  | X |
| query_string | X |  | X |  |
| range ** |  | X | X |  |
| regexp ** | X | X |  | X |
| script | X |  |  |  |
| simple_query_string | X |  |  |  |
| span_containing | X |  | X |  |
| span_first | X |  | X |  |
| span_multi |  |  |  |  |
| span_near | X |  | X |  |
| span_not | X |  | X |  |
| span_or | X |  | X |  |
| span_term |  | X |  | X |
| span_within | X |  | X |  |
| term ** | X | X | X | X |
| terms | X |  | X |  |
| type |  |  |  |  |
| wildcard |  | X |  | X |

*\* : queries that support the inner object named as the field that gets queried, but support `_name` on the top level object instead.

All of the queries that support _name and boost within the inner object do so in their long version. The corresponding short version usually doesn't support  `_name` and `boost`, but only a single field in the form `field: value`.

1) The main question is where should the `_name` be supported. Should it always be in the top level object, as I was expecting without looking at the code, or does it make sense to wrap everything in the inner object if present?

2) Queries that don't support _name at all: boosting, constant_score, function_score, geohash_cell, limit, match_all, span_multi, type. Support for `_name` needs to be added to these?

3) Queries that don't support boost: indices, simple_query_string (PR opened), span_multi. Support for boost should be added to these?

4) Filters that don't support boost: and, bool, exists, geo_bbox, geo_distance, geo_distance_range, geohash_cell, geo_polygon, limit, missing, not, or, script, type. Now that filters and queries are merged, can these be used as queries? If so would the `boost` make sense if it was supported here too?

5) The following are inconsistencies that surely need to be fixed, according also to outcome of this discussion:
- ~~`GeoShapeQueryBuilder` doesn't support `boost`, but `GeoShapeQueryParser` parses it (fixed in #11810)~~.
- `PrefixQueryParser` supports `_name` in both short and long version, `boost` only in long version.
- `RegexpQueryParser` supports `_name` in both short and long version, `boost` only in long version.
- `RangeQueryParser` supports`_name` in the top level but `boost` in the inner field object.
- `TermQueryParser` supports both  `_name` and `boost` in both long and short version.
- ~~`TermsQueryBuilder` doesn't support `boost`, but parser parses it  (fixed in #11810)~~.
</description><key id="89261225">11744</key><summary>consolidate query _name and boost support in query DSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label></labels><created>2015-06-18T10:59:15Z</created><updated>2015-08-27T10:25:19Z</updated><resolved>2015-08-27T10:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-18T12:14:32Z" id="113134497">&gt; 1) The main question is where should the _name be supported. Should it always be in the top level object, as I was expecting without looking at the code, or does it make sense to wrap everything in the inner object if present?

+1 on only supporting it at the top level,  like our documentation describes.

&gt; 2) Queries that don't support _name at all: boosting, constant_score, function_score, geohash_cell, limit, match_all, span_multi, type. Support for _name needs to be added to these?

Yes.

&gt; 3) Queries that don't support boost: indices, simple_query_string (PR opened), span_multi. Support for boost should be added to these?

+1 on supporting `_boost` on all queries. I'm a bit torn about `indices` however which is not a real query. But on the other hand it would be easy to propagate the boos to the underlying query.

&gt; 4) Filters that don't support boost: and, bool, exists, geo_bbox, geo_distance, geo_distance_range, geohash_cell, geo_polygon, limit, missing, not, or, script, type. Now that filters and queries are merged, can these be used as queries? If so would the boost make sense if it was supported here too?

Yes they can be used as queries, and most of them actually return the value of the boost as a score (like match_all) so I think it makes sense to be consistent and add support for `_boost` to these queries.
</comment><comment author="clintongormley" created="2015-06-18T19:18:38Z" id="113263740">&gt; +1 on only supporting it at the top level, like our documentation describes.

I'm not sure about this.  Every other extra parameter is specified in the inner object. What makes `_name` special?  It also means that you can't query a field called `_name` (admittedly an edge case).

Our documentation actually shows two conflicting syntaxes: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html#search-request-named-queries-and-filters

My feeling is that it should be moved into the inner object for all queries.  It'd be easy to throw an exception if two "field names" are present, and an easy change to make in the application.
</comment><comment author="javanna" created="2015-06-19T08:07:02Z" id="113424538">&gt; My feeling is that it should be moved into the inner object for all queries. 

I see your point, but the majority of queries don't have an inner object, which would mean leaving things pretty much as they are? I find the current situation a bit counter intuitive: if there is an inner object, that's where the `_name` goes, otherwise top level (besides bugs and inconsistencies that need to be fixed). The other way around, if we are up for a big change, not sure but lemme throw it on the table, maybe we shouldn't have an inner object at all and have `field: field_name` in all queries instead and `_name` could be safely moved to the top level, maybe together with `boost`? Maybe we should even look into renaming `_name` to `_query_name`?
</comment><comment author="clintongormley" created="2015-06-23T16:30:38Z" id="114563978">Follow up: @javanna and I talked and decided:
- all queries have a long form
- some queries (single field leaf queries) also have a short form
- in the short form, the only top-level parameter should be the field name
- specifying `_name`, `boost`, and any other parameters should always require the long form
</comment><comment author="javanna" created="2015-07-01T16:02:44Z" id="117728209">I decided to do most of the work associated with this issue in the query-refactoring branch, as we have better tools to test these changes there e.g. unit tests and much more coverage. Also, the change has some breaking aspects which are not a problem in the query-refactoring branch as there we will completely change the way queries are sent over the wire anyway: if a builder prints out a new field that wasn't supported before when sending a search request through java api, the corresponding parser from an older node might throw error as it doesn't support that new field in that position.

The bigger part of this change was done as part of #11974. Besides that what we have left to do is remove/deprecate support for `_name` and `boost` where they are supported but they shouldn't be:
- don't parse `_name` in short version of prefix query
- don't parse `_name` in short version of regexp query
- don't parse `_name` in the top level object of a range query (_name and boost should only be parsed in the inner field)
- don't parse `_name` and `boost` in short version of term query, only long version should support those
</comment><comment author="clintongormley" created="2015-08-13T11:45:42Z" id="130635513">Closed by #11974
</comment><comment author="javanna" created="2015-08-13T12:09:27Z" id="130644857">sorry @clintongormley I still have a couple of things to do so I can close this :) the four changes left listed above. will reopen and work on this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/Strings.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/common/StringsTests.java</file><file>core/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Query DSL: deprecate _name and boost in short variants of queries</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/common/Strings.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/test/java/org/elasticsearch/common/StringsTests.java</file><file>core/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Query DSL: deprecate _name and boost in short variants of queries</comment></comments></commit><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Query refactoring: unify boost and query name</comment></comments></commit><commit><files><file>core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Query refactoring: unify boost and query name</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file></files><comments><comment>Java api: add missing boost support to TermsQueryBuilder</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file></files><comments><comment>Java api: add missing boost support to GeoShapeQueryBuilder</comment></comments></commit></commits></item><item><title>Add DateTime ctors without timezone to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11743</link><project id="" key="" /><description>Using DateTime with default timezone is asking for trouble and should
be added to forbidden APIs
</description><key id="89224038">11743</key><summary>Add DateTime ctors without timezone to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T07:46:44Z</created><updated>2015-06-18T18:15:18Z</updated><resolved>2015-06-18T08:44:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-18T08:08:18Z" id="113069059">LGTM
</comment><comment author="bleskes" created="2015-06-18T08:37:08Z" id="113076540">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace deprecated methods readStringStringMap() and writeStringStringMap()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11742</link><project id="" key="" /><description>Lucene deprecated methods `IndexOutput#writeStringStringMap()` and `IndexInput#readStringStringMap()` but are still using them in elasticsearch 2.0.0 `Store` class:
- https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/store/Store.java#L865-865
- https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/store/Store.java#L1192-1192
</description><key id="89218079">11742</key><summary>Replace deprecated methods readStringStringMap() and writeStringStringMap()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2015-06-18T07:10:22Z</created><updated>2016-01-18T21:39:45Z</updated><resolved>2016-01-18T20:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-18T07:56:14Z" id="113067290">The methods in those links are for LegacyChecksums. Do we not need to keep using those methods for backwards compatibility, to be able to read old indices/segments?
</comment><comment author="clintongormley" created="2016-01-18T20:38:00Z" id="172647502">Closing for now
</comment><comment author="nik9000" created="2016-01-18T21:39:45Z" id="172661279">I think in master we can just blast the legacy checksum support entirely.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace Iterators#emptyIterator by JDK one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11741</link><project id="" key="" /><description>`com.google.common.collect.Iterators#emptyIterator()` is marked as deprecated and will be removed in May 2016. We should use JDK7 `Collections#emptyIterator()`
</description><key id="89214614">11741</key><summary>Replace Iterators#emptyIterator by JDK one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T06:49:03Z</created><updated>2015-06-18T06:53:05Z</updated><resolved>2015-06-18T06:51:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-18T06:50:21Z" id="113054663">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Build: Add Iterators.emptyIterator to forbidden apis</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>core/src/main/java/org/elasticsearch/index/get/GetResult.java</file></files><comments><comment>Merge pull request #11741 from dadoonet/cleanup/emptyiterator</comment></comments></commit></commits></item><item><title>Completion Suggester V2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11740</link><project id="" key="" /><description># Overview

The motivation behind the new completion field is to support auto-complete/search-as-you-type functionality. This is a navigational feature to guide users to relevant results as they are typing, improving search precision. Ideally, auto-complete functionality should be as fast as a user types to provide instant feedback relevant to what a user has already typed in.

This PR introduces a new completion field (https://github.com/elastic/elasticsearch/issues/10746) using Lucene’s new suggest API. The new field has a superset of functionalities provided by the existing completion field (CompletionSuggester and ContextSuggester), namely near-real time search, document retrieval, support for multiple contexts and flexible scoring through index and query-time boosts.
# Use Case

Guide users to relevant documents by suggesting song titles as they type. In this example, the suggestions to serve the user will be indexed as a completion field named `title_suggest`.

The mapping snippet below adds a completion field named `title_suggest`

``` json
...
"properties": {
 "title_suggest": {
  "type": "completion"
 }
}
```

To add title suggestions, index documents with `title_suggest` field.

``` json
{
 "title_suggest" : "Californication",
}
```

You can also specify an array or an object to specify multiple values and/or configure index-time weights for completion values. **Index-time Weights** (maybe call this index-time boost?) are for sorting suggestions that match a prefix. For example, is "Californication" or "Can't Stop" more relevant for a prefix query of "ca"?
### Querying

To query against a CompletionField, use `_suggest` API (see https://github.com/elastic/elasticsearch/issues/10746 for details). 

Apart from simple **Prefix Query**, you can use **Fuzzy Prefix Query** to get typo-tolerant suggestions. This queries a set of similar prefixes (based on edit distance) and scores the suggestions based on how similar they are to what the user has typed in. For example, a prefix query of "cale" would suggest "Californication" and "Can't Stop" in order. 

There is support for expressing a query prefix as a **Regular Expression**. For example, `ca[l-n]` will suggest titles starting with "cal", "cam" or "can".
### Scoring and Filtering

It is often desirable to serve suggestions filtered and/or boosted by some criteria. For example, you want to suggest song titles filtered by certain artists or you want to boost song titles based on their genre. 

To achieve suggestion filtering and/or boosting, you can add **contexts** while configuring a completion field. You can define multiple contexts for a completion field (NOTE: adding contexts increases the index size). Every context has a unique name and a type. Currently, there are two types `category` and `geo`.

To use genre as a context to song suggestion, a completion field is configured with a 'genre_context' context which indexes values from field 'genre' as shown below

``` json
...
"properties": {
 "title_suggest": {
  "type": "completion",
  "contexts" : [
   { "name": "genre_context", "type" : "category", "path": "genre" }
  ]
 }
}
```

Suggestions with contexts can be added by indexing a document as follows:

``` json
{
 "title_suggest" : "Californication",
 "genre" : ["funk rock", "alterative_rock", "funk metal"],
}
```

The default behaviour for context-enabled fields is to suggest from _all_ contexts. You can filter on defined context(s) at query time by adding query contexts. For example, to restrict song suggestions to only 'funk rock' and 'indie' genres.

``` json
"suggest-namespace" : {
 ...
 "completion" : {
  "field" : "title_suggest",
  "contexts": {           
   "genre_context": ["funk rock", "indie"]
  }
 }
}
```

You can boost suggestions for some genres more than others. For example, restrict song suggestions to all genres that start with "funk" but boost song suggestions with "funk rock" genre.

``` json
...
"genre_context": [ 
 { "context": "funk", "prefix": true }, 
 { "context": "funk rock", "boost": 4 } 
]
```

Configuring a completion field with multiple contexts, allows filtering/boosting suggestions by multiple criteria. For example, adding artist and genre contexts as follows:

``` json
...
"title_suggest": {
 "type": "completion",
 "contexts" : [
  { "name": "genre_context", "type" : "category", "path": "genre" },
  { "name": "artist_context", "type" : "category", "path": "artist" }
 ]
}
```

You can restrict song suggestions to "funk rock" and "alternative rock" genre and boost songs by "rhcp" by adding the following query contexts.

``` json
"contexts": {           
 "genre_context": [ "funk rock", "alternative rock" ],
 "artist_context" : { "context" : "rhcp", "boost": 4 }
}
```

The query response for any context-enabled field, will include all the associated context values for that entry, along with the suggestion and weight.

NOTE: Adding more contexts to a completion field will result in search performance degradation (when using match all context) and increase the index size.
### PR status
- This PR excludes the Lucene related changes ([LUCENE-6459](https://issues.apache.org/jira/browse/LUCENE-6459) and smaller changes) to keep the PR es specific.
- Retrieving documents with suggestions are not implemented in this PR, and will be done in a subsequent PR
- Todo: Benchmark context suggester
</description><key id="89190686">11740</key><summary>Completion Suggester V2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label></labels><created>2015-06-18T03:52:00Z</created><updated>2015-09-21T12:41:44Z</updated><resolved>2015-08-07T05:10:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-18T17:06:34Z" id="113222836">For backcompat, why can't the field type name stay the same, and the logic to decide which actual mapper/fieldtype to create be when parsing (where we have the index created version)?
</comment><comment author="areek" created="2015-06-18T17:17:54Z" id="113225222">@rjernst I think that might be confusing to the users, you have the same field type that behaves differently w.r.t. when they were created? 
My thoughts, we use the index created version, to map the the old field type from 'completion' -&gt; 'completion_old' for older versions, and make it clear what is the expected behaviour. If you want to use the new type, you explicitly create a 'completion' field type or if you still rely on features such as attaching payloads (not supported for the new field type) you continue using 'completion_old'. Thoughts?
</comment><comment author="rjernst" created="2015-06-18T17:20:31Z" id="113225738">I think that is no more confusing that what I suggested since someone that doesn't know about the new type will get the "new" behavior (they will get a new completion field when they create another with type `completion`).
</comment><comment author="rjernst" created="2015-06-18T17:21:01Z" id="113225823">And I don't think we should allow creating the old type.
</comment><comment author="areek" created="2015-06-18T17:47:13Z" id="113233927">I agree it being somewhat confusing, it can be solved through documentation (same can be done for your suggestion :)). To me, it is easier to reason about new behaviour tied to a different type name then that of the old? But if we don't allow creating the old type, what you are suggesting makes sense to me. 
So, we will check the index creation version and choose which field-type/suggester to be used. I will explore this, thanks for the suggestion, @rjernst!
</comment><comment author="clintongormley" created="2015-06-23T16:28:25Z" id="114563475">The bwc decision is hard. I agree with @rjernst that we shouldn't allow users to create new indices with the old suggester, but that existing indices should still work.

We can choose which suggester to use a query time based on index creation date/version.  The behaviour will be different (e.g. with the old you get back payloads, with the new you get back documents).  I think that almost everybody would use a suggester on a single index, rather than across  multiple indices, so mixed behaviour probably isn't a problem (although perhaps we should not allow combining results from old and new indices?)

If we leave the field type the same (ie `completion`) there is no visual indication to the user that queries will behave differently, but I don't much like the `completion_old` name either...
</comment><comment author="clintongormley" created="2015-06-23T16:28:46Z" id="114563552">@areek what is `exact`?  does it mean "not fuzzy"?
</comment><comment author="areek" created="2015-06-23T16:35:01Z" id="114565244">@clintongormley when `exact` is false for query context, the context in question is treated as a prefix of indexed contexts.
For example, if you have indexed contexts `type1`, `type2` and `type3`, you could boost/filter by all three contexts by using a query context of 

```
{ "value": "type", "exact": false, "boost": .. }
```

instead of:

```
{ "value": "type1", "boost": .. }
{ "value": "type2", "boost": .. }
{ "value": "type3", "boost": .. }
```

Maybe we could have a better name for this option? thoughts?
</comment><comment author="clintongormley" created="2015-06-23T19:41:14Z" id="114620475">&gt; Maybe we could have a better name for this option? thoughts?

@areek maybe we use `context` instead of `value`, and use `prefix` when it isn't exact?
</comment><comment author="abhijitiitr" created="2015-06-30T06:35:54Z" id="117017092">@areek I tested these changes and it feels wonderful to have such a great deal of flexibility. But I faced a few issues. As @clintongormley said

&gt; (e.g. with the old you get back payloads, with the new you get back documents).

I couldn't find a way to get documents currently. If there are multiple inputs having multiple contexts and weights per doc for a `completion` type field, the suggester currently doesn't differentiate between inputs of same and different docs. 
Can you suggest ways of differentiating output on the basis of their docs with these changes?
</comment><comment author="areek" created="2015-07-02T01:22:02Z" id="117868178">@abhijitiitr Thanks for testing the new suggester. It would be awesome if you could share any details about the data set you tested it with :)

&gt; I couldn't find a way to get documents currently.

This feature is not implemented in this PR yet, but will be done as a subsequent PR

&gt;  If there are multiple inputs having multiple contexts and weights per doc for a completion type field, the suggester currently doesn't differentiate between inputs of same and different docs. 

This is a known issue and will be fixed. The suggested inputs that correspond to the same doc will be deduplicated.

&gt; Can you suggest ways of differentiating output on the basis of their docs with these changes?

Once the document fetch phase is implemented, the associated document will be returned with the suggestion.
</comment><comment author="leonardehrenfried" created="2015-07-02T16:04:50Z" id="118079295">I'm not entirely sure from reading this PR whether it will be possible but it seems that payloads are no longer supported in the new suggester. This would make me quite sad since I discovered a bug regarding payloads in the old suggester and was hoping it would be fixed with the new one.

On the other hand I read the following comment: "(e.g. with the old you get back payloads, with the new you get back documents)". This would indeed solve my problem. Is it true that the new suggest API returns the complete document?
</comment><comment author="areek" created="2015-07-02T16:37:03Z" id="118088298">&gt; Is it true that the new suggest API returns the complete document?

@lenniboy Yes, so instead of storing a value as a payload, you can store it as a field in the document. You will get back the document with the suggestions. 
</comment><comment author="clintongormley" created="2015-07-03T09:56:45Z" id="118303413">@areek @jpountz came up with a simple solution to the bwc issue.  Call the new field type and suggester `complete` instead of `completion`.  Problem solved
</comment><comment author="rjernst" created="2015-07-03T10:16:25Z" id="118305835">That sounds like a horrible idea. Those two names are far too close to each other: _very_ confusing. Why is it a problem to force the new field type on 2.0+ indexes?
</comment><comment author="clintongormley" created="2015-07-03T10:19:08Z" id="118306589">@rjernst more confusing than having the same name return different results, and no easy way to distinguish them, and having an exception thrown if you try to do a completion request across old and new indices?  And what about if you try to add a completion field to an old index - what happens then?
</comment><comment author="rjernst" created="2015-07-03T10:21:43Z" id="118307385">&gt; @rjernst more confusing than having the same name return different results, and no easy way to distinguish them, and having an exception thrown if you try to do a completion request across old and new indices?

Yes.

&gt; And what about if you try to add a completion field to an old index - what happens then?

The same thing that happens for other mapper changes we've done. The behavior stays the same on indexes before 2.0 (for new fields as well) and the new behavior happens with indexes created on or after 2.0.
</comment><comment author="clintongormley" created="2015-07-03T10:28:47Z" id="118309598">&gt; The same thing that happens for other mapper changes we've done. The behavior stays the same on indexes before 2.0 (for new fields as well) and the new behavior happens with indexes created on or after 2.0.

Today you can run a completion suggester across multiple indices. If you try this with what you are proposing, then you'll either (a) not be able to reduce the results or (b) get a mixed result set, some with payloads and some with docs.

Plus the new suggester supports things that the old suggester doesn't, which could lead to more confusing exceptions.  What I don't like about keeping the same name is that it is hard for the user to know which index is going to behave in what way.  Changing the name resolves that: "no field of type `complete` found in index foo"
</comment><comment author="areek" created="2015-07-22T06:51:26Z" id="123581082">Updated PR:
- Backwards compatibility
  - `completion` field created on a pre 2.0 index will use the old implementation
  - `completion` field created on a post 2.0 index will use the new implementation
  - if new and old `completion` field is queried at the same time (e.g. same field name across different indices), then an IAE is thrown at the reduce phase
- Old Completion suggester now resides in `org.elasticsearch.search.suggest.completion.old`
- First draft of API docs for new suggester: [Completion Suggester](https://github.com/areek/elasticsearch/blob/comp_suggester_v2/docs/reference/search/suggesters/completion-suggest.asciidoc), [Context Suggester](https://github.com/areek/elasticsearch/blob/comp_suggester_v2/docs/reference/search/suggesters/context-suggest.asciidoc)
- Increased test coverage
- `geo` context mapping work

@clintongormley would be awesome to get some feedback on the API docs (linked above).
@rjernst @jpountz would be great to get some feedback on the new completion field
</comment><comment author="abhijitiitr" created="2015-07-23T06:17:28Z" id="123989933">@areek Sorry for the late response. We have around 0.5M restaurants for which we wanted to index the dishes they served which could be scoped under different groups and scored differently.

``` javascript
 [
        {
            "input" : ["Palak Paneer", "Shahi Paneer"],
            "contexts" : {
                "cuisine_type" : ["North Indian","Continental"],
                "meal_type" : ["Dinner","Lunch"]
            },
            "weight" : 31
        },
       {
            "input" : ["Upma"],
            "contexts" : {
                "cuisine_type" : ["South Indian","Continental"],
                "meal_type" : ["Breakfast"]
            },
            "weight" : 29
        },
        {
            "input" : ["Pizza", "Rissoto", "Pasta"],
            "contexts" : {
                "cuisine_type" : ["Italian"],
                "meal_type" : ["Lunch","Dinner"]
            },
            "weight" : 34
        }

    ]
```

An example of an input to the suggester is given above. 

Regarding the document linking part, I was studying the codebase and was able to find the trace of new suggest functionality.
The `innerExecute` function in the `CompletionSuggester` class was an intermediary in bringing the suggestions and we need to link these suggestions to the documents . As you had earlier said 

&gt; Once the document fetch phase is implemented, the associated document will be returned with the suggestion.

I'm not being able to find a way to link the documents linked to the `outputs`(i.e `options` which are finally added in the end of the function. I only know the codebase in bits and pieces and would be very grateful for your help.
</comment><comment author="clintongormley" created="2015-07-23T12:02:21Z" id="124076340">@areek looking good. i've added some minor comments and some questions.  One thing about the docs: category and context are used interchangeably, which i found quite confusing. I'd try to define the difference and then them consistently.
</comment><comment author="areek" created="2015-07-28T06:18:37Z" id="125464753">@clintongormley Thanks for the review, I updated the PR incorporating your feedback.
</comment><comment author="clintongormley" created="2015-07-30T10:26:14Z" id="126266294">@areek LGTM from a docs/api point of view.  I'd get a developer review as well.

thanks!
</comment><comment author="mikemccand" created="2015-08-05T00:14:02Z" id="127801499">I left several small comments but overall this looks really nice.

It's a big change, I think we need to get it in soon so it has more time to bake before 2.0 GA (it's already scary close, and it's weird that beta1 won't have it but GA will), and then make new PRs for further additions (e.g. retrieving documents sources/fields for each suggestion, generic filters).

Do we talk about the changes vs the current suggesters in the migration guide?  Or maybe we should do a blog post about the differences?
</comment><comment author="areek" created="2015-08-05T20:03:49Z" id="128132006">Thanks @mikemccand for the review. I have addressed all your feedback. I will create a Lucene issue for the deduping doc ids in TopSuggestDocsCollector and back-port the fix. 
</comment><comment author="mikemccand" created="2015-08-05T22:13:00Z" id="128165094">Thanks @areek, LGTM.
</comment><comment author="areek" created="2015-08-07T05:10:50Z" id="128600459">This PR has been merged to a feature branch https://github.com/elastic/elasticsearch/tree/completion_suggester_v2
</comment><comment author="jrots" created="2015-09-21T12:41:44Z" id="141961625">When will this be available in v2.0?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Suggest returned even when setting "suggest_mode" to be "missing"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11739</link><project id="" key="" /><description>I think when setting `suggest_mode` to be `missing`, no suggestion should be returned if the term exists in the index. But it turns out not to behave like my expectation. Is it a bug or anything?

Below is what i did. (with elasticsearch 1.5)

```
# remove all
$ curl -XDELETE elastic:9200/*
{"acknowledged":true}

# index two documents
$ curl -XPOST elastic:9200/my_index/my_type -d '{"name": "shinagawa hotel"}'
{"_index":"my_index","_type":"my_type","_id":"AU4FdpnwDWV1B-oz3tLK","_version":1,"created":true}
$ curl -XPOST elastic:9200/my_index/my_type -d '{"name": "shirakawa hotel"}'
{"_index":"my_index","_type":"my_type","_id":"AU4Fdta7DWV1B-oz3tLL","_version":1,"created":true}
```

```
$ cat search.json
```

``` json
{
    "explain": true,
    "query": {
        "match": {
            "name": {
                "query": "shinagawa"
            }
        }
    },
    "suggest": {
        "my-suggest": {
            "text": "shinagawa",
            "term": {
                "field": "name",
                "suggest_mode": "missing"
            }
        }
    }
}
```

```
$ curl elastic:9200/my_index/_search?pretty -d@search.json
```

``` json
{
  "took" : 7,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.19178301,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "7UoDQJWRQYqF4Gjq94LiMg",
      "_index" : "my_index",
      "_type" : "my_type",
      "_id" : "AU4FdpnwDWV1B-oz3tLK",
      "_score" : 0.19178301,
      "_source":{"name": "shinagawa hotel"},
      "_explanation" : {
        "value" : 0.19178301,
        "description" : "weight(name:shinagawa in 0) [PerFieldSimilarity], result of:",
        "details" : [ {
          "value" : 0.19178301,
          "description" : "fieldWeight in 0, product of:",
          "details" : [ {
            "value" : 1.0,
            "description" : "tf(freq=1.0), with freq of:",
            "details" : [ {
              "value" : 1.0,
              "description" : "termFreq=1.0"
            } ]
          }, {
            "value" : 0.30685282,
            "description" : "idf(docFreq=1, maxDocs=1)"
          }, {
            "value" : 0.625,
            "description" : "fieldNorm(doc=0)"
          } ]
        } ]
      }
    } ]
  },
  "suggest" : {
    "my-suggest" : [ {
      "text" : "shinagawa",
      "offset" : 0,
      "length" : 9,
      "options" : [ {
        "text" : "shirakawa",
        "score" : 0.7777778,
        "freq" : 1
      } ]
    } ]
  }
}
```

Here `shinagawa` is in the index of `name` field, but suggestion to it is not excluded.
</description><key id="89188404">11739</key><summary>Suggest returned even when setting "suggest_mode" to be "missing"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">czheo</reporter><labels /><created>2015-06-18T03:34:11Z</created><updated>2017-03-22T23:58:13Z</updated><resolved>2015-06-18T18:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="inqueue" created="2015-06-18T06:02:13Z" id="113046639">Have a look at https://github.com/elastic/elasticsearch/issues/7472. They are looking for a repro there as well.
</comment><comment author="clintongormley" created="2015-06-18T18:07:58Z" id="113242409">Hi @czheo 

The reason for this is that you have two documents in two different shards.  Suggestions are calculated per shard, then merged.  So the term specified exists on one shard and is missing on the other, which is why it is included as a suggestion.

A similar problem exists with relevance when searching across a tiny collection of documents. See https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html

Duplicate of #7472
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Term vectors API: term statistics loaded even if not needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11738</link><project id="" key="" /><description>As in title.  Slows down loading term vectors from (typical figures) 0.3ms per term vector to 2ms per term vector.

See pull request:

https://github.com/elastic/elasticsearch/pull/11737

This issue is also present in master.
</description><key id="89177706">11738</key><summary>Term vectors API: term statistics loaded even if not needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wewebber</reporter><labels /><created>2015-06-18T02:15:47Z</created><updated>2015-08-26T19:59:37Z</updated><resolved>2015-08-26T19:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T19:59:36Z" id="135154585">Fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Only load term statistics if required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11737</link><project id="" key="" /><description>Current code loads term statistics even if not requested.  This slows down term vector requests markedly (typical figures for stored term vectors on 3k-character field on 4-core machine: 2ms per vector with current code; 0.3ms with this fix).
</description><key id="89177379">11737</key><summary>Only load term statistics if required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">wewebber</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-18T02:14:04Z</created><updated>2015-07-08T16:49:08Z</updated><resolved>2015-07-08T16:40:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-22T19:46:49Z" id="114235627">can you sign the CLA so we can pull this in?
</comment><comment author="wewebber" created="2015-06-23T23:51:54Z" id="114676880">Done, under github account "wewebber".  Do you want me to make an equivalent pull request for the master branch (same issue, same fix), or will you apply the fix yourself?
</comment><comment author="clintongormley" created="2015-06-24T13:29:39Z" id="114867910">thanks @wewebber - don't worry, we'll handle applying to other branches
</comment><comment author="jpountz" created="2015-07-08T16:49:08Z" id="119656084">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for deprecated percent_terms_to_match REST parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11736</link><project id="" key="" /><description>We should also make sure that `minimum_should_match` is overridden by
`percent_terms_to_match` when the later is set.

Relates to #11574 #11572
</description><key id="89149691">11736</key><summary>Support for deprecated percent_terms_to_match REST parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>regression</label><label>v1.5.3</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-06-17T23:11:57Z</created><updated>2015-06-18T13:47:10Z</updated><resolved>2015-06-18T13:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-18T09:45:55Z" id="113092320">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add -XX:+PrintGCDateStamps when using GC Logs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11735</link><project id="" key="" /><description>Now that Elasticsearch requires Java 7 or later, it's safe to add `-XX:+PrintGCDateStamps` to get human readable times alongside JVM times.

Closes #11733
</description><key id="89131263">11735</key><summary>Add -XX:+PrintGCDateStamps when using GC Logs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T21:32:52Z</created><updated>2015-08-07T10:07:32Z</updated><resolved>2015-07-27T19:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T16:45:13Z" id="119654981">LGTM
</comment><comment author="martijnvg" created="2015-07-16T10:31:48Z" id="121921531">Bumping the version up to 1.7.1 for the today's release.
</comment><comment author="jpountz" created="2015-07-17T11:33:35Z" id="122251874">@pickypg Let's merge?
</comment><comment author="nik9000" created="2015-07-27T17:16:06Z" id="125276732">+1
</comment><comment author="pickypg" created="2015-07-27T19:27:40Z" id="125317832">Merged into 1.7 with https://github.com/elastic/elasticsearch/commit/d0f7e5ef05a69932da6e78a1428e780cbcc1f454
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11735 from pickypg/feature/use-gc-datestamps-11733</comment></comments></commit></commits></item><item><title>[doc] Move lang documentation to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11734</link><project id="" key="" /><description>This commit moves `plugins/lang-javascript/README.md` file to `docs/plugins/lang/javascript/index.asciidoc`.

Same for python plugin.

It will be easier to integrate plugin documentation in our reference guide.
</description><key id="89116092">11734</key><summary>[doc] Move lang documentation to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Lang JS</label><label>:Plugin Lang Python</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T20:21:02Z</created><updated>2015-08-18T11:32:02Z</updated><resolved>2015-07-10T14:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-07-10T14:31:34Z" id="120423221">Closing in favor of #12040 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>GC Logs should add -XX:+PrintGCDateStamps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11733</link><project id="" key="" /><description>In addition to the `-XX:+PrintGCTimeStamps`, which semi-unhelpfully supplies the seconds since the JVM started up, we should add `-XX:+PrintGCDateStamps` so that we get readable times printed in the gc.log files.

Note: this is only relevant when the GC logs are turned on, which requires setting the `ES_GC_LOG_FILE` to some non-empty value (e.g., `on`).
</description><key id="89111627">11733</key><summary>GC Logs should add -XX:+PrintGCDateStamps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>low hanging fruit</label></labels><created>2015-06-17T20:05:56Z</created><updated>2015-07-27T19:20:50Z</updated><resolved>2015-07-27T19:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename Optimize API to Forced Merge API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11732</link><project id="" key="" /><description>The `_optimize` API is great for improving performance of time series indices or other cold indices, but the name of it is incredibly misleading.

Lucene changed from calling it `optimize` to `forceMerge` back in Lucene 3.5 ([from this blast in the past](http://blog.trifork.com/2011/11/21/simon-says-optimize-is-bad-for-you/), eh @s1monw!). Maybe it's time that we change it too?
</description><key id="89077504">11732</key><summary>Rename Optimize API to Forced Merge API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Index APIs</label></labels><created>2015-06-17T17:31:22Z</created><updated>2015-06-17T18:44:40Z</updated><resolved>2015-06-17T18:44:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-17T18:44:38Z" id="112910907">Duplicate of #6621. I searched too strictly before posting.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JBoss EAP module clarification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11731</link><project id="" key="" /><description>Hi,  was hoping to get some clarification around the JBoss module.  I recently started using scripts and template searches in my app.  To do taht, I added an ASM dependency and brought it into the ES module.  However, whenever my tests run I end up getting this in the console

org.objectweb.asm.commons.Method from [Module "org.elasticsearch:main" from local module loader

This seems to be because JBoss ships with an older ASM than what ES is using.  Anything special to do here?
</description><key id="89072052">11731</key><summary>JBoss EAP module clarification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">johnament</reporter><labels><label>docs</label></labels><created>2015-06-17T17:04:46Z</created><updated>2015-11-23T12:27:43Z</updated><resolved>2015-11-23T12:27:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-17T17:28:28Z" id="112884490">Until 2.0, we were shading libs so I guess you should not see such a conflict.

Just double checking, I guess you already read this, right? https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/_deploying_in_jboss_eap6_module.html
</comment><comment author="johnament" created="2015-06-17T17:47:18Z" id="112889538">Yes, I saw that.
The problem is that this page (which looks like its based on old ticket I tracked a while ago w/ the module def) explicitly excludes ASM.  JBoss, even Wildfly 9, still ship ASM 3.1 and it looks like lucene needs a newer version.  I don't believe you guys are shading this dependency.

Great to hear what sounds like you're not shading in 2.0.  Should reduce some of the bloat.
</comment><comment author="lukas-vlcek" created="2015-06-17T19:17:58Z" id="112918508">@dadoonet David, do you think you can point me to some ticket/commit which is relevant to un-shading in 2.x please? I would like to learn more about the background of such change.
</comment><comment author="dadoonet" created="2015-06-17T19:52:19Z" id="112927087">@lukas-vlcek I think this commit is relevant: https://github.com/elastic/elasticsearch/commit/4c981ff4bfc250080d521af105b5e8589c9fc517
</comment><comment author="dadoonet" created="2015-06-17T19:57:13Z" id="112928064">@johnament You're right. ASM was not shaded. I wonder if the documentation about about EAP deployment should include the right version of ASM lib so the right version will be picked up by the classloader first instead of using the one provided by JBoss.

That said, I don't know JBoss EAP. Is it super different than JBoss OSS version? 
(note: last time I deployed JBoss, it was a version 5. Things have changed in the mean time I guess :) )
</comment><comment author="lukas-vlcek" created="2015-06-18T06:18:09Z" id="113048963">@dadoonet thanks.
</comment><comment author="s1monw" created="2015-06-18T07:33:31Z" id="113062638">@lukas-vlcek the PR is here https://github.com/elastic/elasticsearch/pull/11522
We still provide a shaded verison of the JAR so you should still be able to benefit from this?
</comment><comment author="lukas-vlcek" created="2015-06-18T08:17:07Z" id="113070588">@s1monw thanks. I welcome non-shaded packaging (building ES RPM will be easier).
</comment><comment author="s1monw" created="2015-06-18T08:20:57Z" id="113072317">@lukas-vlcek  please report any issues if they come up! thanks!
</comment><comment author="lukas-vlcek" created="2015-06-18T08:31:57Z" id="113075626">@johnament FYI, if you are interested in JBoss EAP 6.x BOMs they can be found here: http://maven.repository.redhat.com/techpreview/all/org/jboss/bom/eap6-supported-artifacts/
For example [EAP 6.4.1.GA](http://maven.repository.redhat.com/techpreview/all/org/jboss/bom/eap6-supported-artifacts/6.4.1.GA/eap6-supported-artifacts-6.4.1.GA.pom) uses ASM 3.3.1.

Also, may be you can use ES directly as part of your web app (WAR, EAR, ...) as opposed to EAP module?
</comment><comment author="johnament" created="2015-06-18T11:35:42Z" id="113122435">@lukas-vlcek I don't think that's the question here.  It looks like Lucene 4.10 requires ASM 4.1 it looks like, as a result I had pull that in to the module itself (to avoid leaking).
</comment><comment author="clintongormley" created="2015-06-18T19:11:36Z" id="113262375">I'm not a Java guy, so I can't tell or not: is this issue resolved?
</comment><comment author="dadoonet" created="2015-06-18T20:18:09Z" id="113277883">@clintongormley I think it's not really solved as we may have at least to update our documentation. Need to understand what we have to update though. :)

Marking as doc for now and assigning it to me.
</comment><comment author="dadoonet" created="2015-09-10T10:10:23Z" id="139194665">TBH I don't know what we should fix for 2.0. I have no experience with JBoss EAP. @lukas-vlcek Do you have any idea on how we should update the [doc](https://github.com/elastic/elasticsearch/blob/master/docs/java-api/index.asciidoc)?
</comment><comment author="lukas-vlcek" created="2015-09-10T11:17:41Z" id="139207592">@dadoonet In our team we do not use ES as EAP module. We start embedded ES node to connect to ES cluster. So I am not able to tell you from my experience how to best configure EAP module for ES.
</comment><comment author="johnament" created="2015-09-10T11:31:48Z" id="139209491">This is what ended up working for us.  Note that we were already packaging groovy in our dist so it was a new reference here instead of embedded.  On the flip side, we've also stopped using groovy due to the CVE's published, and all of our reports are now just expression based.

```
&lt;module xmlns="urn:jboss:module:1.0" name="org.elasticsearch"&gt;
  &lt;resources&gt;
    &lt;resource-root path="elasticsearch-${es.version}.jar"/&gt;
      &lt;resource-root path="elasticsearch-cloud-aws-${es.aws.version}.jar"/&gt;
      &lt;resource-root path="lucene-core-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-analyzers-common-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-codecs-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-expressions-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-queries-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-memory-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-highlighter-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-queryparser-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-sandbox-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-suggest-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-misc-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-join-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-grouping-${lucene.version}.jar"/&gt;
      &lt;resource-root path="lucene-spatial-${lucene.version}.jar"/&gt;
      &lt;resource-root path="antlr-runtime-3.5.jar"/&gt;
      &lt;resource-root path="asm-4.1.jar"/&gt;
      &lt;resource-root path="asm-commons-4.1.jar"/&gt;
      &lt;resource-root path="asm-tree-4.1.jar"/&gt;
  &lt;/resources&gt;
  &lt;dependencies&gt;
    &lt;module name="sun.jdk" export="true" &gt;
      &lt;imports&gt;
        &lt;include path="sun/misc/Unsafe" /&gt;
      &lt;/imports&gt;
    &lt;/module&gt;
    &lt;module name="org.apache.log4j"/&gt;
    &lt;module name="org.apache.commons.logging"/&gt;
    &lt;module name="javax.api"/&gt;
    &lt;module name="com.spatial4j"/&gt;
    &lt;module name="com.amazonaws"/&gt;
    &lt;module name="org.apache.commons.codec"/&gt;
    &lt;module name="org.codehaus.groovy"/&gt;
  &lt;/dependencies&gt;
&lt;/module&gt;
```

This does bundle asm 4.1 directly w/ ES for classloader isolation, but the same could be done with a slot.  I did it this way to ensure that no one else accidentally uses ASM 4.1.
</comment><comment author="dadoonet" created="2015-09-10T12:05:46Z" id="139215249">Thanks @lukas-vlcek 

Thanks @johnament. I guess `&lt;module name="com.amazonaws"/&gt;` is not needed for all, right?
I guess this was for elasticsearch 1.x? Any chance you tested a similar configuration for 2.0?
</comment><comment author="dadoonet" created="2015-11-23T12:27:43Z" id="158919344">@johnament Do you want to come with documentation pull request about this? I have no idea of what is needed to be done here.

Closing but feel free to either:
- comment on this ticket how documentation needs to be fixed in the elasticsearch 2.0 context
- open a documentation PR which fix the doc (even better :p )

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CommonTermsQuery ignores disabling the coordination factor with Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11730</link><project id="" key="" /><description>CommonTermsQueryParser does not check for `disable_coords`, only for `disable_coord`. Yet the builder only outputs `disable_coords`, leading to disabling the coordination factor to be ignored in the java API.
</description><key id="89068007">11730</key><summary>CommonTermsQuery ignores disabling the coordination factor with Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T16:45:27Z</created><updated>2015-06-23T16:57:22Z</updated><resolved>2015-06-19T15:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>CommonTermsQuery fix for ignored coordination factor</comment></comments></commit></commits></item><item><title>Query refactoring: QueryFilterBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11729</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings. In
this case this also includes FQueryFilterParser, since both queries are
closely related.

Relates to #10217
Closes #11729 
</description><key id="89064561">11729</key><summary>Query refactoring: QueryFilterBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-06-17T16:28:29Z</created><updated>2015-09-02T19:18:57Z</updated><resolved>2015-06-22T16:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-18T08:51:30Z" id="113080157">left one comment, looks good!
</comment><comment author="cbuescher" created="2015-06-22T13:56:13Z" id="114112046">With last commit I moved the queryName member which was just used for FQuery from the QueryFilterBuilder to the new FQueryFilterBuilder that we introduced on the feature branch to separate the two variants of this builder. This creates some code duplication but avoids the former confusion between the two cases. Only problem here might be that for the Java-API we slightly break the existing QueryFilterBuilder because now it doesn't support queryName() any longer, but that part conceptually belonged to FQuery anyway. 
</comment><comment author="javanna" created="2015-06-22T14:36:08Z" id="114133925">LGTM besides the few comments I left. Looking forward to seeing these two filters removed to be honest, with the merge of filters and queries they make very little sense (only bw comp). I think we need to create a migrate_query_refactoring.asciidoc page and add our first breaking change (only for the java api).
</comment><comment author="cbuescher" created="2015-06-22T15:38:45Z" id="114156787">@javanna added asciidoc to track breaking changes on the feature branch, mind to have a look if you think this is a good location and this is verbose enough to keep track? Also, this is inteded for internal use mostly, should be merged with other migration doc when we merge with master, right?
</comment><comment author="javanna" created="2015-06-22T15:45:25Z" id="114158353">looks great. Yes it will eventually be merged but given that we have one file per version and we don't know yet when we will merge the branch, let's just keep a separate file and decide later on what to do with it.
</comment><comment author="javanna" created="2015-07-01T13:25:28Z" id="117673528">This change is breaking for the java api as the setter `QueryFilterBuilder#queryName(String queryName)` has no effect anymore. `_name` is not supported in this type of query. Use `FQueryFilterBuilder.queryName(String queryName)` instead when in need to wrap a named query as a filter.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file></files><comments><comment>Merge pull request #11729 from cbuescher/feature/query-refactoring-fquery</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java</file></files><comments><comment>Query refactoring: QueryFilterBuilder and Parser</comment></comments></commit></commits></item><item><title>Script in terms aggregation that is not a value script should fail with descriptive failure message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11728</link><project id="" key="" /><description>When a script is not a value script in a terms aggregation then the request must not contain the "fields" parameter. 
However, there is no warning to the user if the request has the "fields" parameter anyway. The aggregation will just fail with cryptic error messages or even worse just silently not do the right thing.

This fails with an ArrayIndexOutOfBoundsException:

```
{
  "aggs": {
    "2": {
      "terms": {
        "field": "label",
        "script": "doc['label'].value"
      }
    }
  }
}
```

This silently just does not do the right thing (returns one bucket with key 0 and all docs in it):

```
{
  "aggs": {
    "2": {
      "terms": {
        "field": "label",
        "script": "_index['text']['the'].tf()"
      }
    }
  }
}
```
</description><key id="89062262">11728</key><summary>Script in terms aggregation that is not a value script should fail with descriptive failure message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-17T16:19:53Z</created><updated>2015-06-25T08:07:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="luckywin" created="2015-06-24T14:47:44Z" id="114895430">I have same issue. Value script in terms aggregation breaks migration from terms facet (I've already posted question about such migration https://stackoverflow.com/questions/31006983/migration-from-terms-facet-to-terms-aggregation)
</comment><comment author="brwe" created="2015-06-24T16:37:32Z" id="114936232">@luckywin just to be sure: you know that removing the "fields" parameter in the aggregation should make your script work?
</comment><comment author="luckywin" created="2015-06-25T07:34:29Z" id="115146809">@brwe yes, I knew this, thank you. After removing "field" parameter, agg starts responding with "true" and "false" as a result of aggregation, and this is not expected result for me. I also tried to modify script to return doc['field'].value (or null when I want to exclude value from agg), but it not worked correctly too. Finally seems I've found workaround by using _source.field and null as returned values.

Anyway, descriptive failure message for "not value script" can save lot of time for elastic users.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to opt-out of loading packaged REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11727</link><project id="" key="" /><description>this is really just a workaround for plugins to run their own
REST tests instead of the core ones. It opts out of the rest test
loading from the core jar file and tries to load from the classpath instead.
Eventually we need to fix this infrastrucutre to move away from parameterized
tests such that subclasses can override behavior.

Closes #11721
</description><key id="89061910">11727</key><summary>Allow to opt-out of loading packaged REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>test</label></labels><created>2015-06-17T16:18:21Z</created><updated>2015-06-22T14:44:02Z</updated><resolved>2015-06-17T18:38:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-17T17:06:27Z" id="112878702">+1 this is a nice simple step.
</comment><comment author="martijnvg" created="2015-06-17T17:50:23Z" id="112890138">LGTM
</comment><comment author="tlrx" created="2015-06-17T18:05:00Z" id="112896968">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighters sometime highlight additional caracters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11726</link><project id="" key="" /><description>Verified on 1.5.2 and 1.6.0

When using a charfilter to remove some characters (in our case we want to ignore `(`and `)` ) the highlighters will highlight trailing removed caracters

Here is a sample sense session to reproduce the issue. 

```
DELETE test
PUT test
{
  "settings": {
    "analysis": {
      "filter": {       
      },
      "analyzer": {
        "analyzer_with_charfilter": {
          "filter": [
            "asciifolding",
            "lowercase"
          ],
          "char_filter": [
            "sign_mappings"
          ],
          "type": "custom",
          "tokenizer": "whitespace"
        }
      },
      "char_filter": {
        "sign_mappings": {
          "type": "mapping",
          "mappings": [
            "*=&gt;star",
            "+=&gt;plus",
            "(=&gt;",
            ")=&gt;"
          ]
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "string",
          "index_options" : "offsets",
          "analyzer": "analyzer_with_charfilter"
        }
      }
    }
  }
}
PUT test/test/1
{
  "name":"(F31)"
}
PUT test/test/2
{
  "name":"(F31) foobar"
}
GET test/test/_search
{
  "fielddata_fields": ["name"], 
  "query": {
    "match": {
      "name": "F31 foobar"
    }
  },"highlight": {
    "require_field_match" : false,
    "fields": {
      "name":{
        "type" : "postings"
      }
    }
  }
}
```

the highlights for the last query look like 

```
"highlight": {
  "name": [
    "(&lt;em&gt;F31)&lt;/em&gt; &lt;em&gt;foobar&lt;/em&gt;"
  ]
}
```

and

```
"highlight": {
  "name": [
    "(&lt;em&gt;F31)&lt;/em&gt;"
  ]
}

```

I would expect the closing paren not to be highlighted ...
</description><key id="89059570">11726</key><summary>Highlighters sometime highlight additional caracters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">jeantil</reporter><labels><label>:Analysis</label><label>bug</label></labels><created>2015-06-17T16:09:49Z</created><updated>2016-11-16T13:38:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T17:23:28Z" id="113226265">The Lucene docs for the PatternReplaceCharFilter (https://lucene.apache.org/core/5_2_0/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.html) say:

&gt; NOTE: If you produce a phrase that has different length to source string and the field is used for highlighting for a term of the phrase, you will face a trouble.

(I know this isn't the one you're using, but the same advice probably applies to the mapping char filter)

That said, the output from the `_analyze` API seems weird to me.  For instance:

```
 GET test/_analyze?field=name&amp;text=F31
```

returns:

```
{
   "tokens": [
      {
         "token": "f31",
         "start_offset": 0,
         "end_offset": 3,
         "type": "word",
         "position": 1
      }
   ]
}
```

while this:

```
GET test/_analyze?field=name&amp;text=(F31)
```

returns:

```
{
   "tokens": [
      {
         "token": "f31",
         "start_offset": 1,
         "end_offset": 5,
         "type": "word",
         "position": 1
      }
   ]
}
```

I would expect the `end_offset` in the second example to be 4, not 5....
</comment><comment author="mikemccand" created="2015-06-19T15:30:19Z" id="113548854">This definitely looks like a bug, and I've created a simple pure Lucene test case showing it, but I'm not yet sure how to fix it; it could be the API for correcting offsets from CharFilter is too simplistic ... I'll open a Lucene issue for discussion.
</comment><comment author="jeantil" created="2015-06-19T15:47:31Z" id="113553891">Thank you !

Le ven. 19 juin 2015 17:31, Michael McCandless notifications@github.com a
écrit :

&gt; This definitely looks like a bug, and I've created a simple pure Lucene
&gt; test case showing it, but I'm not yet sure how to fix it; it could be the
&gt; API for correcting offsets from CharFilter is too simplistic ... I'll open
&gt; a Lucene issue for discussion.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11726#issuecomment-113548854
&gt; .
</comment><comment author="mikemccand" created="2015-06-20T10:12:01Z" id="113740982">OK I opened https://issues.apache.org/jira/browse/LUCENE-6595 but I'm not sure how to fix it!
</comment><comment author="svola" created="2015-06-25T13:20:40Z" id="115255516">I'm encountering a similar issue:

I'm using a prefix-query on the field I'm highlighting.
I'm searching for the term "uhr" and the highlighted result looks like this:

```
 "Schmuck &gt; Dame&lt;em&gt;nuh&lt;/em&gt;ren &gt; Modische &lt;em&gt;Uhren&lt;/em&gt;" 
```

So the highlight is shifted to the left. 
I'm using a special analyzer for german word decomposition. 

There seems to be some problem with finding the correct starting position. 

```
{
     "token": "damenuhr",
     "start_offset": 13,
     "end_offset": 23,
     "type": "&lt;ALPHANUM&gt;",
     "position": 2
  },
  {
     "token": "dam",
     "start_offset": 13,
     "end_offset": 17,
     "type": "&lt;ALPHANUM&gt;",
     "position": 2
  },
  {
     "token": "uhr",
     "start_offset": 17,
     "end_offset": 20,
     "type": "&lt;ALPHANUM&gt;",
     "position": 2
  },
```
</comment><comment author="mikemccand" created="2015-06-26T12:11:39Z" id="115657246">@svola are you also using a char_filter?  If not, then yours is likely a different issue with maybe the german decomposition not setting the right offsets for the tokens it creates?
</comment><comment author="cristiana93" created="2016-11-16T12:55:23Z" id="260939428">Is there any progress on the problem?
I am also having the same issues. I must remove some chars with char_filter using pattern_replace in order for my tokenizer to work, but my highlight is broken because of the start and end offsets.
</comment><comment author="svola" created="2016-11-16T13:38:34Z" id="260948390">It didn't change in ES 5. Same problem for me. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11725</link><project id="" key="" /><description>I'm getting the following error when I'm trying to do a scan request.

This is the steps I have taken and the response I got:

```
$ curl -XGET 'es1.di.no:9200/customer/_search?search_type=scan&amp;scroll=2m&amp;size=20' -d '{"query" : {"match_all" : {}}}'
{"_scroll_id":"c2Nhbjs1OzEyNzkzODozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzkzOTozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzk0MjozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzk0MTozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzk0MDozSGdydm9mQlREdVYtNlFrS1FDSlBROzE7dG90YWxfaGl0czoxMTMyMzk2ODs=","took":85,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":11323968,"max_score":0.0,"hits":[]}}

$ curl -XGET 'es1.di.no:9200/customer/_search?scroll=2m&amp;size=20' -d 'c2Nhbjs1OzEyNzkzODozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzkzOTozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzk0MjozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzk0MTozSGdydm9mQlREdVYtNlFrS1FDSlBROzEyNzk0MDozSGdydm9mQlREdVYtNlFrS1FDSlBROzE7dG90YWxfaGl0czoxMTMyMzk2ODs='
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[HN7qGhz6QmWNEvjSCMStJg][customer][0]: SearchParseException[[customer][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@47359ae3]; }{[HN7qGhz6QmWNEvjSCMStJg][customer][1]: SearchParseException[[customer][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@47359ae3]; }{[HN7qGhz6QmWNEvjSCMStJg][customer][2]: SearchParseException[[customer][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@47359ae3]; }{[HN7qGhz6QmWNEvjSCMStJg][customer][3]: SearchParseException[[customer][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@47359ae3]; }{[HN7qGhz6QmWNEvjSCMStJg][customer][4]: SearchParseException[[customer][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@47359ae3]; }]","status":400}
```

This the stacktrace i see on the server side (one per shard, I'll just paste one):

```
[2015-06-17 16:15:11,922][DEBUG][action.search.type       ] [Prod-node 1] All shards failed for phase: [query]
org.elasticsearch.search.SearchParseException: [customer][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:721)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:557)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:291)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchParseException: Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@47359ae3
        at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:259)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:691)
        ... 9 more
```

I use elasticsearch version 1.5.2.
</description><key id="89032210">11725</key><summary>Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexanderkjall</reporter><labels /><created>2015-06-17T14:23:29Z</created><updated>2015-06-18T07:24:02Z</updated><resolved>2015-06-17T18:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-17T18:21:12Z" id="112902855">The second command should use special `/_search/scroll` endpoint and not the same search endpoint. Please see [Elasticsearch Reference](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html#search-request-scroll) for more information. By the way, https://discuss.elastic.co/ is much better place to ask questions like this - we are using github to keep track of bug reports and enhancement requests only. Since this issue doesn't look like a bug, I am going to close it.
</comment><comment author="alexanderkjall" created="2015-06-18T07:24:02Z" id="113061217">Oh, I assumed that since it printed a stacktrace it was a bug, I'll make sure to read up more carefully next time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[doc] Move cloud documentation to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11724</link><project id="" key="" /><description>This commit moves `plugins/cloud-gce/README.md` file to `docs/plugins/cloud/gce/index.asciidoc`.

It will be easier to integrate plugin documentation in our reference guide.
</description><key id="89026706">11724</key><summary>[doc] Move cloud documentation to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Cloud Azure</label><label>:Plugin Cloud GCE</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T14:05:27Z</created><updated>2015-08-18T11:31:34Z</updated><resolved>2015-07-10T14:31:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-17T17:44:12Z" id="112888925">@clintongormley I put in this PR doc updates for all cloud plugins.
Let me know if it looks good to you. Thanks!
</comment><comment author="dadoonet" created="2015-07-10T14:31:43Z" id="120423263">Closing in favor of #12040 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>Document TransportDeleteByQueryAction class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11723</link><project id="" key="" /><description>Like Simon said in https://github.com/elastic/elasticsearch/pull/11516#issuecomment-112523037 we should javadocument the DeleteByQuery classes.
</description><key id="89023032">11723</key><summary>Document TransportDeleteByQueryAction class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>docs</label></labels><created>2015-06-17T13:51:29Z</created><updated>2015-06-22T12:35:31Z</updated><resolved>2015-06-22T12:35:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java</file><file>plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java</file><file>plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryResponse.java</file><file>plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file></files><comments><comment>Add high-level javadocs to delete-by-query</comment></comments></commit></commits></item><item><title>ClusterStateObserver should log on trace on timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11722</link><project id="" key="" /><description>This more consistent with the other logging it makes and since it can be used in many operations the output can be more verbose (without adding too much info as to who timed out exactly - which we can fix separately). If need be the caller of the observer can log a higher level message.
</description><key id="89009680">11722</key><summary>ClusterStateObserver should log on trace on timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T12:57:50Z</created><updated>2015-06-29T12:33:48Z</updated><resolved>2015-06-29T09:53:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-06-29T09:37:42Z" id="116574191">there is one other logline in `debug` level in the `ClusterStateObserver`, that one is ok?

LGTM apart from that
</comment><comment author="bleskes" created="2015-06-29T09:49:45Z" id="116585825">@spinscale missed that one. Thx. Pushed another commit
</comment><comment author="spinscale" created="2015-06-29T09:50:06Z" id="116586081">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file></files><comments><comment>Internal: ClusterStateObserver should log on trace on timeout</comment></comments></commit></commits></item><item><title>Allow to execute REST tests in plugins again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11721</link><project id="" key="" /><description>The `ElasticsearchRestTestCase` has changed in 2.0 and now throws an error when executing REST tests in a maven plugin project which depends on `elasticsearch-2.0.0-SNAPSHOT-tests.jar`:

``` java
$ cd plugins/delete-by-query
$ mvn clean test -Dtests.filter="@rest"

Suite: org.elasticsearch.plugin.deletebyquery.test.rest.DeleteByQueryRestTests
ERROR   0.03s J3 | DeleteByQueryRestTests.initializationError &lt;&lt;&lt;
   &gt; Throwable #1: java.nio.file.NoSuchFileException: delete_by_query
   &gt;    at org.elasticsearch.test.rest.support.FileUtils.resolveFile(FileUtils.java:99)
   &gt;    at org.elasticsearch.test.rest.support.FileUtils.findYamlSuites(FileUtils.java:86)
   &gt;    at org.elasticsearch.test.rest.ElasticsearchRestTestCase.collectTestCandidates(ElasticsearchRestTestCase.java:189)
   &gt;    at org.elasticsearch.test.rest.ElasticsearchRestTestCase.createParameters(ElasticsearchRestTestCase.java:174)
   &gt;    at org.elasticsearch.plugin.deletebyquery.test.rest.DeleteByQueryRestTests.parameters(DeleteByQueryRestTests.java:45)
   &gt;    at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
Completed [4/4] on J3 in 0.04s, 1 test, 1 error &lt;&lt;&lt; FAILURES!
```

When executed with `mvn clean test` command, it seems that plugin specific REST tests (declared in POM file using `&lt;tests.rest.suite/&gt;` tag) are resolved using a `ZipFileSystem` instance corresponding to the `elasticsearch-2.0.0-SNAPSHOT-tests.jar`.

When executed in an IDE, tests are resolved using the classpath and it works.

File resolution are a bit cryptic in `ElasticsearchRestTestCase` and `FileUtils` so I'll be happy if someone can point me in the right direction to make this works.
</description><key id="89008464">11721</key><summary>Allow to execute REST tests in plugins again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>bug</label></labels><created>2015-06-17T12:51:21Z</created><updated>2015-06-17T18:38:48Z</updated><resolved>2015-06-17T18:38:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-17T13:04:53Z" id="112792591">I don't think its a bug, i think these tests are overengineered?

The zipfilesystem stuff is the best way I could make it work given what some of the other plugins wanted to do here.

In order to simplify this test the parameterization really has to go. that is what adds a significant majority of complexity to this stuff because things must happen in clinit. 

Please, no more parameterized tests, ever!
</comment><comment author="javanna" created="2015-06-17T13:46:06Z" id="112805956">I'd leave the parameterized discussion out of this issue, we can certainly discuss that but I'd do it separately. 

The problem here is that plugins need to be able to run their own REST tests, pointing to them via -Dtests.rest.suite and -Dtests.rest.spec. That is not possible anymore, given that if `ElasticsearchRestTestCase` comes from within a jar, REST tests and spec can only be taken from that specific jar. Only the usecase where a plugin needs to run REST tests coming from elasticsearch core is handled right now; a plugin cannot run its own tests, unless I am missing a way to do it. We need to add back this feature to the REST test infra, it doesn't need to be able to run tests from any file system location as it used to be though (way too much!).
</comment><comment author="rmuir" created="2015-06-17T13:59:04Z" id="112811162">The parameterized discussion is definitely involved here. Besides being completely useless, parameterized tests just make things complicated, because nothing can happen except in clinit (so no subclassing). Just have a look around and look at what is subclassing this rest test stuff and why.

There is too much going on. Down with parameterized tests! I need to create some kind of github plugin like the CLA-checker that makes the whole PR go completely red if it contains any.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTestCase.java</file><file>plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/test/rest/DeleteByQueryRestTests.java</file></files><comments><comment>Allow to opt-out of loading packaged REST tests</comment></comments></commit></commits></item><item><title>Possible typo in the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11720</link><project id="" key="" /><description>If I look at the docs for [templates](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#pre-registered-templates), there's a mention that the templates are stored in .scripts.  The API listing for them seems a bit off.  Is the URI really /_search/templates?  Is it supposed to be /.scripts/_search/templates or something like that?  If I want to update a template do I use /.scripts/_search/templates/{templateId} ?
</description><key id="88988731">11720</key><summary>Possible typo in the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels /><created>2015-06-17T11:20:54Z</created><updated>2015-06-18T16:46:40Z</updated><resolved>2015-06-18T16:46:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T16:46:39Z" id="113214427">HI @johnament 

The docs are correct.  Use `GET|POST /_search/template/some_id` to index or retrieve a template.  And use `GET /_search/template` to perform a search with the template specified in the body of the search request (either inline, as a file name, or with `some_id` to retrieve an indexed template)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unable to store parsed mustache templates via Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11719</link><project id="" key="" /><description>See this question for reference.

https://discuss.elastic.co/t/java-api-and-search-template-with-params/23769/2

One of the issues seems to be that while an entry like this is valid for storing a template:

```
{
    "terms": {
        "assignedToOrg": {{orgIds}}
    }
}
```

The equivalent Java version

```
                        "              \"terms\": {\n" +
                        "                \"organizationId\": {{orgIds}}" +
                        "              }\n" +
```

Is not valid.  This is because the body of the template is being serialized to JSON, and the use of {{ here is an invalid entry.  I'm using ES 1.5.
</description><key id="88982380">11719</key><summary>Unable to store parsed mustache templates via Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels /><created>2015-06-17T10:45:29Z</created><updated>2015-06-17T14:31:41Z</updated><resolved>2015-06-17T14:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johnament" created="2015-06-17T14:31:41Z" id="112825126">Closing out - wasn't clear to me from reading the docs the amount of escaping required.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename `series_arithmetic` agg to `bucket_script`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11718</link><project id="" key="" /><description /><key id="88971876">11718</key><summary>Rename `series_arithmetic` agg to `bucket_script`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T09:49:39Z</created><updated>2015-06-23T13:01:12Z</updated><resolved>2015-06-23T13:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Query refactoring: FieldMaskingSpanQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11717</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217
</description><key id="88964184">11717</key><summary>Query refactoring: FieldMaskingSpanQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-17T09:14:52Z</created><updated>2015-06-22T18:07:32Z</updated><resolved>2015-06-22T18:07:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-18T08:40:59Z" id="113078021">left a few comments
</comment><comment author="cbuescher" created="2015-06-19T14:20:03Z" id="113529269">@javanna Changed back to use QueryBuilder for inner queries, looked at your generics solution for the constructor but somehow didn't get that to work.
</comment><comment author="cbuescher" created="2015-06-19T19:17:29Z" id="113611591">What if we add `SpanQuery toQuery()` to the SpanQueryBuilder interface? That way we actually reflect the only important distinction of span queries and can drop most of the unsafe casting. I added a commit that show what I mean. Notice that the extra `toQuery()` methods I had to introduce to the remaining span queries will go away once we refactor them.
</comment><comment author="cbuescher" created="2015-06-22T15:02:34Z" id="114142321">Rebased and updated PR to reflect latest changes in QueryBuilder inheritance hierarchy.
</comment><comment author="javanna" created="2015-06-22T15:07:08Z" id="114144491">LGTM besides the small comment I left
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11717 from cbuescher/feature/query-refactoring-fieldmaskingspan</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java</file></files><comments><comment>Query refactoring: FieldMaskingSpanQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Using an es query to find value from a previous day...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11716</link><project id="" key="" /><description> Hi,

Hoping that someone could help....

This is my query:

```
body: {
                "query"=&gt; {
                   "filtered"=&gt; {
                    "filter"=&gt; {
                        "bool"=&gt; {
                         "must"=&gt; [
                           {"term"=&gt; {status=&gt; 500}}, 
                             {"term"=&gt; {"type"=&gt; "iis6"}}, 
                              "range"=&gt; {
                               "@timestamp"=&gt; {
                                 "gt"=&gt; "now-2h" 
                              }
                           }
                        ]
                     }
                    }
                  }
                 },
                "aggs"=&gt; {
                  "0"=&gt; {
                    "date_histogram"=&gt; {
                      "field"=&gt; "@timestamp",
                      "interval"=&gt; "2h"
                     }
                  }
                 },
                 "size"=&gt; 0
                }
```

This runs against the current index(logstash-2015.06.17) and produces the anticipated value which is fine.

I'm constructing a dashboard of metrics comparing values against values from 24 hours ago but haven't been able to find these previous values. This is the query that I have used (I am running this query against the previous day's index which is logstash-2015.06.16...)

```
body: {
                "query"=&gt; {
                   "filtered"=&gt; {
                    "filter"=&gt; {
                        "bool"=&gt; {
                         "must"=&gt; [
                           {"term"=&gt; {status=&gt; 500}}, 
                             {"term"=&gt; {"type"=&gt; "iis6"}}, 
                              "range"=&gt; {
                               "timestamp"=&gt; {
                                 "gt"=&gt; "2015-06-16 10:07:00",
                                 "lt"=&gt; "2015-06-16 08:07:00"
                              }
                           }
                        ]
                     }
                    }
                  }
                 },
                "aggs"=&gt; {
                  "0"=&gt; {
                    "date_histogram"=&gt; {
                      "field"=&gt; timestamp",
                      "interval"=&gt; "2h"
                     }
                  }
                 },
                 "size"=&gt; 0
                }
```

and have received the value of 0 hits when I know that data exists in this time period.

Can you please show me the correct way to construct this query? I'm new to Elasticsearch...
</description><key id="88959322">11716</key><summary>Using an es query to find value from a previous day...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trekr5</reporter><labels /><created>2015-06-17T08:53:15Z</created><updated>2015-06-18T16:37:13Z</updated><resolved>2015-06-18T16:37:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T16:37:12Z" id="113212570">Hi @trekr5 

The right place to ask questions about how to use Elasticsearch is the forum: https://discuss.elastic.co/

this issues list is for bug reports and feature requests
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>copy_to and geo_point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11715</link><project id="" key="" /><description>I can't seem to get it to work with 1.6.0:

``` bash
curl -XPUT localhost:9200/geo
curl -XPUT localhost:9200/geo/geo/_mapping -d '
{
  "geo" : {
    "properties" : {
      "location" : {
        "type" : "geo_point",
        "copy_to" : "other_location"
      },
      "other_location" : {
        "type" : "geo_point"
      }
    }
  }
}'
curl -XPOST localhost:9200/geo/geo -d '{ "location" : [-71, 41] }'
curl -XGET localhost:9200/geo/geo/_search?type=count -d '
{
  "size" : 0,
  "query" : { "match_all" : {} },
  "aggs": {
    "geo": {
      "geohash_grid" : { "field" : "other_location" , "precision" : 2 }
    }
  }
}'
```

Get no results when agg on other_location field, but works with location field.
</description><key id="88958668">11715</key><summary>copy_to and geo_point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Mapping</label></labels><created>2015-06-17T08:50:27Z</created><updated>2016-06-21T06:53:24Z</updated><resolved>2016-06-17T15:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T16:36:15Z" id="113212259">@nknize do geo-points support the `copy_to` parameter? If the answer is no, then would it be feasible (and useful) to add support for them? If not, we should probably throw an exception if `copy_to` is specified.

@jimmyjones2 what was your reason for wanting to use `copy_to` with a geo-point?
</comment><comment author="jimmyjones2" created="2015-06-18T17:00:20Z" id="113220976">I've got some records with multiple geo-point fields and would like to be able to search them all at once and plot them all on a map in Kibana, but retain the ability to do that for the separate fields too.
</comment><comment author="nknize" created="2015-06-18T22:38:24Z" id="113306767">@clintongormley @jimmyjones2  copyTo is not supported with `geo_point` field. It's feasible but I don't know that its needed (especially in 2.0 with a change to GeoPointField default indexing). It should make this use-case feasible/simple with named queries. I think it might be doable with named filters (i'm helping give training atm so I'll revisit when I have some time to have a closer look).
</comment><comment author="clintongormley" created="2015-06-19T07:10:08Z" id="113403581">thanks @nknize - i'll assign to you then
</comment><comment author="FindKim" created="2015-07-11T00:21:17Z" id="120555283">I am also running into this issue. Would it be possible to `copy_to` separate `lat` and `lon` values into `other_location.lat` and `other_location.lon`, respectively?
</comment><comment author="michielhn" created="2015-08-19T09:14:25Z" id="132504340">@nknize can you elaborate how this would look like in the future? I am practically looking for the same solution w/o having to redefine the log output from our applications.
</comment><comment author="blabno" created="2016-06-17T11:20:47Z" id="226744837">I use `position` field in many types, but I need `geohash_prefix:true` only in one type, so I wanted to use `copy_to` to `userPosition` field that has `geohash_prefix` enabled , but it does not work. Right now I have to add `userPosition` to the indexed document itself, which is not too nice.

In general when I get a new requirement and have to change mapping for `position` field (which I can't because other types use it differently) then I have to add new field to the document.
</comment><comment author="clintongormley" created="2016-06-17T15:22:01Z" id="226799015">@blabno the indexing for geo points has changed completely in 2.2/2.3, you can't enable or disable geohashes any more. With the new index format, I can't see the benefit of copying geo-point fields to another field any more, so I'm going to close this
</comment><comment author="blabno" created="2016-06-20T08:57:48Z" id="227086314">@clintongormley is `geohash_prefix` always set to `true` then? I assume that if we index parent geohashes then the index grows. Now what if in 10 documents we have `position` field of `geo_point` type, but we want to have `geohash_prefix:true` only for one document?
Just do not tell me that I have to give the field different name, because that would be really stupid requirement to change data model, because of technology limitation.
</comment><comment author="clintongormley" created="2016-06-20T14:41:00Z" id="227162156">@blabno apparently i'm mistaken, the geohash prefix parameter hasn't been removed: https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html#geo-point-params

That said, I don't understand why you would index the same geopoint in multiple ways.  That is really wasteful.  Choose the highest precision that you need and apply that to a single field.
</comment><comment author="blabno" created="2016-06-21T06:53:24Z" id="227355499">@clintongormley I would love to index `position` property in `user` entity in one way only, unfortunately ES forces me to use the same mapping for the same field across all document types and I use `position` property across a lot of other document types. So now I would have to change mapping for all other document types, just to get support for `geohash_prefix` in just one document.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception on MapperParsingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11714</link><project id="" key="" /><description>I've got following error in elasticsearch.log

```
[2015-06-17 07:13:18,146][DEBUG][action.bulk              ] [Claudette St. Croix] [idx_xxx-2015.06.16][3] failed to execute bulk item (index) index {[idx_xxx-2015.06.16][xxx][AU3_74pBI_BlciXyfFn7], source[{"message":"2015 06 16 17:09:30 ipF=172.16.94.11 country=XX srvname=XX method=GET reqF=\"GET /xx/account/self HTTP/1.1\" Referer=\"-\" sCF=403 sCB=- bsF=- dTF=- dTB=- dTcB=- dTsB=- dTr1B=- dTr2B=- browser=\"xx\" SSLC=- SSLP=- invS=- AddrB=\"-\" trID=- Event=- ","@version":"1","@timestamp":"2015-06-16T15:09:33.904Z","type":"xx","file":"/var/opt/xx/customaccess.log","host":"xx.test","offset":"10940297","ipF":"172.16.94.11","country":"XX","srvname":"xx.test","method":"GET","reqF":"GET /xx/account/self HTTP/1.1","Referer":"-","sCF":403,"sCB":0,"bsF":"-","dTF":0.0,"dTB":0.0,"dTcB":0.0,"dTsB":0.0,"dTr1B":0.0,"dTr2B":0.0,"browser":"xx","SSLC":"-","SSLP":"-","invS":"-","AddrB":"-","trID":"-","Event":"-","received_at":"2015-06-16T15:09:33.904Z","received_from":["xx","test"],"hostname":"xx"}]}
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [bsF]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:411)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
        at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:466)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:418)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:148)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:574)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:440)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "-"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:581)
        at java.lang.Long.parseLong(Long.java:631)
        at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:145)
        at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:288)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:239)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:401)
        ... 13 more
```

Filter is based on kv. Number Detection in Elasticsearch is activated.
</description><key id="88915064">11714</key><summary>Exception on MapperParsingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sybnex</reporter><labels /><created>2015-06-17T05:24:11Z</created><updated>2015-06-17T05:44:17Z</updated><resolved>2015-06-17T05:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-17T05:37:00Z" id="112657560">`-` is not an integer so you cannot map it to one.

Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="sybnex" created="2015-06-17T05:43:00Z" id="112660767">I was never trying to ... here the filter:

```
    kv {
      add_field =&gt; [ "received_at", "%{@timestamp}" ]
      add_field =&gt; [ "received_from", "%{host}" ]
    }
    mutate {
      convert =&gt; [ "sCB", "integer" ]
      convert =&gt; [ "sCF", "integer" ]
      convert =&gt; [ "dTB",   "float" ]
      convert =&gt; [ "dTF",   "float" ]
      convert =&gt; [ "dTcB",  "float" ]
      convert =&gt; [ "dTsB",  "float" ]
      convert =&gt; [ "dTr1B", "float" ]
      convert =&gt; [ "dTr2B", "float" ]
      convert =&gt; [ "bsF"  , "string"]
    }
    if [bsF] == "-" {
      drop { }
    }
```
</comment><comment author="sybnex" created="2015-06-17T05:44:15Z" id="112661462">OK, you right. Just saw it right now on formated screen. The other fields are also crappy.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>highlight force_source have no effect on _source content field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11713</link><project id="" key="" /><description>When i set the highlight fields such likes

```
{
   "query": {},
   "highlight": {
   "fields": {"content": {"force_source": true} }
    }
}
```

There was no tags in the _source content field , and the text with highlighted tags still in highlight field? 
so did I use it in a wrong way? or the highlight force_source field option have no effect ? 
</description><key id="88904173">11713</key><summary>highlight force_source have no effect on _source content field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jkryanchou</reporter><labels><label>feedback_needed</label></labels><created>2015-06-17T03:58:02Z</created><updated>2015-06-24T01:29:33Z</updated><resolved>2015-06-23T12:34:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T13:00:00Z" id="113148076">I don't understand what your question is.  Could you provide more info please?
</comment><comment author="clintongormley" created="2015-06-18T13:00:14Z" id="113148116">... with a complete (but simple) recreation
</comment><comment author="jkryanchou" created="2015-06-19T03:40:03Z" id="113357989">OK. Sorry for incomplete description. I re-describe the question again.
I have a amount of docs in indices in ES. e.g 

```
{
    '_source': {
    'title': 'Foo bar foobar foo barbar',
    'headers': 'HTTP/1.0 200 OK
Server: GeoHttpServer
Date: Thu, 18 Jun 2015 19:02:31 GMT
Content-type: text/html',
    'city': 'US'
    }
}
```

And the query with highlight option e.g.

```
{
   "query": {"match": {"title": "foo"}},
   "highlight": {
   "fields": {"title": {"force_source": true} }
    }
}
```

While the response result was 

```
{
    '_source': {
    'title': 'Foo bar foobar foo barbar',
    'headers': 'HTTP/1.0 200 OK
Server: GeoHttpServer
Date: Thu, 18 Jun 2015 19:02:31 GMT
Content-type: text/html',
    'city': 'US'
    }
    'highlight': [{'title': "Foo bar foobar &lt;em&gt;foo&lt;/em&gt; barbar"}]
}
```

I set `force_source: true` in field highlight-option while the _source.title without the highlight tags `&lt;em&gt;&lt;/em&gt;`.  Did I misunderstand the `force_source` option? or the `force_source` have no effect?
The result i expected likes

```
{
    '_source': {
    'title': 'Foo bar foobar &lt;em&gt;foo&lt;/em&gt; barbar',
    'headers': 'HTTP/1.0 200 OK
Server: GeoHttpServer
Date: Thu, 18 Jun 2015 19:02:31 GMT
Content-type: text/html',
    'city': 'US'
    }
    'highlight': [{'title': "Foo bar foobar &lt;em&gt;foo&lt;/em&gt; barbar"}]
}
```
</comment><comment author="clintongormley" created="2015-06-23T12:34:48Z" id="114478410">OK - you've misunderstood the `force_source` option.  All it does is say "Use the `_source` field to get the field values to use for highlighting, instead of eg retrieving stored field values"
</comment><comment author="jkryanchou" created="2015-06-24T01:29:32Z" id="114692771">OK, I got it . Thanks a lot,  Clintongormley : )
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Optional Delayed Allocation on Node leave</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11712</link><project id="" key="" /><description>Allow to set delayed allocation timeout on unassigned shards when a node leaves the cluster. This allows to wait for the node to come back for a specific period in order to try and assign the shards back to it to reduce shards movements and unnecessary relocations.

The setting is an index level setting under `index.unassigned.node_left.delayed_timeout` and defaults to 0 (== no delayed allocation). We might want to change the default, but lets do it in a different change to come up with the best value for it. The setting can be updated dynamically.

When shards are delayed, a log message with "info" level will notify which shards are being delayed and for how long.

An implementation note, we really only need to care about delaying allocation on unassigned replica shards. If the primary shard is uniassigned, anyhow we are going to wait for a copy of it, so really the only case to delay allocation is for replicas.
</description><key id="88890890">11712</key><summary>Optional Delayed Allocation on Node leave</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>feature</label><label>release highlight</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-17T02:22:43Z</created><updated>2015-06-18T19:57:13Z</updated><resolved>2015-06-18T14:07:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-17T07:33:41Z" id="112693581">Did a review cycle. I like how things come together. One concern I had was the implementation in RoutingService where we maintain a queue of pending reroutes per unassigned (delayed) shard. I think it  will be simpler to just use the single future we already have and set it every time to the next expected change moment (i.e., the minimum delay of all unassigned shards). On a setting change we can do a reroute all the time (which we might do already). Am I missing something?
</comment><comment author="bleskes" created="2015-06-17T10:55:32Z" id="112753098">one more little thing - we need do some docs work as well, it's an important change. I can help if need be.
</comment><comment author="kimchy" created="2015-06-17T19:13:44Z" id="112917366">I pushed another round, mainly simplifying the code, adding more unit tests, and addressing comments. @bleskes once we agree on this as the way forward, I will add docs
</comment><comment author="kimchy" created="2015-06-18T08:13:00Z" id="113069775">@s1monw applied another round of changes
</comment><comment author="kimchy" created="2015-06-18T11:56:44Z" id="113129496">@s1monw @bleskes pushed another set of changes
</comment><comment author="bleskes" created="2015-06-18T13:07:36Z" id="113149979">LGTM +1
</comment><comment author="s1monw" created="2015-06-18T14:01:26Z" id="113165878">LGTM makes sense @kimchy 
</comment><comment author="kimchy" created="2015-06-18T15:33:13Z" id="113193772">pushed to master and 1.x, @clintongormley I forgot to add the docs, where do you think it makes sense to document this?
</comment><comment author="clintongormley" created="2015-06-18T19:47:44Z" id="113269951">@kimchy i'd say in the Index Shard Allocation page: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-allocation.html

plus a note on the cluster health page
</comment><comment author="bleskes" created="2015-06-18T19:57:13Z" id="113273055">I think it’s also good to mention on the (rolling) upgrade docs: docs/reference/setup/upgrade.asciidoc

&gt; On 18 Jun 2015, at 21:47, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; @kimchy i'd say in the Index Shard Allocation page: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-allocation.html
&gt; 
&gt; plus a note on the cluster health page
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Documented delayed allocation settings</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file></files><comments><comment>Optional Delayed Allocation on Node leave</comment><comment>Allow to set delayed allocation timeout on unassigned shards when a node leaves the cluster. This allows to wait for the node to come back for a specific period in order to try and assign the shards back to it to reduce shards movements and unnecessary relocations.</comment></comments></commit></commits></item><item><title>Refactor SnapshotService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11711</link><project id="" key="" /><description>This commit splits the snapshot service into several more granular components:
- SnapshotsService remains the faced class for all snapshot-related operations,
- SnapshotManager is now responsible for starting, deleting and aborting individual snapshots,
- SnapshotsShardService handles shard-level snapshot operations and is responsible for start and stopping snapshots of individual shards,
- SnapshotsShardWatcherService is used to monitor cluster state for the changes that affect the running snapshots
</description><key id="88868429">11711</key><summary>Refactor SnapshotService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label></labels><created>2015-06-16T23:46:03Z</created><updated>2015-06-18T17:03:50Z</updated><resolved>2015-06-17T15:49:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-17T15:15:59Z" id="112839978">I just commented on the color of the bikeshed here really. this PR is too big and the classes are too messy for real reviews. not necessarily your fault but just a fact. This is true for most of the classes that have these huge methods acting on the clusterstate. I can't review this really in detail, just too big
</comment><comment author="imotov" created="2015-06-17T15:49:54Z" id="112857498">@s1monw Thanks! I will split it in smaller PRs. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shield duplicates _search api audits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11710</link><project id="" key="" /><description>A standard GET by ID results in a single log entry in the audit log. Search (_search) results in two identical log entries. Every time. 
- Default ES 1.6.0 installation. 
- Added latest Shield. 
- Added "search_admin" user. 

Everything works great out of the box except I get _search audits duplicated in the log. ONLY "SearchRequest" audits. I haven't found any other api yet which results in this strange behavior.

```
[2015-06-16 18:37:24,705] [esdev-shieldpoc01] [transport] [access_granted]      origin_type=[rest], origin_address=[/10.30.24.36:55308], principal=[search_admin], action=[indices:data/read/search], indices=[test], request=[SearchRequest]
[2015-06-16 18:37:24,705] [esdev-shieldpoc01] [transport] [access_granted]      origin_type=[rest], origin_address=[/10.30.24.36:55308], principal=[search_admin], action=[indices:data/read/search], indices=[test], request=[SearchRequest]
```
</description><key id="88868214">11710</key><summary>Shield duplicates _search api audits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bobbyhubbard</reporter><labels><label>:Search</label><label>bug</label></labels><created>2015-06-16T23:44:26Z</created><updated>2016-02-29T15:10:16Z</updated><resolved>2016-02-29T13:52:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:58:41Z" id="113147861">@jaymode please could you take a look?
</comment><comment author="jaymode" created="2015-06-19T12:53:30Z" id="113506025">@bobbyhubbard thanks for reporting this. This behavior isn't ideal and we'll look at if/how we can improve this. What you're seeing is a side effect of how auditing is implemented and how search requests are executed. Shield audits the individual actions that are executed by elasticsearch. 

The `indices:data/read/search` action name corresponds to multiple actions for search. The reason that you are seeing the message twice, is the API executes a `TransportSearchAction`, which in turn executes a action corresponding to the search type; the default type `query_then_fetch` corresponds to the `TransportSearchQueryThenFetchAction` and the execution of this action is what causes the second log message.
</comment><comment author="javanna" created="2015-06-19T14:26:12Z" id="113530590">This seems to have to do with the fact that the main `TransportSearchAction` uses an inner `TransportSearchTypeAction` (there is a different impl for each search type). Last time I checked I noticed some code that gets executed twice while it shouldn't (e.g. request validation), and a side effect is also the double audit log line. Maybe this inner action shouldn't be a transport action after all? The two (outer and inner) execute calls happen on the same node all the time, seems like also the transport handler registration that happens in `TransportSearchTypeAction` has no actual effect as it's already registered by `TransportSearchAction` with same name, so only used for audit logging.
</comment><comment author="bobbyhubbard" created="2015-11-18T23:20:30Z" id="157896389">This bit us again today when someone else in our org setup a new log drain from shield. It reported nearly double the number of rest requests as expected. Then I remembered this issue... The workaround is simple enough...to hash each message (fingerprint in logstash) and use the hash as the message id. But this WILL bite every single Shield customer who is measuring and auditing rest calls. (How many are reporting invalid results now because they dont even know about this bug like one team here almost did?)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/ParsedScrollId.java</file><file>core/src/main/java/org/elasticsearch/action/search/ScrollIdForNode.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchScrollQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchHelper.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryAndFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryAndFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryThenFetchAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file></files><comments><comment>Cleanup search sub transport actions and collapse o.e.action.search.type package into o.e.action.search</comment></comments></commit></commits></item><item><title>Try another node if retryable ClusterBlockException received for TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11709</link><project id="" key="" /><description>Currently if we hit a ClusterBlockException on a node we retry to the same node and then give up. Only on connection exceptions do we [retry a different node](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L231)

This sort of situation can happen if a users is doing a rolling restart of the nodes, due to an upgrade.

We could be smarter in the TransportClient to retry another node when we hit this ClusterBlockException.
</description><key id="88861885">11709</key><summary>Try another node if retryable ClusterBlockException received for TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-16T22:59:58Z</created><updated>2017-04-23T19:59:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-18T12:27:47Z" id="113137936">I have a simple pull request for this https://github.com/kimchy/elasticsearch/tree/retry_on_cluster_block_transport_client, but there is a problem that I am not sure how best to fix, see the commit note for it
</comment><comment author="Inkromind" created="2016-09-12T10:35:59Z" id="246309950">We ran into this issue when one of our nodes was being restarted. It was receiving requests before it was fully initialized and so it threw a ClusterBlockException. The request was not retried with another node.

Is anything going to happen with @kimchy s changes (for which I couldn't find a pull request) or should we catch this exception ourselves and retry (with the possibility that our request is sent to the same node and the same exception occurs)? Or is there another better way to handle this kind of scenario?
</comment><comment author="ScionOfBytes" created="2017-01-30T05:30:04Z" id="275983106">What's the status on this? I believe we've faced the same issue.</comment><comment author="tanero" created="2017-04-23T19:59:30Z" id="296484398">I receive the same error, when I have connected with TransportClient from java api,  If the node went down and start again, exception occurs.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can index but not search when cluster name changed from "elasticsearch"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11708</link><project id="" key="" /><description>Changing our cluster name from "elasticsearch" has resulted in being
- Able to index new documents
- Able to check to see if an index exists
- NOT able to do searches

We are using Elasticsearch 1.5.2 from the elatic.co debian repository on Ubuntu 14.04 using java version 1.8.0_45.

Here is the TransportClient setup code:

&lt;pre&gt;
final Settings settings = ImmutableSettings
        .settingsBuilder()
        .put("cluster.name", CLUSTER_NAME)
        // .put("client.transport.ignore_cluster_name", true)
        .build();
final TransportClient tcClient = new TransportClient(settings);

for (final String ip : IPS.get()) {
    client.addTransportAddress(new InetSocketTransportAddress(ip, 9300));
}
&lt;/pre&gt;


Here is the code to do a simple search

&lt;pre&gt;
final AndFilterBuilder afb = new AndFilterBuilder();
afb.add(FilterBuilders.termFilter("document.file-path", path));
response = transportClient.prepareSearch(index)
        .setTypes("document")
        .setPostFilter(afb)
        .execute()
        .get();
&lt;/pre&gt;


The cluster name was switched to "elasticsearch-staging". I also tested a name without hyphens. The staging cluster is actually just a single node. I did, however, get the same results on a system in testing having 3 no data master nodes and 2 data non-master nodes. 

Switching the cluster name back to "elasticsearch"
</description><key id="88824031">11708</key><summary>Can index but not search when cluster name changed from "elasticsearch"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imarsman</reporter><labels /><created>2015-06-16T19:50:18Z</created><updated>2015-06-16T21:20:49Z</updated><resolved>2015-06-16T21:20:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imarsman" created="2015-06-16T21:20:48Z" id="112572912">Solved it. It had nothing to do with the code or the general configuration of Elasticsearch. I had neglected to set up the document templates in the new cluster setup with proper type mapping. With this, indexing was allowed but searching had nothing to work with.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo Bounding Box Filter always returns 0 results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11707</link><project id="" key="" /><description>I'm using geo filtering with geo_point and distance. Works as expected!

Now, I want to filter a location in a geo bounding box.

MAPPING:

```
PUT /users
{
    "mappings": {
        "users": {
            "properties":
            {
                "location":
                {
                    "type": "geo_point"
                }
            }
        }
    }
}
```

QUERY:

```
GET /users/users/_search
{
     "query":
     {
         "filtered":
         {
             "filter":
             {
                "geo_bounding_box":
                {
                    "location":
                    {
                        "top_left":
                        {
                            "lat": 53.5551999,
                            "lon": 7.227510199999999
                        },
                        "bottom_right":
                        {
                            "lat": 50.75038379999999,
                            "lon": 3.357962
                        }
                    }
                }
            }
        }
    }
}
```

Geo Box above is 'Europe'.

An user index item has this location (place in province Groningen, The Netherlands):

```
"location": {
    "lat": 53.2408,
    "lon": 6.52455
}
```

All I try has no results. I can't see more what is wrong.
</description><key id="88815694">11707</key><summary>Geo Bounding Box Filter always returns 0 results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Schellingerht</reporter><labels /><created>2015-06-16T19:23:21Z</created><updated>2015-06-17T07:12:33Z</updated><resolved>2015-06-17T07:12:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Schellingerht" created="2015-06-17T07:12:32Z" id="112685054">This solution works for me:

"top_right" instead of "top_left"

and

"bottom_left" instead of "bottom_right"
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE with AbstractNioSelector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11706</link><project id="" key="" /><description>Hi all,
Got lots of NullPointerException with the following StackTrace:

```
[2015-06-16 19:01:45,017][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.NullPointerException
        at sun.nio.ch.EPollArrayWrapper.isEventsHighKilled(EPollArrayWrapper.java:174)
        at sun.nio.ch.EPollArrayWrapper.setUpdateEvents(EPollArrayWrapper.java:190)
        at sun.nio.ch.EPollArrayWrapper.add(EPollArrayWrapper.java:239)
        at sun.nio.ch.EPollSelectorImpl.implRegister(EPollSelectorImpl.java:164)
        at sun.nio.ch.SelectorImpl.register(SelectorImpl.java:133)
        at java.nio.channels.spi.AbstractSelectableChannel.register(AbstractSelectableChannel.java:209)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker$RegisterTask.run(NioWorker.java:151)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

The cluster consists of a data node (8 cors 126GB ram, 32GB heap) and SearchLoadBalancer(master/data = false).

What can cause this ?
</description><key id="88812726">11706</key><summary>NPE with AbstractNioSelector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ayashjorden</reporter><labels><label>:Network</label><label>feedback_needed</label></labels><created>2015-06-16T19:09:15Z</created><updated>2015-07-24T08:34:05Z</updated><resolved>2015-07-24T08:34:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:49:55Z" id="113145815">@spinscale any ideas?
</comment><comment author="ayashjorden" created="2015-06-22T08:52:25Z" id="114043840">@clintongormley @spinscale any update on this ticket?
What additional information can I provide to start investigation?

These exceptions happen to us almost every day.

Thanks,
Yarden
</comment><comment author="spinscale" created="2015-06-22T09:00:16Z" id="114045054">I need to take a closer look at this. Can you provide some more generic information:
- elasticsearch version
- java version
- Any special usage pattern? Do you open a lot of new connections? What load is happening on that system?
- Can you run netstat to check the number of open tcp connections?
- Do you know if this is happening with HTTP or the transport protocol? (If you dont use HTTP, we can rule it out)
- What is the impact of that exception in your system/cluster? Are connections broken after that? Is elasticsearch unresponsive?
</comment><comment author="ayashjorden" created="2015-06-23T06:05:21Z" id="114372161">Please see the requested data below:
1. ES cluster consists of two nodes, data/master node (listed below) and a Search-Load-Balancer (data/master=false) which a collocated Kibana 4(nodeJS) is connected to.

``` json
{
  "status" : 200,
  "name" : "es01",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.5.2",
    "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    "build_timestamp" : "2015-04-27T09:21:06Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
1. java version "1.7.0_79"
   OpenJDK Runtime Environment (IcedTea 2.5.5) (7u79-2.5.5-1~deb8u1)
   OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)
2. logstash 1.5.0 with updated elasticsearch output plugin 0.2.7 . I know there's a bottle neck due to ES machine CPU (2x4cors with hyper-threading)
3. `sudo netstat -tupn|grep -F c.d|wc -l` results in 168, outside high traffic hours.
   5.Logstash 1.5.0 is using the transport protocol, not HTTP.
4. I don't know the impact on my cluster as the only thing I see are these errors in the logs.

Hope this information will be helpful,
Yarden
</comment><comment author="szroland" created="2015-06-23T20:09:36Z" id="114626166">Interesting that the `EPollArrayWrapper.isEventsHighKilled` method got introduced to fix file descriptor registration hitting a NullPointer exception, e.g. http://hg.openjdk.java.net/icedtea/jdk7/jdk/rev/9af467c489cc, you still seem to be hitting (another) NullPointer in that area. Maybe there is an issue with how it determines the number of max open file descriptors (e.g. `IOUtil.fdLimit()` call), but in practice the issue looks to be triggered by an FD value larger than 65535 (or whatever you have set `sun.nio.ch.maxUpdateArraySize` to, if you happen to have customized it). Overall this feels to be a JDK issue to me.

As a workaround, I'd recommend trying either one of these:
- Try setting ulimit to 65535 maximum. Based on the error, you likely have it set higher
- If you must keep it at a value higher than that, you could try set the JVM property `sun.nio.ch.maxUpdateArraySize` to a bit higher than whatever you specified as ulimit
- Try upgrade to a newer version of OpenJDK if you can
- You could try using a recent version of the Oracle JVM distribution, maybe even java 8
</comment><comment author="spinscale" created="2015-06-23T20:17:24Z" id="114630340">@szroland very interesting. Out of curiosity: how did you make the connection between the above revision and this problem? Cant spot the missing link

Thanks a lot for helping!
</comment><comment author="szroland" created="2015-06-23T21:03:16Z" id="114641211">I was just looking at the JDK source, `EPollArrayWrapper` in particular, just out of interest, what can be throwing that NullPointer. On the changelog: http://hg.openjdk.java.net/jdk8/jdk8/jdk/log/687fd7c7986d/src/solaris/classes/sun/nio/ch/EPollArrayWrapper.java I saw a ref to a NullPointer, so first I thought it was that issue exactly. But on the contrary, this change seems to be already included in the version of jvm used (e.g. the `isEventsHighKilled` method). I just referenced it to show JDK issues are not unheard of in this area, and that one in particular was another NullPointer in this same class.

Note that I couldn't actually point to the root cause problem, because the `EPollArrayWrapper` code, and how it handles the `eventsLow` and `eventsHigh` structures seems consistent to me, and still the NullPointer seems to be because the eventsHigh map was not created, but a large FD did come through. The only thing I can think of is `IOUtil.fdLimit()` (a native method, so I didn't check how it worked) reporting a lower value than what FDs can actually occur in the given setup. To work around that, you can either limit FDs in linux manually, use that jvm flag to override the size of the `eventsLow` structure, so that it is always sufficiently large, or upgrade and hope someone spotted the root cause and fixed it.

Admittedly, there is a chance of the version of  `EPollArrayWrapper` in the particular JDK build not being fully consistent with the source I was looking at, which could result in my analysis not being 100% correct. I couldn't actually find the particular revision of EPollArrayWrapper that is exactly in that build. But still, the likelyhood of that is small I believe, since the code is still the same at the head revision of jdk8, http://hg.openjdk.java.net/jdk8/jdk8/jdk/log/687fd7c7986d/src/solaris/classes/sun/nio/ch/EPollArrayWrapper.java. I still recommended the upgrade, as I suspect the issue might be with the fdLimits() call, not with `EPollArrayWrapper`itself. 

I haven't tried actually reproducing this, although now that we know the jdk and elastic version, we could try doing that as well.
</comment><comment author="spinscale" created="2015-06-26T06:35:03Z" id="115543291">@ayashjorden have you tried any of the above suggestions? Eager to get some feedback here!
</comment><comment author="ayashjorden" created="2015-06-26T06:50:31Z" id="115547631">Hi @spinscale,
Sorry for the delayed response.

Lowering file descriptors is too risky for us because the (currently) single data node will (again) go into a 'too many open file' blitz.

JVM maxUpdateArraySize and Java8 options will be in focus next week.

I'll post an informative feedback once I'll have it.

Thanks,
Yarden
</comment><comment author="ayashjorden" created="2015-07-24T08:34:05Z" id="124441715">Hi All,
We resolved the issue by addressing ES limit configuration issues as described at #12404 .

Thanks for all the help,
Yarden
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Packaging: Add LICENSE and NOTICE files for all core dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11705</link><project id="" key="" /><description>Also adds `dev-tools/check_license_and_sha.pl` which will ensure that the
sha1 file in the licenses/ directory is the same as the sha1 of each
dependency shipped with Elasticsearch, and that each dependency has
a LICENSE file.  Can also be used to update the sha1 file when upgrading
dependencies.

Closes #2794 
Closes #10684
</description><key id="88803507">11705</key><summary>Packaging: Add LICENSE and NOTICE files for all core dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T18:28:58Z</created><updated>2015-06-30T12:45:26Z</updated><resolved>2015-06-17T16:06:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-16T18:31:00Z" id="112522372">Not sure if we need to include these files from the groovy JAR:
- groovy-all-ANTLR-LICENSE.txt
- groovy-all-ASM-LICENSE.txt
- groovy-all-CLI-LICENSE.txt
- groovy-all-JSR223-LICENSE.txt

Also, would be good to update maven to execute this at build time:

```
dev-tools/check_license_and_sha.pl --check core
```

/cc @dadoonet 
</comment><comment author="rmuir" created="2015-06-16T18:45:58Z" id="112525762">About checking at build time: maybe we don't want to do it quite yet? How long does it take the perl script to run? At least we want to start with it working via jenkins on linux... but I agree we want a way for a developer to manually check it before pushing (if that is running a perl script manually in both cases, fine for a start).

We could later see if it could be made more portable, e.g. just doing some custom logic in maven-ant-run-plugin here (groovy script or something simple). Maybe it won't be so simple if its in groovy but it means it would work without 'shasum' (windows, etc).

Also i couldnt tell if the verification logic will fail on extra license files that shouldn't be there. I see it definitely will for extra sha's: that is really good to see because it keeps everything tidy. Its just an idea for extra paranoia if its not tough to add.
</comment><comment author="s1monw" created="2015-06-16T18:48:57Z" id="112526663">`mvn clean package` yields this:

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (check-invalid-patterns) on project elasticsearch-parent: An Ant BuildException has occured: The following files contain tabs or
[ERROR] nocommits:
[ERROR] * core/licenses/lucene-LICENSE.txt
[ERROR] around Ant part ...&lt;fail if="validate.patternsFound"&gt;The following files contain tabs or... @ 24:37 in /Users/simon/projects/elasticsearch/target/antrun/build-main.xml
[ERROR] -&gt; [Help 1]
```

I think you need to fix you tabs vs. spaces in your editor :)
</comment><comment author="clintongormley" created="2015-06-16T18:50:44Z" id="112527215">@s1monw tabs removed from Lucene license ;)
</comment><comment author="clintongormley" created="2015-06-16T19:34:10Z" id="112539788">&gt; About checking at build time: maybe we don't want to do it quite yet? How long does it take the perl script to run? 

about 150ms :)

&gt; We could later see if it could be made more portable, e.g. just doing some custom logic in maven-ant-run-plugin here (groovy script or something simple). Maybe it won't be so simple if its in groovy but it means it would work without 'shasum' (windows, etc).

agreed - although `shasum` is just a perl script installed by default on most *nixes.  I could equally roll the module into the script directly so that it doesn't have the command line tool dependency.  

That said, rewriting it in eg java would make it work wherever es works.

&gt; Also i couldnt tell if the verification logic will fail on extra license files that shouldn't be there. I see it definitely will for extra sha's: that is really good to see because it keeps everything tidy. Its just an idea for extra paranoia if its not tough to add.

The only reason I didn't was because of the extra licensing files for groovy.
</comment><comment author="s1monw" created="2015-06-16T20:09:17Z" id="112553169">I think you should move the script into the source folder that way it gets packaged and can be used by plugins etc.

```
 dev-tools/check_license_and_sha.pl -&gt; dev-tools/src/main/resources/license-check/check_license_and_sha.pl
```

if you do this then you can use this diff:

``` DIFF
diff --git a/pom.xml b/pom.xml
index 304a060..43ce99c 100644
--- a/pom.xml
+++ b/pom.xml
@@ -1148,6 +1148,26 @@ org.eclipse.jdt.ui.text.custom_code_templates=&lt;?xml version\="1.0" encoding\="UT
                     &lt;version&gt;1.8&lt;/version&gt;
                     &lt;executions&gt;
                         &lt;execution&gt;
+                            &lt;id&gt;check-license&lt;/id&gt;
+                            &lt;phase&gt;verify&lt;/phase&gt;
+                            &lt;goals&gt;
+                                &lt;goal&gt;run&lt;/goal&gt;
+                            &lt;/goals&gt;
+                            &lt;configuration&gt;
+                                &lt;target&gt;
+                                    &lt;condition property="license.exists"&gt;
+                                        &lt;available file="${basedir}/license" type="dir"/&gt;
+                                    &lt;/condition&gt;
+                                    &lt;echo taskName="license check"&gt;Running license check&lt;/echo&gt;
+                                    &lt;exec failonerror="${license.exists}" executable="perl" dir="${elasticsearch.tools.directory}/license-check" &gt;
+                                        &lt;arg value="check_license_and_sha.pl"/&gt;
+                                        &lt;arg value="--check"/&gt;
+                                        &lt;arg value="${basedir}"/&gt;
+                                    &lt;/exec&gt;
+                                &lt;/target&gt;
+                            &lt;/configuration&gt;
+                        &lt;/execution&gt;
+                        &lt;execution&gt;
                             &lt;id&gt;print-jvm&lt;/id&gt;
                             &lt;phase&gt;validate&lt;/phase&gt;
                             &lt;goals&gt;
```

and `mvn verify -DskipTests` will run the entire build and execute the script on every plugin that has a `license` folder
</comment><comment author="clintongormley" created="2015-06-17T15:40:56Z" id="112854256">@rmuir I've extended the file checks to complain about extra LICENSE files, and added a check to be sure that exactly one NOTICE file exists as well.

@s1monw I've added the POM changes so that this check runs automatically.
</comment><comment author="s1monw" created="2015-06-17T15:49:03Z" id="112857270">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11705 from clintongormley/licenses</comment></comments></commit><commit><files /><comments><comment>Packaging: Add LICENSE, NOTICE, and sha1 files and tests for all core dependencies</comment></comments></commit></commits></item><item><title>Update filter-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11704</link><project id="" key="" /><description>Replace the previous example which leveraged a range filter, which causes unnecessary confusion about when to use a range filter to create a single bucket or a range aggregation with exactly one member in ranges.
</description><key id="88772065">11704</key><summary>Update filter-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">caldwecr</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-16T16:18:51Z</created><updated>2015-06-19T08:53:35Z</updated><resolved>2015-06-19T08:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:47:41Z" id="113144850">Hi @caldwecr 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="caldwecr" created="2015-06-19T02:56:13Z" id="113350687">Done.

On Thu, Jun 18, 2015 at 5:48 AM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @caldwecr https://github.com/caldwecr
&gt; 
&gt; Thanks for the PR. Please could I ask you to sign the CLA so that I can
&gt; merge it in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/11704#issuecomment-113144850
&gt; .

## 

Courtland Caldwell
caldwecr@gmail.com
</comment><comment author="clintongormley" created="2015-06-19T08:53:34Z" id="113437242">thanks @caldwecr - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update filter-aggregation.asciidoc</comment></comments></commit></commits></item><item><title>Query Refactoring: DisMaxQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11703</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217

PR goes agains query-refactoring feature branch.
</description><key id="88749878">11703</key><summary>Query Refactoring: DisMaxQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-16T14:50:31Z</created><updated>2015-06-18T14:16:15Z</updated><resolved>2015-06-18T14:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-18T08:46:34Z" id="113078975">left a couple of comments
</comment><comment author="javanna" created="2015-06-18T12:58:49Z" id="113147883">left a few minor comments, LGTM besides those!
</comment><comment author="cbuescher" created="2015-06-18T13:15:07Z" id="113151341">Addressed last comment. I wouldn't force all query builders to overwrite validation method at this point. As far as I can see, only RangeQueryBuilder and BaseTermQueryBuilder really overwrite this in a meaningful way so far. What about even removing all the empty implementations in the spirit of "less code is good" and only add it if needed later?
</comment><comment author="cbuescher" created="2015-06-18T13:50:22Z" id="113162784">Rebased, added validate() to builder and introduced default tie breaker as a constant.
</comment><comment author="javanna" created="2015-06-18T14:12:43Z" id="113169202">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11703 from cbuescher/feature/query-refactoring-dismax</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java</file></files><comments><comment>Query Refactoring: DisMaxQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>[packaging] Add a way to start master only nodes along side other nodes on the same server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11702</link><project id="" key="" /><description>We just use the deb package to install Elasticsearch but we don't have any spare systems to use as master node. I'm told that its generally good practice in this case to pick three of your nodes and run a master only node in a separate JVM from the data node. We'd do just that, but to do it we'd have to add some hacky configuration on top of the deb install. It'd be awesome if there was a dedicated master package (deb, rpm) we could install that'd install an init script and configuration for a dedicated master node.
</description><key id="88743193">11702</key><summary>[packaging] Add a way to start master only nodes along side other nodes on the same server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-06-16T14:26:47Z</created><updated>2016-06-10T19:45:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:37:51Z" id="113141547">Note that the Puppet module for Elasticsearch supports multiple instances: https://github.com/elastic/puppet-elasticsearch
</comment><comment author="nik9000" created="2015-06-24T14:42:02Z" id="114893140">&gt; Note that the Puppet module for Elasticsearch supports multiple instances: https://github.com/elastic/puppet-elasticsearch

Yup. This is for the folks (like WMF) who aren't willing to do much hacking after the deb does its install. Its one of those convenience/supported use case things.
</comment><comment author="jordansissel" created="2016-06-09T04:58:07Z" id="224800090">@nik9000 I like this proposal, though I am not sure how best to provide it successfully without confusing users.

Rough proposal: 

We could ship with two services (elasticsearch and elasticsearch-master). The former (elasticsearch) would not be master-eligible. The later would be master-eligible. They would have two different homes (configuration directories, etc). Users needing a master and data node on a single server would need to enable _both_ services.

I do worry about users accidentally forgetting to run the master node or that users would accidentally run the data node when they really didn't want to. Thoughts?
</comment><comment author="jasontedor" created="2016-06-10T19:45:10Z" id="225277779">&gt; We could ship with two services (elasticsearch and elasticsearch-master). The former (elasticsearch) would not be master-eligible.

This would mean that people can not download the `elasticsearch` package and immediately start a single node and start experimenting. I think that that would be a usability regression.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create configuration reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11701</link><project id="" key="" /><description>There's no single source of reference for configuration options which makes configuring Elasticsearch awkward. https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html is a good start but I believe more is omitted than included (`path.repo` for example).

Elasticsearch needs a single page that lists every possible configuration option with an explanation of what the setting does, Django has a good example https://docs.djangoproject.com/en/1.8/ref/settings/
</description><key id="88737148">11701</key><summary>Create configuration reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">j0hnsmith</reporter><labels /><created>2015-06-16T14:00:33Z</created><updated>2015-06-17T08:35:37Z</updated><resolved>2015-06-17T08:35:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-17T08:35:37Z" id="112717581">Duplicate of https://github.com/elastic/elasticsearch/issues/11504, closing in favour of that issue. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add shortcut for match_all query in the Delete-By-Query plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11700</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/pull/11516#discussion_r32312451 Simon suggest to add a shortcut when a `match_all` query is executed with the Delete-By-Query plugin:

&gt; We should maybe also think about having a shortcut if there is a match all query passed to this. I think folks to this all the time so I wonder if we should have such a destructive operation as feature on this as well behind the scenes. Doesn't need to be added now but in general we can think about adding this to IndexShard / Engine since in lucene that is quite fast.
</description><key id="88695415">11700</key><summary>Add shortcut for match_all query in the Delete-By-Query plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-16T11:11:08Z</created><updated>2016-01-18T20:32:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Null pointer exception when adding BigDecimal field with value null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11699</link><project id="" key="" /><description>Other captialized numeric types, Integer, Double etc. have a null check when passed in to the XContentBuilder.field method. The XContentBuilder.field(String, BigDecimal) method however throws a NullPointerException.

For consistency it would be better to writeNull() if the BigDecimal value is null.
</description><key id="88689233">11699</key><summary>Null pointer exception when adding BigDecimal field with value null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindrit</reporter><labels><label>:Java API</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-06-16T10:38:01Z</created><updated>2015-06-25T07:45:30Z</updated><resolved>2015-06-25T07:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file></files><comments><comment>Fix #11699</comment></comments></commit></commits></item><item><title>Stored script parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11698</link><project id="" key="" /><description>When script parameters are huge but reused several times it would be great to be able to store them and have them available at execution time without having to pass them with the search request.

Something like:

```
{
  "script_fields": {
    "label": {
      "script": "my_script",
      "stored_parameters": "many_and_huge_parameters"
    }
  }
}
```

where the "many_and_huge_parameters" are retrieved from somewhere inside elasticsearch.
My use case is a classifier. I train the model outside elasticsearch and then use the computed model parameters in es as a native script that implements the model. The model stays the same always but whenever I train it again the parameters change and I would like to not always pass them with the search request. 
</description><key id="88684100">11698</key><summary>Stored script parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Scripting</label><label>stalled</label></labels><created>2015-06-16T10:16:18Z</created><updated>2016-01-18T20:31:53Z</updated><resolved>2016-01-18T20:31:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-17T17:58:25Z" id="112893575">Maybe we can support storing the parameters as a record in another index and then refer to this record in the request similar to what we do with terms lookup? Something like:

```
{
  "script_fields": {
    "label": {
      "script": "my_script",
      "stored_parameters": {
        "index": "models",
        "type": "params",
        "id": "1234",
        "path": "something"
      }
    }
  }
}
```

You can do the same thing in the native script, but if this is a common use case, it might be worth supporting this functionality on the elasticsearch level. 
</comment><comment author="brwe" created="2015-06-22T14:04:24Z" id="114114047">@imotov thanks for the pointer to the workaround! (I assume you mean something like here: https://github.com/imotov/elasticsearch-native-script-example/blob/master/src/main/java/org/elasticsearch/examples/nativescript/script/LookupScript.java).We discussed this briefly and decided that we defer implementation of such a feature until more people actually need it. I can live with the workaround for now. @colings86 correct me if I am wrong.
</comment><comment author="imotov" created="2015-06-22T15:03:11Z" id="114142533">@brwe yes, lookup script is one way to do it. Alternatively, if you don't want to deal with a cache (and it's invalidation and expiration) you can pull the parameters from the index and stash them in the "params" map. The params map is created once per execution per shard and then passed to all script instantiations for this shard. So, it will not be as efficient as with global cache but so much simpler. Basically you would have something like this:

``` java
    public static class Factory extends AbstractComponent implements NativeScriptFactory {

        // ............ 

        @Override
        public ExecutableScript newScript(@Nullable Map&lt;String, Object&gt; params) {
            Map&lt;String, Object&gt; veryLargeParams = params.get("cached_params");
            if( veryLargeParams == null) {
                // This will be executed once per search for every shard
                veryLargeParams = loadVeryLargeParams(params);
                params.put("cached_params", veryLargeParams);
            }
            return new MyScript(veryLargeParams);
        }

        // ............ 

    }
```
</comment><comment author="clintongormley" created="2016-01-18T20:31:53Z" id="172646433">Nothing further in the last 9 months - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reclaim memory from fielddata synchronously when an explicit clear is performed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11697</link><project id="" key="" /><description>The cache we are using for fielddata reclaims memory lazily/asynchronously. While this
helps with throughput this is an issue when a clear operation is issued manually since
memory is not reclaimed immediately. Since our clear methods already perform in linear
time anyway, this commit changes the fielddata cache to reclaim memory synchronously
when a clear command is issued manually. However, it remains lazy in case cache entries
are invalidated because segments or readers are closed, which is important since such
events happen all the time.

Close #11695
</description><key id="88683383">11697</key><summary>Reclaim memory from fielddata synchronously when an explicit clear is performed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T10:12:20Z</created><updated>2015-06-16T15:36:30Z</updated><resolved>2015-06-16T15:36:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-16T15:18:38Z" id="112468146">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file></files><comments><comment>Merge pull request #11697 from jpountz/fix/synchronous_fielddata_removal</comment></comments></commit></commits></item><item><title>Add support for query boost to SimpleQueryStringBuilder.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11696</link><project id="" key="" /><description>As per discussion in #11274 this adds support for query boosting to the SimpleQueryStringQuery.

In the process of refactoring SimpleQueryStringQueryBuilder/-Parser we noticed that this is one of the few queries that doesn't support query level boosting, although it in theory it could. Adding this support here.

@dakrone @javanna @cbuescher can either of you check the PR please?
</description><key id="88675994">11696</key><summary>Add support for query boost to SimpleQueryStringBuilder.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T09:41:04Z</created><updated>2015-06-18T18:14:50Z</updated><resolved>2015-06-18T08:10:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-16T11:54:25Z" id="112397990">LGTM besides the small suggestion on explaining the test case a bit more.
</comment><comment author="MaineC" created="2015-06-18T07:13:51Z" id="113059649">Updated and ready for another round.
</comment><comment author="javanna" created="2015-06-18T07:48:17Z" id="113064776">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringTests.java</file></files><comments><comment>Merge pull request #11696 from MaineC/feature/simple-query-string-addboost</comment></comments></commit></commits></item><item><title>[CI failure] org.elasticsearch.indices.stats.IndexStatsTests.testFieldDataStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11695</link><project id="" key="" /><description>I observed this on the current query refactoring branch but it seems unrelated to the changes there.
I could also reproduce this on master.

http://build-us-00.elastic.co/job/es_feature_query_refactoring/2380

```
org.elasticsearch.indices.stats.IndexStatsTests.testFieldDataStats
Schlägt fehl seit 1 Build (Seit Fehlgeschlagen#2380 )
Dauer: 0.17 Sekunden.
Fehlermeldung

 Expected: &lt;0L&gt;      but: was &lt;284L&gt;

Stacktrace

java.lang.AssertionError: 
Expected: &lt;0L&gt;
     but: was &lt;284L&gt;
    at __randomizedtesting.SeedInfo.seed([B07CF80620C33000:A74387A13149B218]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.indices.stats.IndexStatsTests.testFieldDataStats(IndexStatsTests.java:126)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    [...] 
```

Tried this on master at 56c7cce4b and could also reproduce the same error with:

mvn clean test -Pdev -Dtests.seed=B07CF80620C33000 -Dtests.class=org.elasticsearch.indices.stats.IndexStatsTests -Dtests.method="testFieldDataStats" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=1024m -Dtests.locale=fr_FR -Dtests.timezone=Australia/Queensland
</description><key id="88669334">11695</key><summary>[CI failure] org.elasticsearch.indices.stats.IndexStatsTests.testFieldDataStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">cbuescher</reporter><labels /><created>2015-06-16T09:13:55Z</created><updated>2015-06-16T15:36:30Z</updated><resolved>2015-06-16T15:36:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file></files><comments><comment>Reclaim memory from fielddata synchronously when an explicit clear is performed.</comment></comments></commit></commits></item><item><title>Fix support for `_name` in some queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11694</link><project id="" key="" /><description>Some of our Java api builders had wrong logic when it comes to serializing the query in json format, resulting in missing fields like _name. Also, regexp parser was ignoring the _name field.
</description><key id="88664928">11694</key><summary>Fix support for `_name` in some queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.6.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T09:00:42Z</created><updated>2015-07-14T13:46:07Z</updated><resolved>2015-06-16T10:08:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-16T09:45:28Z" id="112366802">Left one minor comment for better understanding, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Query DSL: fix support for _name in some queries</comment></comments></commit></commits></item><item><title>Simplify doc values handling for _timestamp.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11693</link><project id="" key="" /><description>`_timestamp` uses NumericDocValues instead of SortedNumericDocValues like other
numeric fields since it is guaranteed to be single-valued. However, we don't
need a different fielddata impl for it since DocValues.getSortedNumeric already
falls back to NUMERIC doc values if SORTED_NUMERIC are not available.
</description><key id="88664482">11693</key><summary>Simplify doc values handling for _timestamp.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T08:58:15Z</created><updated>2015-06-17T06:19:34Z</updated><resolved>2015-06-17T06:19:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-16T21:07:17Z" id="112570068">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file></files><comments><comment>Merge pull request #11693 from jpountz/fix/simplify_timestamp_dv</comment></comments></commit></commits></item><item><title>Histogram Agg result fails to generate XContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11692</link><project id="" key="" /><description>I have a simple testcase here:

``` Java
@Test
    public void testExecute_withAggs() throws Exception {

        client().admin().indices().prepareCreate("my-index")
                .addMapping("my-type", "_timestamp", "enabled=true")
                .get();
        ensureGreen("my-index");

        client().prepareIndex("my-index", "my-type").setTimestamp("2005-01-01T00:00").setSource("{}").get();
        client().prepareIndex("my-index", "my-type").setTimestamp("2005-01-01T00:10").setSource("{}").get();
        client().prepareIndex("my-index", "my-type").setTimestamp("2005-01-01T00:20").setSource("{}").get();
        client().prepareIndex("my-index", "my-type").setTimestamp("2005-01-01T00:30").setSource("{}").get();
        refresh();

        SearchResponse response = client().prepareSearch("my-index")
                .addAggregation(AggregationBuilders.dateHistogram("rate").field("_timestamp").interval(DateHistogramInterval.HOUR).order(Histogram.Order.COUNT_DESC))
                .get();
        response.toString();
    }
```

which fails generating the XContent with this:

```

java.lang.UnsupportedOperationException: Printing not supported
    at __randomizedtesting.SeedInfo.seed([4C10016566504505:7B5D40D0D89482B4]:0)
    at org.joda.time.format.DateTimeFormatter.requirePrinter(DateTimeFormatter.java:695)
    at org.joda.time.format.DateTimeFormatter.print(DateTimeFormatter.java:642)
    at org.elasticsearch.search.aggregations.support.format.ValueFormatter$DateTime.format(ValueFormatter.java:127)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram$Bucket.toXContent(InternalHistogram.java:156)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.doXContentBody(InternalHistogram.java:535)
    at org.elasticsearch.search.aggregations.InternalAggregation.toXContent(InternalAggregation.java:202)
    at org.elasticsearch.search.aggregations.InternalAggregations.toXContentInternal(InternalAggregations.java:196)
    at org.elasticsearch.search.aggregations.InternalAggregations.toXContent(InternalAggregations.java:187)
    at org.elasticsearch.search.internal.InternalSearchResponse.toXContent(InternalSearchResponse.java:91)
    at org.elasticsearch.action.search.SearchResponse.toXContent(SearchResponse.java:181)
    at org.elasticsearch.action.search.SearchResponse.toString(SearchResponse.java:225)
    at org.elasticsearch.search.aggregations.bucket.HistogramTests.testExecute_withAggs(HistogramTests.java:1041)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
```

to me this looks wrong :) but not my part of the system to figure out what the hack is going on... @colings86 maybe?

happens only on 2.0
</description><key id="88647114">11692</key><summary>Histogram Agg result fails to generate XContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T07:41:42Z</created><updated>2015-06-26T11:30:45Z</updated><resolved>2015-06-26T11:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-16T08:37:07Z" id="112338462">This bug will be fixed with https://github.com/elastic/elasticsearch/pull/11482 which adds a printer to the epoch_second and epoch_millis formats. I pushed the test case suggested in this issue to master.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file></files><comments><comment>Tests: Add test case from #11692</comment></comments></commit></commits></item><item><title>Clarify refresh parameter in the `_bulk` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11691</link><project id="" key="" /><description>See #11690
</description><key id="88640461">11691</key><summary>Clarify refresh parameter in the `_bulk` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label></labels><created>2015-06-16T07:05:10Z</created><updated>2015-06-17T06:49:21Z</updated><resolved>2015-06-17T06:49:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-16T07:05:46Z" id="112315492">LGTM
</comment><comment author="tlrx" created="2015-06-16T07:10:28Z" id="112316940">LGTM, minor typo
</comment><comment author="bleskes" created="2015-06-17T06:49:15Z" id="112678684">@kevinkluge fyi - some automation in your name reopen this PR.. closing.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Clarify refresh parameter in the `_bulk` API</comment></comments></commit></commits></item><item><title>"refresh" parameter is not honored in bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11690</link><project id="" key="" /><description>According the documentation (https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html#bulk-refresh), there's a "refresh" parameter that can be used to force refresh after bulk operation and we have been using this parameter for a while. However, since 1.6.0, the parameter no longer works, with an exception thrown: 
{"error":"IllegalArgumentException[Action/metadata line [1] contains an unknown parameter [refresh]]","status":500}

I spent some time to dig into it, and found it's related to issue #10977 and  commit https://github.com/elastic/elasticsearch/commit/acb07c72b9d2fd6e21f9710ffbdf4573c01c9f22.

The issue is:
The "refresh" parameter is not checked in the source code before 1.6.0 which is simply ignored, began from 1.6.0, the unchecked "refresh" parameter will cause an exception. So I doubt the "refresh" parameter is never honoured.

The sample request is:
{"update":{"_id":"test", "refresh" : true}}
{"doc_as_upsert":true,"doc":{"testkey":"testvalue"}}
</description><key id="88634151">11690</key><summary>"refresh" parameter is not honored in bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niu-lin</reporter><labels /><created>2015-06-16T06:31:51Z</created><updated>2015-06-16T10:11:39Z</updated><resolved>2015-06-16T10:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-16T07:08:07Z" id="112316080">The `refresh` paramater is one that can be set for the entire request. Refreshing is relatively expensive and therefore is only supported for the entire operation and is done while all items have been executed. I made a PR to improve the docs.
</comment><comment author="niu-lin" created="2015-06-16T08:07:29Z" id="112326467">Thanks for your comment. So my understanding is I should write it as below:

POST /index_name/type_name/_bulk?refresh=true
blabla

Is that correct?
</comment><comment author="niu-lin" created="2015-06-16T08:12:35Z" id="112327498">BTW, I understand it is very expensive, but since we are only use it for some special rare cases, we can afford that cost.
</comment><comment author="bleskes" created="2015-06-16T08:46:52Z" id="112341537">yeah, that’s correct.

&gt; On 16 Jun 2015, at 10:07, niu-lin notifications@github.com wrote:
&gt; 
&gt; Thanks for your comment. So my understanding is I should write it as below:
&gt; 
&gt; POST /index_name/type_name/_bulk?refresh=true
&gt; blabla
&gt; 
&gt; Is that correct?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="niu-lin" created="2015-06-16T10:11:38Z" id="112373495">Boaz, thank you very much for clarifying! I'll close this issue then.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Clarify refresh parameter in the `_bulk` API</comment></comments></commit></commits></item><item><title>Allow executable expression scripts for aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11689</link><project id="" key="" /><description>Added several classes to support expressions being used for numerical
calculations in aggregations.  Expressions will still not compile
when used with mapping and update script contexts.

Closes #11596
</description><key id="88580059">11689</key><summary>Allow executable expression scripts for aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T01:01:17Z</created><updated>2015-06-18T20:03:49Z</updated><resolved>2015-06-18T18:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-06-16T01:03:51Z" id="112247965">@colings86 Please take a look when you get a chance to see if this meets your requirements.  Thanks!
</comment><comment author="colings86" created="2015-06-16T09:29:30Z" id="112362269">@jdconrad this looks great! Exactly what I was needing, thanks for implementing it so quickly. Just pulled down your PR and works as I was expecting. Left a small comment and I agree with all of @rjernst comments.
</comment><comment author="jdconrad" created="2015-06-17T20:51:13Z" id="112946613">@rjernst Just posted a new commit.  Please take a look when you get a chance.
</comment><comment author="rjernst" created="2015-06-18T17:25:28Z" id="113226706">LGTM, just a couple minor comments.
</comment><comment author="jdconrad" created="2015-06-18T18:13:36Z" id="113243572">Thanks @rjernst @colings86 for the reviews!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java</file><file>core/src/test/java/org/elasticsearch/script/CustomScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexedScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/OnDiskScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java</file></files><comments><comment>Scripting: Allow executable expression scripts for aggregations</comment></comments></commit></commits></item><item><title>Add option to `_cat/indices` to return index creation date #11524</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11688</link><project id="" key="" /><description>Returning index creation date, both as a numeric millisecond value and
as a string. This implements #11524
</description><key id="88576892">11688</key><summary>Add option to `_cat/indices` to return index creation date #11524</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">szroland</reporter><labels><label>:CAT API</label><label>enhancement</label><label>feedback_needed</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T00:38:53Z</created><updated>2015-08-05T12:42:57Z</updated><resolved>2015-08-03T18:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:12:14Z" id="113133445">Hi @szroland 

Thanks for the PR.  I note that no tests have been added or updated (specifically the REST tests).  Please could you add.

thanks
</comment><comment author="szroland" created="2015-06-21T08:42:37Z" id="113877924">Hi @clintongormley,

I added tests for the new fields to the default cat.indices test case.
Let me know if anything else is needed.

Cheers,
Roland
</comment><comment author="nik9000" created="2015-07-29T13:41:01Z" id="125955995">Looks good to me.
</comment><comment author="clintongormley" created="2015-07-30T13:51:08Z" id="126333478">@nik9000 want to merge it in?
</comment><comment author="nik9000" created="2015-07-30T14:12:45Z" id="126342385">@clintongormley sure.
</comment><comment author="nik9000" created="2015-07-30T14:39:49Z" id="126351148">@szroland, I'm getting test failures:

```
Suite: org.elasticsearch.test.rest.Rest0Tests
  2&gt; REPRODUCE WITH: mvn test -Pdev -Dtests.seed=F28F8AF862FDDF5 -Dtests.class=org.elasticsearch.test.rest.Rest0Tests -Dtests.slow=true -Dtests.method="test {yaml=cat.indices/10_basic/Test cat indices output}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fi_FI -Dtests.timezone=Pacific/Auckland
FAILURE 0.14s | Rest0Tests.test {yaml=cat.indices/10_basic/Test cat indices output} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: field [$body] was expected to match the provided regex but didn't
   &gt; Expected: ^(
   &gt;    index1                                                    \s+
   &gt;    (\d+)                                                     \s+
   &gt;    (\d\d\d\d\-\d\d\-\d\d T\d\d:\d\d:\d\d.\d\d\d\+\d\d:\d\d)  \s+
   &gt;    (\d+)                                                     \s+
   &gt;    (\d\d\d\d\-\d\d\-\d\d T\d\d:\d\d:\d\d.\d\d\d\+\d\d:\d\d)  \s*
   &gt;  )
   &gt;  $
   &gt;      but: was "index1 1438266964787 2015-07-30T10:36:04.787-04:00 1438266964787 2015-07-30T10:36:04.787-04:00 \n"
```

I'll do a bit of debugging and then let you know what I find. Maybe locale issues? Those times look like they are printed in a pretty ISO8601-ish format so I wouldn't expect them to vary based on locale but, you know, computers are tricky beasts.
</comment><comment author="nik9000" created="2015-07-30T14:59:35Z" id="126359920">Ok - DateTime's toString is documented as spitting out its results in ISO8601 which doesn't have that space between the date and the T. It doesn't look locale dependent. So, I revise my review: doesn't look good to me, remove the space in those regexes and then it will.
</comment><comment author="nik9000" created="2015-07-30T15:01:26Z" id="126360400">I've dropped the review tag and added feedback_needed so we can track that this is waiting another (tiny) patch. I don't think feedback_needed is really the right label but its the closest one I can find.
</comment><comment author="szroland" created="2015-08-02T20:09:46Z" id="127065840">Interesting. Strictly speaking the error message makes sense since the date string does not contain that space. I did go ahead and remove it, also while at it brought the PR up in line with the latest commits.

Still, this test wasn't failing for me, since the rest tests do pattern matching using the Pattern.COMMENTS flag, which supposedly allows comments and ignores white space in the regular expression. This seems to be the case for me on Windows with both a 1.7 and 1.8 Oracle jvm. E.g. the test passes with or without that space. Out of curiosity, what platform / JVM did you run this test on?
</comment><comment author="nik9000" created="2015-08-03T13:37:41Z" id="127235999">&gt; Out of curiosity, what platform / JVM did you run this test on?

Its OSX.

```
$ java -version
java version "1.8.0_51"
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)
```

I'm not really sure what is up. I hadn't thought about the COMMENTS flag but its obvious from looking at the regexes it was on. I'll rerun the tests and recheck.
</comment><comment author="nik9000" created="2015-08-03T15:40:33Z" id="127281797">/me shakes fist at time zones. Its because you are in a GMT + and I'm in a GMT -. It wasn't the space. One moment.
</comment><comment author="nik9000" created="2015-08-03T16:02:04Z" id="127288024">@szroland I sent you a pull request for the branch you've used for this pull request. If you merge it it should fix the test for me and it'll become part of this pull request.
</comment><comment author="szroland" created="2015-08-03T18:39:53Z" id="127365525">Ah right, I didn't notice that either. Makes more sense than having weird consistency issues with the behavior of Pattern.COMMENTS. Merged your PR.
</comment><comment author="nik9000" created="2015-08-03T18:43:25Z" id="127366287">OK! That looks great and everything passes now so I've merged it. Thanks @szroland!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file></files><comments><comment>Use UTC instead of default timezone for creation date in CAT endpoint</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file></files><comments><comment>Merge pull request #11688 from szroland/master</comment></comments></commit></commits></item><item><title>Url repository should respect repo.path for file urls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11687</link><project id="" key="" /><description>Otherwise its operation will not be allowed by the security manager
</description><key id="88572678">11687</key><summary>Url repository should respect repo.path for file urls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>bug</label><label>v1.6.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-16T00:05:00Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-07-14T23:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-17T15:09:00Z" id="112837163">@rmuir  can you please take a look at this
</comment><comment author="rmuir" created="2015-07-02T18:35:08Z" id="118118595">this looks good to me. i left one minor comment.
</comment><comment author="clintongormley" created="2015-07-12T11:21:35Z" id="120709141">@imotov let's get this in?
</comment><comment author="clintongormley" created="2015-07-13T09:59:03Z" id="120879200">We should disable the use of URLs for repositories by default.  Later perhaps we can look at adding a white list
</comment><comment author="imotov" created="2015-07-14T01:52:27Z" id="121109564">@rmuir I added URL whitelist. Could you take another look when you have a chance?
</comment><comment author="rmuir" created="2015-07-14T02:05:42Z" id="121110892">looks good
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove SmartNameObjectMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11686</link><project id="" key="" /><description>This was previously a container for an ObjectMapper, along with the
DocumentMapper that ObjectMapper came from. However, there was
only one use of needing the associated DocumentMapper, and that
wasn't actually used.
</description><key id="88559406">11686</key><summary>Remove SmartNameObjectMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T22:41:43Z</created><updated>2015-06-18T12:14:50Z</updated><resolved>2015-06-16T07:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-06-16T06:36:02Z" id="112309526">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Merge pull request #11686 from rjernst/remove/smart-object-wrapper</comment></comments></commit></commits></item><item><title>Bulk "create" via Java API (1.4.2) doesn't error on duplicate docs but does return negative version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11685</link><project id="" key="" /><description>I am creating via BulkRequestBuilder, doing

```
add(client.prepareIndex(...).setOpType(OpType.CREATE)
```

I have a test where I bulk insert and a subset of the objects inserted already exist. I then attach a listener to the bulk request, and expect to get .hasFailures() etc. (This is running vs an in memory server created in JUnit, if that makes any difference)

Instead what happens is hasFailures() returns false, and the inserts silently fail (ie they appear in the requests() array with isFailed()==false, but aren't updated). 

One difference I do see in the listener is that the new objects have version==1 whereas the failed indexes have ==-3, eg (my duplicate objects start at "id51"):

```
WORKED2: 29...id50...CREATE...{"_id":"id50","test_string":"test_string250"}...1
WORKED2: 30...id51...CREATE...{"_id":"id51","test_string":"test_string251"}...-3
```

(for

```
System.out.println("WORKED2: " + bir.getItemId() + "..." + bir.getId() + "..." + ir.opType().toString() + "..." + ir.source().toUtf8() + "..." + ir.version());
```

)

I confirm the object is not updated in the DB, eg:

```
---------------- {"_id":"id50","test_string":"test_string250"} // (newly inserted object)
---------------- {"_id":"id51","test_string":"test_string51"} // (original object)
```

Any ideas what's going on here?

(EDIT: note if i change from OpType.CREATE to OpType.INDEX then as expected I see versions 1 for new objects, 2 for updated objects, and the objects are updated in the DB)

Is this because I'm using a listener and there's a second wave of verification that occurs after the onResponse()? I'm "happy" to use negative version to indicate "duplicate insert failure" if that's going to be consistent behavior.

(Apologies if this behavior is explained somewhere, I couldn't find it!)
</description><key id="88548269">11685</key><summary>Bulk "create" via Java API (1.4.2) doesn't error on duplicate docs but does return negative version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels><label>:Bulk</label><label>feedback_needed</label></labels><created>2015-06-15T21:42:46Z</created><updated>2015-06-18T19:19:49Z</updated><resolved>2015-06-18T13:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:09:34Z" id="113132346">Hi @Alex-Ikanow 

Could you include the actual code that you use?
</comment><comment author="Alex-Ikanow" created="2015-06-18T13:16:26Z" id="113151963">Ugh file this one under "can't reproduce/operator error", apologies - I pulled out the code into a standalone method and (of course!) it worked as expected (version is still -3; but isFailed() is now correctly returning true, and getFailureMessage() is returning the correct message instead of null as before). 

I then tried my original test inside my more complicated code (which has been modified since the original report) and this time it reported the duplicates correctly. 

I think it's safe to assume I was doing something stupid... apologies again!
</comment><comment author="clintongormley" created="2015-06-18T19:19:49Z" id="113263971">thanks for letting us know @Alex-Ikanow 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>in case of scripted updates it would be good if we get the operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11684</link><project id="" key="" /><description>is it possible to get the result.operation() in the UpdateResponse too, so that it is easy to determine if NONE/DELETE/UPSERT which of them actually happened?
because ctx.get("op") is after the execution of script, it would be good to know for the client whether actual update happened or not
</description><key id="88547625">11684</key><summary>in case of scripted updates it would be good if we get the operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vedil</reporter><labels><label>:CRUD</label><label>enhancement</label></labels><created>2015-06-15T21:39:30Z</created><updated>2016-01-18T20:31:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T12:07:54Z" id="113131569">Related to https://github.com/elastic/elasticsearch/pull/9736
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix exception for plain highlighter and huge terms for Lucene 4.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11683</link><project id="" key="" /><description>In Lucene 5x the exception thrown when highlighter encounters a huge term
is a BytesRefHash.MaxBytesLengthExceededException but in Lucene 4x it is
wrapped in a RuntimeException. We have to catch that as well.

closes #11599
</description><key id="88500446">11683</key><summary>Fix exception for plain highlighter and huge terms for Lucene 4.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Highlighting</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label></labels><created>2015-06-15T18:14:06Z</created><updated>2015-06-25T10:23:18Z</updated><resolved>2015-06-25T10:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-25T10:00:25Z" id="115195852">LGTM
</comment><comment author="kimchy" created="2015-06-25T10:03:01Z" id="115196314">left a small comment, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup GeoShapeFieldMapper defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11682</link><project id="" key="" /><description>GeoShapeFieldMapper is a confusing mess when it comes to setting treeLevel and precision parameters. This makes it difficult for developers to follow what parameters should be for the different tree options (leading to potential bugs in future development). It also contains nonsensical initializations such as treeLevels = 0 - which should never occur.  This minor task is to clean up the mapper default values to make it easier to follow.  
</description><key id="88485005">11682</key><summary>Cleanup GeoShapeFieldMapper defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2015-06-15T17:05:11Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Tried to recover failed shards of index endlessly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11681</link><project id="" key="" /><description>I saw other users reported the same issue at  https://github.com/elastic/elasticsearch/issues/2598 and the developers suggested to create a new issue for this.,  but I did not find a new ticket is created.  So I created this one.

When a shard of index fails during recovery,  ElasticSearch  keeps to recover the same shard every 10 seconds.   It would be nice for users to configure the number of times to try to recover instead of trying it forever.
</description><key id="88477939">11681</key><summary>Tried to recover failed shards of index endlessly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mimiyan</reporter><labels><label>feedback_needed</label></labels><created>2015-06-15T16:40:30Z</created><updated>2015-06-16T14:32:05Z</updated><resolved>2015-06-16T14:31:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-15T17:02:14Z" id="112139410">@mimiyan which version do you witness this behavior with? It should be much better in 1.6.
</comment><comment author="clintongormley" created="2015-06-15T17:25:16Z" id="112146380">@mimiyan also what errors are you seeing in the logs?
</comment><comment author="mimiyan" created="2015-06-15T17:40:57Z" id="112149906">I am using version 1.5.

The logs show:
2015-05-22 08:32:25,311][WARN ][indices.cluster ] .....marking and sending shard failed due to [failed recovery]
[2015-05-22 08:32:25,312][WARN ][cluster.action.shard ] [node1] [messages-20150509][0] received shard failed for [messages-20150509][0], node[8YxdI8jsThuwjUk1qzjvaA], [P], s[INITIALIZING], indexUUID [bHbn-05QSYSB9GTuZxZjHA], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[messages-20150509][0] failed recovery]; nested: EngineCreationFailureException[messages-20150509][0] failed to upgrade 3x segments]; nested: EOFException[read past EOF: NIOFSIndexInput(path="/data/.../index/segments_6")]; ]]

The above messages keep popping up every 10 seconds.
</comment><comment author="bleskes" created="2015-06-15T17:48:13Z" id="112151507">@mimiyan how many nodes do you have in the cluster? If there is no other node to put the shard, I suspect you run into https://github.com/elastic/elasticsearch/pull/10558 ...
</comment><comment author="mimiyan" created="2015-06-15T18:17:05Z" id="112159810">@bleskes We only have 1 node in our cluster now.
</comment><comment author="bleskes" created="2015-06-16T14:27:48Z" id="112449982">@mimiyan I see. In this case the no other options and there is no way to recovery. I agree we shouldn't spin with endless attempts. A temporary work around would be to move the corrupted shard folder. Note that your index will stay red as they is no other copy to use. You can issue a manual _reroute command to force allocation of a new _empty_ primary.
</comment><comment author="bleskes" created="2015-06-16T14:31:55Z" id="112450853">at second thought - https://github.com/elastic/elasticsearch/pull/11269 (released in 1.6.0) should fix this (we will no longer think the shard copy is good during assignment, only to discover it's bad in recovery). I'm going to close this as we got to the root cause of the matter. If upgrading to 1.6.0 somehow doesn't help please do reopen.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>QueryParsingException with a geo bounding box query on a non-existant field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11680</link><project id="" key="" /><description>I have an index with the document below for example:

``` json
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 14450,
      "max_score": 1,
      "hits": [
         {
            "_index": "myindex",
            "_type": "Myindex",
            "_id": "AU3333jJGMfBHjZWiltw",
            "_score": 1,
            "_source": {
               "snippet": "Following in Twitter () and Overstockâs (s) footsteps, Amazon has picked up the domains A.Co, Z.Co, K.Co and interestingly enough Cloud.co in aÂ deal made with Colombia-based domain registry .Co.",
               "image": "",
               "pmonth": 5,
               "localname": "E51E4E69",
               "author": "Alexia Tsotsis",
               "pdate": "2011-05-17 00:00:00.0",
               "foundnlpref": "true",
               "source": "TechCrunch",
               "title": "Amazon Buys The A.Co, Z.Co, K.Co And Cloud.CoÂ Domains",
               "url": "http://techcrunch.com/2011/05/17/amazon-buys-the-a-co-z-co-k-co-and-cloud-co-domains/",
               "pyear": 2011,
               "companyid": "http://semgel.com/2013/entity/www.crunchbase.com/company/amazon#self",
               "nlpref": "Twitter Inc.",
               "id": "http://sindicetech.com/data/techmeme/article/E51E4E69",
               "foundcbref": "true"
            }
         }
      ]
   }
}
```

Executing the following query returns no hit; as expected, since the field `countrycode` is not part of the index.

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "query": {
                "match": {
                  "countrycode": {
                    "query": "USA",
                    "type": "phrase"
                  }
                }
              }
            }
          ],
          "must_not": []
        }
      },
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "*"
        }
      }
    }
  },
  "size": 0
}
```

However, If I add a `geo_bounding_box` filter, a `QueryParsingException[[myindex] failed to find geo_point field [Location]]` is thrown. The field `Location` is indexed with the type `geo_point` and the option `geohash` set to `true`.

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "query": {
                "match": {
                  "countrycode": {
                    "query": "USA",
                    "type": "phrase"
                  }
                }
              }
            },
            {
              "geo_bounding_box": {
                "Location": {
                  "bottom_right": {
                    "lat": 21.94304553343818,
                    "lon": -68.90625
                  },
                  "top_left": {
                    "lat": 57.136239319177434,
                    "lon": -74.1796875
                  }
                }
              }
            }
          ],
          "must_not": []
        }
      },
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "*"
        }
      }
    }
  },
  "size": 0
}
```

I imagine the behaviour with both queries should be the same.
</description><key id="88476348">11680</key><summary>QueryParsingException with a geo bounding box query on a non-existant field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scampi</reporter><labels><label>feedback_needed</label></labels><created>2015-06-15T16:33:53Z</created><updated>2017-01-03T12:19:29Z</updated><resolved>2015-06-23T18:16:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jccq" created="2015-06-16T14:17:55Z" id="112447484">- 
</comment><comment author="clintongormley" created="2015-06-18T12:36:29Z" id="113140989">Hi @scampi 

Could you provide a full example showing how you create the index, index the doc, and run the query?  

As you can see from this example, if the location field is mapped but doesn't exist in the document, the query just doesn't return the document:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "loc": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT t/t/1
{}

GET t/t/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "geo_bounding_box": {
          "loc": {
            "top_left": {
              "lat": 40.73,
              "lon": -74.1
            },
            "bottom_right": {
              "lat": 40.717,
              "lon": -73.99
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="scampi" created="2015-06-22T17:23:56Z" id="114188979">Hi @clintongormley 

I am using the JAVA API, but I reproduced it using the JavaScript API instead.

I forgot to say that I had two indices, and only one of them has the geo field `Location`. Sorry for the incomplete bug report. Below is the JS script that reproduces the problem.

``` js
var elasticsearch = require('elasticsearch');
var client = new elasticsearch.Client({
  apiVersion: '1.4',
  host: 'localhost:9200',
  log: 'trace'
});

// Two indices. Only eliot has a geo_point mapping
client.indices.create({ index: 'eliot' });
client.indices.create({ index: 'test' });

client.indices.putMapping({
  index: 'eliot',
  type: 'Eliot',
  body: {
    properties: {
      loc: {
        type: 'geo_point'
      }
    }
  }
});

// this works although the field `nothing` does not exist. It simply returns 0 hit.
client.search({
  index: 'test',
  body: {
    query: {
      term: {
        nothing: {
          value: 'value'
        }
      }
    }
  }
});

// this fails. The issue at hand is that it should behave like in the previous case, I think
client.search({
  index: 'test',
  body: {
    query: {
      constant_score: {
        filter: {
          geo_bounding_box: {
            loc: {
              top_left: {
                lat: 40.73,
                lon: -74.1
              },
              bottom_right: {
                lat: 40.717,
                lon: -73.99
              }
            }
          }
        }
      }
    }
  }
});
```
</comment><comment author="clintongormley" created="2015-06-23T18:16:36Z" id="114595557">&gt; I forgot to say that I had two indices, and only one of them has the geo field Location. 

@scampi A crucial clue to the problem :)

Yes, running a geo query on an index without that field is not supported.
</comment><comment author="scampi" created="2017-01-03T12:19:29Z" id="270102861">Please consider to  reopen this, see related kibana issue.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tribe node binding to incorrect interface after upgrade to 1.5.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11679</link><project id="" key="" /><description>Current configuration consists of 2 tribe nodes connecting to 3 clusters, with each cluster containing 3 data nodes and a dedicated client instance.  One tribe node connects to clusters 1-2, and one
connects to clusters 1-3.  

 All the client nodes and Tribe nodes run on the same server..  This server has 2 network
interfaces, one is a public interface, the other is on a private interface on bond0.  Each Tribe node and Client node provide http connection via the public interface and and transport connection via the bond0 interface.  This setup works fine for both Tribe and Client in 1.4.2 but only works
for the Client node in 1.5.2.  The Tribe node, even when using the exact same elasticsearch.yml file as the 1.4.2 version, wants to bind to the public interface for the both the http and transport when starting up using the 1.5.2 version.

Log comparison (actual values changed)

Starts out:
INFO ][node                     ] [abc-es-tribe2-t01] starting ...
INFO ][transport                ] [abc-es-tribe2-t01] bound_address {inet[/192.168.52.62:8550]}, publish_address {inet[/192.168.52.62:8550]}

8 lines later:

[2015-06-02 09:05:22,242][INFO ][node                     ] [abc-es-tribe2-t01/routing] starting ...
[2015-06-02 09:05:22,314][INFO ][transport                ] [abc-es-tribe2-t01/routing] bound_address {inet[/0.0.0.0:9301]}, publish_address {inet[/xxx.xx.12.82:9301]}

The Tribe is coming up on the correct interfaces per the elasticsearch.yml file settings.  But, when the routing starts, it binds the transport to the wrong interface, resulting in:

INFO ][discovery.zen            ] [abc-es-tribe2-t01/t1] failed to send join request to master ...internal:discovery/zen/join]]; 
nested: ConnectTransportException[[abc-es-tribe2-t01/t1][inet[/xxx.xx.12.82:9302]] connect_timeout[2m]]; nested: SocketException[Network is unreachable]; ]

network:
  bind_host:                    _bond0_
  publish_host:                _bond0_

transport:
  host: 192.168.52.62:8550
  tcp:
    port:                       8550-8999
</description><key id="88470384">11679</key><summary>Tribe node binding to incorrect interface after upgrade to 1.5.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nemonster</reporter><labels><label>:Tribe Node</label><label>feedback_needed</label></labels><created>2015-06-15T16:08:10Z</created><updated>2016-01-18T20:29:21Z</updated><resolved>2016-01-18T20:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-18T11:52:14Z" id="113128640">Hi @nemonster 

This is probably due to this bug fix: https://github.com/elastic/elasticsearch/pull/9721. Probably the tribe was relying on settings that are now excluded from the tribe client settings.  Can you upload the config files?

/cc @javanna 
</comment><comment author="javanna" created="2015-06-18T12:51:46Z" id="113146634">Yes sounds like #9721 might be the cause, I think it can be solved by adding the needed settings to the tribes configuration specifically, rather than relying on global settings being forwarded to the tribe automatically. Let's have a look at the config file to find out which settings this could be.
</comment><comment author="dhramveersingh" created="2015-07-20T16:34:27Z" id="122942053">Hi, I am facing the similar problem. details as below:

Software versions
a) Elasticsearch 1.6.0
b) Logstash 1.5.1
c) Kibana 4.1.0-linux-x64

Configuration of tribe node: 
        "-Des.http.port=29200"
        "-Des.transport.tcp.port=29300-29349"
        "-Des.network.publish_host=localhost"
        "-Des.node.name=global.node1"
        "-Des.tribe.hk.cluster.name=dev.hk"
        "-Des.tribe.uk.cluster.name=dev.uk"
        "-Des.discovery.zen.ping.unicast.hosts=localhost:29350,localhost:29351"
        "-Des.node.data=false"
        "-Des.node.master=false"

Configuration of node UK: 
        "-Des.http.port=29250"
        "-Des.transport.tcp.port=29350"
        "-Des.network.publish_host=localhost"
        "-Des.discovery.zen.ping.unicast.hosts=localhost:29200"
        "-Des.discovery.zen.ping.multicast.enabled=false" 

Configuration of node HK: 
        "-Des.http.port=29251"
        "-Des.transport.tcp.port=29351"
        "-Des.network.publish_host=localhost"
        "-Des.discovery.zen.ping.unicast.hosts=localhost:29200"
        "-Des.discovery.zen.ping.multicast.enabled=false" 

Issue
log of the tribe, where it is clearly visible that, global.node1/uk and global.node1/hk are trying to connect to the 9300 and 9301 port instead of the 29300 and 29301 ports as specified in the configuration files.

[2015-07-20 17:06:50,872][INFO ][node                     ] [global.node1] starting ...
[2015-07-20 17:06:50,973][INFO ][transport                ] [global.node1] bound_address {inet[/0.0.0.0:29300]}, publish_address {inet[localhost/10.117.20.24:29300]}
[2015-07-20 17:06:50,984][INFO ][discovery                ] [global.node1] elasticsearch/bUO32MNNScSAjZYHNwJIlw
[2015-07-20 17:06:50,984][WARN ][discovery                ] [global.node1] waited for 0s and no initial state was set by the discovery
[2015-07-20 17:06:51,002][INFO ][http                     ] [global.node1] bound_address {inet[/0.0.0.0:29200]}, publish_address {inet[localhost/10.117.20.24:29200]}
[2015-07-20 17:06:51,002][INFO ][node                     ] [global.node1/hk] starting ...
[2015-07-20 17:06:51,046][INFO ][transport                ] [global.node1/hk] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/169.254.95.119:9300]}
[2015-07-20 17:06:51,069][INFO ][discovery                ] [global.node1/hk] es.hk/Ja3FpoMXTKqMs4_q7AhIpw
</comment><comment author="clintongormley" created="2016-01-18T20:29:21Z" id="172645957">This has been fixed in recent releases. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postrm script should not fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11678</link><project id="" key="" /><description>This commit changes the postrm script so that it prints error messages instead of failing &amp; exiting when the deletion of directories failed while removing a RPM/DEB package.

Closes #11373
</description><key id="88457465">11678</key><summary>Postrm script should not fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.6.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T15:23:00Z</created><updated>2015-07-14T13:48:10Z</updated><resolved>2015-07-01T09:08:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-15T15:23:53Z" id="112106441">@spinscale can you have a look when you have time please? No hurry, thanks :)
</comment><comment author="tlrx" created="2015-07-01T09:13:06Z" id="117556213">@electrical @spinscale I merged this without the `userdel` message error, leaving as it was before.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow dynamic template deletion by defining an empty template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11677</link><project id="" key="" /><description>Currently is not possible to delete a dynamic template once added to a type mapping.
This PR aims to add this functionality.

See Issue https://github.com/elastic/elasticsearch/issues/11627

As proposed by @peschlowp here https://github.com/elastic/elasticsearch/issues/11627#issuecomment-111598151 you can delete a dynamic template by setting the "value" of the template to empty.

```
"name_of_template_to_delete": {}
```
</description><key id="88457157">11677</key><summary>Allow dynamic template deletion by defining an empty template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">salyh</reporter><labels><label>:Mapping</label><label>enhancement</label></labels><created>2015-06-15T15:22:07Z</created><updated>2015-07-16T09:36:11Z</updated><resolved>2015-07-10T21:18:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-07-08T17:10:05Z" id="119666589">Thanks for the PR @salyh , I left some comments.
</comment><comment author="salyh" created="2015-07-10T10:01:25Z" id="120331708">Incorporated feedback from @jpountz
</comment><comment author="clintongormley" created="2015-07-12T10:27:56Z" id="120706536">@salyh i think this was automatically closed when we deleted the 1.x branch.  Could you reopen against master please?
</comment><comment author="salyh" created="2015-07-13T10:05:33Z" id="120880759">i adapted for master (es 2.0), pls look at the following commit https://github.com/salyh/elasticsearch/commit/a8a896bab2621f3dda6ce001c776ae2923be5197

Currently the SimpleDynamicTemplatesTests.testRemove() test is failing but i think thats because of improper merge result simulation handling. It seems that the mapping update gets lost during the simulation.
</comment><comment author="jpountz" created="2015-07-15T10:15:38Z" id="121566538">@salyh Can you open a pull request based on this commit? I'll give a look at the lost mapping update.
</comment><comment author="salyh" created="2015-07-16T09:36:11Z" id="121900860">see https://github.com/elastic/elasticsearch/pull/12285
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[maven] clean pom.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11676</link><project id="" key="" /><description>In Maven parent project, in dependency management, we should only declare which versions of 3rd party jars we want to use but not force any scope.
It makes then more obvious in modules what is exactly the scope of any dependency.

For example, one could imagine importing `jimfs` as a `compile` dependency in another module/plugin with:

``` xml
&lt;dependency&gt;
   &lt;groupId&gt;com.google.jimfs&lt;/groupId&gt;
   &lt;artifactId&gt;jimfs&lt;/artifactId&gt;
&lt;/dependency&gt;
```

But it won't work as expected as the default maven `scope` should be `compile` but here it's `test` as defined in the parent project.

So, if you want to use this lib for tests, you should simply define:

``` xml
&lt;dependency&gt;
   &lt;groupId&gt;com.google.jimfs&lt;/groupId&gt;
   &lt;artifactId&gt;jimfs&lt;/artifactId&gt;
   &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
```

We also remove `maven-s3-wagon` from gce plugin as it's not used.
</description><key id="88453695">11676</key><summary>[maven] clean pom.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T15:09:05Z</created><updated>2015-06-16T07:13:31Z</updated><resolved>2015-06-16T07:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-15T21:20:53Z" id="112215481">Not a Maven expert but the explanation sounds good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11676 from dadoonet/maven/clean-pom</comment></comments></commit></commits></item><item><title>Making bin/plugin compatible with Cygwin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11675</link><project id="" key="" /><description>Currently this script fails with the following error under Cygwin:

```
$ bin/plugin -install mobz/elasticsearch-head
Erreur : impossible de trouver ou charger la classe principale org.elasticsearch.plugins.PluginManager
```

In English:

```
Could not find the main class: org.elasticsearch.plugins.PluginManager
```
</description><key id="88448773">11675</key><summary>Making bin/plugin compatible with Cygwin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Lucas-C</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label></labels><created>2015-06-15T14:50:16Z</created><updated>2016-02-29T20:00:56Z</updated><resolved>2016-02-29T20:00:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Lucas-C" created="2015-06-15T14:51:09Z" id="112095724">I actually stole that code from: https://github.com/elastic/elasticsearch/blob/bd5c7d0/core/bin/elasticsearch#L128
</comment><comment author="clintongormley" created="2015-06-17T19:58:03Z" id="112928236">Hi @Lucas-C 

Thanks for the PR.  Please could I ask you to sign the CLA? https://www.elastic.co/contributor-agreement/
</comment><comment author="tlrx" created="2015-06-18T06:52:55Z" id="113055206">Which Cygwin/Windows versions has this been tested on?
</comment><comment author="Lucas-C" created="2015-06-18T08:09:05Z" id="113069162">@clintongormley : done

@tlrx : CYGWIN_NT-6.3 / Windows 8.1 x64
</comment><comment author="Lucas-C" created="2015-08-28T10:59:47Z" id="135740162">I updated the pull request to be compatible with master.
It is still exactly the `case` as in [distribution/src/main/resources/bin/elasticsearch](https://github.com/elastic/elasticsearch/blob/2.0/distribution/src/main/resources/bin/elasticsearch#L117)
</comment><comment author="nik9000" created="2015-08-28T13:04:50Z" id="135768918">It looks like @Lucas-C has signted the CLA so I'll clear that.
</comment><comment author="nik9000" created="2015-08-28T13:06:35Z" id="135769200">LGTM. I don't know cygwin very well nor do I have a test machine but if it was good enough for bin/elasticsearch its probably good enough for bin/plugin.
</comment><comment author="jasontedor" created="2016-02-29T20:00:56Z" id="190357817">We do not support Cygwin.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create PID_DIR in init.d script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11674</link><project id="" key="" /><description>Since the /var/run/elasticsearch directory is cleaned when the operating system starts, the init.d script must ensure that the PID_DIR is correctly created.

Closes #11594
</description><key id="88444342">11674</key><summary>Create PID_DIR in init.d script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T14:33:32Z</created><updated>2015-07-08T10:23:10Z</updated><resolved>2015-06-16T14:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-15T14:33:50Z" id="112090783">@spinscale Can you have a look please? thanks
</comment><comment author="GlenRSmith" created="2015-06-16T07:57:46Z" id="112325136">:beer:
</comment><comment author="spinscale" created="2015-06-16T09:26:47Z" id="112361002">tested on an old ubuntu installation. works! Thanks for fixing this!
</comment><comment author="tlrx" created="2015-06-16T14:35:45Z" id="112451729">@spinscale thanks for the review
</comment><comment author="Rylon" created="2015-07-06T11:33:52Z" id="118826853">Hi there, would it be possible to push out a 1.6.0 revision with this fix in it rather than waiting for the 1.6.1 release?
</comment><comment author="spinscale" created="2015-07-06T11:54:03Z" id="118832123">@Rylon I'm sorry, but we do not do anything like a revision fix. 1.6.1 will include the fix, but there wont be any other backport. If you need to fix it manually, just put the init script part in your current init script or repackage the debian package manually and replace the init script. You could also check out the `v1.6.0` tag, apply the patch and run `mvn clean -DskipTests package` manually to get a patched 1.6.0 debian package
</comment><comment author="Rylon" created="2015-07-06T12:40:12Z" id="118841906">Cool thanks for the update. I'll put something in our config to handle this temporarily, do you know roughly when 1.6.1 will be appearing?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Master receives anonymous cluster-shutdown request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11673</link><project id="" key="" /><description>I have a 1-master, 7-slave elasticsearch setup on amazon ec2. Recently the ec2 instance on which the master node was hosted failed and all applications were shutdown. It has since been restarted and its stable, but this failure has somehow affected the master node. Each time elasticsearch is started on the master node, it receives an anonymous shutdown request and proceeds to shutdown. There is no error message of any sort, just the log statement indicating that a shutdown request was received. I have ensured that none of the clients or browsers are connected to the cluster when the master is restarted, but I always end up with the same issue. The entire log is below. Any help debugging this will be appreciated

[2015-06-15 14:14:47,372][INFO ][node                     ] [NoDataMasterNode] version[1.5.2], pid[18672], build[62ff986/2015-04-27T09:21:06Z]
[2015-06-15 14:14:47,372][INFO ][node                     ] [NoDataMasterNode] initializing ...
[2015-06-15 14:14:47,372][DEBUG][node                     ] [NoDataMasterNode] using home [/d/d1/elasticsearch/elasticsearchMasterNode], config [/d/d1/elasticsearch/elasticsearchMasterNode/config], data [[/d/d1/elasticsearch/elasticsearchMasterNode/data]], logs [/d/d1/elasticsearch/elasticsearchMasterNode/logs], work [/mnt/elasticsearch/tmp], plugins [/d/d1/elasticsearch/elasticsearchMasterNode/plugins]
[2015-06-15 14:14:47,387][DEBUG][plugins                  ] [NoDataMasterNode] lucene property is not set in plugin es-plugin.properties file. Skipping test.
[2015-06-15 14:14:47,387][DEBUG][plugins                  ] [NoDataMasterNode] [/d/d1/elasticsearch/elasticsearchMasterNode/plugins/cloud-aws/_site] directory does not exist.
[2015-06-15 14:14:47,392][DEBUG][plugins                  ] [NoDataMasterNode] [/d/d1/elasticsearch/elasticsearchMasterNode/plugins/cloud-aws/_site] directory does not exist.
[2015-06-15 14:14:47,396][INFO ][plugins                  ] [NoDataMasterNode] loaded [cloud-aws], sites [bigdesk, head]
[2015-06-15 14:14:47,427][DEBUG][common.compress.lzf      ] using encoder [VanillaChunkDecoder] and decoder[{}] 
[2015-06-15 14:14:47,440][DEBUG][env                      ] [NoDataMasterNode] using node location [[/d/d1/elasticsearch/elasticsearchMasterNode/data/dataindexer/nodes/0]], local_node_id [0]
[2015-06-15 14:14:49,288][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [generic], type [cached], keep_alive [30s]
[2015-06-15 14:14:49,295][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [index], type [fixed], size [2], queue_size [200]
[2015-06-15 14:14:49,297][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [bulk], type [fixed], size [2], queue_size [50]
[2015-06-15 14:14:49,297][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [get], type [fixed], size [2], queue_size [1k]
[2015-06-15 14:14:49,297][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [search], type [fixed], size [6], queue_size [1k]
[2015-06-15 14:14:49,298][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [suggest], type [fixed], size [2], queue_size [1k]
[2015-06-15 14:14:49,298][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [percolate], type [fixed], size [2], queue_size [1k]
[2015-06-15 14:14:49,298][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2015-06-15 14:14:49,299][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [listener], type [fixed], size [1], queue_size [null]
[2015-06-15 14:14:49,299][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]
[2015-06-15 14:14:49,300][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]
[2015-06-15 14:14:49,300][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]
[2015-06-15 14:14:49,300][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]
[2015-06-15 14:14:49,300][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]
[2015-06-15 14:14:49,301][DEBUG][threadpool               ] [NoDataMasterNode] creating thread_pool [optimize], type [fixed], size [1], queue_size [null]
[2015-06-15 14:14:49,320][DEBUG][monitor.jvm              ] [NoDataMasterNode] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}}]
[2015-06-15 14:14:49,828][DEBUG][monitor.os               ] [NoDataMasterNode] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@7e8910c7] with refresh_interval [1s]
[2015-06-15 14:14:49,832][DEBUG][monitor.process          ] [NoDataMasterNode] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@31b0afa9] with refresh_interval [1s]
[2015-06-15 14:14:49,836][DEBUG][monitor.jvm              ] [NoDataMasterNode] Using refresh_interval [1s]
[2015-06-15 14:14:49,837][DEBUG][monitor.network          ] [NoDataMasterNode] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@1b3b9b36] with refresh_interval [5s]
[2015-06-15 14:14:49,842][DEBUG][monitor.network          ] [NoDataMasterNode] net_info
host [ip-10-220-7-14]
eth0    display_name [eth0]
        address [/fe80:0:0:0:1057:3eff:fe4e:e24e%2] [/10.220.7.14] 
        mtu [9001] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
lo  display_name [lo]
        address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1] 
        mtu [65536] multicast [false] ptp [false] loopback [true] up [true] virtual [false]

[2015-06-15 14:14:49,848][DEBUG][monitor.fs               ] [NoDataMasterNode] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@39c21328] with refresh_interval [1s]
[2015-06-15 14:14:49,854][DEBUG][common.netty             ] using gathering [true]
[2015-06-15 14:14:49,889][DEBUG][discovery.zen.elect      ] [NoDataMasterNode] using minimum_master_nodes [-1]
[2015-06-15 14:14:49,892][DEBUG][discovery.zen.ping.unicast] [NoDataMasterNode] using initial hosts [], with concurrent_connects [10]
[2015-06-15 14:14:49,894][DEBUG][discovery.zen            ] [NoDataMasterNode] using ping.timeout [3s], join.timeout [1m], master_election.filter_client [true], master_election.filter_data [false]
[2015-06-15 14:14:49,895][DEBUG][discovery.zen.fd         ] [NoDataMasterNode] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2015-06-15 14:14:49,898][DEBUG][discovery.zen.fd         ] [NoDataMasterNode] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2015-06-15 14:14:50,277][DEBUG][script                   ] [NoDataMasterNode] using script cache with max_size [100], expire [null]
[2015-06-15 14:14:50,369][DEBUG][cluster.routing.allocation.decider] [NoDataMasterNode] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2015-06-15 14:14:50,370][DEBUG][cluster.routing.allocation.decider] [NoDataMasterNode] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2015-06-15 14:14:50,371][DEBUG][cluster.routing.allocation.decider] [NoDataMasterNode] using [cluster_concurrent_rebalance] with [2]
[2015-06-15 14:14:50,373][DEBUG][indices.recovery         ] [NoDataMasterNode] using max_bytes_per_sec[100mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2015-06-15 14:14:50,377][DEBUG][gateway.local            ] [NoDataMasterNode] using initial_shards [quorum], list_timeout [30s]
[2015-06-15 14:14:50,470][DEBUG][http.netty               ] [NoDataMasterNode] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb-&gt;512kb], pipelining[true], pipelining_max_events[10000]
[2015-06-15 14:14:50,489][DEBUG][indices.store            ] [NoDataMasterNode] using indices.store.throttle.type [MERGE], with index.store.throttle.max_bytes_per_sec [20mb]
[2015-06-15 14:14:50,490][DEBUG][indices.memory           ] [NoDataMasterNode] using index_buffer_size [305.5mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2015-06-15 14:14:50,491][DEBUG][indices.cache.filter     ] [NoDataMasterNode] using [node] weighted filter cache with size [10%], actual_size [305.5mb], expire [null], clean_interval [1m]
[2015-06-15 14:14:50,492][DEBUG][indices.fielddata.cache  ] [NoDataMasterNode] using size [-1] [-1b], expire [null]
[2015-06-15 14:14:50,515][DEBUG][gateway.local.state.meta ] [NoDataMasterNode] using gateway.local.auto_import_dangled [YES], gateway.local.delete_timeout [30s], with gateway.local.dangling_timeout [2h]
[2015-06-15 14:14:50,569][DEBUG][gateway.local.state.meta ] [NoDataMasterNode] took 53ms to load state
[2015-06-15 14:14:50,573][DEBUG][bulk.udp                 ] [NoDataMasterNode] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]
[2015-06-15 14:14:50,662][INFO ][node                     ] [NoDataMasterNode] initialized
[2015-06-15 14:14:50,670][INFO ][node                     ] [NoDataMasterNode] starting ...
[2015-06-15 14:14:50,703][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500
[2015-06-15 14:14:50,703][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false
[2015-06-15 14:14:50,729][DEBUG][transport.netty          ] [NoDataMasterNode] using profile[default], worker_count[4], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/3/6/1/1], receive_predictor[512kb-&gt;512kb]
[2015-06-15 14:14:50,757][DEBUG][transport.netty          ] [NoDataMasterNode] Bound profile [default] to address [/0:0:0:0:0:0:0:0:9300]
[2015-06-15 14:14:50,759][INFO ][transport                ] [NoDataMasterNode] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.220.7.14:9300]}
[2015-06-15 14:14:50,769][INFO ][discovery                ] [NoDataMasterNode] dataindexer/aYPnhc2KRBKD2X5lwJl7AQ
[2015-06-15 14:14:50,770][DEBUG][cluster.service          ] [NoDataMasterNode] processing [initial_join]: execute
[2015-06-15 14:14:50,770][DEBUG][cluster.service          ] [NoDataMasterNode] processing [initial_join]: no change in cluster_state
[2015-06-15 14:14:50,770][TRACE][discovery.zen            ] [NoDataMasterNode] starting to ping
[2015-06-15 14:14:50,774][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] connecting to [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}
[2015-06-15 14:14:50,794][DEBUG][transport.netty          ] [NoDataMasterNode] connected to node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}]
[2015-06-15 14:14:50,795][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] connected to [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}
[2015-06-15 14:14:50,795][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] sending to [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}
[2015-06-15 14:14:50,829][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] received response from [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}: [ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[1], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}, ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[2], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}]
[2015-06-15 14:14:54,775][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] sending to [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}
[2015-06-15 14:14:54,784][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] received response from [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}: [ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[1], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}, ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[3], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}, ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[4], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}]
[2015-06-15 14:14:58,784][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] sending to [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}
[2015-06-15 14:14:58,788][TRACE][discovery.zen.ping.unicast] [NoDataMasterNode] [1] received response from [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}: [ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[1], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}, ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[3], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}, ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[5], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}, ping_response{node [[NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}], id[6], master [null], hasJoinedOnce [false], cluster_name[dataindexer]}]
[2015-06-15 14:14:58,789][TRACE][discovery.zen            ] [NoDataMasterNode] full ping responses: {none}
[2015-06-15 14:14:58,789][DEBUG][discovery.zen            ] [NoDataMasterNode] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2015-06-15 14:14:58,790][DEBUG][cluster.service          ] [NoDataMasterNode] processing [zen-disco-join (elected_as_master)]: execute
[2015-06-15 14:14:58,795][DEBUG][cluster.service          ] [NoDataMasterNode] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]
[2015-06-15 14:14:58,796][INFO ][cluster.service          ] [NoDataMasterNode] new_master [NoDataMasterNode][aYPnhc2KRBKD2X5lwJl7AQ][ip-10-220-7-14][inet[/10.220.7.14:9300]]{data=false, max_local_storage_nodes=1, master=true}, reason: zen-disco-join (elected_as_master)
[2015-06-15 14:14:58,796][DEBUG][cluster.service          ] [NoDataMasterNode] publishing cluster state version 1
[2015-06-15 14:14:58,797][DEBUG][cluster.service          ] [NoDataMasterNode] set local cluster state to version 1
[2015-06-15 14:14:58,799][DEBUG][river.cluster            ] [NoDataMasterNode] processing [reroute_rivers_node_changed]: execute
[2015-06-15 14:14:58,799][DEBUG][river.cluster            ] [NoDataMasterNode] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-06-15 14:14:58,799][TRACE][discovery.zen            ] [NoDataMasterNode] cluster joins counter set to [1](elected as master)
[2015-06-15 14:14:58,799][DEBUG][cluster.service          ] [NoDataMasterNode] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state (version: 1)
[2015-06-15 14:14:58,819][DEBUG][cluster.service          ] [NoDataMasterNode] processing [local-gateway-elected-state]: execute
[2015-06-15 14:14:58,834][INFO ][http                     ] [NoDataMasterNode] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.220.7.14:9200]}
[2015-06-15 14:14:58,834][INFO ][node                     ] [NoDataMasterNode] started
[2015-06-15 14:14:58,841][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,841][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,841][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,841][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,841][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,841][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,842][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,843][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,844][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,845][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,846][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:14:58,847][DEBUG][cluster.service          ] [NoDataMasterNode] cluster state updated, version [2], source [local-gateway-elected-state]
[2015-06-15 14:14:58,848][DEBUG][cluster.service          ] [NoDataMasterNode] publishing cluster state version 2
[2015-06-15 14:14:58,848][DEBUG][cluster.service          ] [NoDataMasterNode] set local cluster state to version 2
[2015-06-15 14:14:58,849][DEBUG][river.cluster            ] [NoDataMasterNode] processing [reroute_rivers_node_changed]: execute
[2015-06-15 14:14:58,849][DEBUG][river.cluster            ] [NoDataMasterNode] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-06-15 14:14:58,872][INFO ][gateway                  ] [NoDataMasterNode] recovered [6] indices into cluster_state
[2015-06-15 14:14:58,872][DEBUG][cluster.service          ] [NoDataMasterNode] processing [local-gateway-elected-state]: done applying updated cluster_state (version: 2)
[2015-06-15 14:15:08,797][DEBUG][cluster.service          ] [NoDataMasterNode] processing [routing-table-updater]: execute
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,800][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,801][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,802][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][3]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,803][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h1][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][6]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,804][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,805][DEBUG][gateway.local            ] [NoDataMasterNode] [test_result][1]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,805][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][5]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,805][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2014h2][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,805][DEBUG][gateway.local            ] [NoDataMasterNode] [testdocument][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,805][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h2][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,805][DEBUG][gateway.local            ] [NoDataMasterNode] [sourcedocument2015h1][4]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-06-15 14:15:08,806][DEBUG][cluster.service          ] [NoDataMasterNode] processing [routing-table-updater]: no change in cluster_state
[2015-06-15 14:15:14,682][INFO ][action.admin.cluster.node.shutdown] [NoDataMasterNode] [cluster_shutdown]: requested, shutting down in [1s]
[2015-06-15 14:15:15,684][INFO ][action.admin.cluster.node.shutdown] [NoDataMasterNode] [cluster_shutdown]: done shutting down all nodes except master, proceeding to master
[2015-06-15 14:15:15,687][INFO ][action.admin.cluster.node.shutdown] [NoDataMasterNode] shutting down in [200ms]
[2015-06-15 14:15:15,888][INFO ][action.admin.cluster.node.shutdown] [NoDataMasterNode] initiating requested shutdown...
[2015-06-15 14:15:15,888][INFO ][node                     ] [NoDataMasterNode] stopping ...
[2015-06-15 14:15:15,902][INFO ][node                     ] [NoDataMasterNode] stopped
[2015-06-15 14:15:15,903][INFO ][node                     ] [NoDataMasterNode] closing ...
[2015-06-15 14:15:15,927][INFO ][node                     ] [NoDataMasterNode] closed
</description><key id="88442732">11673</key><summary>Master receives anonymous cluster-shutdown request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rhycce</reporter><labels /><created>2015-06-15T14:25:46Z</created><updated>2015-06-17T23:27:22Z</updated><resolved>2015-06-17T19:56:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-17T19:56:00Z" id="112927837">Hi @rhycce 

Something somewhere is sending a shutdown request to your cluster.  Are you sure your cluster is properly firewalled?  What about putting a proxy in front of it and logging requests?

We've actually removed the shutdown API in #10831, but if your cluster is open to abusive requests, you'll have other problems as well, even without this API.
</comment><comment author="rhycce" created="2015-06-17T23:27:22Z" id="112979174">That is what I suspected. The cluster  has all the security policies in place and even has its own security group with limited access on Amazon. Also the techOps guys confirmed that access to the master node was limited to IP's coming from our office. We even dumped all tcp requests on that port to see if there were any incoming requests during the failure but it seemed there was none. Since we couldn't resolve the issue, we shut it down, zipped up the master node, moved it to a new instance and restarted it. So far it has been working without any issues. This might be caused by a corrupted EC2 instance. Unfortunately, since the new instance is working, techOps had to shutdown the possibly corrupted instance so I could not debug further.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException indexing percolator query with "inner_hits"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11672</link><project id="" key="" /><description>With ElasticSearch 1.5.2

I have a mapping with a nested object and try to index a percolator query like this:

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "nested": {
          "path": "nestedObject",
          "query": {
            "filtered": {
              "query": {
                "match_all": {}
              }
            }
          },
          "inner_hits": {
            "size": 1000
          }
        }
      }
    }
  }
}
```

That produces this exception:

```
org.elasticsearch.index.percolator.PercolatorException: [test] failed to parse query [498370ec09aecc8ac355befa1f41]
        at org.elasticsearch.index.percolator.PercolatorQueriesRegistry.parsePercolatorDocument(PercolatorQueriesRegistry.java:196)
        at org.elasticsearch.index.percolator.PercolatorQueriesRegistry$RealTimePercolatorOperationListener.preIndex(PercolatorQueriesRegistry.java:324)
        at org.elasticsearch.index.indexing.ShardIndexingService.preIndex(ShardIndexingService.java:139)
        at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:493)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:196)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.query.QueryParseContext.addInnerHits(QueryParseContext.java:268)
        at org.elasticsearch.index.query.NestedQueryParser$ToBlockJoinQueryBuilder.build(NestedQueryParser.java:153)
        at org.elasticsearch.index.query.NestedFilterParser.parse(NestedFilterParser.java:97)
        at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
        at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
        at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
        at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:302)
        at org.elasticsearch.index.query.IndexQueryParserService.parseInnerQuery(IndexQueryParserService.java:321)
        at org.elasticsearch.index.percolator.PercolatorQueriesRegistry.parseQuery(PercolatorQueriesRegistry.java:222)
        at org.elasticsearch.index.percolator.PercolatorQueriesRegistry.parsePercolatorDocument(PercolatorQueriesRegistry.java:193)
        ... 9 more
```

I can index the query if I remove the useless "inner_hits" field. Maybe Elasticsearch should return a better error than `NullPointerException`
</description><key id="88437993">11672</key><summary>NullPointerException indexing percolator query with "inner_hits"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Volune</reporter><labels><label>:Inner Hits</label><label>:Percolator</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T14:09:38Z</created><updated>2015-06-23T13:05:14Z</updated><resolved>2015-06-23T13:05:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-06-16T06:48:29Z" id="112312667">@Volune agreed, ES should fail with a better error with something like: `inner hits isn't supported in the percolate api`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>percolator: Fail nicely if `nested` query with `inner_hits` is used in a percolator query.</comment></comments></commit></commits></item><item><title>NullPointerException when using bulk import</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11671</link><project id="" key="" /><description>While using the bulk import, I got an error message of `NullPointerException[null]`

From the logs:

```
[2015-06-15 08:03:03,831][DEBUG][action.bulk              ] [Autolycus] [products][3], node[ZrSwGU7bR_Oh9htebqstEA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@5f410b12]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:129)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:565)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-06-15 08:03:03,841][DEBUG][action.bulk              ] [Autolycus] [products][4], node[ZrSwGU7bR_Oh9htebqstEA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@77230a7b]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:129)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:565)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-06-15 08:03:03,865][DEBUG][action.bulk              ] [Autolycus] [products][1], node[ZrSwGU7bR_Oh9htebqstEA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@7d9b753]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:129)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:565)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-06-15 08:03:03,881][DEBUG][action.bulk              ] [Autolycus] [products][2], node[ZrSwGU7bR_Oh9htebqstEA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@1542dc3d]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:129)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:565)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Running 1.4.2 locally on Ubuntu 14.04 LTS

```
[ericu-destroyer-of-worlds] elasticsearch$ curl localhost:9200
{
  "status" : 200,
  "name" : "Autolycus",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.4.2",
    "build_hash" : "927caff6f05403e936c20bf4529f144f0c89fd8c",
    "build_timestamp" : "2014-12-16T14:11:12Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.2"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="88425719">11671</key><summary>NullPointerException when using bulk import</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hydrogen18</reporter><labels><label>:Bulk</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-06-15T13:22:09Z</created><updated>2016-09-16T06:41:53Z</updated><resolved>2016-09-16T06:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-15T15:30:56Z" id="112110467">@hydrogen18 can you give us an example bulk request that is causing the null pointer exception?
</comment><comment author="bleskes" created="2015-06-15T16:42:34Z" id="112131587">@hydrogen18 do you create the index dynamically by indexing into it or is it created in advance?
</comment><comment author="hydrogen18" created="2015-06-16T17:30:24Z" id="112507270">The index is created in advance. I could give you an example, but the documents are boring.
</comment><comment author="bleskes" created="2015-06-17T09:08:29Z" id="112730285">@hydrogen18 thx. The stack trace (which we can do better reporting dealing with) suggest that index was removed while the operation was in flight. Can that be?
</comment><comment author="hydrogen18" created="2015-06-17T14:02:13Z" id="112812222">I did not intentionally do that. This box is on our local network, so I suppose it is possible. Would there be a log statement indicating when the index was dropped?
</comment><comment author="bleskes" created="2015-06-18T07:34:46Z" id="113062865">on the  master, you should see the result of this log:

```
                logger.info("[{}] deleting index", request.index);
```
</comment><comment author="hydrogen18" created="2015-06-18T15:13:21Z" id="113188608">It does appear the indices were deleted in the same time period. I am unsure how that happened. There is probably no 'correct' way for the software to handle such a situation. However, it'd be nice if the message was something other than `NullPointerException`

```
elasticsearch.log.2015-06-15:[2015-06-15 08:03:03,744][INFO ][cluster.metadata         ] [Autolycus] [products] deleting index
elasticsearch.log.2015-06-15:[2015-06-15 08:03:09,115][INFO ][cluster.metadata         ] [Autolycus] [scripts] deleting index
```
</comment><comment author="bleskes" created="2015-06-18T15:15:29Z" id="113189076">Yeah, the NullPointerException is totally wrong. I’ll fix that….

Thanks for digging it up.

&gt; On 18 Jun 2015, at 17:13, Eric Urban notifications@github.com wrote:
&gt; 
&gt; It does appear the indices were deleted in the same time period. I am unsure how that happened. There is probably no 'correct' way for the software to handle such a situation. However, it'd be nice if the message was something other than "NullPointerException"
&gt; 
&gt; elasticsearch.log.2015-06-15:[2015-06-15 08:03:03,744][INFO ][cluster.metadata         ] [Autolycus] [products] deleting index
&gt; elasticsearch.log.2015-06-15:[2015-06-15 08:03:09,115][INFO ][cluster.metadata         ] [Autolycus] [scripts] deleting index
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="gfyoung" created="2016-09-16T04:04:36Z" id="247513375">@bleskes : It seems some like logging a warning of some kind (after checking for `null`) would be best?
</comment><comment author="bleskes" created="2016-09-16T06:41:53Z" id="247529403">@gfyoung we should totally avoid the NPE and deal with concurrent index deletion correctly. This has long been fixed (somewhere in the 2.x history line) so I'm closing this now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11670</link><project id="" key="" /><description /><key id="88416604">11670</key><summary>0.90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nixlnixl</reporter><labels /><created>2015-06-15T12:49:05Z</created><updated>2015-06-15T12:56:02Z</updated><resolved>2015-06-15T12:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-15T12:56:01Z" id="112058647">I assume this has been open by mistake.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove non-default fielddata formats.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11669</link><project id="" key="" /><description>Now that doc values are the default for fielddata, specialized in-memory
formats are becoming an esoteric option. This commit removes such formats:
- `fst` on string fields,
- `compressed` on geo points.

I also removed documentation and tests that the fielddata cache is shared if
you change the format, since this is only true for in-memory fielddata formats
(given that for doc values, the caching is done directly in Lucene).

In terms of backward compatibility, IndexFielddataService already falls back to
the default fielddata format in case the configured fielddata format can't be found,
and logs a warning.
</description><key id="88413597">11669</key><summary>Remove non-default fielddata formats.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>deprecation</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T12:39:07Z</created><updated>2015-06-22T11:37:16Z</updated><resolved>2015-06-15T14:19:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-06-15T12:48:24Z" id="112054768">nice cleanup, LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesAtomicFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedAtomicFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/FSTPackedBytesStringFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTest.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Merge pull request #11669 from jpountz/remove/esoteric_fielddata_formats</comment></comments></commit></commits></item><item><title>Do not prompt for node name twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11668</link><project id="" key="" /><description>We allow setting the node's name a few different ways: the `name` system
property, the setting `name`, and the setting `node.name`. There is an order
of preference to these settings that gets applied will copy values from the
system property or `node.name` setting to the `name` setting. When setting
only `node.name` to one of the prompt placeholders, the user would be
prompted twice as the value of `node.name` is copied to `name` prior to
prompting for input. Additionally, the value entered by the user for `node.name`
would not be used and only the value entered for `name` would be used.

This fix changes the behavior to only prompt once when `node.name` is set and
`name` is not set. This is accomplished by waiting until all values have been
prompted and replaced, then the logic for determining the node's name is
executed.

Closes #11564
</description><key id="88407503">11668</key><summary>Do not prompt for node name twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>bug</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T12:17:35Z</created><updated>2015-07-08T18:15:49Z</updated><resolved>2015-07-08T18:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="djschny" created="2015-06-29T18:52:01Z" id="116795180">Should the javadocs about `__prompt__` be updated to match the end chosen `${prompt.text}` while at it with this change?
</comment><comment author="jaymode" created="2015-06-30T14:07:12Z" id="117199143">@djschny I pushed another commit that cleaned up the javadocs
</comment><comment author="djschny" created="2015-06-30T14:37:52Z" id="117210460">I pulled and built this branch and then tested and it worked. LGTM, thanks for fixing this.

```
djschny:elasticsearch-2.0.0-SNAPSHOT djschny$ bin/elasticsearch --node.name="$\{prompt.text}"
Enter value for [node.name]: success
[2015-06-30 10:36:14,767][INFO ][node                     ] [success] version[2.0.0-SNAPSHOT], pid[1345], build[634769a/2015-06-30T14:30:00Z]
[2015-06-30 10:36:14,768][INFO ][node                     ] [success] initializing ...
[2015-06-30 10:36:14,774][INFO ][plugins                  ] [success] loaded [], sites []
[2015-06-30 10:36:14,915][INFO ][env                      ] [success] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [118.9gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2015-06-30 10:36:17,663][INFO ][node                     ] [success] initialized
[2015-06-30 10:36:17,664][INFO ][node                     ] [success] starting ...
[2015-06-30 10:36:17,813][INFO ][transport                ] [success] bound_address {inet[/127.0.0.1:9300]}, publish_address {inet[/127.0.0.1:9300]}
[2015-06-30 10:36:17,839][INFO ][discovery                ] [success] elasticsearch/RX2qVD1HQyuHp74Glt4iag
[2015-06-30 10:36:21,635][INFO ][cluster.service          ] [success] new_master [success][RX2qVD1HQyuHp74Glt4iag][djschny][inet[/127.0.0.1:9300]], reason: zen-disco-join (elected_as_master)
[2015-06-30 10:36:21,679][INFO ][http                     ] [success] bound_address {inet[/127.0.0.1:9200]}, publish_address {inet[/127.0.0.1:9200]}
[2015-06-30 10:36:21,679][INFO ][node                     ] [success] started
[2015-06-30 10:36:21,700][INFO ][gateway                  ] [success] recovered [0] indices into cluster_state
```
</comment><comment author="jpountz" created="2015-07-08T17:13:05Z" id="119667561">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem with highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11667</link><project id="" key="" /><description>##### Hi Guys,

I am using **elasticsearch 1.6.0** and having issues with highlighting.
When I use the following query:

``` json
{
  "from" : 0,
  "size" : 10,
  "query" : {
      "query_string" : {
      "query" : "attachment:Y",
      "use_dis_max" : true,
      "analyzer" : "snowball"
    }
  },
  "highlight" : {
    "pre_tags" : [ "&lt;span class=\"mark\"&gt;" ],
    "post_tags" : [ "&lt;/span&gt;" ],
    "encoder" : "html",
    "order": "score",
    "require_field_match" : false,
    "fields" : {
       "*" : { }
      }
  }
 }
```

I get in the highlight response fields that I have not queried specifically about.
My query was: attachment:Y
and I got:

``` json
 "highlight": {
               "fields.attachment": [
                  "&lt;span class=\"mark\"&gt;Y&lt;/span&gt;"
               ],
               "fields.reproducible": [
                  "&lt;span class=\"mark\"&gt;Y&lt;/span&gt;"
               ],
               "fields.has-linkage": [
                  "&lt;span class=\"mark\"&gt;Y&lt;/span&gt;"
               ],
               "fields.has-others-linkage": [
                  "&lt;span class=\"mark\"&gt;Y&lt;/span&gt;"
               ]
            }
```

Which is not the behavior I want...
I can't set `require_field_match=true` because I also want to query for free text without specifying any fields and I expect elasticsearch to highlight everything he finds matching the text, and if `require_field_match=true` nothing is highlighted when I query for any text like 

``` json
"query" : {
      "query_string" : {
      "query" : "Y",
      "use_dis_max" : true,
      "analyzer" : "snowball"
    }
  }
```

Is there something else I can configure to get this to work?

Thanks in advance any help would be appreciated!
</description><key id="88397935">11667</key><summary>Problem with highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">litalk</reporter><labels /><created>2015-06-15T11:37:19Z</created><updated>2015-06-19T03:57:43Z</updated><resolved>2015-06-17T19:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-17T19:37:45Z" id="112923178">Hi @litalk 

You're looking for this option: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#field-match
</comment><comment author="litalk" created="2015-06-17T19:46:57Z" id="112926027">Hi @clintongormley,
Thanks for your answer, but as I said in my post I cannot use this option because sometimes I am searching for free text, not binded to any field and I would expect elasticsearch to highlight every field he finds with my text query. If I set require_field_match=true nothing is highlighted. 

Can you reopen my request? 
</comment><comment author="clintongormley" created="2015-06-18T17:43:28Z" id="113232484">Somehow I missed the whole section you wrote about `require_field_match`.  My apologies.

However, when you say:

&gt; I am searching for free text, not binded to any field

... this is not what you are doing.  If you don't specify a field, then you are querying the `_all` field, which contains the values of all fields concatenated as one big string.   This is why various fields get highlighted.  Instead, try querying just the fields that you want to query, along with setting `require_field_match` to true.
</comment><comment author="litalk" created="2015-06-18T17:50:01Z" id="113234959">But this is not my use case. 
I don't always know the specific fields in my application. I know I am searching for some phrase but I am not sure if it's in the name field or description,  or maybe in the comments...  This is just an example,  but the idea of the application is a global search and not always the specific field is known. In that case, highlighting does a great job. Bug in the case I do want  to filter by a specific field it does not work as expected... 
</comment><comment author="clintongormley" created="2015-06-18T19:55:13Z" id="113272360">then for the first use case you can use `_all`, and for the second use case you can specify the fields directly
</comment><comment author="litalk" created="2015-06-19T03:57:43Z" id="113359141">But that requires me to parse the query, I don't want to do that... Sometimes I have combination of both use cases. E.G: '''fieldA:query AND sometext'''
And for that I can't use this solution even if I parse the query. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] remove redundant tests and move to different suite</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11666</link><project id="" key="" /><description>Some of the test for meta data are redundant. Also, since they
somewhat test service disruptions (start master with empty
data folder) we might move them to DiscoveryWithServiceDisruptionsTests.
Also, this commit adds a test for
https://github.com/elastic/elasticsearch/issues/11665
</description><key id="88389136">11666</key><summary>[TEST] remove redundant tests and move to different suite</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-06-15T10:53:29Z</created><updated>2015-07-27T10:45:32Z</updated><resolved>2015-07-27T10:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-06-17T12:56:44Z" id="112789662">thanks fr the review! addressed all comments. want to have another look?
</comment><comment author="bleskes" created="2015-06-24T13:25:31Z" id="114866709">This looks great. Left some minor comments.
</comment><comment author="brwe" created="2015-07-22T09:38:03Z" id="123643074">@bleskes I added a method now that lets me restart any node I want and made all other helpers I used before private again. Let me know if that is OK. 
</comment><comment author="bleskes" created="2015-07-23T09:58:27Z" id="124038296">Thx @brwe . This looks great. I  left some minor comments.
</comment><comment author="brwe" created="2015-07-23T16:53:03Z" id="124166168">@bleskes so, turns out there was a bug in the meta data write with closed indices, see https://github.com/brwe/elasticsearch/commit/f141911390dd5a8177f5eaf4f56eb920c1061357 . comment not so minor after all.
</comment><comment author="bleskes" created="2015-07-24T09:29:47Z" id="124455195">LGTM. Thx @brwe . Do we also want/need to add a unit test for https://github.com/brwe/elasticsearch/commit/f141911390dd5a8177f5eaf4f56eb920c1061357 (opening a closed index) ?  (as a different change)
</comment><comment author="brwe" created="2015-07-27T10:45:32Z" id="125162934">&gt; Do we also want/need to add a unit test for brwe@f141911 (opening a closed index) ? (as a different change)

Yes, I think we should. I opened https://github.com/elastic/elasticsearch/issues/12475 to track this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesTests.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Merge pull request #11666 from brwe/meta-data-zen</comment></comments></commit></commits></item><item><title>Concurrent deletion of indices and master failure can cause indices to be reimported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11665</link><project id="" key="" /><description>Currently, a data node deletes indices by evaluating the cluster state. If a new cluster state comes in it is compared to the last known cluster state, and if the new state does not contain an index that the node has in its last cluster state, then this index is deleted.

This could cause data to be deleted if the data folder of all master nodes was lost (#8823): 

All master nodes of a cluster go down at the same time and their data folders cannot be recovered. 
A new master is brought up but it does not have any indices in its cluster state because the data was lost.
Because all other node are data nodes it cannot get the cluster state from them too and therefore sends a cluster state without any indices in it to the data nodes. The data nodes then delete all their data. 

On the master branch we prevent this now by checking if the current cluster state comes from a different master than the previous one and if so, we keep the indices and import them as dangling (see #9952, ClusterChangedEvent).

While this prevents the deletion, it also means that we might in other cases not delete indices although we should.

Example:
1. two masters eligible nodes, m1 is master, one data node (d).
2. m1, m2 and d are on cluster state version 1 that contains and index
3. The index is deleted through the API, causing m1 to send cluster state 2 which does not contain the index to m2 and d that should trigger the actual index deletion.
4. m1 goes down
5. m2 receives the new cluster state but d does not (network issues etc)
6. m2 is elected master and sends cluster state 3 to d which again does not contain the index
7. d will not delete the index because the state comes from a different master than cluster state 1 (the last one it knows of) and will therefore not delete the index and instead import it back into the cluster 

Currently there is no way for a data node to decide if an index should actually be deleted or not if the cluster state that triggers the delete comes from a new master. We chose between: (1) deleting all data in case a node receives an empty cluster state or (2) run the risk to keep indices around that should actually be deleted.

We decided for (2) in #9952. Just opening this issue so that this behavior is documented.
</description><key id="88374771">11665</key><summary>Concurrent deletion of indices and master failure can cause indices to be reimported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Cluster</label><label>adoptme</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-06-15T09:49:15Z</created><updated>2016-03-01T14:53:22Z</updated><resolved>2016-03-01T14:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-15T10:00:43Z" id="112003072">@brwe what about making the delete index request wait for responses from the data nodes? then the request can report success/failure?
</comment><comment author="bleskes" created="2015-06-15T11:39:50Z" id="112028653">@clintongormley the delete index API does wait for data nodes to confirm the deletion. The above scenario will trigger the call to time out (it waits for an ack from the data node that will not come). If people then check the CS, they will see that the index was deleted. However, at a later stage, once the data rejoins the cluster and the new master, the index will be reimported.
</comment><comment author="clintongormley" created="2015-06-15T11:43:19Z" id="112030100">Ok understood.  +1
</comment><comment author="clintongormley" created="2016-01-18T20:28:13Z" id="172645767">@bleskes is this still an issue?
</comment><comment author="bleskes" created="2016-01-19T10:54:08Z" id="172816676">Sadly it is. However, thinking about it again I realized that we can easily detect the “new empty” master danger by comparing cluster uuid - a new master will generate a new one. Agreed with marking as adopt me. Although it sounds scary it’s quite an easy fix and is a good entry point to the cluster state universe. If anyone wants to pick this up, please ping me :)

&gt; On 18 Jan 2016, at 21:28, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; @bleskes is this still an issue?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterChangedEventTests.java</file><file>core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java</file></files><comments><comment>Index deletes not applied when cluster UUID has changed</comment></comments></commit></commits></item><item><title>Fix missing dependencies for RPM/DEB packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11664</link><project id="" key="" /><description>Since elasticsearch doesn't shade artifacts anymore (see #11522), the dependencies list for RPM/DEB must be updated. In this commit, we package all maven libs by default except the generated -shaded/-tests/-test-cours JARs and slf4j-api (marked as optionnal).
</description><key id="88372173">11664</key><summary>Fix missing dependencies for RPM/DEB packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T09:37:14Z</created><updated>2015-06-23T11:24:01Z</updated><resolved>2015-06-23T11:23:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-15T09:48:24Z" id="112000307">I did not try to run it but it looks good to me.

That said, I had a discussion with @rmuir about how we should declare what we want to embed in plugins. The conclusion was that we should somehow declare explicitly what we want to add instead of excluding... See discussion here: https://github.com/elastic/elasticsearch/issues/11647#issuecomment-111712856

So may be we should be consistent here with that position. WDYT?
</comment><comment author="tlrx" created="2015-06-15T09:57:03Z" id="112002346">@dadoonet thanks! I think this case is different from plugins. The deb/rpm is just another way to package elasticsearch: there's no additional code or dependencies needed and there won't be any conflicting lib like in plugins. Here we need to be sure to include everything, so excluding only what we don't need  makes sense to me.
</comment><comment author="spinscale" created="2015-06-19T09:29:21Z" id="113445274">@tlrx LGTM

@dadoonet agreeing with @tlrx here, I think we can keep the behaviour. We already broke deb/rpm packaging once, by not including ant-lr in rpm/deb and thus having lucene expressions not working.
</comment><comment author="tlrx" created="2015-06-23T11:23:40Z" id="114448798">thanks @spinscale !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elasticsearch shield</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11663</link><project id="" key="" /><description>shield‘s price? License expiration
</description><key id="88366850">11663</key><summary>elasticsearch shield</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LiangAllen</reporter><labels /><created>2015-06-15T09:17:18Z</created><updated>2015-06-15T09:43:14Z</updated><resolved>2015-06-15T09:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-15T09:43:13Z" id="111999536">Hi @LiangAllen 

Please contact sales@elastic.co 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 5.2.1.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11662</link><project id="" key="" /><description>I have one remaining nocommit to fix in SimpleLuceneTests because of DirectoryReader.openIfChanged returning a non-null reader after IndexWriter.prepareCommit has been called. I'll dig.
</description><key id="88361604">11662</key><summary>Upgrade to Lucene 5.2.1.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T08:53:21Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-06-15T10:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-15T08:55:54Z" id="111983769">This looks related to [LUCENE-6523](https://issues.apache.org/jira/browse/LUCENE-6523) but I'm not sure yet if this is the expected behaviour or if it hides a bug.
</comment><comment author="jpountz" created="2015-06-15T09:28:35Z" id="111992079">Since we don't use prepareCommit in Elasticsearch, I removed this test and opened https://issues.apache.org/jira/browse/LUCENE-6565. I think it's ready now.
</comment><comment author="mikemccand" created="2015-06-15T10:09:38Z" id="112004643">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/Version.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/IndexCacheableQueryTests.java</file><file>core/src/test/java/org/elasticsearch/deps/lucene/SimpleLuceneTests.java</file><file>core/src/test/java/org/elasticsearch/test/IBMJ9HackThreadFilters.java</file><file>core/src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file></files><comments><comment>Merge pull request #11662 from jpountz/upgrade/lucene-5.2.1</comment></comments></commit></commits></item><item><title>Make explicit the requirement for intervals to be integers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11661</link><project id="" key="" /><description>The rounding function implies this, but this has still caught some people out.
https://github.com/elastic/elasticsearch/issues/4847
</description><key id="88358700">11661</key><summary>Make explicit the requirement for intervals to be integers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pjcard</reporter><labels><label>docs</label></labels><created>2015-06-15T08:40:17Z</created><updated>2015-06-15T09:42:36Z</updated><resolved>2015-06-15T09:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pjcard" created="2015-06-15T08:40:43Z" id="111979734">I have just signed the CLA. Perhaps there's a delay?
</comment><comment author="clintongormley" created="2015-06-15T09:42:36Z" id="111999442">thanks @pjcard - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11661 from pjcard/patch-1</comment></comments></commit></commits></item><item><title>Add detail response support for _analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11660</link><project id="" key="" /><description>Add detail option for `_analyze` API.

This PR add these function to `_analyze` API
- output all Token's attributes
  - can specify attribute name with `attributes` parameter
  - &lt;strike&gt;can output short attribute name with `shor_attr_name` parameter&lt;/strike&gt;
- output token array after each steps, that are charfilters, tokenizer, tokenfilters

Closes #11076
</description><key id="88348552">11660</key><summary>Add detail response support for _analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-06-15T07:45:19Z</created><updated>2015-12-14T10:46:53Z</updated><resolved>2015-12-10T16:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-06-19T16:27:12Z" id="113565531">Request example:

``` json
GET localhost:9200/_analyze?pretty
{
  "text": "&lt;text&gt;This is troubled&lt;/text&gt;",
  "char_filters": ["html_strip"], 
  "tokenizer": "standard", 
  "filters": ["snowball"], 
  "detail": true, 
  "short_attr_name": true, 
  "attributes": ["KeywordAttribute"]
}
```

Response example: 

``` json
{
  "detail" : {
    "custom_analyzer" : true,
    "charfilters" : [ {
      "name" : "html_strip",
      "filtered_text" : [ "\nThis is troubled\n" ]
    } ],
    "tokenizer" : {
      "name" : "standard",
      "tokens" : [ {
        "token" : "This",
        "start_offset" : 6,
        "end_offset" : 10,
        "type" : "&lt;ALPHANUM&gt;",
        "position" : 0,
        "attributes" : { }
      }, {
        "token" : "is",
        "start_offset" : 11,
        "end_offset" : 13,
        "type" : "&lt;ALPHANUM&gt;",
        "position" : 1,
        "attributes" : { }
      }, {
        "token" : "troubled",
        "start_offset" : 14,
        "end_offset" : 22,
        "type" : "&lt;ALPHANUM&gt;",
        "position" : 2,
        "attributes" : { }
      } ]
    },
    "tokenfilters" : [ {
      "name" : "snowball",
      "tokens" : [ {
        "token" : "This",
        "start_offset" : 6,
        "end_offset" : 10,
        "type" : "&lt;ALPHANUM&gt;",
        "position" : 0,
        "attributes" : {
          "KeywordAttribute" : {
            "keyword" : false
          }
        }
      }, {
        "token" : "is",
        "start_offset" : 11,
        "end_offset" : 13,
        "type" : "&lt;ALPHANUM&gt;",
        "position" : 1,
        "attributes" : {
          "KeywordAttribute" : {
            "keyword" : false
          }
        }
      }, {
        "token" : "troubl",
        "start_offset" : 14,
        "end_offset" : 22,
        "type" : "&lt;ALPHANUM&gt;",
        "position" : 2,
        "attributes" : {
          "KeywordAttribute" : {
            "keyword" : false
          }
        }
      } ]
    } ]
  }
}
```
</comment><comment author="dadoonet" created="2015-07-02T08:41:28Z" id="117959743">@johtani I left comments. Hope this helps!
</comment><comment author="johtani" created="2015-07-02T16:38:49Z" id="118088642">@dadoonet Thanks for your comment. This was helpful for me especially about English:)
I've updated the PR with your comments.
And I found and fixed the bug about the order of tokenfilters/charfilters in the response.
Could you take another look?
</comment><comment author="johtani" created="2015-07-03T02:13:17Z" id="118211495">Add the example of response to document
</comment><comment author="dadoonet" created="2015-07-03T06:44:56Z" id="118252084">I left some other comments. 
</comment><comment author="clintongormley" created="2015-07-12T11:29:19Z" id="120709558">@johtani any progress here?
</comment><comment author="johtani" created="2015-07-13T05:31:14Z" id="120818715">@clintongormley I need some other reviews for this. https://github.com/elastic/elasticsearch/pull/11660#discussion_r33843116 
</comment><comment author="bleskes" created="2015-07-16T09:42:40Z" id="121903750">I replied to the comment.

&gt; On 13 Jul 2015, at 07:31, Jun Ohtani notifications@github.com wrote:
&gt; 
&gt; @clintongormley I need some other reviews for this. #11660 (comment)
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="johtani" created="2015-07-24T07:44:32Z" id="124396801">Thanks comments, @dadoonet @bleskes 
Fix comments.
</comment><comment author="dadoonet" created="2015-08-07T08:25:51Z" id="128639434">I left a small last comment. Not sure if you want to fix it. LGTM
</comment><comment author="johtani" created="2015-08-07T10:51:35Z" id="128671663">@dadoonet Agreed. Push fixing
</comment><comment author="dadoonet" created="2015-08-07T11:38:01Z" id="128680387">LGTM
</comment><comment author="johtani" created="2015-08-19T03:29:51Z" id="132433236">@rjernst @mikemccand @jpountz **if you have time** and interested in reviewing this, please review this. I need a second reviewer. 
</comment><comment author="johtani" created="2015-09-29T13:42:47Z" id="144063422">@rjernst Thanks for your comments. I pushed only fixing "removing classname". I think it is easy to review what is different. If it is OK, then I will rebase and fix checking version.
</comment><comment author="dadoonet" created="2015-11-23T11:52:52Z" id="158913810">@johtani What is the status on this? Do you need a final review? 

May be you should squash and rebase on master branch? Master branch changed a lot in the meantime.

For example `rest-api-spec/test/indices.analyze/10_analyze.yaml` should probably move now to `rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml`.
</comment><comment author="johtani" created="2015-11-24T06:35:58Z" id="159170287">@dadoonet Ah, thank you for comment. I squashed and rebased on master branch.
I need a final review!
</comment><comment author="dadoonet" created="2015-11-24T07:12:23Z" id="159179444">I left some comments 
</comment><comment author="johtani" created="2015-11-24T08:45:12Z" id="159197363">@dadoonet Thanks for reviewing. I pushed fixing.
</comment><comment author="dadoonet" created="2015-11-27T11:32:02Z" id="160120027">LGTM
</comment><comment author="johtani" created="2015-11-30T02:27:23Z" id="160500168">Thanks @dadoonet , I will merge master and v2.2.0
</comment><comment author="colings86" created="2015-11-30T08:35:57Z" id="160554668">@johtani I left a few comments. I also wonder if it would be more consistent with other APIs if we called the option `explain` rather than `detail` since it explains the analysis chain/output?

@clintongormley what do you think?
</comment><comment author="clintongormley" created="2015-11-30T09:19:06Z" id="160567450">++ for `explain`
</comment><comment author="johtani" created="2015-12-01T03:16:54Z" id="160836826">@colings86 Thanks for your comment! I will change `explain` and fixing your comment.
</comment><comment author="johtani" created="2015-12-09T13:27:45Z" id="163231148">@colings86 Changed `detail` param to `explain` and fixed your comment.
Could you review again?
</comment><comment author="colings86" created="2015-12-09T15:06:14Z" id="163283581">@johtani I left one minor comment but LGTM
</comment><comment author="johtani" created="2015-12-10T16:14:28Z" id="163673908">Closed by master : fab44398d9d48f12319bc018d4b436f723b6508e 
2.x : f427cf4ae459b864894c8ba1a3eff589ad5248a0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade AWS dependency to 1.10.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11659</link><project id="" key="" /><description /><key id="88345955">11659</key><summary>Upgrade AWS dependency to 1.10.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T07:31:33Z</created><updated>2015-06-18T19:12:31Z</updated><resolved>2015-06-18T12:05:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-15T08:12:55Z" id="111973816">Release note is here: http://aws.amazon.com/releasenotes/6413123902727197

Note that it comes with:
- Apache Httpclient 4.3.6 (we are using 4.3.5)
- Jackson 2.5.3 (same)
- Joda-Time 2.8 (we are using 2.7 - patched version)

I think we should also upgrade elasticsearch core with recent httpclient and joda BTW. (but this is outside of this PR scope)

And then remove non needed libs in plugins. See related discussion at #11647.

LGTM
</comment><comment author="s1monw" created="2015-06-16T18:29:11Z" id="112521954">@dadoonet I pushed upgrades for those libs too...
</comment><comment author="dadoonet" created="2015-06-16T19:27:35Z" id="112537939">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make explicit the requirement for intervals to be integers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11658</link><project id="" key="" /><description>The rounding function implies this, but this has still caught some people out:
https://github.com/elastic/elasticsearch/issues/4847#issuecomment-111540770
</description><key id="88345410">11658</key><summary>Make explicit the requirement for intervals to be integers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-06-15T07:27:22Z</created><updated>2015-06-15T08:29:40Z</updated><resolved>2015-06-15T08:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-15T08:18:42Z" id="111974600">thanks for the PR @pjcardy. Please could you sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="ghost" created="2015-06-15T08:29:21Z" id="111976119">I've just realised I've somehow put 'bee' instead of 'be'. I'm in the middle of merging my two accounts, so I'll finish that now and submit a fixed PR from the merged account (with which I'll sign the CLA). Cheers.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log cluster health state (green/yellow/red) change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11657</link><project id="" key="" /><description>If we go from one state to another we should explicitly log this.

If not at info/warn, even though that'd be _super dooper handy_, at least at trace/debug.
</description><key id="88312488">11657</key><summary>Log cluster health state (green/yellow/red) change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Logging</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-06-15T04:33:51Z</created><updated>2015-11-16T18:03:44Z</updated><resolved>2015-11-16T14:16:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-15T09:13:39Z" id="111988232">We do already, as DEBUG:

```
[2015-06-15 11:13:16,339][DEBUG][cluster.service          ] [Gladiator] set local cluster state to version 10
```
</comment><comment author="bleskes" created="2015-06-15T09:17:27Z" id="111988799">few more handy tips: 

debug logging also gives you the reason for the change - we try to give it good names. He it says it got a new cluster state from the master.

```
1&gt; [2015-06-14 04:51:45,068][DEBUG][cluster.service          ] [node_t1] processing [zen-disco-receive(from master [[node_t0][UUL8MVMFT1iz_2G5po6Haw][hotel][local[node_0]]{enable_custom_paths=true, mode=local}])]: execute

```

if you set the logging level of `cluster.service` to `TRACE`  you will get the complete cluster state with every change. Note though that it can be huge and so use with care.
</comment><comment author="markwalkom" created="2015-06-16T03:46:06Z" id="112273803">Sorry, by state I meant green/yellow/red :)
</comment><comment author="danielmitterdorfer" created="2015-11-03T07:46:40Z" id="153281325">I've looked into this and thought how it could be implemented. The problem is that currently the cluster health status is calculated on a variety of parameters (see `ClusterHealthResponse`). The cluster health status is only evaluated when a cluster health REST request is handled. So this calculation is triggered by some user action but not based on an event from within ES itself currently.

So we have basically two problems to solve:
1. Where do we calculate the cluster health status?
2. How to recognize cluster state changes in general?

Regarding (1), I can think of different approaches:
- We could precalculate the cluster health status and put it into `ClusterState`. However, I am not really convinced that this a good solution as it increases the size of `ClusterState` further.
- We could factor out the common logic and recalculate it every time it's needed (i.e. on state change and when handling the before-mentioned REST request).

Regarding (2) I think we'd need to implement a `ClusterStateListener` which logs the message.

Apart from that, the old cluster health status could be kept around to suppress logging when the cluster health status did not change.

What do think @clintongormley, @bleskes?
</comment><comment author="bleskes" created="2015-11-03T13:38:03Z" id="153355021">I'm +1 on have a generic method to calculate the cluster status (it is fairly encapsulated in ClusterHealthResponse ). Note that status change only happens after specific events. Going from green to yellow to red happens on shard failures and nodes leaving the cluster. Going from red to yellow to green happens on shard started event. We can just do the calculation there and do some logging there (including the reason / what have changed).
</comment><comment author="danielmitterdorfer" created="2015-11-03T14:10:42Z" id="153366874">Thanks for your thoughts. I'll give it a shot and ping you.
</comment><comment author="nik9000" created="2015-11-03T14:30:06Z" id="153371535">&gt; We can just do the calculation there and do some logging there (including the reason / what have changed).

I wonder if we could go so far as to _always_ log something at those events and raise the log level if the cluster state changed. Something like

``` java
public static void logClusterStateChangeEvent(String eventMessage) {
  ClusterHealth oldHealth = fetchOldHealth();
  ClusterHealth newHealth = calculateNewHealth();
  if (oldHealth != newHealth) {
    logger.info("{} and cluster state changed from {} to {}", eventMessage, oldHealth, newHealth);
  } else {
    logger.debug("{} and cluster state stayed {}", eventMessage, oldHealth);
  }
}
```

That way if you switched to DEBUG level logging you'd see this stream of events and it'd be really obvious which one might impact the cluster state. I kinda figure we already have logging at these events so it'd be a matter of hunting them down.
</comment><comment author="bleskes" created="2015-11-03T14:31:18Z" id="153371845">@nik9000 I think we can defiantly do better but watch out with logging cluster states. They can be _huge_.
</comment><comment author="nik9000" created="2015-11-03T14:32:10Z" id="153372055">&gt; @nik9000 I think we can defiantly do better but watch out with logging cluster states. They can be huge.

Oh I didn't mean the whole thing! I just meant the health. Yeah. I know full well that they can take up megabytes of text.....
</comment><comment author="nik9000" created="2015-11-03T14:33:03Z" id="153372258">&gt; Oh I didn't mean the whole thing! I just meant the health. Yeah. I know full well that they can take up megabytes of text.....

And I've fixed the comment so it looks like what I meant the first time around.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterHealthStatus.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterIndexHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterShardHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/health/ClusterStateHealth.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java</file><file>core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIT.java</file><file>core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java</file><file>core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterIndexHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/ClusterStateHealthTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/health/RoutingTableGenerator.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationPriorityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestCase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ExpectedShardSizeAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/QuorumGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java</file><file>core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java</file><file>core/src/test/java/org/elasticsearch/indices/settings/UpdateNumberOfReplicasIT.java</file><file>core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/state/SimpleIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisIT.java</file><file>core/src/test/java/org/elasticsearch/search/nested/SimpleNestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/preference/SearchPreferenceIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java</file><file>core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/tribe/TribeIT.java</file><file>test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file><file>test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java</file></files><comments><comment>Log cluster health status changes</comment></comments></commit></commits></item><item><title>Updated groovy docs link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11656</link><project id="" key="" /><description>The previous link we had was out of date.

Closes #11655
</description><key id="88311584">11656</key><summary>Updated groovy docs link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-06-15T04:27:08Z</created><updated>2015-06-16T03:46:26Z</updated><resolved>2015-06-15T09:17:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-15T04:28:00Z" id="111918692">LGTM
</comment><comment author="s1monw" created="2015-06-15T07:14:38Z" id="111962593">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Updated groovy docs link</comment></comments></commit></commits></item><item><title>Groovy docs link is bad</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11655</link><project id="" key="" /><description>We're pointing to http://groovy.codehaus.org/ when it should be http://groovy-lang.org/
</description><key id="88311526">11655</key><summary>Groovy docs link is bad</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-06-15T04:26:55Z</created><updated>2015-06-15T15:37:22Z</updated><resolved>2015-06-15T15:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-15T15:37:21Z" id="112112875">Closed in #11656
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shards in newly created dummy indices are not allocating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11654</link><project id="" key="" /><description>Hi,

I created an index using command
curl -XPUT http://135.250.193.206:9200/some-index

It doesn't get allocated. We are unable to insert data as it throws UnavailableShardsException

I have tried to set the allocation using the below command but the issue persists. 

curl -XPUT 'localhost:9200/_cluster/settings' -d '{
    "transient" : {
        "cluster.routing.allocation.enable" : "all"
    }
}'

I am using ES version 1.4.4
</description><key id="88306857">11654</key><summary>Shards in newly created dummy indices are not allocating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-06-15T03:57:15Z</created><updated>2015-06-15T07:17:03Z</updated><resolved>2015-06-15T07:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-15T04:16:22Z" id="111916645">Do you have enough disk space? Have you checked your ES logs?

However please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="nithyanv" created="2015-06-15T04:17:39Z" id="111916730">Hi, 

I have 78% diskspace. 

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   277  100   277    0     0   8656      0 --:--:-- --:--:-- --:--:-- 17312
{
  "cluster_name" : "devteam",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 217,
  "active_shards" : 217,
  "relocating_shards" : 0,
  "initializing_shards" : 4,
  "unassigned_shards" : 239
}
</comment><comment author="s1monw" created="2015-06-15T07:17:02Z" id="111963111">please use the discuss list for those kind of questions as @markwalkom  suggested

&gt; However please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Unassigned meta data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11653</link><project id="" key="" /><description>Unassigned meta includes additional information as to why a shard is unassigned, this is especially handy when a shard moves to unassigned due to node leaving or shard failure.

The additional data is provided as part of the cluster state, and as part of `_cat/shards` API.

The additional meta includes the timestamp that the shard has moved to unassigned, allowing us in the future to build functionality such as delay allocation due to node leaving until a copy of the shard is found.
</description><key id="88289930">11653</key><summary>Add Unassigned meta data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-15T02:13:09Z</created><updated>2015-06-18T12:45:15Z</updated><resolved>2015-06-16T15:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-15T07:14:06Z" id="111962372">left some minor comments mainly structural... good change!
</comment><comment author="clintongormley" created="2015-06-15T09:23:58Z" id="111990217">Awesome change, but it's missing documentation?
</comment><comment author="bleskes" created="2015-06-15T09:55:32Z" id="112001623">+1  - left a couple of minor comments, plus a suggestion to split the Reason.NEW into "new from API" and recovered.
</comment><comment author="kimchy" created="2015-06-15T15:35:53Z" id="112112503">I implemented the first round of feedback, biggest change is several additional reasons for unassigned (quite a few actually :) ), and now its verified that when a shard is created unassigned, we provide the meta data for it
</comment><comment author="kimchy" created="2015-06-15T16:03:23Z" id="112120423">also, would love to hear if the names I came up with for `Reason` make sense (https://github.com/elastic/elasticsearch/pull/11653/files#diff-d0e0ede81cd92c4e0b6b707b254f5fcaR45)
</comment><comment author="bleskes" created="2015-06-16T11:36:26Z" id="112392501">LGTM. Left some minor suggestions. 
</comment><comment author="s1monw" created="2015-06-16T11:51:21Z" id="112396838">this also LGTM the only thing that I really don't like is the `UnassignedMeta` name. everything that has `Meta` attached to it is like a smell to it that it rather belongs somewhere else. I think we should rename the class to something like `UnassignmentEvent` or something like this or maybe `UnassignedTrigger` something that makes it clear that we are tracking the cause of the unassignemnet.
</comment><comment author="kimchy" created="2015-06-16T11:57:38Z" id="112399338">pushed the first round of better names (not yet for UnassignedMeta), and addressed comments
</comment><comment author="bleskes" created="2015-06-16T12:08:23Z" id="112402623">Namings LGTM. My vote goes to UnassignmentInfo (I like unassignment as it doesn't suggest it is unassigned now, Meta and Info are the same imho). I don't feel strongly about this and I'm good with all of the above as well.
</comment><comment author="s1monw" created="2015-06-16T12:38:36Z" id="112409667">ok with `UnassignmentInfo` or `UnassignmentEvent` but I am leaning towards the latter
</comment><comment author="kimchy" created="2015-06-16T14:49:10Z" id="112457698">renamed to UnassignedInfo, I think its the best suggestion so far, will push soonish
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java</file><file>core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>core/src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file></files><comments><comment>Add Unassigned meta data</comment><comment>Unassigned meta includes additional information as to why a shard is unassigned, this is especially handy when a shard moves to unassigned due to node leaving or shard failure.</comment></comments></commit></commits></item><item><title>Script_File support for NEST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11652</link><project id="" key="" /><description>Hi,

ElasticSearch supports referencing a script which is stored in a file. Eg:-
GET /_search
{
    "script_fields": {
        "my_field": {
            "script_file": "my_script",
            "params": {
              "my_var": 2
            }
        }
    }
}

However the .NET NEST client does not have any support for this yet. Is this going to be added soon?

https://github.com/elastic/elasticsearch-net/blob/4fcecae93dcf16a4ff7e110135da81eed409dda5/src/Nest/DSL/Filter/ScriptFilterDescriptor.cs#L36

Thanks,
Vineet
</description><key id="88241856">11652</key><summary>Script_File support for NEST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels /><created>2015-06-14T20:15:48Z</created><updated>2015-06-14T20:22:41Z</updated><resolved>2015-06-14T20:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-14T20:22:41Z" id="111873462">This issue was moved to elastic/elasticsearch-net#1456
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"plugin" script not named in a linux-friendly manner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11651</link><project id="" key="" /><description>I'd like to see the script named "es-plugin" or something similar so that it doesn't overlap with other programs that also have a plugin system (like LogStash, who also has a script named "plugin").

This would be particularly useful for the RPMs which, if I understand what's going on correctly, install "plugin" to /usr/share/bin.

I can submit a pull request if that helps speed things up.
</description><key id="88201523">11651</key><summary>"plugin" script not named in a linux-friendly manner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">proegssilb</reporter><labels><label>:Plugins</label></labels><created>2015-06-14T15:25:26Z</created><updated>2015-06-23T18:08:50Z</updated><resolved>2015-06-23T18:08:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T18:08:49Z" id="114593841">Closing in favour of #11797
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Number fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11650</link><project id="" key="" /><description>I found this explanation about my issue:

http://elasticsearch-users.115913.n3.nabble.com/Number-Format-Exception-td1474946.html

&gt; Ahh, yes, this is a limitation in elasticsearch. Basically, all types are broken down into Lucene Documents, with their "typeness" added by elasticsearch. The limit to that is the fact that a field with the same name across different types _within the same index_ must be of the same type (long/integer/string). The problem does not exists between indices.

So is this going to change? Because why one type field is realted to another?
</description><key id="88153540">11650</key><summary>Number fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dopesong</reporter><labels /><created>2015-06-14T09:13:55Z</created><updated>2015-06-14T20:17:32Z</updated><resolved>2015-06-14T20:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-14T20:17:32Z" id="111872766">Hi @dopesong 

No it isn't going to change. In fact we're getting stricter about enforcing this restriction.  See https://github.com/elastic/elasticsearch/issues/8870

A blog post will be coming soon to explain the problems, and the changes we've made to resolve them.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify Java requirements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11649</link><project id="" key="" /><description>Looks like the getting started and setup pages have slightly different wording about the Java requirements, which might cause some confusion (e.g. as reported on [Gentoo bug 551880](https://bugs.gentoo.org/show_bug.cgi?id=551880#c0)). I believe this small commit helps clarifying the situation.
</description><key id="88148664">11649</key><summary>Clarify Java requirements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ferki</reporter><labels /><created>2015-06-14T08:42:35Z</created><updated>2015-06-14T20:14:29Z</updated><resolved>2015-06-14T20:14:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-14T20:14:22Z" id="111872413">Thanks @ferki - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11649 from adjust/jdk_docs</comment></comments></commit></commits></item><item><title>script debugging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11648</link><project id="" key="" /><description>Currently, it's impossible to debug scripts in ES. I see no user-end documentation on that.

Can we get tools created for those who use something more complex than "2+2" one-line scripts?
</description><key id="87984713">11648</key><summary>script debugging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">celesteking</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-06-13T11:02:30Z</created><updated>2016-01-18T20:27:32Z</updated><resolved>2016-01-18T20:27:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-13T12:13:29Z" id="111704897">The groovy console is a great tool for debugging groovy scripts: http://www.groovy-lang.org/groovyconsole.html

Any suggestions for what you'd like to see?
</comment><comment author="celesteking" created="2015-06-13T12:28:26Z" id="111706355">How do I load ES environment in groovy console? ctx and all that.

I'd like to have at least println() immlemented/documented or something that would aid with debugging a complex script. 
</comment><comment author="nik9000" created="2015-06-13T12:53:30Z" id="111708656">Inject a logger into the script context?
On Jun 13, 2015 8:28 AM, "celesteking" notifications@github.com wrote:

&gt; How do I load ES environment in groovy console? ctx and all that.
&gt; 
&gt; I'd like to have at least println() immlemented/documented or something
&gt; that would aid with debugging a complex script.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/11648#issuecomment-111706355
&gt; .
</comment><comment author="askaliuk" created="2015-06-19T19:24:14Z" id="113613422">Interested in the topic as well
</comment><comment author="tomasaftalion" created="2015-07-17T00:14:24Z" id="122135083">Me too.
</comment><comment author="clintongormley" created="2016-01-18T20:27:32Z" id="172645654">Closing in favour of #13084
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[build] include in plugins only needed jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11647</link><project id="" key="" /><description>Follow up for https://github.com/elastic/elasticsearch-analysis-kuromoji/issues/61

We don't shade anymore elasticsearch dependencies, so plugins might include jars in the distribution ZIP file which might not be needed anymore.

For example, `elasticsearch-cloud-aws` comes with:

```
Archive:  cloud-aws/target/releases/elasticsearch-cloud-aws-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
  1920788  05-18-15 09:42   aws-java-sdk-ec2-1.9.34.jar
   503963  05-18-15 09:42   aws-java-sdk-core-1.9.34.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   915096  01-19-15 09:24   jackson-databind-2.3.2.jar
   252288  05-18-15 09:42   aws-java-sdk-kms-1.9.34.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
    35058  01-19-15 09:24   jackson-annotations-2.3.0.jar
   229998  05-29-15 12:28   jackson-core-2.5.3.jar
   589289  01-19-15 09:24   joda-time-2.7.jar
   562858  05-18-15 09:42   aws-java-sdk-s3-1.9.34.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
    44854  06-12-15 19:22   elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar
 --------                   -------
  6221815                   13 files
```

A lot of those files are already distributed with elasticsearch itself so classes are available within the classloader.

That's leading me to another question. We might now hit incompatibility issues. Let say we upgrade jackson to 3.0.0 in elasticsearch but aws sdk requires 2.x version. We might have a clash. @s1monw @rmuir WDYT?
That might not happen though.
</description><key id="87968671">11647</key><summary>[build] include in plugins only needed jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>blocker</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-13T09:16:20Z</created><updated>2015-07-04T16:51:41Z</updated><resolved>2015-07-01T19:37:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-13T09:19:44Z" id="111690306">Adding here an interesting article about building artifacts with assembly plugin http://books.sonatype.com/mvnref-book/reference/assemblies-sect-controlling-contents.html
</comment><comment author="rmuir" created="2015-06-13T14:14:03Z" id="111712856">&gt; A lot of those files are already distributed with elasticsearch itself so classes are available within the classloader.

thats why i say, list _exactly_ what we are putting in our zips. no stupid maven transient dependencies: it does not work here anyway!!!! 

And for things like tracking licensing, knowing exactly what is going into the zips is a must-have anyway.
</comment><comment author="dadoonet" created="2015-06-15T11:09:14Z" id="112019587">I found a configuration for analysis plugins which does what we expect.

src/main/assembly/plugins.xml

``` xml
&lt;?xml version="1.0"?&gt;
&lt;assembly&gt;
    &lt;id&gt;plugin&lt;/id&gt;
    &lt;formats&gt;
        &lt;format&gt;zip&lt;/format&gt;
    &lt;/formats&gt;
    &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt;
    &lt;dependencySets&gt;
        &lt;dependencySet&gt;
            &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
            &lt;useProjectArtifact&gt;true&lt;/useProjectArtifact&gt;
            &lt;useTransitiveFiltering&gt;true&lt;/useTransitiveFiltering&gt;
            &lt;useTransitiveDependencies&gt;true&lt;/useTransitiveDependencies&gt;
            &lt;excludes&gt;
                &lt;exclude&gt;org.elasticsearch:elasticsearch&lt;/exclude&gt;
            &lt;/excludes&gt;
        &lt;/dependencySet&gt;
    &lt;/dependencySets&gt;
&lt;/assembly&gt;
```

Test script:

``` sh
#!/bin/bash

for i in * ; do
  if [ -d "$i" ]; then
    echo "**** $i"
    cd $i
    mvn clean install -DskipTests 1&gt;&gt; ../make.logs
    unzip -l target/releases/elasticsearch-$i-2.0.0-SNAPSHOT.zip
    cd ..
  fi
done
```

It gives:

```
**** analysis-icu
Archive:  target/releases/elasticsearch-analysis-icu-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
    78854  06-11-15 14:52   lucene-analyzers-icu-5.2.0.jar
 11126867  12-03-14 18:48   icu4j-54.1.jar
    23791  06-15-15 12:47   elasticsearch-analysis-icu-2.0.0-SNAPSHOT.jar
 --------                   -------
 11229512                   3 files
**** analysis-kuromoji
Archive:  target/releases/elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
  4598265  06-11-15 14:52   lucene-analyzers-kuromoji-5.2.0.jar
    20858  06-15-15 12:47   elasticsearch-analysis-kuromoji-2.0.0-SNAPSHOT.jar
 --------                   -------
  4619123                   2 files
**** analysis-phonetic
Archive:  target/releases/elasticsearch-analysis-phonetic-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
   284184  11-25-14 11:46   commons-codec-1.10.jar
    15810  06-15-15 12:47   elasticsearch-analysis-phonetic-2.0.0-SNAPSHOT.jar
    26152  06-11-15 14:52   lucene-analyzers-phonetic-5.2.0.jar
 --------                   -------
   326146                   3 files
**** analysis-smartcn
Archive:  target/releases/elasticsearch-analysis-smartcn-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
  3603150  06-11-15 14:52   lucene-analyzers-smartcn-5.2.0.jar
    12121  06-15-15 12:47   elasticsearch-analysis-smartcn-2.0.0-SNAPSHOT.jar
 --------                   -------
  3615271                   2 files
**** analysis-stempel
Archive:  target/releases/elasticsearch-analysis-stempel-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
    10452  06-15-15 12:48   elasticsearch-analysis-stempel-2.0.0-SNAPSHOT.jar
   517989  06-11-15 14:52   lucene-analyzers-stempel-5.2.0.jar
 --------                   -------
   528441                   2 files
```

What I like with this approach is that we can share the same configuration file accros multiple plugins (may be all of them).

When running that on other plugins (with same assembly descriptor), I get:

```
**** cloud-aws
Archive:  target/releases/elasticsearch-cloud-aws-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
   503963  05-18-15 09:42   aws-java-sdk-core-1.9.34.jar
   915096  01-19-15 09:24   jackson-databind-2.3.2.jar
   252288  05-18-15 09:42   aws-java-sdk-kms-1.9.34.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
    35058  01-19-15 09:24   jackson-annotations-2.3.0.jar
   589289  01-19-15 09:24   joda-time-2.7.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
   229998  05-29-15 12:28   jackson-core-2.5.3.jar
   562858  05-18-15 09:42   aws-java-sdk-s3-1.9.34.jar
    44854  06-15-15 12:48   elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar
  1920788  05-18-15 09:42   aws-java-sdk-ec2-1.9.34.jar
 --------                   -------
  6221815                   13 files
**** cloud-azure
Archive:  target/releases/elasticsearch-cloud-azure-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
   662947  02-04-15 16:33   azure-storage-2.0.0.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   465649  10-30-14 13:57   jersey-core-1.13.jar
    18323  10-30-14 13:57   jackson-jaxrs-1.9.2.jar
    27075  10-30-14 13:57   jackson-xc-1.9.2.jar
    96737  02-05-15 11:45   azure-management-0.7.0.jar
   229998  05-29-15 12:28   jackson-core-2.5.3.jar
   149432  02-04-15 16:28   azure-core-0.7.0.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
     2497  10-25-14 15:13   javax.inject-1.jar
   131269  10-30-14 13:48   jersey-client-1.13.jar
    67758  10-30-14 13:54   jettison-1.1.jar
   890168  10-30-14 13:55   jaxb-impl-2.2.3-1.jar
    25689  10-31-14 13:19   slf4j-api-1.6.2.jar
   634271  02-04-15 17:23   azure-management-compute-0.7.0.jar
   508143  10-30-14 13:57   mail-1.4.5.jar
   765648  10-30-14 13:57   jackson-mapper-asl-1.9.2.jar
   412739  10-31-14 13:19   commons-lang3-3.3.2.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
    62983  10-25-14 15:13   activation-1.1.jar
   163417  10-30-14 13:54   jersey-json-1.13.jar
    26514  10-27-14 17:18   stax-api-1.0.1.jar
   105134  10-30-14 13:55   jaxb-api-2.2.2.jar
    23346  10-30-14 13:55   stax-api-1.0-2.jar
   228286  10-30-14 13:56   jackson-core-asl-1.9.2.jar
    48482  06-15-15 12:48   elasticsearch-cloud-azure-2.0.0-SNAPSHOT.jar
 --------                   -------
  6914128                   27 files
**** cloud-gce
Archive:  target/releases/elasticsearch-cloud-gce-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
   194265  04-24-15 12:25   google-api-client-1.20.0.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   286717  04-24-15 12:25   google-http-client-1.20.0.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
   466069  04-24-15 12:24   google-api-services-compute-v1-rev59-1.20.0.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
  1791384  04-24-15 12:25   guava-jdk5-13.0.jar
    61092  04-24-15 12:25   google-oauth-client-1.20.0.jar
    33015  10-25-14 15:12   jsr305-1.3.9.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
     6720  04-24-15 12:25   google-http-client-jackson2-1.20.0.jar
   229998  05-29-15 12:28   jackson-core-2.5.3.jar
    18219  06-15-15 12:49   elasticsearch-cloud-gce-2.0.0-SNAPSHOT.jar
 --------                   -------
  4255102                   13 files
```

Finding the dependencies to include/update looks as a nightmare to me and that's somehow what maven is good at. 
That said, too many libs are still included in cloud plugins. A global workaround would be to mark all elasticsearch core dependencies as `provided` in `plugins/pom.xml`. In that case, shared libs won't be included in the plugin distribution as maven will resolve that.

@rmuir I understand that you are not a fan of this and the reasons you are against this.
To me it's a tradeoff between `plugin.xml` complexity and the risk of missing something. It looks super easy for simple plugins like analysis but more complex for cloud plugins.
I can give a try to your solution though and see where is goes.
</comment><comment author="dadoonet" created="2015-06-15T11:11:08Z" id="112020138">Ha. I forgot to say that it works well for analysis plugins because I added in `plugins/pom.xml`:

``` xml
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
            &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
            &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
```
</comment><comment author="dadoonet" created="2015-06-15T11:17:35Z" id="112022573">And here is the result for lang plugins:

```
**** lang-javascript
Archive:  target/releases/elasticsearch-lang-javascript-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
  1134765  12-12-14 19:05   rhino-1.7R4.jar
    23725  06-15-15 13:16   elasticsearch-lang-javascript-2.0.0-SNAPSHOT.jar
 --------                   -------
  1158490                   2 files
**** lang-python
Archive:  target/releases/elasticsearch-lang-python-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
    10063  06-15-15 13:17   elasticsearch-lang-python-2.0.0-SNAPSHOT.jar
 37021723  05-27-15 22:59   jython-standalone-2.7.0.jar
 --------                   -------
 37031786                   2 files
```
</comment><comment author="dadoonet" created="2015-06-15T11:32:30Z" id="112026034">By explicitly marking elasticsearch core dependencies as provided, it reduced the number of libs of cloud plugins. We now have:

```
**** cloud-aws
Archive:  target/releases/elasticsearch-cloud-aws-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
  1920788  05-18-15 09:42   aws-java-sdk-ec2-1.9.34.jar
   503963  05-18-15 09:42   aws-java-sdk-core-1.9.34.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   562858  05-18-15 09:42   aws-java-sdk-s3-1.9.34.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
    35058  01-19-15 09:24   jackson-annotations-2.3.0.jar
   252288  05-18-15 09:42   aws-java-sdk-kms-1.9.34.jar
    44854  06-15-15 13:27   elasticsearch-cloud-aws-2.0.0-SNAPSHOT.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
  1143162  06-15-15 11:11   jackson-databind-2.5.3.jar
 --------                   -------
  5630594                   11 files
**** cloud-azure
Archive:  target/releases/elasticsearch-cloud-azure-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
   662947  02-04-15 16:33   azure-storage-2.0.0.jar
     2497  10-25-14 15:13   javax.inject-1.jar
   131269  10-30-14 13:48   jersey-client-1.13.jar
    67758  10-30-14 13:54   jettison-1.1.jar
    26514  10-27-14 17:18   stax-api-1.0.1.jar
   890168  10-30-14 13:55   jaxb-impl-2.2.3-1.jar
   149432  02-04-15 16:28   azure-core-0.7.0.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   163417  10-30-14 13:54   jersey-json-1.13.jar
   105134  10-30-14 13:55   jaxb-api-2.2.2.jar
    23346  10-30-14 13:55   stax-api-1.0-2.jar
   765648  10-30-14 13:57   jackson-mapper-asl-1.9.2.jar
   634271  02-04-15 17:23   azure-management-compute-0.7.0.jar
    62983  10-25-14 15:13   activation-1.1.jar
   465649  10-30-14 13:57   jersey-core-1.13.jar
    18323  10-30-14 13:57   jackson-jaxrs-1.9.2.jar
    27075  10-30-14 13:57   jackson-xc-1.9.2.jar
    96737  02-05-15 11:45   azure-management-0.7.0.jar
    48481  06-15-15 13:28   elasticsearch-cloud-azure-2.0.0-SNAPSHOT.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
   508143  10-30-14 13:57   mail-1.4.5.jar
   228286  10-30-14 13:56   jackson-core-asl-1.9.2.jar
 --------                   -------
  6245701                   24 files
**** cloud-gce
Archive:  target/releases/elasticsearch-cloud-gce-2.0.0-SNAPSHOT.zip
  Length     Date   Time    Name
 --------    ----   ----    ----
   466069  04-24-15 12:24   google-api-services-compute-v1-rev59-1.20.0.jar
   282269  10-31-14 13:19   httpcore-4.3.2.jar
    18217  06-15-15 13:28   elasticsearch-cloud-gce-2.0.0-SNAPSHOT.jar
   194265  04-24-15 12:25   google-api-client-1.20.0.jar
   590533  10-31-14 13:19   httpclient-4.3.5.jar
    61092  04-24-15 12:25   google-oauth-client-1.20.0.jar
   232771  01-19-15 09:24   commons-codec-1.6.jar
   286717  04-24-15 12:25   google-http-client-1.20.0.jar
    33015  10-25-14 15:12   jsr305-1.3.9.jar
    62050  01-19-15 09:24   commons-logging-1.1.3.jar
     6720  04-24-15 12:25   google-http-client-jackson2-1.20.0.jar
  1791384  04-24-15 12:25   guava-jdk5-13.0.jar
 --------                   -------
  4025102                   12 files
```

aws: 13 -&gt; 11
azure: 27 -&gt; 24
gce: 13 -&gt; 12
</comment><comment author="rmuir" created="2015-06-15T11:39:35Z" id="112028466">&gt; @rmuir I understand that you are not a fan of this and the reasons you are against this.
&gt; To me it's a tradeoff between plugin.xml complexity and the risk of missing something.

I think maven's transitive dependency mechanism is _broken_ here either way.

The way i see it, there are two choices:
1 manually maintain list of all transitive dependencies for elasticsearch/lucene/anything else ES brings in 
2 manually maintain list of all dependencies for each plugin.

Given those two choices, I argue that 2 is easier than 1. It is also easier to reason about: you specify exactly what should be in the zip, versus trying to navigate through tons of confusing maven dependencies to figure out why something that should not be there..

So i am not insisting on number 2... I just think its the less evil of the two choices. If there is a third choice that is simpler then this is not really relevant.
</comment><comment author="rmuir" created="2015-06-29T17:37:07Z" id="116770634">By the way @dadoonet if you have a PR for this, we should move forward. We can followup with improved ways to do it later. Today our plugins have duplicate jars (jar hell): I think its a big problem.

I'm gonna make this issue as a blocker.
</comment><comment author="rmuir" created="2015-06-29T17:38:27Z" id="116771044">and again, your method is fine, method number 1 versus method number 2 is not so important when today we have number 3: jar hell.
</comment><comment author="clintongormley" created="2015-06-29T18:48:40Z" id="116794478">@dadoonet any progress here?
</comment><comment author="dadoonet" created="2015-06-29T22:02:23Z" id="116860050">Not at this time. Focussing all efforts on distribution modules. Trying to fix remaining RPM issue.
</comment><comment author="rmuir" created="2015-06-30T00:29:56Z" id="116887223">I can try to implement the approach david took. First i am looking at jar hell, and maybe we can improve testing around this area once we have that (at least a script or something, try to load all plugins up...)
</comment><comment author="dadoonet" created="2015-06-30T08:47:58Z" id="117059853">@clintongormley @rmuir Good news. I think I'm done with the distribution stuff: https://github.com/elastic/elasticsearch/pull/11523

So I looked again at my WIP and pushed my branch here: https://github.com/dadoonet/elasticsearch/tree/plugins/reduce-analysis-size

I'll travelling today and hopefully will be able to push that forward and come with a PR.
Will update here.
</comment><comment author="dadoonet" created="2015-06-30T11:56:32Z" id="117147477">@rmuir I finally opened #11944. Was having some issues when I rebased my code with the license-checker as some jars are not provided anymore within the plugin artifact. :) 

Let me know what you think of it.
</comment><comment author="apatrida" created="2015-07-04T15:37:15Z" id="118523861">So is this creating a new dependency hell?  If we bring in ElasticSearch, say via Gradle, then we end up with all the versions it uses of many libraries all possible conflicting with other frameworks and even our application code.  All to get access to the client, or to run embedded ES.  And now to resolve it, the application has to shade things of its own to avoid unknown failures happening in ES which probably requires more specific and well tested versions of things  But on accident, if they don't know all this, their Gradle will change version numbers used to the most recent, causing unknown failures, silent or obvious.  And if they want to fix it, the only place they really can then is shading their application's usage, and hope that the somewhat-not-really-maintained-with-every-feature Gradle Shadow plugin can handle only shading the right things and not mucking up others.  And when you add that plugin, then you might have issues with Gradle Application plugin knowing to use a partial Uber JAR with some shaded things, then other JARs that were left untouched (if even possible), and whack-bam-fun-and-games you are in a new hell.  

up until now, ElasticSearch was one library you could include that didn't screw you over.  Now we get to play games with it and a longer list of dependencies!  

Obviously this issue was from the viewpoint of the creators of ES, and not from the viewpoint of using it as a library.  Or am I missing some magical benefit this provides?  I only see problems.  

btw, I think you broke some plugins on Master, cloud-was doesn't register s3 repos anymore.

```
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.s3.S3Repository
  while locating org.elasticsearch.repositories.Repository

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:140)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:368)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:55)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:110)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:378)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:209)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: org/apache/http/protocol/HttpContext
    at com.amazonaws.AmazonWebServiceClient.&lt;init&gt;(AmazonWebServiceClient.java:129)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:432)
    at com.amazonaws.services.s3.AmazonS3Client.&lt;init&gt;(AmazonS3Client.java:414)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.getClient(InternalAwsS3Service.java:153)
    at org.elasticsearch.cloud.aws.InternalAwsS3Service.client(InternalAwsS3Service.java:82)
    at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(S3Repository.java:125)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: org.apache.http.protocol.HttpContext
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 37 more

```
</comment><comment author="dadoonet" created="2015-07-04T16:48:52Z" id="118533375">@jaysonminard you can still use the shaded version of elasticsearch as we still provide it with the classifier `shaded`: https://github.com/elastic/elasticsearch/blob/master/core/pom.xml#L374

Would that help you?

Thank you for raising the S3 issue. We need to check that.
</comment><comment author="dadoonet" created="2015-07-04T16:51:41Z" id="118533488">I just opened #12034 for the last issue you reported.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[build] include in plugins only needed jars</comment></comments></commit></commits></item><item><title>Add '?' (one character) matching in index name wildcard patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11646</link><project id="" key="" /><description>Expands wildcard comparison patterns with `?`, that represents exactly one character. Uses Pattern and Matcher classes instead of previous custom implementation. Closes #8824
Does this feature make sense? I used Pattern and Matcher classes for this, with whitelisted `.` and `.*` sequences. I also have a custom matching implementation, but currently it doesn't work for some rare cases.
</description><key id="87930434">11646</key><summary>Add '?' (one character) matching in index name wildcard patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexkuk</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>review</label></labels><created>2015-06-13T04:53:36Z</created><updated>2015-11-21T22:14:45Z</updated><resolved>2015-11-21T22:14:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-13T09:43:25Z" id="111693235">Relates to #11334 and #11560
</comment><comment author="clintongormley" created="2015-06-26T09:33:58Z" id="115610787">This should probably only be used for index name matching only, because of bwc concerns with eg field names or aggregation names.

Also, would be better to keep this implemented as a state machine rather than using regular expressions (or possibly using Lucene regular expressions instead)
</comment><comment author="clintongormley" created="2015-11-21T22:14:45Z" id="158686200">Closed in favour of https://github.com/elastic/elasticsearch/pull/12209
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Per Shard Optimize API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11645</link><project id="" key="" /><description>Currently, the `_optimize` API exists to optimize an entire index by merging segments in the background. In general, it's key to merge all segments to 1 (`max_num_segments=1`). However, if you have a very high workload, then it can be a very slow and expensive operation. Worse, optimizing an index that has replicas requires that both the primary and replica(s) be optimized separately.

It would be great if we could provide an improved optimize API that allows further control:
- Optimize specific shards of an index
  - This would be particularly helpful with routing scenarios where shards could be tuned as-needed without impacting the rest of the shards (note: for time based indices, it's still optimal to optimize all of them, but it may be easier to wait for the remaining shards)
  - It also has the added benefit that you can better predict its impact, both in terms of file capacity _and_ processing. Since you may have multiple shards per node, it also avoids running them in parallel.
- Choose to optimize a specific shard (not only the shard, but which node if replica shards exist)
- Offer the ability to optimize a single shard, then copy the optimized shard rather than duplicating the effort on its primary/replica peer.
  - This would allow people to choose to spend a little bit of short lived memory and network bandwidth rather than the current overhead associated (processing, longer lived memory, and more IO).
</description><key id="87923472">11645</key><summary>Per Shard Optimize API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Index APIs</label><label>discuss</label></labels><created>2015-06-13T04:02:23Z</created><updated>2015-08-26T19:59:05Z</updated><resolved>2015-08-26T19:59:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-06-13T10:40:36Z" id="111697420">I don't think we should overengineer optimize in this way.

Maybe it is slow/cumbersome in lucene 4.x but time to start looking at lucene 5, its been out for a long time.
</comment><comment author="jpountz" created="2015-08-26T19:59:05Z" id="135154473">Closing for the reasons that Robert gave.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add equals/hashcode to fieldtypes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11644</link><project id="" key="" /><description>In order to restrict a single set of field type settings for a given
field name across an index, we need the ability to compare field types.
This change adds equals and hashcode, as well as tests for every field
type.
</description><key id="87881291">11644</key><summary>Add equals/hashcode to fieldtypes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T23:34:55Z</created><updated>2015-06-15T22:42:59Z</updated><resolved>2015-06-15T22:42:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-12T23:36:30Z" id="111641456">Note there were also some bugs found in GeoShapeFieldMapper that are fixed here. Notably the default strategy, tree level, and tree precision were not correctly serialized.
</comment><comment author="jpountz" created="2015-06-15T08:04:52Z" id="111972751">I left some minor comments but I like the PR in general, especially the careful unit tests.
</comment><comment author="nknize" created="2015-06-15T17:06:29Z" id="112141034">Geo field changes LGTM
</comment><comment author="rjernst" created="2015-06-15T17:56:20Z" id="112153217">@jpountz I pushed a new commit.
</comment><comment author="jpountz" created="2015-06-15T21:16:19Z" id="112214467">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/FieldTypeTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/MappedFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/ByteFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/CompletionFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/DateFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/DoubleFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/FloatFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/IntegerFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/LongFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/ShortFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/core/StringFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/AllFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/IdFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/IndexFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/ParentFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/RoutingFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/SourceFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/TimestampFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/TypeFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/UidFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/internal/VersionFieldTypeTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Merge pull request #11644 from rjernst/refactor/field-type-equality</comment></comments></commit></commits></item><item><title>Categorize index by size and make ClusterInfo available to all nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11643</link><project id="" key="" /><description>This commit makes the `ClusterInfo` object available to all nodes via a
custom ClusterState object. The cluster info contains the following
information:
1. Disk usage information about each node in the cluster.
2. Shard sizes for all primary shards in the cluster.
3. Relative classifications for the size of each index.

Each index is classified from SMALLEST to LARGEST based on its size,
with SMALL, MEDIUM, and LARGE falling equidistantly between smallest and
largest.

This information can be used in the future for determining the best data
path for a particular shard, enhancing the `DiskThresholdDecider`, or
making routing decidings for a shard based on its relative size in the
cluster. It can be accessed by using something like:

``` java
ClusterInfo info = clusterService.state().custom(ClusterInfo.TYPE);
Map&lt;String, DiskUsage&gt; nodeDiskUsage = info.getNodeDiskUsages();
Map&lt;ShardId, Long&gt; shardSizes = info.getShardSizes();
IndexClassification classification = info.getIndexClassification();
```

The `ClusterInfo` object is also available to inspect from the cluster
state HTTP endpoint, which returns a response like:

``` json
{
  ... other cluster state ...
  "cluster_info" : {
    "node_disk_usage" : {
      "nFlT3nFzQRyvixcYv6Yfuw" : {
        "free_bytes" : 37556461568,
        "used_bytes" : 23225233408
      }
    },
    "shard_size" : {
      "[test][4]" : 156,
      "[test][2]" : 156,
      "[test][3]" : 3007,
      "[foo][0]" : 127,
      "[wiki][0]" : 12728802,
      "[test2][4]" : 156,
      "[test2][3]" : 2904,
      "[wiki2][1]" : 23338909,
      "[test2][2]" : 156,
      "[test2][1]" : 156,
      "[test2][0]" : 156,
      "[test][0]" : 156,
      "[test][1]" : 156
    },
    "classifications" : {
      "test2" : "SMALLEST",
      "test" : "SMALLEST",
      "wiki2" : "LARGEST",
      "foo" : "SMALLEST",
      "wiki" : "MEDIUM"
    }
  }
}
```

Relates to work on #11185
</description><key id="87869636">11643</key><summary>Categorize index by size and make ClusterInfo available to all nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label></labels><created>2015-06-12T22:34:58Z</created><updated>2016-03-14T10:15:20Z</updated><resolved>2016-03-10T15:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-15T14:50:55Z" id="112095679">Thanks @dakrone, this new API looks great.

I don't really like this new index classification levels; it seems like it only hides information (the true size of a shard) behind labels whose meaning changes over time (what was once LARGE can later become SMALL), but maybe I don't understand its intended future use cases...

For #11185 I would just use the actual incoming shard size (if it's being relocated).
</comment><comment author="clintongormley" created="2016-03-10T11:03:09Z" id="194792428">@dakrone is this PR still relevant or can it be closed?
</comment><comment author="dakrone" created="2016-03-10T15:57:08Z" id="194920721">Yeah, it can be closed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reduce the size of the XContent parsing exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11642</link><project id="" key="" /><description>The content that caused the exception can be potentially very big and in most cases it's not very useful for debugging.
</description><key id="87862842">11642</key><summary>Reduce the size of the XContent parsing exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T21:51:29Z</created><updated>2015-06-13T09:36:45Z</updated><resolved>2015-06-13T00:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-12T22:11:53Z" id="111630701">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`moving_avg` forecasts should not include current point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11641</link><project id="" key="" /><description>- Forecasts should include all points up to, but not including, the current point. 
- Fixes some tests
- Removes the "Gap" tests, which have proven to be super fragile due to accumulated edge cases, and aren't even very useful anymore because the mockHisto stuff generates randomly sized gaps.
- Removes the concrete implementation of `predict()`, which makes things simpler / more intuitive
</description><key id="87838140">11641</key><summary>`moving_avg` forecasts should not include current point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T20:16:50Z</created><updated>2015-06-22T15:20:52Z</updated><resolved>2015-06-22T15:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-06-18T19:35:39Z" id="113267611">@uboness @colings86 Sorry for delay, fixes applied

As an aside, the work I've been doing on the optimizer is going to need a fair amount of change to the `next()` and `predict()` interfaces...the current setup just isn't going to be ergonomic enough to deal with an optimizer changing model parameters constantly.
</comment><comment author="colings86" created="2015-06-22T08:35:29Z" id="114039228">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java</file></files><comments><comment>Merge pull request #11641 from polyfractal/bugfix/movavg_predict</comment></comments></commit></commits></item><item><title>Revert "Removing top-level filter parameter from search API."</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11640</link><project id="" key="" /><description>Reverts elastic/elasticsearch#11600

Tests are failing so reverting PR for now. Need to re-run tests with `-Dtests.slow=true`
</description><key id="87808082">11640</key><summary>Revert "Removing top-level filter parameter from search API."</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels /><created>2015-06-12T18:10:58Z</created><updated>2015-08-26T17:35:21Z</updated><resolved>2015-06-12T18:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T17:29:04Z" id="135116677">@ycombinator Any plans to get back to it?
</comment><comment author="ycombinator" created="2015-08-26T17:35:21Z" id="135118561">@jpountz I'd love to but probably won't have time any more. So, no. Someone else can take this. Sorry :(
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file></files><comments><comment>Merge pull request #11640 from elastic/revert-11600-gh-8862</comment></comments></commit></commits></item><item><title>Unable to install plugin in several paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11639</link><project id="" key="" /><description>I would like to launch elasticsearch 1.6 on demand (not as service) inside a folder like

```
/Users/me/Documents/99. Project/my_shared/tools/elasticsearch-1.6.0
```

(Please, note DOT+WHITESPACE after 99 in the fourth level of the path)

ElasticSearch works fine, but there is a problem if I go inside bin folder to install a plugin; for example:

```
./plugin -install mobz/elasticsearch-head
```

The enlgish translation of the error is:

&gt;    Error: impossible to find or to load main class Project.my_shared.tools.elasticsearch-1.6.0.config

As you can see it starts reporting the path just after the WHITESPACE.

I'm using a MacOS system.
</description><key id="87807852">11639</key><summary>Unable to install plugin in several paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierpaolocira</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-06-12T18:10:14Z</created><updated>2016-01-18T20:25:38Z</updated><resolved>2016-01-18T20:25:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T20:25:37Z" id="172645321">This has been fixed in recent versions.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Include global parameters in the REST spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11638</link><project id="" key="" /><description>Currently, global parameters like `source` and `filter_path` aren't included in the spec since it's implied that they exist on all APIs, and so defining them on all endpoints becomes unnecessarily repetitive/redundant.

However, I think it's useful to keep a definition of them somewhere other than the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html), since after all...they are still part of the REST API.  This is especially useful for the .NET client, since we rely on the spec to generate the respective C# methods for each query parameter.  Without them defined, we have to modify our code generator to include them manually.

I'm not sure how this would fit into the current spec since we define query parameters on a per endpoint basis...but we could think of something.  Maybe a  single `_global.json` file?

Just a thought.  If others agree that this is a good idea, I'd be happy to devise some solution.
</description><key id="87797980">11638</key><summary>Include global parameters in the REST spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:REST</label><label>discuss</label></labels><created>2015-06-12T17:30:39Z</created><updated>2017-01-17T12:35:16Z</updated><resolved>2017-01-17T12:35:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T18:04:24Z" id="111576642">+1
</comment><comment author="javanna" created="2015-06-15T07:19:41Z" id="111963925">+1
</comment><comment author="clintongormley" created="2016-01-18T20:25:05Z" id="172645246">@gmarz do you still want this?
</comment><comment author="javanna" created="2017-01-11T19:36:38Z" id="271971376">I think we should do this, the `global.json` idea was ok with me. Does anybody at @elastic/es-clients have other ideas?</comment><comment author="gmarz" created="2017-01-11T19:46:26Z" id="271973917">I opened #22569.  I opted for `_global` rather than `global` to signal that it's not an actual endpoint.  Open to any suggestions.</comment><comment author="karmi" created="2017-01-12T08:22:54Z" id="272102255">What if, instead of `_global.json`, we had something like `_common.json` (or similar), which could list not only global URL parameters, but also other things like that in the future?</comment><comment author="javanna" created="2017-01-12T12:15:35Z" id="272148996">I have no opinion on global vs common, they both sound good to me.</comment><comment author="gmarz" created="2017-01-12T17:32:38Z" id="272228265">I'm also good with `_common.json`.  @karmi what other things do you see us adding in the future other than query params?</comment><comment author="karmi" created="2017-01-12T17:43:55Z" id="272231114">@gmarz, not really sure, probably just a case of my regular "just in case" condition :)</comment><comment author="gmarz" created="2017-01-12T20:40:57Z" id="272277238">@karmi 👍 I've updated the PR and renamed it to _common.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Include global query string parameters in the REST spec</comment></comments></commit></commits></item><item><title>[build] mark elasticsearch as provided in plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11637</link><project id="" key="" /><description>When we build a plugin, we suppose it will be executed within elasticsearch server.
So we should mark it as `provided`.

If a java developer needs to embed the plugin and elasticsearch, it will make sense to declare both in its `pom.xml` file.
</description><key id="87788366">11637</key><summary>[build] mark elasticsearch as provided in plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T16:58:31Z</created><updated>2015-06-16T07:32:34Z</updated><resolved>2015-06-16T07:31:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-12T16:59:35Z" id="111557099">@rmuir I started with this before working on reducing size of generated plugin artifacts. (PR coming)...
</comment><comment author="jpountz" created="2015-06-16T07:20:26Z" id="112318520">LGTM
</comment><comment author="s1monw" created="2015-06-16T07:20:47Z" id="112318660">LGTM 2
</comment><comment author="dadoonet" created="2015-06-16T07:21:12Z" id="112318814">:)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11637 from dadoonet/plugins/elasticsearch-provided</comment></comments></commit></commits></item><item><title>[doc] Move Smartcn documentation to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11636</link><project id="" key="" /><description>This commit moves `plugins/analysis-smartcn/README.md` file to `plugins/analysis-smartcn/src/main/asciidoc/index.asciidoc`.

It will be easier to integrate plugin documentation in our reference guide.
</description><key id="87783223">11636</key><summary>[doc] Move Smartcn documentation to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Smartcn</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T16:37:05Z</created><updated>2015-08-18T11:32:23Z</updated><resolved>2015-07-10T14:31:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-17T13:41:02Z" id="112803373">@clintongormley PR updated.
</comment><comment author="dadoonet" created="2015-07-10T14:31:52Z" id="120423284">Closing in favor of #12040 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>[doc] Move Phonetic documentation to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11635</link><project id="" key="" /><description>This commit moves `plugins/analysis-phonetic/README.md` file to `plugins/analysis-phonetic/src/main/asciidoc/index.asciidoc`.

It will be easier to integrate plugin documentation in our reference guide.
</description><key id="87781068">11635</key><summary>[doc] Move Phonetic documentation to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Phonetic</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T16:29:11Z</created><updated>2015-08-18T11:32:13Z</updated><resolved>2015-07-10T14:32:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T17:39:13Z" id="111570215">Same changes needed as for #11632
</comment><comment author="dadoonet" created="2015-06-17T13:37:53Z" id="112802175">@clintongormley Updated as well.
</comment><comment author="dadoonet" created="2015-07-10T14:32:02Z" id="120423319">Closing in favor of #12040 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>Simplify ShardRouting and centralize move to unassigned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11634</link><project id="" key="" /><description>Make sure there is a single place where shard routing move to unassigned, so we can add additional metadata when it does, also, simplify shard routing implementations a bit
</description><key id="87778167">11634</key><summary>Simplify ShardRouting and centralize move to unassigned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T16:19:47Z</created><updated>2015-06-12T20:52:12Z</updated><resolved>2015-06-12T20:52:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-12T16:31:54Z" id="111548538">LGTM
</comment><comment author="s1monw" created="2015-06-12T18:27:55Z" id="111581503">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/CatAllocationTestBase.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Simplify ShardRouting and centralize move to unassigned</comment><comment>Make sure there is a single place where shard routing move to unassigned, so we can add additional metadata when it does, also, simplify shard routing implementations a bit</comment><comment>closes #11634</comment></comments></commit></commits></item><item><title>snapshot restore "failed to parse repository source" </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11633</link><project id="" key="" /><description>![image](https://cloud.githubusercontent.com/assets/12627595/8133988/55656c0a-115e-11e5-8338-4dd92a4f7fba.png)
I want to revise the parameter "shard_per_node"
My ES　version is 1.4.4.I copied the eg from es.co.Because of my version?
</description><key id="87771372">11633</key><summary>snapshot restore "failed to parse repository source" </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Judyccb</reporter><labels /><created>2015-06-12T15:59:51Z</created><updated>2015-06-12T16:34:43Z</updated><resolved>2015-06-12T16:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-06-12T16:34:43Z" id="111550221">The ability to change index settings during restore was added in 1.5.0. It's not available in the version of elasticsearch that you are using. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[doc] Move ICU documentation to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11632</link><project id="" key="" /><description>This commit moves `plugins/analysis-icu/README.md` file to `plugins/analysis-icu/src/main/asciidoc/index.asciidoc`.

It will be easier to integrate plugin documentation in our reference guide.

@clintongormley follow up #11630 
</description><key id="87771353">11632</key><summary>[doc] Move ICU documentation to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T15:59:45Z</created><updated>2015-08-18T11:31:43Z</updated><resolved>2015-07-10T14:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T17:38:35Z" id="111570106">Also, please add the installation/removal/usage sections 
</comment><comment author="dadoonet" created="2015-06-17T13:31:37Z" id="112800102">PR updated @clintongormley 
</comment><comment author="dadoonet" created="2015-07-10T14:32:12Z" id="120423350">Closing in favor of #12040 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>Documentation: Search template docs use outdated syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11631</link><project id="" key="" /><description>The search syntax for pre-registered templates describes the deprecated syntax and not the new one: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#pre-registered-templates

The template name should now be passed in a root-level `file` parameter and the docs describe the old syntax where it is nested under a `template` parameter.
</description><key id="87766449">11631</key><summary>Documentation: Search template docs use outdated syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>bug</label><label>docs</label></labels><created>2015-06-12T15:40:22Z</created><updated>2015-06-15T09:27:08Z</updated><resolved>2015-06-15T08:13:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-06-15T08:13:51Z" id="111973946">The link references 1.6 branch. These changes were made in master only (v2.0). The following link (the master version of the same doc) looks to have correct syntax already:

https://www.elastic.co/guide/en/elasticsearch/reference/master/search-template.html#pre-registered-templates
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[doc] Move kuromoji documentation to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11630</link><project id="" key="" /><description>This commit moves `plugins/analysis-kuromoji/README.md` file to `plugins/analysis-kuromoji/src/main/asciidoc/index.asciidoc`.

It will be easier to integrate plugin documentation in our reference guide.

@clintongormley I propose that I first adapt documentation for all plugins using this structure. Then if we decide to have all docs within the same dir, that will be easy to move.
I think that if we keep documentation under each module, we can bring in the maven asciidoc plugin so it's super easy when you change the doc to see immediately all the changes. Just for checking of course because to build the actual reference doc, we use our own doc generation infra.

WDYT?
</description><key id="87759149">11630</key><summary>[doc] Move kuromoji documentation to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Kuromoji</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T15:18:04Z</created><updated>2015-08-18T11:31:54Z</updated><resolved>2015-07-10T14:32:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T17:28:09Z" id="111567046">Hi @dadoonet 

I think that all the docs need to be under the root `docs/` directory, otherwise we'll end up rebuilding everything whenever there is any commit.  

I don't think we should use the asciidoctor plugin as we have made a number of customizations to asciidoc for our purposes, and asciidoctor is a completely different code base.  Building the docs is just one command.  If you want to make it a maven target, why not just add a call to build_docs.pl to the pom.  That just requires you to have build_docs.pl in your $PATH.

After chatting to @skearns64 we're planning on moving plugins out of the reference docs and adding an Elasticsearch Plugins under the Elasticsearch section of the guide homepage. The "book" would be divided into sections such as Discovery Plugins etc, would provide the docs for each core plugin, and a page listing community contributed plugins.

On the contents of this PR itself:
- I like the Installation, Removal, and Usage sections that @tlrx has in https://github.com/elastic/elasticsearch/pull/11584/files
- For the code examples, why not use AUTOSENSE to make them runnable?
</comment><comment author="dadoonet" created="2015-06-17T06:28:41Z" id="112670874">@clintongormley Thanks for the review.

I applied your comments and the ones you did in ICU PR.
Let me know if it's what you were thinking about. I'll apply the same then for other plugins.
</comment><comment author="dadoonet" created="2015-07-10T14:32:22Z" id="120423398">Closing in favor of #12040 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Prepare plugin and integration docs for 2.0</comment></comments></commit></commits></item><item><title>Query refactoring: ConstantScoreQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11629</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

PR goes agains query-refactoring feature branch.
</description><key id="87753267">11629</key><summary>Query refactoring: ConstantScoreQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-12T14:56:26Z</created><updated>2015-06-16T09:52:18Z</updated><resolved>2015-06-16T09:52:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-15T07:36:36Z" id="111967670">left a couple of comments, looks good though!
</comment><comment author="cbuescher" created="2015-06-15T12:47:17Z" id="112054237">Adressed the last two comments, not sure about the queryBuilder/filterBuilder naming thing. Happy to change that back since it's a filter as you said. 
</comment><comment author="cbuescher" created="2015-06-15T16:29:33Z" id="112128210">Revisited null-checks for inner query builder again. It seems that current DSL allows inner empty queries like `{ "constant_score" : { "filter" : { } }` at the moment, so we need to gracefully handle this on all levels. That's why I removed the null-check for inner query builder from validation since it is legal coming from the DSL. We return `null` from `toQuery()` in this cases which is consistent with the current behaviour in the old `parse()` method.
</comment><comment author="javanna" created="2015-06-15T16:31:00Z" id="112128755">LGTM
</comment><comment author="cbuescher" created="2015-06-15T16:43:31Z" id="112131980">Final squash and rebase, will merge this then as soon as I ran the tests again another time.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11629 from cbuescher/feature/query-refactoring-constantscore</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java</file></files><comments><comment>Query refactoring: ConstantScoreQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Query refactoring: AndQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11628</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

PR goes agains query-refactoring feature branch.
</description><key id="87737014">11628</key><summary>Query refactoring: AndQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-12T13:56:00Z</created><updated>2015-06-16T12:56:36Z</updated><resolved>2015-06-16T12:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-15T08:16:06Z" id="111974256">left two minor comments, LGTM besides those
</comment><comment author="cbuescher" created="2015-06-16T12:19:10Z" id="112404399">Renamed the test case, keeping the `null` return value for reasons statet above. 
</comment><comment author="javanna" created="2015-06-16T12:49:44Z" id="112414942">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11628 from cbuescher/feature/query-refactoring-and</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java</file><file>core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java</file></files><comments><comment>Query refactoring: AndQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>Allow deletion of dynamic_templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11627</link><project id="" key="" /><description>Currently is not possible to delete a dynamic template once added to a type mapping.
The below outlined patch would introduce this functionality. 

To not break backward compatibility maybe adding a dedicated switch to enable this functionality would be useful.&lt;pre&gt;PUT /testindex/_mapping/testtype?allow_dynamic_template_deletion=true&lt;/pre&gt;

```
diff --git a/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java b/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java
index e4f5a8d..2c565ca 100644
--- a/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java
+++ b/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java
@@ -259,7 +259,17 @@
        RootObjectMapper mergeWithObject = (RootObjectMapper) mergeWith;
        if (!mergeResult.simulate()) {
            // merge them
-            List&lt;DynamicTemplate&gt; mergedTemplates = Lists.newArrayList(Arrays.asList(this.dynamicTemplates));
+            List&lt;DynamicTemplate&gt; mergedTemplates = Lists.newArrayList();
+            outer:
+            for (int k = 0; k &lt; this.dynamicTemplates.length; k++) {
+                for (int i = 0; i &lt; mergeWithObject.dynamicTemplates.length; i++) {
+                    if (this.dynamicTemplates[k].name().equals(mergeWithObject.dynamicTemplates[i])) {
+                        mergedTemplates.add(this.dynamicTemplates[k]);
+                        continue outer;
+                    }
+                }
+            }
+
            for (DynamicTemplate template : mergeWithObject.dynamicTemplates) {
                boolean replaced = false;
                for (int i = 0; i &lt; mergedTemplates.size(); i++) {
```

I'am happy to prepare a PR for that.
</description><key id="87727590">11627</key><summary>Allow deletion of dynamic_templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">salyh</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-12T13:22:35Z</created><updated>2017-05-05T16:06:47Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="peschlowp" created="2015-06-12T19:49:03Z" id="111598151">How about this alternate approach which doesn't require a flag and seems more natural to me: Deleting a dynamic template can be done by setting the name of the template to delete to {} when updating the mapping. At least that's what I intuitively tried when looking for this functionality.

``` javascript
PUT /my_index
{
    "mappings": {
        "my_type": {
            "dynamic_templates": [
                { "name_of_template_to_delete": {} }
            ]
}}}
```
</comment><comment author="clintongormley" created="2015-06-13T09:22:59Z" id="111690411">@peschlowp I like this much more - nice. 
</comment><comment author="salyh" created="2015-06-13T09:46:54Z" id="111693650">Agreed
</comment><comment author="salyh" created="2015-07-13T10:06:15Z" id="120880913">see https://github.com/elastic/elasticsearch/pull/11677#issuecomment-120880759
</comment><comment author="salyh" created="2015-07-16T09:36:50Z" id="121901270">see https://github.com/elastic/elasticsearch/pull/12285
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use System.nanoTime for ThreadPool's estimated time, since it's less likely to go backwards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11626</link><project id="" key="" /><description /><key id="87725713">11626</key><summary>Use System.nanoTime for ThreadPool's estimated time, since it's less likely to go backwards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T13:17:31Z</created><updated>2015-06-17T14:56:49Z</updated><resolved>2015-06-12T14:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-12T14:05:28Z" id="111504134">LGTM
</comment><comment author="bleskes" created="2015-06-17T10:19:17Z" id="112746192">does this mean we should add System.currentTimeMillis() to the fobidden APIs?
</comment><comment author="s1monw" created="2015-06-17T10:31:00Z" id="112748443">@bleskes I guess that would make sense... lets see if it's possible though
</comment><comment author="mikemccand" created="2015-06-17T14:56:49Z" id="112832310">&gt; does this mean we should add System.currentTimeMillis() to the fobidden APIs?

+1

We talked about this in #11058 but there are a number of public APIs that expose their System.currentTimeMillis and it wasn't clear if we could change them ... i.e. whether the caller somehow expected to be able to compare results returned from those APIs with their own calls to System.currentTimeMillis.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>Merge pull request #11626 from mikemccand/thread_pool_nano_time</comment></comments></commit></commits></item><item><title>[DOCS] Make build from source read correct for new structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11625</link><project id="" key="" /><description>The 'Building From Source' section of README.textile was misleading as it said that the release can be found in `target/releases` when they are actually now found in the `target/releases` folder of each module.
</description><key id="87722440">11625</key><summary>[DOCS] Make build from source read correct for new structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T13:05:33Z</created><updated>2015-06-12T13:11:50Z</updated><resolved>2015-06-12T13:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure ThreadPool#estimatedTimeInMillis never goes backwards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11624</link><project id="" key="" /><description /><key id="87718968">11624</key><summary>Ensure ThreadPool#estimatedTimeInMillis never goes backwards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label></labels><created>2015-06-12T12:50:59Z</created><updated>2015-06-12T17:09:05Z</updated><resolved>2015-06-12T12:57:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-12T12:54:31Z" id="111482393">Can we cutover to System.nanoTime() instead?

System.currentTimeMillis() sometimes goes backwards on my machine by 3 hours.
</comment><comment author="s1monw" created="2015-06-12T12:57:47Z" id="111483795">yeah I like that much better I guess
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugin install broken in 1.6 when using JMX remote</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11623</link><project id="" key="" /><description>Since #10721, the plugin script uses the system wide defaults file and thus inherits all JAVA_OPTIONS for the server process. We're using JMX to monitor our JVM with a remote port set. Since the plugin script uses the same defaults file it uses the same port and fails spectacularly since the port is already in use by the running ES process. 
</description><key id="87710112">11623</key><summary>Plugin install broken in 1.6 when using JMX remote</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Xylakant</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-06-12T12:08:44Z</created><updated>2016-01-18T20:23:10Z</updated><resolved>2016-01-18T20:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Xylakant" created="2015-06-12T12:20:09Z" id="111474984">There are more entries in the defaults file which the plugin install script should not inherit:
- ES_GC_LOGGING
- ES_HEAP_SIZE
- ulimits

etc.
</comment><comment author="clintongormley" created="2016-01-18T20:23:10Z" id="172644908">Fixed by #12801
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[build] update maven-shade-plugin to 2.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11622</link><project id="" key="" /><description>## Release Notes - Apache Maven Shade - Version 2.4

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317921&amp;version=12331393
### Bugs:
- [MSHADE-155] - dependency-reduced-pom should use shadedArtifactId
- [MSHADE-169] - Typos in warning message
- [MSHADE-172] - "java.lang.ArithmeticException: / by zero" in MinijarFilter
- [MSHADE-174] - Unable to shade Java 8 jarfiles with static interface methods using minimizeJar
- [MSHADE-183] - Getting "Error creating shaded jar: java.util.jar.Attributes cannot be
               cast to java.lang.String" error when using ManifestResourceTransformer with Maven 3.2.5
- [MSHADE-185] - systemPath content is interpolated for system dependencies
### Improvements:
- [MSHADE-177] - MavenProject/MavenSession Injection as a paremeter instead as a component.
- [MSHADE-178] - Removing plexus-container-default dependency
- [MSHADE-179] - Fix RAT Report
- [MSHADE-180] - Upgrade plexus-utils to 3.0.18
- [MSHADE-188] - Upgrade maven-dependency-tree to 2.2
- [MSHADE-191] - Upgrade plexus-utils to 3.0.22
- [MSHADE-192] - Upgrade maven-invoker to 1.10
- [MSHADE-193] - Upgrade to fluido skin 1.4.0
</description><key id="87707197">11622</key><summary>[build] update maven-shade-plugin to 2.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T11:57:39Z</created><updated>2015-06-16T07:36:54Z</updated><resolved>2015-06-16T07:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-16T07:34:05Z" id="112321505">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11622 from dadoonet/maven/shade-plugin</comment></comments></commit></commits></item><item><title>Query refactoring: BoostingQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11621</link><project id="" key="" /><description>As part of the refactoring of queries this PR splits the parsers parse() method into a parsing and a query building part, adding serialization, hashCode(), equals() to the builder.

In addition this changes the BoostingQueryBuilder constructor from a no-arg constructor to one requiring the mandatory postive and negative query to be set, also making those fields final internally. For this reason also the setters for these two fields were deleted.

PR goes agains query-refacoring feature branch.
</description><key id="87705302">11621</key><summary>Query refactoring: BoostingQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>breaking</label></labels><created>2015-06-12T11:50:01Z</created><updated>2015-08-28T10:11:42Z</updated><resolved>2015-06-16T14:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-15T08:21:31Z" id="111975032">left a couple of minor comments
</comment><comment author="cbuescher" created="2015-06-16T10:34:15Z" id="112380940">Just posted an update reverting the changes in the constructor and adressing other comments.
</comment><comment author="javanna" created="2015-06-16T11:03:18Z" id="112385845">left one small comment, LGTM otherwise
</comment><comment author="javanna" created="2015-08-28T10:11:42Z" id="135727003">This change is breaking for the java api as it removes setters for mandatory positive/negative query. Both arguments have to be supplied at construction time instead and have to be non-null.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file></files><comments><comment>Merge pull request #11621 from cbuescher/feature/query-refactoring-boostingquery</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java</file></files><comments><comment>Query refactoring: BoostingQueryBuilder and Parser</comment></comments></commit></commits></item><item><title>ExpressionScriptEngineService not available when running embedded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11620</link><project id="" key="" /><description>When running Elasticsearch embedded in Java, I cannot use scripts of language 'expression'. I'm getting these errors:
script_score the script could not be loaded]; nested: ElasticsearchIllegalArgumentException[script_lang not supported [expression]];
Debugging shows that the ExpressionScriptEngineService is not injected into the ScriptEngine. 

Running the exact same query on a standalone elasticsearch runs fine. 

I'm using 1.5.2, but tested also on 1.6.0, with the same behaviour. 
</description><key id="87701720">11620</key><summary>ExpressionScriptEngineService not available when running embedded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hendrikdonvil</reporter><labels /><created>2015-06-12T11:34:55Z</created><updated>2016-11-10T03:28:17Z</updated><resolved>2015-06-12T12:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-12T11:43:30Z" id="111465739">the expression lib is marked as `optional` in the pom

```
 &lt;dependency&gt;
            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
            &lt;artifactId&gt;lucene-expressions&lt;/artifactId&gt;
            &lt;version&gt;${lucene.maven.version}&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
```

if you pull it in manually does this fix it?
</comment><comment author="hendrikdonvil" created="2015-06-12T12:02:54Z" id="111468890">This fixes it! Thanks a lot! 

Just a little annoying that we have to keep this version in sync with the version elasticsearch uses, if we upgrade in the future, but that's not that big a deal.
</comment><comment author="s1monw" created="2015-06-12T12:22:29Z" id="111475282">cool thanks
</comment><comment author="synhershko" created="2016-05-29T08:30:47Z" id="222349057">What is the up-to-date guidance here, now that lang-expression is it's own module? Including both org.elasticsearch.lang-expression and org.apache.lucene.lucene-expressions as dependencies doesn't seem to work.
</comment><comment author="synhershko" created="2016-05-29T14:40:20Z" id="222364127">Ok, figured it out :)

For the future reader - make sure you include both lang-expression from org.elasticsearch.module and the lucene package mentioned above; and then add ExpressionPlugin.class to nodePlugins in your InternalCluster configs
</comment><comment author="statut" created="2016-11-09T14:32:58Z" id="259427972">@synhershko could you, please, explain where InternalCluster configs are located and how to add ExpressionPlugin.class? Thanks.
</comment><comment author="synhershko" created="2016-11-10T03:28:17Z" id="259593067">InternalCluster configs are located in one of the methods in the class where the cluster is initialized
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove deprecated script APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11619</link><project id="" key="" /><description>The script APIs have been deprecated long ago we can now remove them.
This commit still keeps the parsing code since it might be used in a
query that is still stuck in transaction log. This issue should be discussed
elsewhere.
</description><key id="87691471">11619</key><summary>Remove deprecated script APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Scripting</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T10:57:23Z</created><updated>2015-06-13T09:45:36Z</updated><resolved>2015-06-13T08:12:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-12T15:24:47Z" id="111525688">LGTM
</comment><comment author="s1monw" created="2015-06-13T08:07:36Z" id="111686048">@rjernst I pushed a new commit
</comment><comment author="rjernst" created="2015-06-13T08:09:46Z" id="111686112">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/support/BaseInnerHitBuilder.java</file><file>core/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>core/src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>core/src/main/java/org/elasticsearch/script/Script.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/ValuesSourceAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/test/java/org/elasticsearch/action/update/UpdateRequestTests.java</file><file>core/src/test/java/org/elasticsearch/document/BulkTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTest.java</file><file>core/src/test/java/org/elasticsearch/index/query/TemplateQueryTest.java</file><file>core/src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexLookupTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexedScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/OnDiskScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptFieldTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreBackwardCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionTests.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptTests.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateTests.java</file><file>plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptSearchTests.java</file><file>plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptSearchTests.java</file></files><comments><comment>Remove deprecated script APIs</comment></comments></commit></commits></item><item><title>Clarify the "Disable HTTP" description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11618</link><project id="" key="" /><description>Description should be clear that http is disabled on the data nodes , not the non data nodes.
</description><key id="87584778">11618</key><summary>Clarify the "Disable HTTP" description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sunildabre</reporter><labels /><created>2015-06-12T02:55:25Z</created><updated>2015-06-12T02:56:07Z</updated><resolved>2015-06-12T02:55:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add snapshot name validation logic to all snapshot operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11617</link><project id="" key="" /><description>Make sure snapshot name validation occurs earlier in all snapshot operations.
</description><key id="87549018">11617</key><summary>Add snapshot name validation logic to all snapshot operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-12T00:15:03Z</created><updated>2015-06-18T17:56:41Z</updated><resolved>2015-06-18T01:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-17T15:08:04Z" id="112836722">left some minor comments
</comment><comment author="imotov" created="2015-06-17T16:09:31Z" id="112862452">@s1monw pushed some fixes
</comment><comment author="s1monw" created="2015-06-17T16:19:26Z" id="112866177">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>degradation performance bulk insert</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11616</link><project id="" key="" /><description>Hello.

I use ElasticSearch for collection applications log. But it's huge logs. I have 17 physical servers and 8 node on each servers. Each servers has 4 disks.
My config looks like this:

``` json
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.timeout: 5s
discovery.zen.ping.unicast.hosts: ["192.168.3.23:9300"]
gateway.expected_nodes: 2
gateway.recover_after_nodes: 3
gateway.recover_after_time: 5m
gateway.type: local
index.indexing.slowlog.threshold.index.debug: 2s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.trace: 500ms
index.indexing.slowlog.threshold.index.warn: 10s
index.number_of_replicas: 2
index.number_of_shards: 4
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.trace: 200ms
index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.trace: 500ms
index.search.slowlog.threshold.query.warn: 10s
monitor.jvm.gc.young.debug: 400ms
monitor.jvm.gc.young.info: 700ms
monitor.jvm.gc.young.warn: 1000ms
network.bind_host: 192.168.3.7
network.publish_host: 192.168.3.7
node.data: true
node.master: true
node.name: "server_1"
path.data: /var/www/elastic,/var/www/elastic,/var/www/elastic,/var/www/elastic
path.logs: /var/log/elasticsearch
#
transport.tcp.port: 9300
http.port: 9200
cluster.routing.allocation.disk.watermark.low: 1gb
cluster.routing.allocation.disk.watermark.high: 500mb
cluster.routing.allocation.node_concurrent_recoveries: 4
cluster.routing.allocation.node_initial_primaries_recoveries: 8
indices.recovery.concurrent_streams: 8
indices.recovery.max_bytes_per_sec: 100mb
threadpool.bulk.queue_size: 5000
```

Each node has using own path on all disks.
For insert of data I used to Logstash. For transport data from my servers to Logstash I used to logstash_forwarder with spool-size=512.

When cluster has not much nodes all works fine. But, when cluster is increased, Logstash begins to wait ES with next messages:

"Failed to flush outgoing items", :outgoing_count=&gt;4872, :exception=&gt;java.lang.OutOfMemoryError: Java heap space, :backtrace=&gt;[], :level=&gt;:warn}"

But, my attempts increases memory for Logstash have not been successful.
Maybe I have a wrong configuration for ElasticSearch?
</description><key id="87516330">11616</key><summary>degradation performance bulk insert</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">swood</reporter><labels /><created>2015-06-11T22:10:29Z</created><updated>2015-06-12T16:55:01Z</updated><resolved>2015-06-12T16:55:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T16:55:00Z" id="111555829">Hi @swood 

The right place to ask questions like these is in the forum: https://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>AsyncShardFetch can hang if there are new nodes in cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11615</link><project id="" key="" /><description>The AsyncShardFetch retrieves shard information from the different nodes in order to detirment the best location for unassigned shards. The class uses TransportNodesListGatewayStartedShards and TransportNodesListShardStoreMetaData in order to fetch this information. These actions, inherit from TransportNodesAction and are activated using a list of node ids. Those node ids are extracted from the cluster state that is used to assign shards.

If we perform a reroute and adding new nodes in the same cluster state update task, it is possible that the AsyncShardFetch administration is based on
a different cluster state then the one used by TransportNodesAction to resolve nodes. This can cause a problem since TransportNodesAction filters away unknown nodes, causing the administration in AsyncShardFetch to get confused.

This commit fixes this allowing to override node resolving in TransportNodesAction and uses the exact node ids transfered by AsyncShardFetch.

NOTE: this is currently not an issue as we never add nodes and reroute in the same task. This is however dangerous and should be fixed.
</description><key id="87504940">11615</key><summary>AsyncShardFetch can hang if there are new nodes in cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T21:24:44Z</created><updated>2015-06-12T17:02:41Z</updated><resolved>2015-06-12T07:57:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-06-11T22:49:40Z" id="111298071">LGTM, even though this we don't have this problem today, I think its worth back porting it.
</comment><comment author="bleskes" created="2015-06-12T10:21:22Z" id="111440231">Pushed to 1.x &amp; 1.6
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java</file><file>core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java</file><file>core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java</file><file>core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/state/RareClusterStateTests.java</file></files><comments><comment>Merge pull request #11615 from bleskes/async_fetch_non_existent_nodes</comment></comments></commit></commits></item><item><title>Fix typo in upgrade docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11614</link><project id="" key="" /><description>Fixed a typo in the docs.
</description><key id="87501101">11614</key><summary>Fix typo in upgrade docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oyiadom</reporter><labels><label>docs</label></labels><created>2015-06-11T21:15:04Z</created><updated>2015-06-13T09:35:41Z</updated><resolved>2015-06-13T09:28:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T16:53:04Z" id="111555029">Hi @oyiadom 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in? 
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="oyiadom" created="2015-06-13T00:22:08Z" id="111645958">Hey, @clintongormley.  You are welcome.  I just signed the agreement.
</comment><comment author="clintongormley" created="2015-06-13T09:35:41Z" id="111690976">thanks @oyiadom - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11614 from oyiadom/patch-1</comment></comments></commit></commits></item><item><title>Consistently add one more maxMerge in ConcurrentMergeSchedulerProvider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11613</link><project id="" key="" /><description>we pass maxMergeCount+1 here so that CMS will allow one too many
merges to kick off which then allows InternalEngine.IndexThrottle
to detect too-many-merges and throttle. We miss to add this when the setting
is updated.
</description><key id="87465164">11613</key><summary>Consistently add one more maxMerge in ConcurrentMergeSchedulerProvider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v1.5.3</label><label>v1.6.1</label></labels><created>2015-06-11T19:24:20Z</created><updated>2015-06-12T16:48:55Z</updated><resolved>2015-06-11T19:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-11T19:24:32Z" id="111252246">@mikemccand  can you look 
</comment><comment author="mikemccand" created="2015-06-11T19:28:55Z" id="111253627">I left one comment else LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES 1.6 broke installing plugins in uninstalled package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11612</link><project id="" key="" /><description>First, the use case:

To ensure a single, tested, artifact to be installed across the fleet, including all plugins needed, our build process roughly works like this:
- Grab the version of ES as a debian package
- Extract this debian package in a temp dir
- cd into this directory and run: `bin/plugin install ...`
- go back to the temp dir, and repackage as debian package

The reason for the debian package is that the init scripts for ES do a lot of work (setting ulimit, users, groups, etc), and we don't want to duplicate that and maintain it; otherwise we'd just use the tarball.

This process worked fine with 1.5.x, but starting 1.6, the 'plugin' command can no longer find the needed config files. Here are the specific issues:

1) Even though it finds 'CONF_DIR' just fine, it insists on setting 'CONF_FILE' separately, and does not use the same heuristics as for 'CONF_DIR'. 

2) It now requires /etc/default/elasticsearch to be present to run, which requires an installed version, or another override.

The workaround is now to do this, which is ugly and prone to breakage going forward:

```
$ ES_INCLUDE=`pwd`/etc/default/elasticsearch CONF_FILE=`pwd`/etc/elasticsearch/elasticsearch.yaml CONF_DIR=`pwd`/etc/elasticsearch  usr/share/elasticsearch/bin/plugin  -h
```

The full diff below, to illustrate the offending parts:

```
diff elasticsearch-1.{5.2,6.0}/usr/share/elasticsearch/bin/plugin
23a26,62
&gt; # Sets the default values for elasticsearch variables used in this script
&gt; if [ -z "$CONF_DIR" ]; then
&gt;   CONF_DIR="/etc/elasticsearch"
&gt;
&gt;   if [ -z "$CONF_FILE" ]; then
&gt;     CONF_FILE="$CONF_DIR/elasticsearch.yml"
&gt;   fi
&gt; fi
&gt;
&gt; if [ -z "$CONF_FILE" ]; then
&gt;   CONF_FILE="/etc/elasticsearch/elasticsearch.yml"
&gt; fi
&gt;
&gt; # The default env file is defined at building/packaging time.
&gt; # For a deb package, the value is "/etc/default/elasticsearch".
&gt; ES_ENV_FILE="/etc/default/elasticsearch"
&gt;
&gt; # If an include is specified with the ES_INCLUDE environment variable, use it
&gt; if [ -n "$ES_INCLUDE" ]; then
&gt;     ES_ENV_FILE="$ES_INCLUDE"
&gt; fi
&gt;
&gt; # Source the environment file
&gt; if [ -n "$ES_ENV_FILE" ]; then
&gt;
&gt;   # If the ES_ENV_FILE is not found, try to resolve the path
&gt;   # against the ES_HOME directory
&gt;   if [ ! -f "$ES_ENV_FILE" ]; then
&gt;       ES_ENV_FILE="$ELASTIC_HOME/$ES_ENV_FILE"
&gt;   fi
&gt;
&gt;   . "$ES_ENV_FILE"
&gt;   if [ $? -ne 0 ]; then
&gt;       echo "Unable to source environment file: $ES_ENV_FILE" &gt;&amp;2
&gt;       exit 1
&gt;   fi
&gt; fi
48c87,96
&lt; exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS -Xmx64m -Xms16m -Delasticsearch -Des.path.home="$ES_HOME" $properties -cp "$ES_HOME/lib/*" org.elasticsearch.plugins.PluginManager $args

---
&gt; # check if properties already has a config file or config dir
&gt; if [ -e "$CONF_DIR" ]; then
&gt;   case "$properties" in
&gt;     *-Des.default.path.conf=*|*-Des.path.conf=*)
&gt;     ;;
&gt;     *)
&gt;       properties="$properties -Des.default.path.conf=$CONF_DIR"
&gt;     ;;
&gt;   esac
&gt; fi
49a98,110
&gt; if [ -e "$CONF_FILE" ]; then
&gt;   case "$properties" in
&gt;     *-Des.default.config=*|*-Des.config=*)
&gt;     ;;
&gt;     *)
&gt;       properties="$properties -Des.default.config=$CONF_FILE"
&gt;     ;;
&gt;   esac
&gt; fi
&gt;
&gt; export HOSTNAME=`hostname -s`
&gt;
&gt; exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS -Xmx64m -Xms16m -Delasticsearch -Des.path.home="$ES_HOME" $properties -cp "$ES_HOME/lib/*" org.elasticsearch.plugins.PluginManager $args
```
</description><key id="87444286">11612</key><summary>ES 1.6 broke installing plugins in uninstalled package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">jib</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-06-11T18:45:10Z</created><updated>2016-01-18T20:22:43Z</updated><resolved>2016-01-18T20:22:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T20:22:43Z" id="172644820">This has all changed a lot since 1.6, and CONF_FILE is not longer supported.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test blob during repository verification should be created in a test subdirectory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11611</link><project id="" key="" /><description>The repository verification process should create a subdirectory to make sure we check permission of newly created directories in case elasticsearch processes on different nodes are running using different uids and creating blobs with incompatible permissions.
</description><key id="87440743">11611</key><summary>Test blob during repository verification should be created in a test subdirectory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T18:32:11Z</created><updated>2015-07-31T16:36:33Z</updated><resolved>2015-07-31T16:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-16T09:54:38Z" id="121912478">Bumping the version up to 1.7.1 for the release today.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file></files><comments><comment>Create a directory during repository verification</comment></comments></commit></commits></item><item><title>Use task's class name if not a TimedPrioritizeRunnable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11610</link><project id="" key="" /><description>This is helpful to track down the origin of pending_tasks that aren't
expected. In tests we catch this with an assert, but in production
asserts may not be enabled so we should at least add the class name.
</description><key id="87440493">11610</key><summary>Use task's class name if not a TimedPrioritizeRunnable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>bug</label><label>v1.5.3</label><label>v1.6.1</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T18:30:54Z</created><updated>2015-06-12T16:43:22Z</updated><resolved>2015-06-11T18:40:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-11T18:31:30Z" id="111233207">This is an enhancement, but I'd like to treat it as a bug and backport this as far as possible. Without it, debugging large queue lengths of unknown tasks is very difficult.
</comment><comment author="bleskes" created="2015-06-11T18:31:59Z" id="111233326">LGTM. Left one minor suggestion
</comment><comment author="s1monw" created="2015-06-11T18:34:29Z" id="111233762">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>'elasticsearch' user and group id's should be consistent across multiple nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11609</link><project id="" key="" /><description>For filesystem snapshots, if the 'elasticsearch' group and user have differing id numbers between nodes in the system, snapshots will not work.  The master will create subdirectories for each index, but the permissions will reflect ownership by the 'elasticsearch' user on the master.  If the master's elasticsearch user does not match the elasticsearch user id on another node, that node will not be able to write to those directories.

When we create the 'elasticsearch' user and group at installation time, it would be helpful to have a consistent id number.  I don't know if the best solution is to pick an arbitrary high user ID number (9200?) and try to use it by default, or just to document that it needs to be set the same across all nodes.
</description><key id="87439299">11609</key><summary>'elasticsearch' user and group id's should be consistent across multiple nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Packaging</label><label>docs</label></labels><created>2015-06-11T18:26:19Z</created><updated>2016-12-13T11:49:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Xylakant" created="2015-06-16T10:17:54Z" id="112374662">I'd consider a fixed user/group ID a bug. For example we assign ranges for user accounts depending on whether it's a "system" account or a "real user" account and 9200 would fall smack into our "real user" account range and probably conflict. I know of more organizations that use a similar scheme, so whatever value you choose you'll step on somebodies toe.
</comment><comment author="clintongormley" created="2016-01-18T20:21:49Z" id="172644492">Let's opt for documenting it
</comment><comment author="gogularaja" created="2016-12-13T11:49:40Z" id="266718948">Hi All,

I am facing an issue while taking elasticsearch snapshot in cluster. I found it is because of different permission of elasticsearch user in one of my node.

I have 2 servers. Configured as cluster
server1: 10.0.0.0
server2: 10.0.0.1

I have changed the userid &amp; groupid of elasticsearch user in server2. So that to keep it same across all the nodes.
But after changing it, i am unable to start the elasticsearch service in server2. Also log file is also not getting updated.


Please guide me in solving this issue..


Thanks,
Gogul</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] expand testing of compound queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11608</link><project id="" key="" /><description>Extracted random query creation code from tests and created one class per query. This is a bit verbose but allows us to avoid having to create a new instance of a test class in order to create a random instance of a specific query. Also, we can reuse the same code to add random queries to compound queries for more coverage and deeper testing. In the end this is a way to share code between different tests through intermediate classes.

I want to open this up for discussion, this solution is not perfect but it improve things a bit, although it might be seen as making our query test infra even more complicated, let's discuss it here.

This PR goes against the feature/query-refactoring branch.
</description><key id="87417112">11608</key><summary>[TEST] expand testing of compound queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2015-06-11T17:10:37Z</created><updated>2015-10-31T09:51:36Z</updated><resolved>2015-08-25T08:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-15T13:02:10Z" id="112061712">Without going much into the details, I like the idea of being able to have a central utility class to create random queries (also nested). This is not only useful for the current refactoring test but also later on for intergration and performance test. 

I'm a little bit unsure if spreading the test query code will be confusing after a while. At the moment we are familiar with the query test setup, but it's already getting a bit complicated and if someone new steps in and wants to write a test he already needs to understand quiet a bit about the test architecture. The need for separate classes and registries would add to that.

Another point just recently discovered in the RangeQueryBuilder tests: there are edge cases there that make it advisable to make createTestQuery() to depend on the global test setup (e.g. are certain mappers there or not). In this PR creating test queries is stateless and could almost be static (only requiring random()), this might not be enough for some of the test setups we might encounter. I think we need some way to compare pros/cons and how much the added complexity here will help in the future.
</comment><comment author="javanna" created="2015-06-15T13:09:52Z" id="112064297">&gt; there are edge cases there that make it advisable to make createTestQuery() to depend on the global test setup (e.g. are certain mappers there or not).

createTestQuery can be static at the moment only because mapped fields are static too, it does depend on the defined mappers defined in the beforeClass, if I understood what you wrote.

I see why you are unsure about this PR and I am too. But I think that having to do e.g. `new TermQueryBuilderTest.createTestQuery` to get a test term query is quite ugly. This PR also tries to make it easier to generate whatever random query rather than one of the three that get returned now. So it is not perfect but I think it still is an improvement. Maybe we could find some other way to address the same issues without requiring one more test class per query.
</comment><comment author="cbuescher" created="2015-06-15T13:21:45Z" id="112067883">Also if we allow recursively nested queries (e.g. in BoolQueryBuilder draw inner queries from the registry of test queries), there should be a way to control the depth of the tree. I think we don't immediately need that for the unit tests now, but might be handy later.
</comment><comment author="javanna" created="2015-06-15T13:23:18Z" id="112068557">Yes indeed I already ran into too deep trees there :) but it did uncover some bugs. I think we should test this, but we should limit the depth, totally agree.
</comment><comment author="javanna" created="2015-06-18T08:32:19Z" id="113075691">What do we do with this PR? @cbuescher @MaineC @alexksikes can we come to some conclusion? What are your opinions?
</comment><comment author="MaineC" created="2015-06-18T08:36:14Z" id="113076294">I guess the correct vote from my side would be a +0

For translation: http://www.apache.org/foundation/voting.html
</comment><comment author="cbuescher" created="2015-06-18T08:55:29Z" id="113080826">I'm hesitating for the following reasons:

Pros:
- it would be great to have a Query-Genrating framework for all kinds of tests, not only the current refactoring
- having some deeper level of nestedness would also be great
- avoiding code duplication

Cons:
- added complexity
- loss of control: by that I mean we are currently creating the queries specifically for the unit tests. When this random queries are used by other tests and extended for their uses cases as well, there might soon be me more need for variants, tests get more difficult to write etc...
- we might need test context for random setup, I think I already needed to depend on test context when allowing certain query options **

*\* (at least in https://github.com/elastic/elasticsearch/blob/feature/query-refactoring/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java#L67 and I think we might need it again) 

I think I'm still conflicted about how independent of the tests we are writing at the moment those query generators should be. But happy to discuss, there's also lots to gain from random query generators.
</comment><comment author="alexksikes" created="2015-06-18T15:11:32Z" id="113188209">I'm thinking that the need for something like this might get clearer as we get more refactored queries down the pipeline. Right now, I have mixed feelings as I would really like to avoid further added complexity.
</comment><comment author="javanna" created="2015-08-25T08:55:29Z" id="134530741">Closing this, it got way too outdated to be restored. Will open a new PR when I get to this again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixing edge case in RangeQueryBuilder when using time zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11607</link><project id="" key="" /><description>When specifying a time zone, RangeQueryBuilder currently throws an exeption when the field is not using DateFieldMapper, but if there are no mappers at all for that field it doesn't complain. 
To be more consistent we should also throw expection in this case.

PR goes agains query-refacoring feature branch.
</description><key id="87398060">11607</key><summary>Fixing edge case in RangeQueryBuilder when using time zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-06-11T16:10:53Z</created><updated>2015-06-11T16:24:04Z</updated><resolved>2015-06-11T16:24:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-11T16:18:31Z" id="111190851">LGTM can we test this edge case?
</comment><comment author="cbuescher" created="2015-06-11T16:22:58Z" id="111192402">@javanna We already test this, that's how I found out ;-)
Will merge then.
</comment><comment author="javanna" created="2015-06-11T16:23:44Z" id="111192628">great get it in then!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file></files><comments><comment>Merge pull request #11607 from cbuescher/feature/query-refactoring-rangequery-fix</comment></comments></commit></commits></item><item><title>Fold ShardGetService creation away from Guice  into IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11606</link><project id="" key="" /><description>it's always acccessed via IndexShard and has crazy circular dependencies or
rather had. It just makes IndexShard ctor bigger for no reason.
</description><key id="87395298">11606</key><summary>Fold ShardGetService creation away from Guice  into IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T16:01:24Z</created><updated>2015-06-15T07:46:40Z</updated><resolved>2015-06-15T07:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-11T17:09:39Z" id="111209403">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11606 from s1monw/fold_in_get_service</comment></comments></commit></commits></item><item><title>Create ShardSuggestService/Metrics manually outside of guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11605</link><project id="" key="" /><description>This ShardSuggestService is a simple metrics counter and doesn't need
to be injected. It just makes IndexShard ctor bigger for no reason.
</description><key id="87386619">11605</key><summary>Create ShardSuggestService/Metrics manually outside of guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T15:34:28Z</created><updated>2015-06-12T17:04:04Z</updated><resolved>2015-06-12T11:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-11T17:06:19Z" id="111208687">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When no parent param is passed, partial update of  a child document removes the _parent value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11604</link><project id="" key="" /><description>PUT /myindex/anytype/?routing=rk&amp;parent=daddy

``` JSON
{
  "field":"value"
}
```

Everything works fine

PUT /myindex/anytype/?routing=rk

``` JSON
{
  "field":"value"
}
```

The _parent field is removed. In a parent-child approach, you lose the link to your parent.
According to the documentation, you should not be able to update the `parent` value. (nor remove it ?)
https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#_parameters_3

ES version : 1.4.5
</description><key id="87379228">11604</key><summary>When no parent param is passed, partial update of  a child document removes the _parent value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ls-pluriels</reporter><labels /><created>2015-06-11T15:10:50Z</created><updated>2015-06-12T16:30:54Z</updated><resolved>2015-06-12T16:30:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T16:30:53Z" id="111548191">@ls-pluriels you linked to the update API docs, but you're not using `_update`, you're reindexing the document completely (which deletes the old doc and adds the new)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Partial text in Completion Suggester doesn't (immediately) matches with dataset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11603</link><project id="" key="" /><description>Hi. We work on ElasticSearch 1.3.4. and have a very big set of data. 

Our completion field is configured as follows:
_suggest_company_by_name: {
max_input_length: 50
preserve_separators: true
payloads: true
analyzer: simple
preserve_position_increments: true
type: completion
}

Example of how we index:
_suggest_company_by_name: {
input: [
Trailburning
]
output: Trailburning
payload: {
company_identifier: 9f3974f8f10c4028b6ec2ccfcc67aaa6
}
}

We query the data as follows:
{
        "name" : {
            "text" : "&lt;partial text&gt;",
            "completion" : {
                "field" : "_suggest_company_by_name",
                "fuzzy" : {},
                "size" : 10,
            }
        }
    }

We experience the following problem. When entering a partial text (in this case a company name), there is a point where the partial text is already so specific it should respond with the exact name from the dataset matching it, but instead it gives results which are different. 

Example 1:
I search for the name 'Travel Rhythm'. As partial text I enter 'Travel R', but the system responds with the names 'Travel Distribution Systems', '1travel.gr' and 'Travel Agent CMS' (which obviously match with the 'Travel' part, but not with 'Travel&lt;space&gt;R'. Not untill the partial text is 'Travel Rhy' I get 'Travel Rhythm' as only result. 

Example 2
I search for the name iLost. As partial text I enter 'iLost' but the system responds with 'iGost Technologies' and 'iLastic'. Not untill I add a space in the partial text (so 'iLost&lt;space&gt;') I see 'iLost' in addition to the previous results.

What steps can we take to solve this? Thank you for your suggestions.
</description><key id="87377189">11603</key><summary>Partial text in Completion Suggester doesn't (immediately) matches with dataset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">michielkraijkamp</reporter><labels><label>:Suggesters</label></labels><created>2015-06-11T15:03:53Z</created><updated>2015-06-18T12:16:22Z</updated><resolved>2015-06-18T12:16:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T16:27:02Z" id="111545998">Hi @michielkraijkamp 

You're using fuzzy matching, so it produces a lot more matches.  I think, in the new suggester that we're implementing, that we can possibly favour exact matches over fuzzy matches.  @areek is that possible?
</comment><comment author="michielkraijkamp" created="2015-06-15T08:32:24Z" id="111977321">Thanks for your quick reply @clintongormley 

It looks kind of fuzzy indeed :-) Showing suggestions if your partial text doesn't match data isn't so wrong, but if you look for something particular it is difficult if it's there but you need a workaround to find it.
</comment><comment author="areek" created="2015-06-16T04:14:59Z" id="112282359">@michielkraijkamp @clintongormley with the new suggester implementation, the fuzzy matches that share the most number of characters with the user query would be ranked higher. For example, searching for 'iLost' will always yield 'iLost' higher than 'iLastic' or 'iGost'. (see https://github.com/elastic/elasticsearch/issues/10746)
</comment><comment author="michielkraijkamp" created="2015-06-16T07:28:01Z" id="112320530">@clintongormley @areek That sounds perfect :-) 
</comment><comment author="clintongormley" created="2015-06-18T12:16:21Z" id="113134867">Closing in favour of #10746
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup MergeScheduler infrastrucutre</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11602</link><project id="" key="" /><description>This commit cleans up all the MergeScheduler infrastructure
and simplifies / removes all unneeded abstractions. The MergeScheduler
itself is now private to the Engine and all abstractions like Providers
that had support for multiple merge schedulers etc. are removed.

@mikemccand do you wanna review this please
</description><key id="87375412">11602</key><summary>Cleanup MergeScheduler infrastrucutre</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T14:57:59Z</created><updated>2015-06-12T16:46:19Z</updated><resolved>2015-06-11T18:54:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-11T15:55:05Z" id="111182190">LGTM, this is a great cleanup ... I just left a few comments.
</comment><comment author="s1monw" created="2015-06-11T16:08:31Z" id="111187393">pushed a new commit
</comment><comment author="mikemccand" created="2015-06-11T17:50:30Z" id="111221599">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/index/OneMergeHelper.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ElasticsearchConcurrentMergeScheduler.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/merge/scheduler/ConcurrentMergeSchedulerProvider.java</file><file>core/src/main/java/org/elasticsearch/index/merge/scheduler/MergeSchedulerProvider.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShardModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/MergeSchedulerConfig.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/indices/settings/UpdateSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Cleanup MergeScheduler infrastrucutre</comment></comments></commit></commits></item><item><title>Allow users to perform simple arithmetic operations on histogram aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11601</link><project id="" key="" /><description>A script can be used to calculate a metric to add to a bucket based on other metrics present in the bucket

Closes #11029
</description><key id="87369339">11601</key><summary>Allow users to perform simple arithmetic operations on histogram aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T14:40:21Z</created><updated>2015-08-30T14:50:38Z</updated><resolved>2015-06-12T08:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-11T15:47:24Z" id="111178749">LGTM
</comment><comment author="tarunsapra" created="2015-08-30T14:48:44Z" id="136151488">Would be nice if a link is added, this link could point to the pages in the docs complementing this feature.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removing top-level filter parameter from search API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11600</link><project id="" key="" /><description>Closes #8862.
</description><key id="87341113">11600</key><summary>Removing top-level filter parameter from search API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2015-06-11T13:10:05Z</created><updated>2015-10-20T10:50:36Z</updated><resolved>2015-06-12T17:07:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-11T15:51:57Z" id="111180781">Thanks for taking care of this! The change to QueryPhase looks good however I don't think we should keep tests for features we removed, so I think we should just push the change to QueryPhase.
</comment><comment author="ycombinator" created="2015-06-11T16:19:39Z" id="111191292">@jpountz Thanks. I was wondering about "negative" tests. I'll remove it.
</comment><comment author="jpountz" created="2015-06-11T21:17:01Z" id="111279550">LGTM
</comment><comment author="clintongormley" created="2015-06-12T16:54:05Z" id="111555400">@ycombinator you want to merge it in? master branch only, thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file></files><comments><comment>Merge pull request #11600 from ycombinator/gh-8862</comment></comments></commit></commits></item><item><title>Kibana highlighting issue not fixed?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11599</link><project id="" key="" /><description>New 1.6.0 install with new indices, in Kibana discover searching for "execute":

```
[2015-06-11 13:18:59,718][DEBUG][action.search.type       ] [Laura Dean] [29] Failed to execute fetch phase
org.elasticsearch.search.fetch.FetchPhaseExecutionException: [logstash-2015.06.11][4]: query[filtered(_all:execute)-&gt;BooleanFilter(+cache(@timestamp:[1433938739612 TO 1434025139612]
))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@31c250a5&gt;!]: Fetch Failed [Failed to highlight field [m
essage.raw]]
        at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:133)
        at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:133)
        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:194)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:504)
        at org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:452)
        at org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:449)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.lucene.util.BytesRefHash$MaxBytesLengthExceededException: bytes can be at most 32766 in length; got 173493
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:493)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:396)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:371)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:351)
        at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getLeafContext(WeightedSpanTermExtractor.java:360)
        at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:216)
        at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
        at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
        at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
        at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
        at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:119)
        ... 9 more
Caused by: org.apache.lucene.util.BytesRefHash$MaxBytesLengthExceededException: bytes can be at most 32766 in length; got 173493
        at org.apache.lucene.util.BytesRefHash.add(BytesRefHash.java:284)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:467)
        ... 19 more
```

I thought this was fixed in #9881 - but for some reason ES is still trying to highlight message.raw which is not_analyzed. The comment in the Kibana issue https://github.com/elastic/kibana/issues/2782#issuecomment-105224019 says that long terms shouldn't get into the index and not_analyzed should be skipped - but neither seem to be happening here.

Reproduction in this [gist](https://gist.github.com/jimmyjones2/0be6664a471a30b2e624) then add index to Kibana and in discover search for "execute". My original setup involves Elasticsearch sending its own logs to logstash via log4j socket, then ingesting into another elasticsearch instance - can include details here if needed.
</description><key id="87333926">11599</key><summary>Kibana highlighting issue not fixed?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-06-11T12:44:16Z</created><updated>2015-08-28T17:03:15Z</updated><resolved>2015-06-25T10:36:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-11T12:51:03Z" id="111121974">Thanks for reporting @jimmyjones2 

@brwe please can you investigate?
</comment><comment author="brwe" created="2015-06-15T18:27:29Z" id="112161991">Indeed, the problem was not fixed on 1.x branch. Two things happened: 1) the test I wrote for the huge terms actually did not trigger the exception on 1.x because on 1.x this only happens with certain queries like prefix query but not match query (which I use in the test) and 2) I fixed the issue by catching a MaxBytesLengthExceededException in PlainHighlighter but on 1.x this is actually wrapped in a RuntimeException. So this did not fix it on 1.x but the tests passed anyway. I made a pull request to change this now here #11683. Sorry. 
</comment><comment author="jimmyjones2" created="2015-06-15T19:18:55Z" id="112178962">@brwe Thanks! Might this fix get into 1.6.1?
</comment><comment author="Filirom1" created="2015-06-25T08:58:33Z" id="115171033">+1
</comment><comment author="brwe" created="2015-06-25T10:36:55Z" id="115205782">I pushed to 1.6 and 1.x branch, the fix will be in 1.6.1. Thanks again for reporting!
</comment><comment author="asatsi" created="2015-06-30T08:56:09Z" id="117062603">+1 simple searches not happening due to this when using with ELK :-( Kind of annoying and frustrating. Is there any workaround?
</comment><comment author="Filirom1" created="2015-07-17T07:59:50Z" id="122209859">Thanks, it works like a charm now :beers: 
</comment><comment author="tmartensen" created="2015-08-28T15:36:44Z" id="135808513">This still doesn't work for me when using Kibana 4.1.1 and Elasticsearch 1.7.1. Is there something else that needs to be done to existing indexes to get this to work?
</comment><comment author="clintongormley" created="2015-08-28T15:40:31Z" id="135809806">@tmartensen you shouldn't have to do anything, as long as we're talking about the same exception.  More details?
</comment><comment author="tmartensen" created="2015-08-28T15:58:56Z" id="135814133">I'm still getting the same exception after upgrading to 1.7.1. I'm trying to log SOAP messages that are larger than normal, but not outrageously large, by filtering on the `org.springframework.ws.client.messagetracing*` logger name.

```
Caused by: org.elasticsearch.search.fetch.FetchPhaseExecutionException: [logstash-2015.08.26][4]: query[filtered(logger_name:org.springframework.ws.client.messagetracing* logger_name:com.xxx.xxx.xxx)-&gt;BooleanFilter(+QueryWrapperFilter(ConstantScore(*:*)) +cache(@timestamp:[1440306000000 TO 1440777217294]))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@2f9f7c14&gt;!]: Fetch Failed [Failed to highlight field [message.raw]]
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:126)
    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:501)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:868)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:862)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.lucene.util.BytesRefHash$MaxBytesLengthExceededException: bytes can be at most 32766 in length; got 47034
```

I've added the workaround to the Kibana code as described in https://github.com/elastic/kibana/issues/2782#issuecomment-81925059 as well, and it seems to have no effect.
</comment><comment author="jimmyjones2" created="2015-08-28T16:27:02Z" id="135825086">Fix worked for me, after remembering to upgrade the client node
</comment><comment author="tmartensen" created="2015-08-28T16:31:36Z" id="135826035">@jimmyjones2 what does that entail? I've upgraded the index through the upgrade API for the offending indexes.
</comment><comment author="clintongormley" created="2015-08-28T16:34:40Z" id="135826781">It means: make sure you upgrade all Elasticsearch nodes, including the client nodes (if you're using them)
</comment><comment author="tmartensen" created="2015-08-28T16:38:17Z" id="135827528">I see. I'm using ES unclustered at the moment, straight out of the box. How do you upgrade the nodes?

**EDIT:** I saw that I had quite a few older nodes running out there and shut them all down. After a restart, everything seems to be fine. Thanks for your help, @jimmyjones2 and @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>UNICODE_CHARACTER_CLASS fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11598</link><project id="" key="" /><description>Closes #10146
</description><key id="87314128">11598</key><summary>UNICODE_CHARACTER_CLASS fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">srogljan</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-06-11T11:41:18Z</created><updated>2015-12-11T17:33:04Z</updated><resolved>2015-12-11T10:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T20:41:10Z" id="162244129">Sorry this was lost. Are you still interested in this?  If so, could I ask you to sign the CLA?  http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2015-12-05T20:41:56Z" id="162244167">I wonder if this should go through parseFields and we can deprecate the old form.

/cc @MaineC 
</comment><comment author="srogljan" created="2015-12-08T13:21:34Z" id="162877726">Sure you can. :-) I hope I will sign it at the evening. Can you wait, that long?
</comment><comment author="MaineC" created="2015-12-08T16:20:22Z" id="162933952">Yeah, sure.
</comment><comment author="MaineC" created="2015-12-08T16:43:07Z" id="162940641">@clintongormley wrt. to passing this through ParseField: while the deprecation handling of ParseField would indeed be handy to have here, ParseField comes with a bit more logic for supporting both, underscore based and camel case names, inferring one from the other. 

Switching things like this to a vanilla ParseField would make things more confusing. Might be worthwhile to extract the deprecation logic from it though if there are more bugs/ requests for improvement like this one.
</comment><comment author="srogljan" created="2015-12-09T10:26:49Z" id="163180092">signed! feel free to use this code :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/regex/Regex.java</file><file>core/src/test/java/org/elasticsearch/common/regex/RegexTests.java</file></files><comments><comment>Merge pull request #11598 from srogljan/master</comment></comments></commit></commits></item><item><title>Deprecate doc and byte size MergePolicy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11597</link><project id="" key="" /><description /><key id="87295760">11597</key><summary>Deprecate doc and byte size MergePolicy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v1.6.0</label></labels><created>2015-06-11T10:28:36Z</created><updated>2015-06-11T10:41:54Z</updated><resolved>2015-06-11T10:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-11T10:31:32Z" id="111080611">Plus add a new line before the deprecated tag
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for Lucene Expression scripts in pipeline aggregators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11596</link><project id="" key="" /><description>At the moment Lucene Expressions scripts only implement SearchScript and throw an UnsupportedOperationException if you call ExpressionScriptService.executable() or ExpressionScriptService.execute(). The message in the exception says "Cannot use expressions for updates" which is understandable since the language is not built for update operations. However, in the pipeline aggregators we want to be able to use scripts to perform arbitrary calculations on the aggregations responses, which are outside of a search context since they are performed in the reduce phase. It would be great if we can update the ExpressionScriptService to be able to support this.
</description><key id="87290798">11596</key><summary>Support for Lucene Expression scripts in pipeline aggregators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-11T10:10:23Z</created><updated>2015-06-18T18:13:23Z</updated><resolved>2015-06-18T18:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-06-11T21:25:21Z" id="111281841">After looking at the code, this can't really be accomplished in the short-term.  Expressions is missing numerous features that are specified in the example here (https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-scripted-metric-aggregation.html) including if/else blocks (though it does have conditionals), loops, variable assignment, and different types such as strings or ints.  The expressions architecture is built exclusively to support doubles with other numeric values being converted to doubles when used.  Expressions also have no notion of a long-term variable meaning that the variables passed between scripts would have to be coded in as well.  There are simply too many limitations to overcome before the upcoming beta for 2.0.
</comment><comment author="jdconrad" created="2015-06-11T21:33:24Z" id="111283258">@colings86 Also, would you please provide an example aggregation that you're thinking of doing using expressions?  Thanks!
</comment><comment author="jdconrad" created="2015-06-11T21:49:36Z" id="111286012">@colings86 It might be worth looking into what we do with multi-value fields where we allow them have member methods like sum, avg, median, min, and max.  If that could be provided for a single known variable, would that be enough for what you're trying to do?
</comment><comment author="jdconrad" created="2015-06-12T15:22:55Z" id="111525254">Just spoke with @colings86 -- My apologies for the misunderstanding, the goal is simply to be able to calculate numeric expressions through an executable script instead of a search script.  I believe this can be done now for the 2.0 beta.  I will always assume that the Objects in the map of vars passed in will be doubles.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java</file><file>core/src/main/java/org/elasticsearch/script/ScriptService.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionExecutableScript.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstFunctionValues.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ReplaceableConstValueSource.java</file><file>core/src/test/java/org/elasticsearch/script/CustomScriptContextTests.java</file><file>core/src/test/java/org/elasticsearch/script/IndexedScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/OnDiskScriptTests.java</file><file>core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java</file></files><comments><comment>Scripting: Allow executable expression scripts for aggregations</comment></comments></commit></commits></item><item><title>[TEST] remove node from nodes list if disruption is removed from node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11595</link><project id="" key="" /><description>If we don't remove the node from the nodes list then later clearDisruption might fail
in case we shut down the node before.
</description><key id="87238252">11595</key><summary>[TEST] remove node from nodes list if disruption is removed from node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-06-11T07:09:38Z</created><updated>2015-06-17T19:46:32Z</updated><resolved>2015-06-15T14:09:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-11T07:13:39Z" id="111020168">good catch. Left one comment about the test
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/test/disruption/NetworkPartition.java</file><file>core/src/test/java/org/elasticsearch/test/disruption/NetworkPartitionTests.java</file></files><comments><comment>Merge pull request #11595 from brwe/netowrk-partition-test</comment></comments></commit></commits></item><item><title>Path to file elasticsearch.pid not exist after restart </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11594</link><project id="" key="" /><description>During start elasticsearch (version 1.6) service after restart ubuntu machine I have following error:

touch: cannot touch ‘/var/run/elasticsearch/elasticsearch.pid’: No such file or directory

This occurs because elasticsearch.pid move to /elasticsearch directory (PID_DIR="/var/run/elasticsearch", PID_FILE="$PID_DIR/$NAME.pid") and ubuntu remove this after restart. Command "touch" does not create parent directories.

In version 1.5 the .pid file was in /var/run/ directiry (PID_FILE="/var/run/$NAME.pid") and so was no error.
</description><key id="87237723">11594</key><summary>Path to file elasticsearch.pid not exist after restart </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">konstantinkostin28</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-06-11T07:06:36Z</created><updated>2017-06-20T12:06:27Z</updated><resolved>2015-06-16T14:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-11T07:35:58Z" id="111030053">@konstantinkostin28 thanks for your reporting and you're right, the PID_DIR should be created before `touch` the file.
</comment><comment author="GlenRSmith" created="2015-06-11T10:48:45Z" id="111083326">I get the same behavior on an Ubuntu 14.04 VM. Was fine on v1.5.2, broken on 1.6.0.
</comment><comment author="yett" created="2015-06-11T12:30:04Z" id="111115792">@tlrx , 
1. Should I change the PID_DIR value to `/var/run/' or 
2. Change  script to create "$PID_DIR" before touch as following?

```
154   mkdir -p "$LOG_DIR" "$DATA_DIR" "$WORK_DIR" "$PID_DIR" &amp;&amp; chown "$ES_USER":"$ES_GROUP" "$LOG_DIR" "$DATA_DIR" "$WORK_DIR" "$PID_DIR"
155   touch "$PID_FILE" &amp;&amp; chown "$ES_USER":"$ES_GROUP" "$PID_FILE"

```

I did the approach 2, and it works. I am just curious about the proper way. Thanks! 
</comment><comment author="tlrx" created="2015-06-11T12:36:10Z" id="111117369">@yett My personal preference goes to `/var/run/elasticsearch/elasticsearch.pid` (approach 2). The thing is to have a consistent, secured way to read/write this file.
</comment><comment author="yett" created="2015-06-11T13:51:39Z" id="111142430">Thanks for the tips. Cheers! 
</comment><comment author="jerrac" created="2015-06-11T19:30:29Z" id="111253902">I just upgraded to 1.6 today and ran into this issue. Annoyingly, the /var/run/elasticsearch directory I created before upgrading to 1.6 (in an attempt to avoid this issue on other nodes...) went away after I rebooted (I had non-ES reasons to reboot). 

This is on Ubuntu 12.04. I was going from 1.4.5 to 1.6, using the apt repo.

Is there a reason apt doesn't deal with this for us?
</comment><comment author="GlenRSmith" created="2015-06-12T15:42:32Z" id="111531579">On my Ubuntu 14.04 instance, there is, in fact a /usr/lib/tmpfiles.d/elasticsearch.conf:
d    /var/run/elasticsearch   0755 root root - -
This doesn't seem to get handled. In fact, I have other tmpfile entries that don't seem to be processed.
</comment><comment author="uschindler" created="2015-06-13T12:33:05Z" id="111706516">Same issue here. I fixed this by putting `PID_DIR="/var/run"` into `/etc/default/elasticsearch`.
In my opinion, on Ubuntu/Debian all pid files are directly in this folder, so PID_DIR by default should just be `/var/run`
</comment><comment author="DASAR" created="2015-06-13T13:15:58Z" id="111709697">Please fix this issue in official Elasticsearch repository (ppa) for Ubuntu. 
</comment><comment author="GlenRSmith" created="2015-06-14T11:40:05Z" id="111815279">@uschindler Can you elaborate on "on Ubuntu/Debian all pid files are directly in this folder"? On my Ubuntu 14.04 LTS install with only Elasticsearch as an additional install, I've got a number of services whose PID files are in their on subdirectories in /var/run (CUPS, for example).
</comment><comment author="uschindler" created="2015-06-14T22:05:59Z" id="111879194">Daemons generally only create a sub-dir if they create more than just a PID as run file. But generally, the pid file should be placed in /var/run and have ending ".pid".

I have to first lookup the packaging rules in Debian, but there is something about that in it!
</comment><comment author="tlrx" created="2015-06-15T14:45:45Z" id="112094433">Sorry for the annoyance. I pushed a fix in #11674, it basically creates &amp; chown the PID_DIR in init.d scripts and had been tested on Ubuntu 12.04, 14.04, Debian 7.8, CentOS6.6 and few others.

The Debian Guidelines are pretty open concerning PID files as long as it is stored in `/var/run` (source: https://www.debian.org/doc/packaging-manuals/fhs/fhs-2.3.txt): 

&gt; Programs may have a subdirectory of /var/run; this is encouraged for programs that use more than
&gt; one run-time file. 

For defaults, we'd like to keep the current `/var/run/elasticsearch` directory. It was already the case on RPM based distributions and keeping things consistent across platforms make scripts easier to maintain. It also helps when multiple instances runs on the same machine and make things simplier when the JVM Security Manager is enabled. Note that it can been overridden using the `PID_DIR` environment variable as @uschindler already mentioned. 
</comment><comment author="arbourd" created="2015-06-15T15:26:11Z" id="112108150">@tlrx Thanks for your work on this. :)
</comment><comment author="uschindler" created="2015-06-15T16:19:16Z" id="112125181">@tlrx Thanks. Patch looks good.

In any case the packaging manuals already say what I mentioned in my first post:

&gt; /var/run : Run-time variable data
&gt; 
&gt; Purpose
&gt; 
&gt; This directory contains system information data describing the system since it
&gt; was booted. Files under this directory must be cleared (removed or truncated as
&gt; appropriate) at the beginning of the boot process. Programs may have a
&gt; subdirectory of /var/run; this is encouraged for programs that use more than
&gt; one run-time file. [42] Process identifier (PID) files, which were originally
&gt; placed in /etc, must be placed in /var/run. The naming convention for PID files
&gt; is &lt;program-name&gt;.pid. For example, the crond PID file is named /var/run/
&gt; crond.pid.

Your text was just a bit clipped. Basically it says: If there is only the PID file and nothing more of runtime data it should be placed directly in /var/run. Otherwise a subdirectory with the name of the program should be created ("this is encouraged for programs that use more than one run-time file").

But I am also fine with your pull request. Thanks in any case!
</comment><comment author="GlenRSmith" created="2015-06-16T07:50:26Z" id="112324118">@tlrx Thanks!
</comment><comment author="bskern" created="2015-07-02T20:01:47Z" id="118151187">When will this get released? I noticed this version [here](https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.6.0.deb) does not contain these changes
</comment><comment author="tlrx" created="2015-07-03T07:20:07Z" id="118260095">@bskern the issue has been reported for 1.6.0, and the package you indicate is 1.6.0 too so it does not contain the fix.

The fix will will be released with 1.6.1, hopefully soon.
</comment><comment author="baelter" created="2015-11-02T09:18:30Z" id="152964436">Also an issue on manjaro
</comment><comment author="yashvit" created="2017-06-16T12:42:32Z" id="309016402">Linux Version: Ubuntu 16.04
Elasticsearch Version: 1.7.3

I have recently added some new nodes to my existing cluster, and randomly one server would not start elasticsearch on restart. When manually trying to start it gives the following error:

&gt;sudo systemctl status elasticsearch

```
* elasticsearch.service - Elasticsearch
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Fri 2017-06-16 12:32:28 UTC; 3s ago
     Docs: http://www.elastic.co
  Process: 1437 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -Des.pidfile=$PID_DIR/elasticsearch.pid -Des.default.path.home=$ES_HOME -Des.default.p
 Main PID: 1437 (code=exited, status=3)

Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]: java.io.FileNotFoundException: /var/run/elasticsearch/elasticsearch.pid (No such file or directory)
Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]:         at java.io.FileOutputStream.open0(Native Method)
Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]:         at java.io.FileOutputStream.open(FileOutputStream.java:270)
Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]:         at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)
Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]:         at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:162)
Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]:         at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:194)
Jun 16 12:32:28 els02.jobsoid.net elasticsearch[1437]:         at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Jun 16 12:32:28 els02.jobsoid.net systemd[1]: elasticsearch.service: Main process exited, code=exited, status=3/NOTIMPLEMENTED
Jun 16 12:32:28 els02.jobsoid.net systemd[1]: elasticsearch.service: Unit entered failed state.
Jun 16 12:32:28 els02.jobsoid.net systemd[1]: elasticsearch.service: Failed with result 'exit-code'.
```


If I create a folder /var/run/elasticsearch and change ownership to elasticsearch:elasticsearch then elasticsearch starts. On restart again the same issue.

I understand this above issue is related to version 1.6 but still I tried changing the PID_DIR folder to /var/run in the service file at /usr/lib/systemd/system/elasticsearch.service

with that it gives an error: java.io.FileNotFoundException: /var/run/elasticsearch.pid (Permission denied) instead of "no such file or directory"

One other thing i noticed, when I manually create the dir in /var/run and force elasticsearch to start, it seems to work fine but the GET /_nodes does not return any stats of the OS or Filesystem. 

Makes me think its some permission issue?</comment><comment author="clintongormley" created="2017-06-20T12:06:27Z" id="309732872">@yashvit it's really time to upgrade :)</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Create PID_DIR in init.d script</comment></comments></commit></commits></item><item><title>Unable To Enable Writes Due To ClusterBlockException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11593</link><project id="" key="" /><description>So while debugging a separate issue in the cluster, I issued the following command via API:

curl -XPUT 'localhost:9200/test-*/_settings' -d '
{
    "index" : {
        "blocks": {
            "write": true,
            "metadata": true
        }
    }
}'

The reasoning is unimportant and related to my stupidity in what exactly this would do, but my plan was to flip right back to false. However, this cannot be done.

{"error":"RemoteTransportException[[es-master2][inet[/&lt;ip&gt;:&lt;port&gt;]][indices:admin/settings/update]]; nested: ClusterBlockException[blocked by: [FORBIDDEN/9/index metadata (api)];]; ","status":403}

After learning a bit more of my actions, it sort of makes sense - after all I did forbid it. However, there seems to be no way to undo this change forcefully that I have discovered. If I try to set to false via localhost on master, same response. If I shutdown all master nodes, and add the following to elasticsearch.yml, no luck:

index.blocks.metadata: false
index.blocks.write: false

I'm assuming because API + index specific settings override yml.

How can this be undone? If I could simply delete these indices, I would. But delete fails because of the same reason. I cannot ignore either, because simple things like localhost:9200/_cat/shards also fails (why, I'm not sure, maybe because index.blocks.metadata is set to true?). While these particular indexes are not so important, other more important ones are on the cluster...

So, how does one sudo or force their way out of this? There must be a way.
</description><key id="87141465">11593</key><summary>Unable To Enable Writes Due To ClusterBlockException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rholloway</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-06-10T23:22:08Z</created><updated>2016-01-18T20:20:52Z</updated><resolved>2016-01-18T20:20:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rholloway" created="2015-06-11T03:29:30Z" id="110986139">Edit, this has been solved! The issue for me was I was trying to revert the change with

curl -XPUT 'localhost:9200/test-*/_settings' -d '
{
    "index" : {
        "blocks": {
            "write": false,
            "metadata": false
        }
    }
}'

which appears to attempt ta change the write block first, which tosses the metadata exception. If I break it into two separate queries, then I was able to revert. Lesson learned - don't lose control changing various settings you aren't familiar with while trying to fix an issue.

curl -XPUT 'localhost:9200/test-*/_settings' -d '
{
    "index" : {
        "blocks": {
            "metadata": false
        }
    }
}'

curl -XPUT 'localhost:9200/test-*/_settings' -d '
{
    "index" : {
        "blocks": {
            "write": false  
        }
    }
}'
</comment><comment author="rholloway" created="2015-06-11T03:44:27Z" id="110991082">Not sure if it's worth reopening issue for, but if anyone ever does see this and happens to know where this setting is persisted to disk, I'd definitely be curious to know. I spent a while trying to track down and was going to shutdown cluster and edit manually, but it's not persisted in the global cluster state file where I thought it might be. 
</comment><comment author="clintongormley" created="2015-06-12T15:35:09Z" id="111529890">Hi @rholloway 

I think this is at least worth discussing.  Reopening
</comment><comment author="clintongormley" created="2016-01-18T20:20:52Z" id="172644084">With https://github.com/elastic/elasticsearch/issues/15278 this setting can now be unset.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw exception if no `elasticsearch.yml` file is found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11592</link><project id="" key="" /><description>Log message to indicate that no `elasticsearch.yml` file was found

Added a warning despite other methods throw an exception to avoid breaking the former behaviour.
Closes https://github.com/elastic/elasticsearch/issues/11510 
</description><key id="87118983">11592</key><summary>Throw exception if no `elasticsearch.yml` file is found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">saschamarkus</reporter><labels><label>:Settings</label><label>enhancement</label><label>feedback_needed</label></labels><created>2015-06-10T21:50:05Z</created><updated>2015-08-12T13:22:08Z</updated><resolved>2015-08-12T13:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T15:32:11Z" id="111529097">Hi @saschamarkus 

Thanks for the PR. I think we should go for the exception instead of the warning.  Anybody disagree?
</comment><comment author="metadave" created="2015-07-30T13:59:30Z" id="126337186">@saschamarkus would you mind rebasing against master to see if that helps with merge conflicts?
</comment><comment author="clintongormley" created="2015-08-05T09:02:58Z" id="127924122">Hi @saschamarkus 

Just pinging in case you didn't see this before.
</comment><comment author="saschamarkus" created="2015-08-05T09:21:07Z" id="127927980">Hi,
Sorry for not answering to your messsage. I'll try to rebase this week.
Right now I've a bit busy but I think I'll manage it by the weekend.

Hi @saschamarkus https://github.com/saschamarkus

Just pinging in case you didn't see this before.

—
Reply to this email directly or view it on GitHub
https://github.com/elastic/elasticsearch/pull/11592#issuecomment-127924122
.
</comment><comment author="saschamarkus" created="2015-08-12T13:22:08Z" id="130299304">Hi,
I created another pull request with me changes on top of the current sources: 
https://github.com/elastic/elasticsearch/pull/12833
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Request to change name of WrapperQueryBuilder (org.elasticsearch.index.query)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11591</link><project id="" key="" /><description>I spent a considerable time today looking for a query builder that would accept a JSON string query (as per the REST API).

I wonder if the term JSON could be incorporated into the name i.e. JsonWrapperQueryBuilder?

Not a big deal but it wasn't obvious to me despite numerous Google searches and lots of hunting, forum posts etc...

Thanks.
</description><key id="87114581">11591</key><summary>Request to change name of WrapperQueryBuilder (org.elasticsearch.index.query)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">MattFriedmanAfilias</reporter><labels><label>:Java API</label><label>adoptme</label><label>docs</label></labels><created>2015-06-10T21:32:26Z</created><updated>2016-01-15T12:41:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-11T12:36:42Z" id="111117508">I agree that the name is not the best... not sure I would associate it with json specifically though, given that it allows to wrap binary content in general, which could be json, yaml or smile. Maybe XContentQueryBuilder or something along those lines?
</comment><comment author="MattFriedmanAfilias" created="2015-06-11T15:33:13Z" id="111174831">Thanks for this. One idea is to create _alias_ methods in QueryBuilders such as (or something like)

```
QueryBuilders.jsonWrapperQuery(String)
QueryBuilders.jsonWrapperQuery(bytes)
QueryBuilders.yamlWrapperQuery(String)
QueryBuilders.yamlWrapperQuery(bytes)
```

etc...

These methods would simply forward the input to the target method: `wrapperQuery()` so they would only exist for clarity.

If you can tolerate these extra convenience methods it might make it easier for someone to locate the method they need.

Or maybe there's a cleaner way someone will think of. Or maybe it is just a documentation thing; I found it hard to locate the method I needed. 

Thanks very much,
Matt
</comment><comment author="clintongormley" created="2015-06-12T09:28:08Z" id="111427812">XContentQueryBuilder is probably no more meaningful than WrapperQuery.  The aliases imply that eg they would convert JSON or YAML into the right format for including in a query, where really this method just injects the raw string...

Probably the best solution is to just document the query builder here: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/query-dsl-queries.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recovering from "Could not find a state file to recover" automatically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11590</link><project id="" key="" /><description>Have a scenario where the user upgraded (to 1.5.2) and downgraded and re-upgraded multiple times, and shards are showing the following exceptions:

```
Caused by: org.elasticsearch.ElasticsearchIllegalStateException: Could not find a state file to recover from among [[id:21, legacy:false, file:/opt/something/data/elasticsearch/data/cluster_name/nodes/0/indices/index_name/1/_state/state-21.st], [id:29, legacy:true, file:/opt/something/data/elasticsearch/data/cluster_name/nodes/0/indices/index_name/1/_state/state-29]]
```

While they can workaround this by moving the state file without the .st out of the shard directories, this can cause other issues if the user is not careful (eg. if they have a large number of shards affected and script it to move the files without .st out of the directory and end up moving the only state file out in some cases). 

It will be nice for the product to handle this workaround automatically without manual intervention and gracefully recover.
</description><key id="87113631">11590</key><summary>Recovering from "Could not find a state file to recover" automatically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2015-06-10T21:28:48Z</created><updated>2016-01-18T20:18:55Z</updated><resolved>2016-01-18T20:18:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T20:18:55Z" id="172643292">This seems to have been fixed. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot/Restore: Add checksums to metadata files in repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11589</link><project id="" key="" /><description>Currently, we only checksums of the data files, leaving possibility for metadata files to get corrupted without us noticing it. We should add checksums to the metadata files and verify them on restore. 
</description><key id="87101202">11589</key><summary>Snapshot/Restore: Add checksums to metadata files in repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T20:44:46Z</created><updated>2015-07-15T23:19:52Z</updated><resolved>2015-07-15T23:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RestoreSource.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/store/ByteArrayIndexInput.java</file><file>core/src/main/java/org/elasticsearch/common/lucene/store/IndexOutputOutputStream.java</file><file>core/src/main/java/org/elasticsearch/common/xcontent/FromXContentBuilder.java</file><file>core/src/main/java/org/elasticsearch/gateway/MetaDataStateFormat.java</file><file>core/src/main/java/org/elasticsearch/index/shard/StoreRecoveryService.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file><file>core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreFormat.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java</file><file>core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/Snapshot.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotMissingException.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/store/ByteArrayIndexInputTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/BlobStoreFormatTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Add checksum to snapshot metadata files</comment></comments></commit></commits></item><item><title>Bake in TieredMergePolicy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11588</link><project id="" key="" /><description>Today we provide the ability to plug in MergePolicy and
we provide the once lucene ships with. We do not recommend to change
the default and even only a small number of expert users would ever touch
this. This commit removes the ancient log byte size and log doc count
merge policy providers, simplifies the MergePolicy wiring and makes the
tiered MP the one and only default. All notions of a merge policy has been
removed from the docs and should be deprecated in the previous version.
</description><key id="87081503">11588</key><summary>Bake in TieredMergePolicy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T19:38:42Z</created><updated>2015-06-11T18:48:17Z</updated><resolved>2015-06-11T10:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-06-10T20:25:23Z" id="110900876">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/merge/policy/AbstractMergePolicyProvider.java</file><file>core/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>core/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>core/src/main/java/org/elasticsearch/index/merge/policy/MergePolicyModule.java</file><file>core/src/main/java/org/elasticsearch/index/merge/policy/MergePolicyProvider.java</file><file>core/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ElasticsearchMergePolicy.java</file><file>core/src/main/java/org/elasticsearch/index/shard/FilterDocValuesProducer.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/MergePolicyConfig.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/VersionFieldUpgrader.java</file><file>core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineMergeTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/merge/policy/MergePolicySettingsTest.java</file><file>core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTest.java</file><file>core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTest.java</file><file>core/src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>core/src/test/java/org/elasticsearch/indices/settings/UpdateSettingsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ParentFieldLoadingBwcTest.java</file><file>core/src/test/java/org/elasticsearch/search/child/ParentFieldLoadingTest.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/index/merge/NoMergePolicyProvider.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Bake in TieredMergePolicy</comment></comments></commit></commits></item><item><title>RPM: Allow to create RPM with &amp; without signing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11587</link><project id="" key="" /><description>In order to create an RPM for integration tests (as well as centos 5), we need to create an RPM that is not signed when it is built.

This can probably be sovled by having another maven profile with a different RPM configuration, that stores the RPM in a different path. This RPM should be used when uploading in the build release script to S3, the signed one should be used for the repositories.
</description><key id="87052298">11587</key><summary>RPM: Allow to create RPM with &amp; without signing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>blocker</label><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T17:55:05Z</created><updated>2015-06-30T12:26:11Z</updated><resolved>2015-06-30T12:26:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-10T19:57:44Z" id="110892877">@spinscale I think that it should be super easy to solve that once #11523 will be merged.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Release: Build two RPMS, signed and unsigned</comment></comments></commit></commits></item><item><title>Shortcut `exists` and `missing` queries when no types/docs exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11586</link><project id="" key="" /><description>There used to be a null check for _field_names mapper not existing. This
was recently removed. However, there is a corner case when the mapper
may be missing: when no types or docs exist at all in the index.

This change adds back a null check and just returns no docs.
</description><key id="87035730">11586</key><summary>Shortcut `exists` and `missing` queries when no types/docs exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T16:55:10Z</created><updated>2015-06-12T15:33:08Z</updated><resolved>2015-06-10T21:58:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-10T19:47:05Z" id="110890040">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/test/java/org/elasticsearch/search/query/ExistsMissingTests.java</file></files><comments><comment>Merge pull request #11586 from rjernst/fix/field-names-null</comment></comments></commit></commits></item><item><title>Remove MergeScheduler pluggability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11585</link><project id="" key="" /><description>Nobody should really plug in a different merge scheduler for elasticsearch.
This is too expert and might cause catastrophic failures.
</description><key id="87015376">11585</key><summary>Remove MergeScheduler pluggability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T15:50:00Z</created><updated>2015-07-05T18:19:50Z</updated><resolved>2015-06-10T18:31:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-06-10T15:51:39Z" id="110808389">LGTM
</comment><comment author="jpountz" created="2015-06-10T15:52:01Z" id="110808501">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add documentation for delete by query plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11584</link><project id="" key="" /><description>Add documentation for delete by query plugin (see #11516). This page is placed in a /plugins directory until we figure where to place all plugins documentation.
</description><key id="87006756">11584</key><summary>Add documentation for delete by query plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T15:25:05Z</created><updated>2015-06-23T11:27:12Z</updated><resolved>2015-06-23T11:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-06-10T17:16:44Z" id="110841528">@tlrx I made a proposal here https://github.com/elastic/elasticsearch-analysis-kuromoji/pull/62 about plugin (module) documentation.
I think it should goes into `src/main/asciidoc` to meet maven convention for `asciidoc` docs.

So I'd make that consistent. In maven world, source files (code, documentation, resources...) should always be in `src` dir.

Would love to get @clintongormley thoughts about this so I can move my PR from the plugin repo to elasticsearch repo.
</comment><comment author="tlrx" created="2015-06-16T11:53:37Z" id="112397875">@clintongormley I just updated with your modification, thanks for your review.

@dadoonet I think I created too many sub sections :) Thanks for your review by the way.
</comment><comment author="clintongormley" created="2015-06-18T12:28:54Z" id="113138321">LGTM - we'll need to change the `&lt;&lt; &gt;&gt;` links when we move it into the plugin docs, but we can do that then.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix MapperException detection during translog ops replay</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11583</link><project id="" key="" /><description>The current ExceptionsHelper.unwrapCause(exception) requires the incoming exception to support ElasticsearchWrapperException , which TranslogRecoveryPerformer.BatchOperationException doesn't implement. I opted for a more generic solution

Example failure: http://build-us-00.elastic.co/job/es_g1gc_master_metal/8534/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/

See #11363
</description><key id="87004195">11583</key><summary>Fix MapperException detection during translog ops replay</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T15:16:23Z</created><updated>2015-06-10T17:06:59Z</updated><resolved>2015-06-10T17:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-10T15:51:08Z" id="110808225">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file></files><comments><comment>Merge pull request #11583 from bleskes/mapper_exception_unwrap</comment></comments></commit></commits></item><item><title>ES waiting for observer timeout before failing consistency check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11582</link><project id="" key="" /><description>When consistency policy isn't satisfied, rather than bail out in the precheck, we are waiting the default cluster state observer timeout (60s) before sending the error response.  The only way to tell this is happening if you don't have the response is the timeout getting logged, in `DEBUG`:

```
[2015-06-10 09:44:35,628][DEBUG][action.index             ] [Scaleface] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
```

On a busy cluster with lots of writes, you will start to see metadata operations slow down even though CPU and memory aren't necessarily getting any pressure.

To reproduce, start a single 1.5.2 node and create an index with more than one replica (so there isn't quorum):

```
curl -s -XPUT localhost:9200/foo -d'---
number_of_shards: 1
number_of_replicas: 2
'
```

Verify topology:

```
% es shards
foo 0 p STARTED    0 115b 10.0.1.200 Scaleface 
foo 0 r UNASSIGNED                             
foo 0 r UNASSIGNED                             
```

Index a doc:

```
curl -s -XPUT localhost:9200/foo/t/1 -d'{"f": 1}'
```

After a minute you'll get:

```
error: "UnavailableShardsException[[foo][0] Not enough active copies to meet write\
  \ consistency of [QUORUM] (have 1, needed 2). Timeout: [1m], request: index {[foo][t][1],\
  \ source[{\"f\":1}]}]"
status: 503
```

It looks like the great refactoring in 5bdfdc42d99b12e7999fb36c8f1a72d43d7b5606 may have switched the behavior?  If that's the correct behavior, maybe we can log something in `ERROR` after `N` indexing requests have failed consistency check?

Here's the same operation with `TRACE`:

```
[2015-06-10 10:08:39,271][TRACE][action.index             ] [Impossible Man] primary shard [[foo][0]] is not yet active or we do not know the node it is assigned to [null], scheduling a retry.
[2015-06-10 10:08:39,279][TRACE][action.index             ] [Impossible Man] observer: sampled state rejected by predicate (version [2], status [APPLIED]). adding listener to ClusterService
[2015-06-10 10:08:39,279][TRACE][action.index             ] [Impossible Man] observer: postAdded - predicate rejected state (version [2], status [APPLIED])
[2015-06-10 10:09:39,272][DEBUG][action.index             ] [Impossible Man] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-06-10 10:09:39,274][TRACE][action.index             ] [Impossible Man] primary shard [[foo][0]] is not yet active or we do not know the node it is assigned to [null], scheduling a retry.
```
</description><key id="87004039">11582</key><summary>ES waiting for observer timeout before failing consistency check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">drewr</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2015-06-10T15:15:47Z</created><updated>2015-12-04T03:00:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-17T12:14:43Z" id="112778311">&gt; On a busy cluster with lots of writes, you will start to see metadata operations slow down even though CPU and memory aren't necessarily getting any pressure.

Can you clarify what you mean by slowing down? I can think of a scenario where many waiting indexing operations slow down cluster state processing (we try the indexing operation on every cluster state change and can be more efficient.)

&gt; It looks like the great refactoring in 5bdfdc4 may have switched the behavior? If that's the correct behavior,

This behavior has been there from the beginning time, or something very close to it. The rational is that the needed replicas might be assigned soon and it's worth while waiting (think about a newly create index).   Can you sure more around why you think something changed?

&gt; if you don't have the response is the timeout getting logged, in DEBUG
&gt;  maybe we can log something in ERROR after N indexing requests have failed consistency check?

That log message should actually be TRACE as it's may be very high volume (like in your case). I think there is merit to have a logging infra that throttle logging but that would be a separate issue.
</comment><comment author="celesteking" created="2015-09-22T14:12:37Z" id="142300526">started getting same message once server ran out of disk space.

I allocated more, but it didn't go away. Also, cluster is now in red health, which is just plain stupid as the source of problem has been rectified. 
</comment><comment author="kamakazikamikaze" created="2015-10-13T16:24:23Z" id="147767496">SImilar to celesteking but Java ran out of memory which required a restart. Logs are now full of this message and server fails to write even 30% of documents sent to it.
</comment><comment author="bachkhoabk47" created="2015-12-03T03:31:18Z" id="161505649">&gt; SImilar to celesteking but Java ran out of memory which required a restart. Logs are now full of this message and server fails to write even 30% of documents sent to it.

Hi Kamakazikamikaze and everybody. 
I'm Tan and I come from Vietnam.
You know, I would like to sent data from Syslog-ng to Elasticsearch with around 1 million message line, but in ES side, I just see 515,007 docs. It means ES just received and processed haft past of the number messages that sent from Syslog-ng.
Of course, I also checked the bandwidth, hdd, ram and cpu. They worked well and just used around 50% their capacity.

Mr.Kamakazikamikaze. I found your comment here and I would like you give me some your experience about that.
Does has any parameters that I have to config for tuning my system?

Thank you very much!
</comment><comment author="clintongormley" created="2015-12-03T14:27:02Z" id="161655170">@bachkhoabk47 This sounds like a question for the forums (http://discuss.elastic.co/) not for the issues list.
</comment><comment author="bachkhoabk47" created="2015-12-04T03:00:36Z" id="161858438">Hi Clintongormley!
Thanks for you feedback. 
I've posted my question to the forums.
Thank you so much
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Must + Should combined in Bool : unexpected result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11581</link><project id="" key="" /><description>I'm using ElasticSearch 1.6.0

I have to combine a must with a should in a bool.
The must filter results 1229 items. Combined with the should filter, I only get 1 result (the item with id 2). It seems that 'should' works as 'must'. But why?

```
{
    "query": {
        "filtered": {
            "query": {
                "match_all": []
            },
            "filter": {
                "bool": {
                    "must": {
                        "range": {
                            "date": {
                                "from": "2015-06-28",
                                "to": "2015-08-08"
                            }
                        }
                    },
                    "should": [
                        {
                            "term": {
                                "id": 2
                            }
                        }
                    ]
                }
            }
        }
    }
}
```
</description><key id="86996262">11581</key><summary>Must + Should combined in Bool : unexpected result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Schellingerht</reporter><labels /><created>2015-06-10T14:52:07Z</created><updated>2015-06-10T17:41:52Z</updated><resolved>2015-06-10T15:04:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-10T15:04:51Z" id="110787907">With a bool filter, at least one should clause must match (otherwise it is meaningless).  You only have one, so it has to match.
</comment><comment author="Schellingerht" created="2015-06-10T17:41:52Z" id="110848167">Thanks. I was really confused between bool query and bool filter.

Documentation: https://www.elastic.co/guide/en/elasticsearch/guide/current/_most_important_queries_and_filters.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `rewrite` query parameter to the `indices.validate_query` API spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11580</link><project id="" key="" /><description>Added in #10147
</description><key id="86992546">11580</key><summary>Add `rewrite` query parameter to the `indices.validate_query` API spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.6.1</label><label>v1.7.0</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T14:39:45Z</created><updated>2015-06-17T19:58:52Z</updated><resolved>2015-06-15T15:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-10T15:12:04Z" id="110790631">thanks @gmarz maybe add a REST test that uses it too?
</comment><comment author="gmarz" created="2015-06-12T17:08:47Z" id="111559739">@javanna thanks for the review, I added it to the basic yaml test.  Let me know if your prefer that I squash these two commits.
</comment><comment author="javanna" created="2015-06-15T07:19:02Z" id="111963801">LGTM thanks @gmarz 
</comment><comment author="gmarz" created="2015-06-15T15:20:13Z" id="112104747">Merged to master (https://github.com/elastic/elasticsearch/commit/8dd7dae5b986148be126a336c937e4c29c0bfd49) 1.x (https://github.com/elastic/elasticsearch/commit/a351f4388497434786b735e467162ec14f719676) and 1.6 (https://github.com/elastic/elasticsearch/commit/a647e1a702b901cafdb4e7c362e4ef1193854772)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make it possible to see if a term matches text in the index or does not.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11579</link><project id="" key="" /><description>When suggestions are found you see these in the options of the results But when a term is spelled correctly you also see no options filled in.

So there is no way to really know if the suggester was not able to find a matching suggestion or if the term was spelled correctly to begin with.
</description><key id="86990859">11579</key><summary>Make it possible to see if a term matches text in the index or does not.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ilanrivers</reporter><labels><label>:Suggesters</label><label>discuss</label></labels><created>2015-06-10T14:34:19Z</created><updated>2017-02-08T11:13:18Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T15:06:36Z" id="111520456">I'm afraid I don't understand what you're asking. You want to expand on your question, perhaps with JSON examples?
</comment><comment author="ilanrivers" created="2015-06-16T14:55:25Z" id="112460922">I will try to give a clear example:
Lets say you fill an index with 3 words:

```
POST /test_es_suggest/type
{
  "word": "Software"  
}

POST /test_es_suggest/type
{
  "word": "Developer"  
}

POST /test_es_suggest/type
{
  "word": "Programmer"  
}
```

Now i use the Suggest API as follows:

```
GET /test_es_suggest/_suggest
{
  "Suggest": {
      "term": {
        "field": "word",
        "suggest_mode": "popular",
        "size": 2,
        "prefix_len": 1,
        "analyzer": "default"
      },
      "text": "Softwarer"
    }
}
```

I receive the following results which is fine because software was spelled bad now it is corrected
The options is filled with the new word:

```
"Suggest": [
      {
         "text": "softwarer",
         "offset": 0,
         "length": 9,
         "options": [
            {
               "text": "software",
               "score": 0.875,
               "freq": 1
            }
         ]
      }
   ]
```

Now if i search for the correct word:

```
GET /test_es_suggest/_suggest
{
  "Suggest": {
      "term": {
        "field": "word",
        "suggest_mode": "popular",
        "size": 2,
        "prefix_len": 1,
        "analyzer": "default"
      },
      "text": "Software"
    }
}
```

The results is as follows:

```
"Suggest": [
      {
         "text": "software",
         "offset": 0,
         "length": 8,
         "options": []
      }
   ]
```

Options is empty so I assume there is nothing to be suggested OR was the word just not found?

If i search for "softwareasdfasdf" i get the exact same response which i know i get that response because there is no suitable word to be suggested.

```
"Suggest": [
      {
         "text": "softwareasdfasdf",
         "offset": 0,
         "length": 16,
         "options": []
      }
   ]
```

In the example with "Software" I would like to know that the word was correct so there is nothing to be suggested. In my opinion there is a clear difference in the reason both responses show an empty "options" array. One word cannot be found and the other word is already correct.

So my question is, can something be added to the response so I know the difference?
</comment><comment author="clintongormley" created="2015-06-18T12:43:47Z" id="113143403">What about just setting the `suggest_mode` to `always`?

See https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-term.html
</comment><comment author="ilanrivers" created="2015-06-18T13:24:33Z" id="113154587">According to the documentation:
Suggest mode Always : Suggest any matching suggestions based on terms in the suggest text. 

But if I use suggest mode always I still do not know if the word was found or not and I still receive an empty options array by just changing the mode to always so it seems this does not guarantee that you receive a suggestion.

```
GET /test_es_suggest/_suggest
{
  "Suggest": {
      "term": {
        "field": "word",
        "suggest_mode": "always",
        "size": 2,
        "prefix_len": 1,
        "analyzer": "default"
      },
      "text": "softwaredev"
    }
}
```
</comment><comment author="ilanrivers" created="2015-07-09T14:17:39Z" id="119997793">Was this question clear now? @clintongormley 
</comment><comment author="ilanrivers" created="2015-09-08T14:32:47Z" id="138583748">Is this issue going to be done? It seems as if a field can be added to indicate if the search term is an exact match with an word in the index and that solves the problem.
</comment><comment author="DaanBiesterbos" created="2015-09-29T13:12:44Z" id="144054158">Indeed. It would be very useful to know why no suggestion was returned exactly. :+1:  
</comment><comment author="Ard-Jan" created="2015-10-08T06:11:56Z" id="146430063">It would be very helpful for me too, to see the difference between a term that is correctly spelled and a term where is no suggestion for!
</comment><comment author="nkelly75" created="2017-02-08T11:13:18Z" id="278299691">Might be simplifying here but my experience is that the term suggester and phrase suggester can be valuable with little custom effort. They are good at suggesting corrections so if the data has 'virtualize' and I type 'virtualise' I'll get a useful suggestion. The suggest_mode 'always' doesn't seem to work as I'd expect so when a user types a term that is _good_ the term suggester can't offer feedback.

In the process of working around this by extracting interesting terms from main index and generating a special index for use with a completion suggester. This is going pretty well but seems strange that the term/phrase suggesters can do the harder job of helping suggest corrections but can't provide results that say yep 'virtuali' is a term that can be completed with 'virtualize'.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reverse nested aggregations for parent child relationships</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11578</link><project id="" key="" /><description>Hi,
I'm noticing the reverse nested aggregation feature is available only for the nested data model but not for parent-child. We need to use parent-child since update latency is very important to us but noticing that aggregations which are several levels deep are not possible in a single query using parent child.

Here is how this can be done with the Nested model. Is this something which can be added for parent-child?

REQUEST:-
POST/document_test/document/_search?search_type=count{
    "aggs": {
        "created_date_agg": {
            "nested": {
                "path": "metadata"
            },
            "aggs": {
                "created_date_agg_filter": {
                    "filter": {
                        "term": {
                            "fieldid": "CREATEDDATE"
                        }
                    },
                    "aggs": {
                        "created_date_agg_values": {
                            "terms": {
                                "field": "fieldvalue"
                            },
                            "aggs": {
                                "metadata_to_parent": {
                                    "reverse_nested": {

```
                                },
                                "aggs": {
                                    "filetype_agg": {
                                        "nested": {
                                            "path": "metadata"
                                        },
                                        "aggs": {
                                            "filetype_agg_filter": {
                                                "filter": {
                                                    "term": {
                                                        "fieldid": "DOCTYPE"
                                                    }
                                                },
                                                "aggs": {
                                                    "filetype_agg_values": {
                                                        "terms": {
                                                            "field": "fieldvalue"
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

}

RESPONSE:-
RESPONSE: -"aggregations": {
    "created_date_agg": {
        "doc_count": 12,
        "created_date_agg_filter": {
            "doc_count": 4,
            "created_date_agg_values": {
                "doc_count_error_upper_bound": 0,
                "sum_other_doc_count": 0,
                "buckets": [
                    {
                        "key": "6/9/2015",
                        "doc_count": 3,
                        "metadata_to_parent": {
                            "doc_count": 3,
                            "filetype_agg": {
                                "doc_count": 9,
                                "filetype_agg_filter": {
                                    "doc_count": 3,
                                    "filetype_agg_values": {
                                        "doc_count_error_upper_bound": 0,
                                        "sum_other_doc_count": 0,
                                        "buckets": [
                                            {
                                                "key": "Excel",
                                                "doc_count": 2
                                            },
                                            {
                                                "key": "Microsoft Word",
                                                "doc_count": 1
                                            }
                                        ]
                                    }
                                }
                            }
                        }
                    },
                    {
                        "key": "6/10/2015",
                        "doc_count": 1,
                        "metadata_to_parent": {
                            "doc_count": 1,
                            "filetype_agg": {
                                "doc_count": 3,
                                "filetype_agg_filter": {
                                    "doc_count": 1,
                                    "filetype_agg_values": {
                                        "doc_count_error_upper_bound": 0,
                                        "sum_other_doc_count": 0,
                                        "buckets": [
                                            {
                                                "key": "Powerpoint",
                                                "doc_count": 1
                                            }
                                        ]
                                    }
                                }
                            }
                        }
                    }
                ]
            }
        }
    }
}
}
</description><key id="86983022">11578</key><summary>Reverse nested aggregations for parent child relationships</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels /><created>2015-06-10T14:07:28Z</created><updated>2015-06-12T15:05:28Z</updated><resolved>2015-06-12T15:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T15:05:27Z" id="111520152">Duplicate of #5306
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rewrite fields with `just_name` option to their actual index names in MLT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11577</link><project id="" key="" /><description>Closes #11573
</description><key id="86955740">11577</key><summary>Rewrite fields with `just_name` option to their actual index names in MLT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:More Like This</label><label>bug</label></labels><created>2015-06-10T12:35:08Z</created><updated>2015-07-12T10:29:14Z</updated><resolved>2015-07-10T21:18:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-06-10T16:30:22Z" id="110823623">LGTM
</comment><comment author="alexksikes" created="2015-06-17T15:27:41Z" id="112846190">Not sure this entirely solves it. The items have fields that should be mapped to their respective index names as well before fetching. Perhaps we should add `just_name` functionality to TVs API?
</comment><comment author="s1monw" created="2015-07-09T11:48:31Z" id="119924813">@alexksikes the TV API is pretty much broken and I am not sure if it's fixable with that regard. I really don't know how to fix this since we might reanalyze etc.
</comment><comment author="alexksikes" created="2015-07-09T12:17:23Z" id="119936609">@s1monw yes so then this is a good enough fix.
</comment><comment author="clintongormley" created="2015-07-12T10:29:14Z" id="120706605">@s1monw this was autoclosed when we deleted 1.x - could you reopen against master?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test: rename waitForConcreteMappingsOnAll &amp;  waitForMappingOnMaster to assertConcreteMappingsOnAll &amp; assertMappingOnMaster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11576</link><project id="" key="" /><description>Now that mapping updates are sync and done before indexing we don't really need the waiting component. Also, removed many places were they were used as safe guard against delayed mapping updates, which are now not needed.
</description><key id="86951388">11576</key><summary>Test: rename waitForConcreteMappingsOnAll &amp;  waitForMappingOnMaster to assertConcreteMappingsOnAll &amp; assertMappingOnMaster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T12:21:04Z</created><updated>2015-06-10T12:38:04Z</updated><resolved>2015-06-10T12:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-06-10T12:33:18Z" id="110732177">:+1: 
</comment><comment author="jpountz" created="2015-06-10T12:36:19Z" id="110733176">This looks great, I like how it simplifies these tests.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsTests.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateTests.java</file><file>core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesTests.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateTests.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Merge pull request #11576 from bleskes/wait_on_mapping_remove</comment></comments></commit></commits></item><item><title>Add Aspire for Elasticsearch to integrations page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11575</link><project id="" key="" /><description>Add Aspire for Elasticsearch to the integrations page, and add two missing colons to other integrations.
</description><key id="86948690">11575</key><summary>Add Aspire for Elasticsearch to integrations page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skearns64</reporter><labels /><created>2015-06-10T12:13:03Z</created><updated>2015-06-11T13:04:05Z</updated><resolved>2015-06-11T13:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #11575 from skearns64/aspire-doc</comment></comments></commit></commits></item><item><title>Add back support for deprectated percent_terms_to_match REST parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11574</link><project id="" key="" /><description>This was removed in 1.5 but should still be supported until the next major release.

Closes #11572
</description><key id="86923196">11574</key><summary>Add back support for deprectated percent_terms_to_match REST parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:More Like This</label><label>regression</label><label>v1.6.1</label></labels><created>2015-06-10T10:41:18Z</created><updated>2015-06-12T15:03:27Z</updated><resolved>2015-06-10T12:31:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-06-10T10:42:23Z" id="110695139">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MoreLikeThis Query doesn't work with `just_path` option pre 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11573</link><project id="" key="" /><description>the term vector API which is now used to build MLT queries get the term vector potentially from a different field (source) than it's executed against. This will result in empty queries and should be rewritten against it's index name field
</description><key id="86917252">11573</key><summary>MoreLikeThis Query doesn't work with `just_path` option pre 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:More Like This</label><label>bug</label></labels><created>2015-06-10T10:23:26Z</created><updated>2016-01-18T20:17:14Z</updated><resolved>2016-01-18T20:17:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-16T09:55:08Z" id="121912541">Bumping the version up to 1.7.1 for the release today.
</comment><comment author="clintongormley" created="2016-01-18T20:17:14Z" id="172642955">The `path` parameter has been removed.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_mlt API ignores deprecated `percent_terms_to_match` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11572</link><project id="" key="" /><description>we moved to `minimum_should_match` but we should still accept the `percent_terms_to_match` parameter until 2.0
</description><key id="86915491">11572</key><summary>_mlt API ignores deprecated `percent_terms_to_match` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>bug</label><label>v1.5.3</label><label>v1.6.1</label></labels><created>2015-06-10T10:19:03Z</created><updated>2015-06-10T12:35:49Z</updated><resolved>2015-06-10T12:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-10T12:35:49Z" id="110733050">fixed see #11574 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update REST API documentation with valid examples</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11571</link><project id="" key="" /><description>One example is cluster stats API:
https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html

Fields that were recently added do not show up in the sample json response.
Documentation should be up to date with the code.
</description><key id="86914759">11571</key><summary>Update REST API documentation with valid examples</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spapin</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-06-10T10:15:42Z</created><updated>2015-08-13T10:10:39Z</updated><resolved>2015-08-13T10:10:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vineelyalamarthy" created="2015-06-12T18:58:29Z" id="111588956">Can I work on this bug, How to get started ?
</comment><comment author="clintongormley" created="2015-06-12T19:08:59Z" id="111591225">@vineelyalamarthy of course - go for it.  See https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md
</comment><comment author="vineelyalamarthy" created="2015-06-12T19:14:07Z" id="111592071">@clintongormley  Thank you so much, may be will ask you, if I have  some questions. Actually, I plan to start few easy ones(Low Hanging Fruit labelled ones), then get some familiarity with it and then get into core ones.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fixes #11571 - update "Cluster Stats" documentation with valid example</comment></comments></commit></commits></item><item><title>Search Templates: Adds API endpoint to render search templates as a response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11570</link><project id="" key="" /><description>Adds a '_render/template' endpoint which will render a search template in the response with a provided `params` object.

Closes #6821
</description><key id="86896801">11570</key><summary>Search Templates: Adds API endpoint to render search templates as a response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Templates</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T09:04:30Z</created><updated>2015-07-01T08:12:36Z</updated><resolved>2015-06-30T15:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-06-12T16:59:43Z" id="111557132">While debugging my templates I wasn't quite sure if my saved (to file) templates had taken effect i.e. reloaded from disk. Might it makes sense to include some form of template version ID in the output - even if its based on something like System.identityHashCode(cachedScriptObject)?
That way I can tell what I am testing against (I couldn't see a way of forcing an immediate script reload)
</comment><comment author="polyfractal" created="2015-06-12T19:24:52Z" id="111594363">Two comments, but LGTM (although admittedly I don't have much experience with this part of the code).

Agree with @markharwood, would be nice to see visual indication when a template changes.  

On a similar note, when validating against static file templates...does this always validate using local templates?  Because if it redirects to a different node, an indicator of what node executed the validation may be helpful too, in case nodes have different static files.
</comment><comment author="clintongormley" created="2015-06-15T09:10:45Z" id="111987815">&gt; Agree with @markharwood, would be nice to see visual indication when a template changes.

The same issue occurs when actually running scripts/templates.  How do you know that it has been reloaded?  The easiest way is just to watch the log (which you're likely to be able to see during development anyway).  Also, we have plans (on linux at least) to support inotify, which will cause reloads to happen immediately.  I don't think we need to add metadata here.

&gt; On a similar note, when validating against static file templates...does this always validate using local templates? Because if it redirects to a different node, an indicator of what node executed the validation may be helpful too, in case nodes have different static files.

Again, same problem when executing scripts/templates.  Making sure you have the same file on all nodes is the developer's job.  We could provide some API to (eg) check that a particular file exists and is the same on all nodes, but I don't think this API is the place to do that.
</comment><comment author="colings86" created="2015-06-15T11:32:24Z" id="112026005">@markharwood @polyfractal @clintongormley I pushed a commit with changes based on your reviews
</comment><comment author="clintongormley" created="2015-06-17T19:35:21Z" id="112922476">Hmmm it would be nice to just run a quick JSON parsing check on the output.
</comment><comment author="colings86" created="2015-06-18T08:55:55Z" id="113080901">I have renamed the classes and the endpoint to use `render` instead of `validate`. The original intent of this PR was to show the user what the template and params creates as a final output which would be run on a search, not to catch all the cases where the resulting request may be invalid. This can be done easily by running the template in a search request. The missing bit was something to show the user what the template becomes befor the search is run and this endpoint is intended to fill that gap.
</comment><comment author="clintongormley" created="2015-06-18T18:16:07Z" id="113244430">+1
</comment><comment author="markharwood" created="2015-06-30T15:55:11Z" id="117236994">Rest tests currently failing due to the way the build process works but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: Rename caches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11569</link><project id="" key="" /><description>With the merge of queries and filters, it would make sense to rename our caches:
- filter cache -&gt; query cache
- shard query cache -&gt; shard request cache

I also have hopes that the latter renaming will help remove confusion that the request cache is short-lived since it is invalidated on every refresh.
</description><key id="86873927">11569</key><summary>Core: Rename caches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>blocker</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T07:34:21Z</created><updated>2015-06-29T08:21:19Z</updated><resolved>2015-06-29T08:21:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-10T07:38:06Z" id="110636253">++
</comment><comment author="jpountz" created="2015-06-10T07:40:33Z" id="110637442">One challenge here is that the new setting for the size of the query cache would be `index.cache.query.size` while it is also the old setting for the request cache size. So we need somehow to be able to differenciate the two.
</comment><comment author="s1monw" created="2015-06-11T10:14:15Z" id="111077074">&gt; One challenge here is that the new setting for the size of the query cache would be index.cache.query.size while it is also the old setting for the request cache size. So we need somehow to be able to differentiate the two.

yeah we can't really... we can go to `index.shard.query.cache.size` maybe?
</comment><comment author="clintongormley" created="2015-06-12T14:13:27Z" id="111507511">And `node.cache.query.size` given that it is node level?
</comment><comment author="bleskes" created="2015-06-12T14:27:38Z" id="111512517">we typically prefix the node level index related settings with `indices`. I like the cache prefix as well.

How about `indices.cache.shard_requests.*` (using adrien’s naming of `shard request cache`)?

&gt; On 12 Jun 2015, at 16:13, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; And node.cache.query.size given that it is node level?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="jpountz" created="2015-06-15T08:13:31Z" id="111973905">@bleskes That works for me, but the issue here is not the new name of the request cache, but the new name of the query cache (formerly known as filter cache) so that it would be different from the previous name of the request cache.
</comment><comment author="bleskes" created="2015-06-15T14:09:15Z" id="112082409">@jpountz yeah, I got confused. The important part for me was to point out the usage of "indices." as node level settings which related to indices. 

As far as the name collision goes - I wonder how many people use the current query cache (i.e. shard requests caching) and thus how bad it would be to make a breaking change and change the semantics of that settings.

Another alternative, which I'm on the fence about but think it's worth mentioning, is using the word segments in the name  - as the queries are cached on a segment level. So that will be `indices.cache.segment.query` &amp; `indices.cache.shard.requests`  . 
</comment><comment author="jpountz" created="2015-06-18T06:41:40Z" id="113053456">If there are no objections, I will try to implement Boaz's last proposal.
</comment><comment author="s1monw" created="2015-06-22T12:45:45Z" id="114088886">&gt; If there are no objections, I will try to implement Boaz's last proposal.

++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/filter/FilterCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/index/IndexQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/none/NoneQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/RequestCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineSearcherFactory.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java</file><file>core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file><file>core/src/test/java/org/elasticsearch/test/search/MockSearchService.java</file></files><comments><comment>Rename caches.</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>core/src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/filter/FilterCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheModule.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/index/IndexQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/query/none/NoneQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/RequestCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineSearcherFactory.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/IndicesQueryCache.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java</file><file>core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java</file><file>core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksTests.java</file><file>core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/cache/query/IndicesRequestCacheTests.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/search/scriptfilter/ScriptQuerySearchTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file><file>core/src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file><file>core/src/test/java/org/elasticsearch/test/engine/MockEngineSupport.java</file><file>core/src/test/java/org/elasticsearch/test/search/MockSearchService.java</file></files><comments><comment>Rename caches.</comment></comments></commit></commits></item><item><title>Rivers removal.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11568</link><project id="" key="" /><description>While we had initially planned to keep rivers around in 2.0 to ease migration,
keeping support for rivers is challenging as it conflicts with other important
changes that we want to bring to 2.0 like synchronous dynamic mappings updates.
Nothing impossible to fix, but it would increase the complexity of how we
deal with dynamic mappings updates and manage rivers, while handling dynamic
mappings updates correctly is important for resiliency and rivers are on the go.
So removing rivers in 2.0 may well be a better trade-off.
</description><key id="86871295">11568</key><summary>Rivers removal.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>breaking</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-06-10T07:24:37Z</created><updated>2015-06-17T06:21:36Z</updated><resolved>2015-06-17T06:20:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-06-10T07:39:44Z" id="110637015">LGTM
</comment><comment author="dadoonet" created="2015-06-17T06:21:36Z" id="112669371">w00t! :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/river/AbstractRiverComponent.java</file><file>core/src/main/java/org/elasticsearch/river/River.java</file><file>core/src/main/java/org/elasticsearch/river/RiverComponent.java</file><file>core/src/main/java/org/elasticsearch/river/RiverException.java</file><file>core/src/main/java/org/elasticsearch/river/RiverIndexName.java</file><file>core/src/main/java/org/elasticsearch/river/RiverModule.java</file><file>core/src/main/java/org/elasticsearch/river/RiverName.java</file><file>core/src/main/java/org/elasticsearch/river/RiverNameModule.java</file><file>core/src/main/java/org/elasticsearch/river/RiverSettings.java</file><file>core/src/main/java/org/elasticsearch/river/RiversManager.java</file><file>core/src/main/java/org/elasticsearch/river/RiversModule.java</file><file>core/src/main/java/org/elasticsearch/river/RiversPluginsModule.java</file><file>core/src/main/java/org/elasticsearch/river/RiversService.java</file><file>core/src/main/java/org/elasticsearch/river/RiversTypesRegistry.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/PublishRiverClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/RiverClusterChangedEvent.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/RiverClusterService.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/RiverClusterState.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/RiverClusterStateListener.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/RiverClusterStateUpdateTask.java</file><file>core/src/main/java/org/elasticsearch/river/cluster/RiverNodeHelper.java</file><file>core/src/main/java/org/elasticsearch/river/dummy/DummyRiver.java</file><file>core/src/main/java/org/elasticsearch/river/dummy/DummyRiverModule.java</file><file>core/src/main/java/org/elasticsearch/river/routing/RiverRouting.java</file><file>core/src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file><file>core/src/main/java/org/elasticsearch/river/routing/RiversRouting.java</file><file>core/src/test/java/org/elasticsearch/river/RiverTests.java</file></files><comments><comment>Merge pull request #11568 from jpountz/remove/rivers</comment></comments></commit></commits></item><item><title>1.6 hangs on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11567</link><project id="" key="" /><description>Hello,

Just trying out 1.6 and after running ./elasticsearch it just hangs at the "gateway….recovered indices" line. Also, in your instructions it shows "recovered [1] indices into cluster" - on mine it says [0]. 

Here is a copy of what's on my screen….thanks!

[2015-06-10 02:56:51,667][INFO ][node                     ] [War Machine] version[1.6.0], pid[6371], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-06-10 02:56:51,668][INFO ][node                     ] [War Machine] initializing ...
[2015-06-10 02:56:51,679][INFO ][plugins                  ] [War Machine] loaded [], sites []
[2015-06-10 02:56:51,817][INFO ][env                      ] [War Machine] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [89.1gb], net total_space [124.9gb], types [ext4]
[2015-06-10 02:56:57,978][INFO ][node                     ] [War Machine] initialized
[2015-06-10 02:56:57,978][INFO ][node                     ] [War Machine] starting ...
[2015-06-10 02:56:58,150][INFO ][transport                ] [War Machine] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.102.138.168:9300]}
[2015-06-10 02:56:58,184][INFO ][discovery                ] [War Machine] elasticsearch/g-8pZesRQMeh9DUCcy_OPw
[2015-06-10 02:57:01,997][INFO ][cluster.service          ] [War Machine] new_master [War Machine][g-8pZesRQMeh9DUCcy_OPw][trikjer][inet[/10.102.138.168:9300]], reason: zen-disco-join (elected_as_master)
[2015-06-10 02:57:02,046][INFO ][http                     ] [War Machine] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.102.138.168:9200]}
[2015-06-10 02:57:02,046][INFO ][node                     ] [War Machine] started
[2015-06-10 02:57:02,425][INFO ][gateway                  ] [War Machine] recovered [0] indices into cluster_state
</description><key id="86815489">11567</key><summary>1.6 hangs on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trident60</reporter><labels /><created>2015-06-10T03:06:15Z</created><updated>2015-06-10T03:34:54Z</updated><resolved>2015-06-10T03:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-10T03:14:22Z" id="110571085">It will only ever report &gt;0 recovered indices if there is data in the cluster, it looks like this is a brand new cluster with no data in it.
Have you tried to check if ES is responding to a curl, it looks like it will! That last log line is what you'd see once ES has started up and is ready to do it's thing.

Also please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="trident60" created="2015-06-10T03:31:39Z" id="110573920">Thank you for the quick reply!
So it will only report [0] if there IS data in the cluster? And this looks like a new cluster with NO data in it? Yet it's reporting [0]…I'm even more confused…lol

If it's ready to do it's thing shouldn't it "finish" and leave me at a prompt? As it is, I have to cntrl+c to get out of that process and ES is not running. 

Here is the response back from a curl attempt:
curl: (7) Failed to connect to localhost port 9200: Connection refused

Okay…I will try Freenode…this just seemed like a bug as I've used past versions and never
had this happen ;)
</comment><comment author="markwalkom" created="2015-06-10T03:32:33Z" id="110574086">You need to run it as daemon mode, -d, for it ti return you to the console.
</comment><comment author="trident60" created="2015-06-10T03:34:11Z" id="110574190">Ok….I did that.
./elasticsearch -d 
It didn't display anything, just returned me to the console it's not running and curl still failed to connect.

Maybe I missed something?
</comment><comment author="trident60" created="2015-06-10T03:34:54Z" id="110574281">Never mind…lol

I didn't miss anything…it just took a second to start ;)

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SearchParseException and transporterror</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11566</link><project id="" key="" /><description>I have an error as bellows, can anyone help?
"index": "guba",
"shard": 0,
"status": 400,
"reason": "RemoteTransportException[[ubuntu11][inet[/219.224.135.96:9301]][indices:data/read/search[phase/query]]]; nested: SearchParseException[[guba][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"bool":{"must":[{"match":{"stock_name":"郑煤机"}}],"must_not":[],"should":[]}}}]]]; nested: NullPointerException; "

there is another error as belows:
File "mongo_oplog_zmq_work.py", line 43, in parse_op
    res = es.update(index=INDEX_NAME, doc_type=collection, id=id, body={"doc": doc}, params={"doc_as_upsert": True})
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch-1.5.0-py2.7.egg/elasticsearch/client/utils.py", line 69, in _wrapped
    return func(_args, params=params, *_kwargs)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch-1.5.0-py2.7.egg/elasticsearch/client/__init__.py", line 423, in update
    params=params, body=body)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch-1.5.0-py2.7.egg/elasticsearch/transport.py", line 307, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch-1.5.0-py2.7.egg/elasticsearch/connection/http_urllib3.py", line 89, in perform_request
    self._raise_error(response.status, raw_data)
  File "/usr/local/lib/python2.7/dist-packages/elasticsearch-1.5.0-py2.7.egg/elasticsearch/connection/base.py", line 105, in _raise_error
    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
elasticsearch.exceptions.TransportError: TransportError(500, u'RemoteTransportException[[ubuntu10][inet[/219.224.135.95:9301]][indices:data/write/update]]; nested: NullPointerException; ')
</description><key id="86806470">11566</key><summary>SearchParseException and transporterror</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">linhaobuaa</reporter><labels /><created>2015-06-10T02:24:57Z</created><updated>2015-06-10T02:29:01Z</updated><resolved>2015-06-10T02:29:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-10T02:29:00Z" id="110563103">Please join us in  #elasticsearchon Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove leftover sugar methods from FieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11565</link><project id="" key="" /><description>These methods are now all in MappedFieldType. This removes the remaining
callers of the methods on FieldMapper, and cuts down the FieldMapper
API to no longer include them.
</description><key id="86734259">11565</key><summary>Remove leftover sugar methods from FieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-09T21:02:45Z</created><updated>2015-06-12T14:08:19Z</updated><resolved>2015-06-09T21:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-09T21:10:50Z" id="110505453">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java</file></files><comments><comment>Merge pull request #11565 from rjernst/remove/field-mapper-wrappers</comment></comments></commit></commits></item><item><title>${prompt.text} and ${prompt.secret} double prompting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11564</link><project id="" key="" /><description>With the following configuration

```
node.name: ${prompt.text}
```

Elasticsearch 1.6.0 prompts you twice. Once for "node.name" and "name". Ultimately it uses the second one for the value of the configuration item:

```
djschny:elasticsearch-1.6.0 djschny$ ../startElastic.sh 
Enter value for [node.name]: foo
Enter value for [name]: bar
[2015-06-09 16:10:03,405][INFO ][node                     ] [bar] version[1.6.0], pid[4836], build[cdd3ac4/2015-06-09T13:36:34Z]
[2015-06-09 16:10:03,405][INFO ][node                     ] [bar] initializing ...
```
</description><key id="86720842">11564</key><summary>${prompt.text} and ${prompt.secret} double prompting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">djschny</reporter><labels><label>:Settings</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-06-09T20:13:45Z</created><updated>2016-03-17T16:21:47Z</updated><resolved>2016-03-14T00:07:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-12T14:04:01Z" id="111503550">@jaymode please could you take a look
</comment><comment author="joshuar" created="2015-12-17T23:47:41Z" id="165617665">Looks to be still a problem on 2.1.  Regression here?  @GlenRSmith and I can confirm this occurs with the 2.1 release archive.
</comment><comment author="joshuar" created="2015-12-17T23:49:53Z" id="165618030">It seems to be independent of which config item you want to prompt for:

```
elasticsearch-2.1.0  bin/elasticsearch
Enter value for [cluster.name]: foo
Enter value for [cluster.name]: bar
[2015-12-18 10:48:50,219][INFO ][node                     ] [Wendell Vaughn] version[2.1.0], pid[21231], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-12-18 10:48:50,220][INFO ][node                     ] [Wendell Vaughn] initializing ...
[2015-12-18 10:48:50,280][INFO ][plugins                  ] [Wendell Vaughn] loaded [], sites []
[2015-12-18 10:48:50,304][INFO ][env                      ] [Wendell Vaughn] using [1] data paths, mounts [[/ (/dev/mapper/fedora_josh--xps13-root)]], net usable_space [79.1gb], net total_space [233.9gb], spins? [no], types [ext4]
[2015-12-18 10:48:51,761][INFO ][node                     ] [Wendell Vaughn] initialized
[2015-12-18 10:48:51,762][INFO ][node                     ] [Wendell Vaughn] starting ...
[2015-12-18 10:48:51,902][INFO ][transport                ] [Wendell Vaughn] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2015-12-18 10:48:51,912][INFO ][discovery                ] [Wendell Vaughn] bar/azOFxYXMQ6muCpGOpqvxSw
```
</comment><comment author="GlenRSmith" created="2015-12-18T00:00:40Z" id="165619714">Just confirmed on 2.1.1.
</comment><comment author="rjernst" created="2015-12-18T06:52:08Z" id="165692770">This is different than the original issue. I think what is happening is we first initialize the settings/environment in bootstrap so that we can eg init logging, but then when bootstrap creates the node, it passes in a fresh Settings, and the Node constructor again initializes settings/environment.
</comment><comment author="clintongormley" created="2016-01-18T20:16:45Z" id="172642851">@jaymode could you take a look at this please?
</comment><comment author="jaymode" created="2016-01-19T15:07:27Z" id="172880737">As @rjernst said, this is a different issue that the original. `BootstrapCLIParser` was added which extends CLITool. CLITool prepares the settings and environment which includes prompting since CLITools are usually run outside of the bootstrap process. This causes the first prompt that is ignored. The second prompt that is used, comes from `Bootstrap`, which is passed to the node.

Part of the issue is that the `BootstrapCLIParser` sets properties that will change the value of settings. I think we can solve this a few different ways:
1. Pass in empty settings/null environment for this CLITool. If we try to create a valid environment here then we have to prepare the settings to ensure we parse the paths from the settings for our directories
2. Do not use the `CLITool` infrastructure
3. Prepare the environment in bootstrap, pass to `BootstrapCLIParser`. Re-prepare the settings/environment, passing in the already prepared settings.

@spinscale @rjernst any thoughts?
</comment><comment author="rjernst" created="2016-01-19T19:46:38Z" id="172964786">I would say preparing the environment once would be the best option, it will just take some refactoring to pass it through. Running prepare already creates Environment twice (the first time so it can try and load the config file, which might have other paths like plugin path or data path). Really I think we should simply not allow paths to be configured in elasticsearch.yml, instead it should only be through sysprops.  It might be something that could simplify the settings/env prep, to make this a little easier.
</comment><comment author="jasontedor" created="2016-02-24T02:40:12Z" id="188027362">I have a fix for this that will come as part of #16579.
</comment><comment author="jasontedor" created="2016-03-14T00:07:24Z" id="196086272">Closed by #17024.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java</file></files><comments><comment>do not prompt for node name twice</comment></comments></commit></commits></item><item><title>`ignore_above` uses string length, not utf-8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11563</link><project id="" key="" /><description>ignore_above is intended to guard against the lucene limitation
that a term cannot exceed 32766 bytes.

However, the implementation just used the character count, which
doesn't take into account the fact that some characters have
multi-byte utf-8 encodings.

This commit causes the string field mapper to use the byte length
of the UTF-8 encoding instead, which should correctly filter out
terms whose lengths exceed the limit in bytes.
</description><key id="86715051">11563</key><summary>`ignore_above` uses string length, not utf-8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvcephei</reporter><labels /><created>2015-06-09T19:51:14Z</created><updated>2015-07-10T16:48:26Z</updated><resolved>2015-07-10T16:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vvcephei" created="2015-06-09T19:52:19Z" id="110485038">Ok, now I'm not sure why it still says I've not signed the CLA. Any ideas? Do you want me to email you the pdf?
</comment><comment author="rmuir" created="2015-06-09T20:14:55Z" id="110490442">A single UTF-16 code unit can expand to at most 3 UTF-8 bytes. Can the code use this to avoid unnecessary conversion to UTF-8 when String.length() \* 3 &lt; limit (that should be the typical case).
</comment><comment author="rmuir" created="2015-06-09T20:23:02Z" id="110494069">Personally i have some performance concerns, related to all the unicode conversion and so on. A better approach IMO is to continue to count characters, but use a limit of 32766 / 3 instead. This is safe and always works.

Nobody needs 10 kilobyte terms in their index. The lucene limit is already obnoxiously high and could/should easily be reduced to a smaller value anyway.
</comment><comment author="vvcephei" created="2015-06-11T18:06:00Z" id="111225339">cool. Thanks for the feedback wrt performance. It was a concern for me as well, but I couldn't find another way to get the count. I'll definitely use StandardCharsets.UTF_8 in the future.

I agree with you about using string length and setting the limit at 32766 / 3. I needed to put in a workaround in our clusters immediately, and that's essentially why I did. It certainly seems crazy to want 10KB terms in your queries.

If you think that approach is acceptable for all your users, I'll update the PR. 
</comment><comment author="clintongormley" created="2015-06-12T14:03:06Z" id="111503140">@vvcephei I wonder if it should be a documentation update instead.  Otherwise if somebody deals mostly with ASCII and they specify `256` they'd be surprised when strings of 100 characters get rejected.
</comment><comment author="vvcephei" created="2015-06-12T15:23:54Z" id="111525491">@clintongormley  That would at least clear it up, but I wonder if that's not pushing too much responsibility to the user.

AFIAK, the whole intent of ignore_above is to protect users from the exception that lucene will throw if you have an excessively lengthy term. If lucene hadn't started throwing instead of ignoring, would we even have ignore_above?

If there are other use cases for it, then I'd agree with you, since I don't typically thing of the "length" of a string as the size of its byte representation.

But if the main/only use case is the lucene guard, I wonder if it would be better just to give a boolean option "ignore_unindexably_long_terms" (tounge-in-cheek, but you get the idea). Then it'd be up to ES to ensure we never see that exception from Lucene, by whatever mechanism.

In that world, the "right thing" to do would probably be to explore the most efficient way to find the byte-length of the string in UTF-8, and just do that. It's work that lucene is going to do anyway for the term, so it's not going to add a huge amount of overhead. My main concern is the extra memory for allocating the array, but I'm sure it can be avoided.
</comment><comment author="clintongormley" created="2015-06-12T16:37:43Z" id="111551028">@vvcephei while Lucene has a high fixed limit on the token size, `ignore_above` is very useful in (eg) the logging use case where you don't want to index long (but nowhere near as long as 32k) terms.  eg the Logstash template defaults `ignore_above` to 256.

Most logging is almost entirely ASCII, so I think it'd be quite confusing to just divide by 3. Hence the documentation change makes more sense to me
</comment><comment author="vvcephei" created="2015-06-12T17:58:33Z" id="111575496">Cool. Thanks for the context. I'll change the PR to a documentation change.
</comment><comment author="jpountz" created="2015-07-08T17:20:07Z" id="119669132">@vvcephei ping, do you have plans to change the PR anytime soon?
</comment><comment author="vvcephei" created="2015-07-09T16:32:24Z" id="120060321">Hey! Sorry; I got side-tracked. I'll do it now.
</comment><comment author="vvcephei" created="2015-07-09T16:46:11Z" id="120065632">@jpountz If you have a chance, can you give me a little guidance on updating the docs?

It looks like I should clone `github/elastic/docs.git` and then update all of `html/en/elasticsearch/reference/*/mapping-core-types.html` to add the desired comment.

Is this correct?
</comment><comment author="jpountz" created="2015-07-09T16:48:01Z" id="120066020">@vvcephei docs have moved to the main repo, for instance the core types docs are now at https://github.com/elastic/elasticsearch/blob/master/docs/reference/mapping/types/core-types.asciidoc
</comment><comment author="vvcephei" created="2015-07-09T19:06:27Z" id="120110346">oops. I didn't mean to close the PR, just update it. 

@jpountz, can you take a look at the diff and see of you think this is what we ought to say?
</comment><comment author="clintongormley" created="2015-07-10T16:48:16Z" id="120456629">thanks @vvcephei - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docfix: ignore_above uses string length, not utf-8</comment></comments></commit></commits></item><item><title>Bugfix: ignore_above uses string length, not utf-8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11562</link><project id="" key="" /><description>ignore_above is intended to guard against the lucene limitation
that a term cannot exceed 32766 bytes.

However, the implementation just used the character count, which
doesn't take into account the fact that some characters have
multi-byte utf-8 encodings.

This commit causes the string field mapper to use the byte length
of the UTF-8 encoding instead, which should correctly filter out
terms whose lengths exceed the limit in bytes.
</description><key id="86710104">11562</key><summary>Bugfix: ignore_above uses string length, not utf-8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvcephei</reporter><labels /><created>2015-06-09T19:35:01Z</created><updated>2015-06-09T19:40:13Z</updated><resolved>2015-06-09T19:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vvcephei" created="2015-06-09T19:40:05Z" id="110480865">I've just signed the CLA. I'll resubmit the PR.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[API] correct link for flush_synced docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11561</link><project id="" key="" /><description /><key id="86708832">11561</key><summary>[API] correct link for flush_synced docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2015-06-09T19:30:08Z</created><updated>2015-06-12T13:58:45Z</updated><resolved>2015-06-09T19:39:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-09T19:35:12Z" id="110478842">LGTM. @HonzaKral thx for catching. Do you want to do the push song and dance? I can do it if you prefer.
</comment><comment author="HonzaKral" created="2015-06-09T19:38:56Z" id="110480230">I can do it - master and 1.x I presume.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add escaping for JSON response body filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11560</link><project id="" key="" /><description>Please add the ability to handle escaping for certain characters with the filter_path option. For example to allow the following to work for our common .marvel-\* indices.

```
GET _stats?filter_path=indices.\.marvel*
```
</description><key id="86696921">11560</key><summary>add escaping for JSON response body filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:REST</label><label>adoptme</label><label>enhancement</label></labels><created>2015-06-09T18:52:40Z</created><updated>2015-11-16T16:29:23Z</updated><resolved>2015-11-16T16:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-11T15:20:48Z" id="111170736">I think we should add escaping to the `filter_path` option. Index names or field names can include dots or comma so we need to handle escaping to filter responses on those names.
</comment><comment author="djschny" created="2015-06-11T15:37:26Z" id="111175886">Sorry about the v1.6.0 label. How do we indicate that a particular bug is only for a certain version?
</comment><comment author="javanna" created="2015-06-11T15:45:04Z" id="111178195">hey @djschny no worries! we use versions to indicate when things get fixed, rather than which versions are affected. We actually have no way at the moment to specify which version is affected by a bug.
</comment><comment author="clintongormley" created="2015-06-11T16:24:04Z" id="111192717">Here's a regex that will tokenize properly:

```
/(                 # capture either
    (?:            
        [^.,\\]    # any character except a comma, period, or backslash
      | \\.        # or a backslash followed by any character
    )+             # repeated as many times as possible
  | 
    [.,]           # or either a single comma or period
 )
/x
```

So this string: `indices.\.marvel*.foo\,bar,indices.\.marvel*.\*` would result in the following tokens:
- `indices`
- `.`
- `\.marvel*`
- `.` 
- `foo\,bar`
- `,`
- `indices`
- `.`
- `\.marvel*`
- `.`
- `\*`

Then real asterisks should be separated from escaped asterisks, and then any other escapes should be removed.
</comment><comment author="clintongormley" created="2015-06-12T09:30:18Z" id="111428639">Probably need to support `+` and `-` as well
</comment><comment author="clintongormley" created="2015-06-12T09:34:37Z" id="111429338">Related to https://github.com/elastic/elasticsearch/issues/11334
</comment><comment author="clintongormley" created="2015-06-12T09:40:17Z" id="111430050">In order to handle escaped wildcards, we either need a state machine, or to reuse Lucene's automaton (which creates a lot of objects).  Perhaps we just don't allow escaped wildcards, in which case we can use the simple pattern matching that we have today.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make index level mapping apis use MappedFieldType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11559</link><project id="" key="" /><description>The MapperService is the "index wide view" of mappings. Methods on it
are used at query time to lookup how to query a field. This
change reduces the exposed api so that any information returned
is limited to that api exposed by MappedFieldType. In the future,
MappedFieldType will be guaranteed to be the same across all
document types for a given field.

Note CompletionFieldType needed some more settings moved to it. Other
than that, this change is almost purely cosmetic.
</description><key id="86667398">11559</key><summary>Make index level mapping apis use MappedFieldType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-06-09T17:03:36Z</created><updated>2015-06-12T13:56:29Z</updated><resolved>2015-06-09T18:04:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-09T17:55:58Z" id="110446403">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java</file><file>core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>core/src/main/java/org/elasticsearch/action/fieldstats/TransportFieldStatsTransportAction.java</file><file>core/src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/IndexIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/fieldvisitor/SingleFieldsVisitor.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java</file><file>core/src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeQuery.java</file><file>core/src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxQuery.java</file><file>core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>core/src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/FieldContext.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/FieldLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/LeafDocLookup.java</file><file>core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java</file><file>core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java</file><file>core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>core/src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file><file>core/src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Merge pull request #11559 from rjernst/pr/field-type-users</comment></comments></commit></commits></item><item><title>Getting no results while using TermVector API with Transport Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11558</link><project id="" key="" /><description>``` scala
class jClient(clusterName: String, ip: String="localhost", port: Int=9300) {
  def initTC: Client = {
    val new_setting: ImmutableSettings.Builder = ImmutableSettings.builder().put("cluster.name", clusterName)
    var transportClient: TransportClient = new TransportClient(new_setting)
    transportClient = transportClient.addTransportAddress(new InetSocketTransportAddress(ip, port))
    transportClient
  }
  def init: Client = {
    val new_setting: ImmutableSettings.Builder = ImmutableSettings.builder().put("cluster.name", clusterName)
    val nodeBuilder: NodeBuilder = NodeBuilder.nodeBuilder().settings(new_setting).local(false)
    val node: Node = nodeBuilder.node()
    val client: Client = node.client()
    client
  }
}

class termVectors {
  def getTermsVector(client: Client, index: String, `type`: String, id: String, field: String) {
    val builder = new TermVectorRequestBuilder(client, index, `type`, id).setSelectedFields(field)
    builder.setTermStatistics(true).setFieldStatistics(true)
    builder.execute(new ActionListener[TermVectorResponse] {
      def onResponse(response: TermVectorResponse) {
        val builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint()
        response.toXContent(builder, ToXContent.EMPTY_PARAMS)
        println("TermsVector: " + builder.string)
        client.close()
      }
      def onFailure(e: Throwable) {e.printStackTrace}
    })
  }
}

object TestTV {
  def main(args: Array[String]) {
    val client = new jClient("elasticsearch").initTC
   // val client = new jClient("elasticsearch").init      //this works but very slow
    val temp = new termVectors()
    temp.getTermsVector(client, "new_index", "documents", "AP890101-0056", "text")
   }
}
```

I am trying to retrieve Term Vector of a document using Scala. I am using Elasticsearch 1.5.2 with Scala 2.10.5.
Here, if I use `NodeBuilder` I get the answer but it is very slow, like 7-9 seconds for one vector. I do not get any answer using the `TransportClient`. The code compiles correctly in both cases.

What should I do to increase speed and make `TransportClient` work?
</description><key id="86649227">11558</key><summary>Getting no results while using TermVector API with Transport Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apanimesh061</reporter><labels /><created>2015-06-09T16:02:42Z</created><updated>2015-06-10T07:23:45Z</updated><resolved>2015-06-10T07:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-06-10T07:23:44Z" id="110629233">When you call builder.execute() the request is sent but then you don't wait for the result to come back. main() might therefore finish before the result has actually been returned. Try waiting until onResponse() is called, for example with a latch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item></channel></rss>